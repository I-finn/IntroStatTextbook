# Confidence intervals with bootstrapping  {#foundations-bootstrapping}

```{r, include = FALSE}
source("_common.R")
```

::: {.chapterintro}
TODO
:::

## Motivating example: Martian alphabet revisited

### Observed data


### Variability of the statistic

But wait---we're not done! We have evidence that humans tend to prefer Bumba on the left, but by how much? To answer this, we need a confidence interval---an interval of plausible values for the true probability humans will select Bumba as the left letter. The width of this interval is determined by how variable sample proportions are from sample to sample. It turns out, there is a mathematical model for this variability that we will explore later in this chapter. For now, let's take the standard deviation from our simulated sample proportions as an estimate for this variability: 0.08. Since the simulated distribution of proportions is bell-shaped, we know about 95% of sample proportions should fall within two standard deviations of the true proportion, so we can add and subtract this **margin of error**\index{margin of error} to our sample proportion to calculate an approximate 95% confidence interval[^05-inference-cat-5]: $$
\frac{34}{38} \pm 2\times 0.08 = 0.89 \pm 0.16 = (0.73, 1)
$$ Thus, based on this data, we are 95% confident that the probability a human guesses Bumba on the left is somewhere between 73% and 100%.

[^05-inference-cat-5]: If you carry out the calculations, you'll note that the upper bound is actually $0.89 + 0.16 = 1.05$, but since a sample proportion cannot be greater than 1, we truncated the interval to 1.

```{r include=FALSE}
terms_chp_5 <- c(terms_chp_5, "margin of error")
```


### Confidence intervals {#ConfidenceIntervals}

A point estimate provides a single plausible value for a parameter. However, a point estimate is rarely perfect---usually there is some error in the estimate. In addition to supplying a point estimate of a parameter, a next logical step would be to provide a plausible *range of values* for the parameter.

A plausible range of values for the population parameter is called a **confidence interval**. Using only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net. We can throw a spear where we saw a fish, but we will probably miss. On the other hand, if we toss a net in that area, we have a good chance of catching the fish.

If we report a point estimate, we probably will not hit the exact population parameter. On the other hand, if we report a range of plausible values---a confidence interval---we have a good shot at capturing the parameter.

This reasoning also explains why we can never prove a null hypothesis. Sample statistics will vary from sample to sample. While we can quantify this uncertainty (e.g., we are 95% sure the statistic is within 0.15 of the parameter), we can never be certain that the parameter is an exact value. For example, suppose you want to test whether a coin is a fair coin, i.e., $H_0: \pi = 0.50$ versus $H_0: \pi \neq 0.50$, so you toss the coin 10 times to collect data. In those 10 tosses, 6 land on heads and 4 land on tails, resulting in a p-value of 0.754[^05-inference-cat-10]. We don't have enough evidence to show that the coin is biased, but surely we wouldn't say we just proved the coin is fair!

[^05-inference-cat-10]: You will get more practice calculating p-values such as these in this Chapter.

::: importantbox
There are only *two* possible decisions in a hypothesis test: (1) reject $H_0$, or (2) fail to reject $H_0$. Since one can never prove a null hypothesis---we can only disprove[^05-inference-cat-11] it---we never have the ability to "accept the null." You may have seen this phrase in other textbooks or articles, but it is incorrect.
:::

[^05-inference-cat-11]: Since statistical methods are grounded in probability, technically we can only find strong evidence against a hypothesis, not disprove it.

::: guidedpractice
If we want to be very certain we capture the population parameter, should we use a wider interval or a smaller interval?[^05-inference-cat-12]
:::

[^05-inference-cat-12]: If we want to be more certain we will capture the fish, we might use a wider net. Likewise, we use a wider confidence interval if we want to be more certain that we capture the parameter.

We will explore both simulation-based methods (bootstrapping) and theory-based methods for creating confidence intervals in this text. Though the details change with different scenarios, theory-based confidence intervals will always take the form: $$
\mbox{statistic} \pm (\mbox{multiplier}) \times (\mbox{standard error of the statistic})
$$ The statistic is our best guess for the value of the parameter, so it makes sense to build the confidence interval around that value. The standard error, which is a measure of the uncertainty associated with the statistic, provides a guide for how large we should make the confidence interval. The multiplier is determined by how confident we'd like to be, and tells us how many standard errors we need to add and subtract from the statistic. The amount we add and subtract from the statistic is called the **margin of error**.

::: onebox
**General form of a confidence interval.**

The general form of a **theory-based confidence interval** for an unknown parameter is $$
\mbox{statistic} \pm (\mbox{multiplier}) \times (\mbox{standard error of the statistic})
$$ The amount we add and subtract to the statistic to calculate the confidence interval is called the **margin of error**. $$
\mbox{margin of error} = (\mbox{multiplier}) \times (\mbox{standard error of the statistic})
$$
:::

In Section \@ref(theory-prop) we will discuss different percentages for the confidence interval (e.g., 90% confidence interval or 99% confidence interval). Section \@ref(two-prop-boot-ci) provides a longer discussion on what "95% confidence" actually means.

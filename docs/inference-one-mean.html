<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 17 Inference for a single mean | Montana State Introductory Statistics with R</title>
<meta name="author" content="Nicole Carnegie, Stacey Hancock, Elijah Meyer, Jade Schmidt, Melinda Yager">
<meta name="description" content="Focusing now on statistical inference for quantitative data, we will revisit and expand upon the foundational aspects of hypothesis testing from Chapter 9. The important data structure for this...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="Chapter 17 Inference for a single mean | Montana State Introductory Statistics with R">
<meta property="og:type" content="book">
<meta property="og:url" content="https://mtstateintrostats.github.io/IntroStatTextbook/inference-one-mean.html">
<meta property="og:description" content="Focusing now on statistical inference for quantitative data, we will revisit and expand upon the foundational aspects of hypothesis testing from Chapter 9. The important data structure for this...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 17 Inference for a single mean | Montana State Introductory Statistics with R">
<meta name="twitter:description" content="Focusing now on statistical inference for quantitative data, we will revisit and expand upon the foundational aspects of hypothesis testing from Chapter 9. The important data structure for this...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet">
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script><script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script><script src="libs/plotly-binding-4.10.0/plotly.js"></script><script src="libs/typedarray-0.1/typedarray.min.js"></script><link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script><link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script><script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="css/ims-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Montana State Introductory Statistics with R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="authors.html">Authors</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="rstudio.html">Preliminaries: Getting started in RStudio</a></li>
<li class="book-part">Introduction to data</li>
<li><a class="" href="data-hello.html"><span class="header-section-number">1</span> Hello data</a></li>
<li><a class="" href="data-design.html"><span class="header-section-number">2</span> Study design</a></li>
<li><a class="" href="data-applications.html"><span class="header-section-number">3</span> Applications: Data</a></li>
<li class="book-part">Exploratory data analysis</li>
<li><a class="" href="explore-categorical.html"><span class="header-section-number">4</span> Exploring categorical data</a></li>
<li><a class="" href="explore-numerical.html"><span class="header-section-number">5</span> Exploring quantitative data</a></li>
<li><a class="" href="explore-regression.html"><span class="header-section-number">6</span> Correlation and regression</a></li>
<li><a class="" href="explore-mult-reg.html"><span class="header-section-number">7</span> Multivariable models</a></li>
<li><a class="" href="explore-applications.html"><span class="header-section-number">8</span> Applications: Explore</a></li>
<li class="book-part">Foundations of inference</li>
<li><a class="" href="foundations-randomization.html"><span class="header-section-number">9</span> Hypothesis testing with randomization</a></li>
<li><a class="" href="foundations-bootstrapping.html"><span class="header-section-number">10</span> Confidence intervals with bootstrapping</a></li>
<li><a class="" href="foundations-mathematical.html"><span class="header-section-number">11</span> Inference with mathematical models</a></li>
<li><a class="" href="foundations-errors.html"><span class="header-section-number">12</span> Errors, power, and practical importance</a></li>
<li><a class="" href="foundations-applications.html"><span class="header-section-number">13</span> Applications: Foundations</a></li>
<li class="book-part">Inference for categorical data</li>
<li><a class="" href="inference-one-prop.html"><span class="header-section-number">14</span> Inference for a single proportion</a></li>
<li><a class="" href="inference-two-props.html"><span class="header-section-number">15</span> Inference for comparing two proportions</a></li>
<li><a class="" href="inference-categ-applications.html"><span class="header-section-number">16</span> Applications: Infer categorical</a></li>
<li class="book-part">Inference for quantitative data</li>
<li><a class="active" href="inference-one-mean.html"><span class="header-section-number">17</span> Inference for a single mean</a></li>
<li><a class="" href="inference-paired-means.html"><span class="header-section-number">18</span> Inference for comparing paired means</a></li>
<li><a class="" href="inference-two-means.html"><span class="header-section-number">19</span> Inference for comparing two independent means</a></li>
<li><a class="" href="inference-num-applications.html"><span class="header-section-number">20</span> Applications: Infer quantitative</a></li>
<li class="book-part">Inference for regression</li>
<li><a class="" href="inference-reg.html"><span class="header-section-number">21</span> Inference for correlation and slope</a></li>
<li><a class="" href="inference-reg-applications.html"><span class="header-section-number">22</span> Applications: Infer regression</a></li>
<li class="book-part">Probability</li>
<li><a class="" href="probability.html"><span class="header-section-number">23</span> Probability with tables</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/MTstateIntroStats/IntroStatTextbook">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="inference-one-mean" class="section level1" number="17">
<h1>
<span class="header-section-number">17</span> Inference for a single mean<a class="anchor" aria-label="anchor" href="#inference-one-mean"><i class="fas fa-link"></i></a>
</h1>
<!-- Old reference: #one-mean -->
<div class="chapterintro">
<p>Focusing now on statistical inference for quantitative data, we will revisit and expand upon the foundational aspects of hypothesis testing from Chapter <a href="foundations-randomization.html#foundations-randomization">9</a>.</p>
<p>The important data structure for this chapter is a quantitative response variable (that is, the outcome is numerical).
The three data structures we detail are:</p>
<ul>
<li>one quantitative response variable, summarized by a single mean,</li>
<li>one quantitative response variable which is a difference across a pair of observations, summarized by a paired mean difference, and</li>
<li>a quantitative response variable broken down by a binary explanatory variable, summarized by a difference in means.</li>
</ul>
<p>When appropriate, each of the data structures will be analyzed using the three methods from Chapters <a href="foundations-randomization.html#foundations-randomization">9</a>, <a href="foundations-bootstrapping.html#foundations-bootstrapping">10</a>, and <a href="foundations-mathematical.html#foundations-mathematical">11</a>: randomization test, bootstrapping, and mathematical models, respectively.</p>
<p>As we build on the inferential ideas, we will visit new foundational concepts in statistical inference. One key new idea rests in estimating how the sample mean (as opposed to the sample proportion) varies from sample to sample; the resulting value is referred to as the standard error of the mean. We will also introduce a new important mathematical model, the <span class="math inline">\(t\)</span>-distribution (as the foundation for the <span class="math inline">\(t\)</span>-test).</p>
</div>
<p>To summarize a quantitative response variable, we focus on the sample mean (instead of, for example, the sample median or the range of the observations) because of the well-studied mathematical model which describes the behavior of the sample mean.
The sample mean will be calculated in one group, two paired groups, and two independent groups. We will not cover mathematical models which describe other statistics, but the bootstrap and randomization techniques described below are immediately extendable to any function of the observed data.
The techniques described for each setting will vary slightly, but you will be well served to find the structural similarities across the different settings.</p>
<p>Similar to how we can model the behavior of the sample proportion <span class="math inline">\(\hat{p}\)</span> using a normal distribution, the sample mean <span class="math inline">\(\bar{x}\)</span> can also be modeled using a normal distribution when certain conditions are met.
However, we’ll soon learn that a new distribution, called the <span class="math inline">\(t\)</span>-distribution, tends to be more useful when working with the sample mean.
We’ll first learn about this new distribution, then we’ll use it to construct confidence intervals and conduct hypothesis tests for the mean.</p>
<p>Below we summarize the notation used throughout this chapter.</p>
<div class="onebox">
<p><strong>Notation.</strong></p>
<ul>
<li>
<span class="math inline">\(n\)</span> = sample size</li>
<li>
<span class="math inline">\(\bar{x}\)</span> = sample mean</li>
<li>
<span class="math inline">\(s\)</span> = sample standard deviation</li>
<li>
<span class="math inline">\(\mu\)</span> = population mean</li>
<li>
<span class="math inline">\(\sigma\)</span> = population standard deviation</li>
</ul>
</div>
<p>A single mean is used to summarize data when we measured a single quantitative variable on each observational unit, e.g., GPA, age, salary. Aside from slight differences in notation, the inferential methods presented in this section will be identical to those for a paired mean difference, as we will see in Chapter <a href="inference-paired-means.html#inference-paired-means">18</a>.</p>
<div id="bootstrap-confidence-interval-for-mu" class="section level2" number="17.1">
<h2>
<span class="header-section-number">17.1</span> Bootstrap confidence interval for <span class="math inline">\(\mu\)</span><a class="anchor" aria-label="anchor" href="#bootstrap-confidence-interval-for-mu"><i class="fas fa-link"></i></a>
</h2>
<p>In this section, we will use <strong>bootstrapping</strong>, first introduced in Chapter <a href="foundations-bootstrapping.html#foundations-bootstrapping">10</a>, to construct a confidence interval for a population mean. Recall that bootstrapping is best suited for modeling studies where the data have been generated through random sampling from a population. Our bootstrapped distribution of sample means will mimic the process of randomly sampling from a population to give us a sense of how sample means will vary from sample to sample.</p>
<div id="observed-data-4" class="section level3" number="17.1.1">
<h3>
<span class="header-section-number">17.1.1</span> Observed data<a class="anchor" aria-label="anchor" href="#observed-data-4"><i class="fas fa-link"></i></a>
</h3>
<p>As an employer who subsidizes housing for your employees, you need to know the average monthly rental price for a three bedroom flat in Edinburgh.
In order to walk through the example more clearly, let’s say that you are only able to randomly sample five Edinburgh flats (if this were a real example, you would surely be able to take a much larger sample size, possibly even being able to measure the entire population!).</p>
<p>Figure <a href="inference-one-mean.html#fig:5flats">17.1</a> presents the details of the random sample of observations where the monthly rent of five flats has been recorded.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:5flats"></span>
<img src="06/figures/5flats.png" alt="Five randomly sampled flats in Edinburgh." width="75%"><p class="caption">
Figure 17.1: Five randomly sampled flats in Edinburgh.
</p>
</div>
<p>The sample average monthly rent of £1648 is a first guess at the price of three bedroom flats. However, as a student of statistics, you understand that one sample mean based on a sample of five observations will not necessarily equal the true population average rent for all three bedroom flats in Edinburgh.
Indeed, you can see that the observed rent prices vary with a standard deviation of £340.232, and surely the average monthly rent would be different if a different sample of size five had been taken from the population.
Fortunately, we can use bootstrapping to approximate the variability of the sample mean from sample to sample.</p>
</div>
<div id="variability-of-the-statistic-3" class="section level3" number="17.1.2">
<h3>
<span class="header-section-number">17.1.2</span> Variability of the statistic<a class="anchor" aria-label="anchor" href="#variability-of-the-statistic-3"><i class="fas fa-link"></i></a>
</h3>
<p>As with the inferential ideas covered in previous chapters, the inferential analysis methods in this chapter are grounded in quantifying how one data set differs from another when they are both taken from the same population.</p>
<p>Figure <a href="inference-one-mean.html#fig:bootstrapping">17.2</a> shows how the unknown original population of all three bedroom flats in Edinburgh can be estimated by using many duplicates of the sample. This estimated population—consisting of infinitely many copies of the original sample—can then be used to generate bootstrapped resamples.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:bootstrapping"></span>
<img src="06/figures/bootstrapping.png" alt="Using the original sample of five Edinburgh flats to generate an estimated population, which is then used to generate bootstrapped resamples. This process of generating a bootstrapped sample is equivalent to sampling five flats from the original sample, with replacement." width="75%"><p class="caption">
Figure 17.2: Using the original sample of five Edinburgh flats to generate an estimated population, which is then used to generate bootstrapped resamples. This process of generating a bootstrapped sample is equivalent to sampling five flats from the original sample, with replacement.
</p>
</div>
<p>In Figure <a href="inference-one-mean.html#fig:bootstrapping">17.2</a>, the repeated bootstrap resamples are obviously different both from each other and from the original sample.
Since the bootstrap resamples are taken from the same (estimated) population, these differences are due entirely to natural variability in the sampling procedure.
By summarizing each of the bootstrap resamples (here, using the sample mean), we see, directly, the variability of the sample mean from sample to sample.
The distribution of <span class="math inline">\(\bar{x}_{boot}\)</span>, the bootstrapped sample means, for the Edinburgh flats is shown in Figure <a href="inference-one-mean.html#fig:flatsbsmean">17.3</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:flatsbsmean"></span>
<img src="17-numerical-one-mean_files/figure-html/flatsbsmean-1.png" alt="Distribution of bootstrapped means from 1,000 simulated bootstrapped samples generated by sampling with replacement from our original sample of five Edinburgh flats. The histogram provides a sense for the variability of the average rent values from sample to sample for samples of size 5." width="90%"><p class="caption">
Figure 17.3: Distribution of bootstrapped means from 1,000 simulated bootstrapped samples generated by sampling with replacement from our original sample of five Edinburgh flats. The histogram provides a sense for the variability of the average rent values from sample to sample for samples of size 5.
</p>
</div>
<p>The bootstrapped average rent prices vary from £1250 to £1995 (with a small observed sample of size 5, a bootstrap resample can sometimes, although rarely, include only repeated measurements of the same observation).</p>
<div class="onebox">
<p><strong>Bootstrapping from one sample.</strong></p>
<ol style="list-style-type: decimal">
<li>Take a random sample of size <span class="math inline">\(n\)</span> from the original sample, <em>with replacement</em>. This is called a <strong>bootstrapped resample</strong>.</li>
<li>Record the sample mean (or statistic of interest) from the bootstrapped resample. This is called a <strong>bootstrapped statistic</strong>.</li>
<li>Repeat steps (1) and (2) 1000s of times.</li>
</ol>
</div>
<p>Due to theory that is beyond this text, we know that the bootstrap means <span class="math inline">\(\bar{x}_{boot}\)</span> vary around the original sample mean, <span class="math inline">\(\bar{x}\)</span>, in a similar way to how different sample (i.e., different data sets which would produce different <span class="math inline">\(\bar{x}\)</span> values) means vary around the true parameter <span class="math inline">\(\mu\)</span>.
Therefore, an interval estimate for <span class="math inline">\(\mu\)</span> can be produced using the <span class="math inline">\(\bar{x}_{boot}\)</span> values themselves. A 95% <strong>bootstrap confidence interval</strong> for <span class="math inline">\(\mu\)</span>, the population mean rent price for three bedroom flats in Edinburgh, is found by locating the middle 95% of the bootstrapped sample means in Figure <a href="inference-one-mean.html#fig:flatsbsmean">17.3</a>.</p>
<div class="onebox">
<p><strong>95% Bootstrap confidence interval for a population mean <span class="math inline">\(\mu\)</span>.</strong></p>
The 95% bootstrap confidence interval for the parameter <span class="math inline">\(\mu\)</span> can be obtained directly using the ordered values <span class="math inline">\(\bar{x}_{boot}\)</span> values — the bootstrapped sample means. Consider the sorted <span class="math inline">\(\bar{x}_{boot}\)</span> values, and let <span class="math inline">\(\bar{x}_{boot, 0.025}\)</span> be the 2.5<sup>th</sup> percentile and <span class="math inline">\(\bar{x}_{boot, 0.025}\)</span> be the 97.5<sup>th</sup> percentile. The 95% confidence interval is given by:
<center>
(<span class="math inline">\(\bar{x}_{boot, 0.025}\)</span>, <span class="math inline">\(\bar{x}_{boot, 0.975}\)</span>)
</center>
</div>
<p>You can find confidence intervals of difference confidence levels by changing the percent of the distribution you take, e.g., locate the middle 90% of the bootstrapped statistics for a 90% confidence interval.</p>
<div class="workedexample">
<p>Using Figure <a href="inference-one-mean.html#fig:flatsbsmean">17.3</a>, find the 90% and 95% confidence intervals for the true mean monthly rental price of a three bedroom flat in Edinburgh.</p>
<hr>
<p>A 90% confidence interval is given by (£1429, £1876). The conclusion is that we are 90% confident that the true average rental price for three bedroom flats in Edinburgh lies somewhere between £1429 and £1876.</p>
<p>A 95% confidence interval is given by (£1389.75, £1916). The conclusion is that we are 95% confident that the true average rental price for three bedroom flats in Edinburgh lies somewhere between £1389.75 and £1916.</p>
</div>
</div>
<div id="bootstrap-percentile-confidence-interval-for-sigma-special-topic" class="section level3" number="17.1.3">
<h3>
<span class="header-section-number">17.1.3</span> Bootstrap percentile confidence interval for <span class="math inline">\(\sigma\)</span> (special topic)<a class="anchor" aria-label="anchor" href="#bootstrap-percentile-confidence-interval-for-sigma-special-topic"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose that the research question at hand seeks to understand how variable the rental price of the three bedroom flats are in Edinburgh.
That is, your interest is no longer in the average rental price of the flats but in the <em>standard deviation</em> of the rental prices of all three bedroom flats in Edinburgh, <span class="math inline">\(\sigma\)</span>.
You may have already realized that the sample standard deviation, <span class="math inline">\(s\)</span>, will work as a good <strong>point estimate</strong> for the parameter of interest: the population standard deviation, <span class="math inline">\(\sigma\)</span>.
The point estimate of the five observations is calculated to be <span class="math inline">\(s =\)</span> £340.23.
While <span class="math inline">\(s =\)</span> £340.23 might be a good guess for <span class="math inline">\(\sigma\)</span>, we prefer to have an interval.
Although there is a mathematical model which describes how <span class="math inline">\(s\)</span> varies from sample to sample, the mathematical model will not be presented in this text.
But even without the mathematical model, bootstrapping can be used to find a confidence interval for the parameter <span class="math inline">\(\sigma\)</span>.</p>
<div class="workedexample">
<p>Describe the bootstrap distribution for the standard deviation shown in Figure <a href="inference-one-mean.html#fig:flatsbssd">17.4</a>.</p>
<hr>
<p>The distribution is skewed left and centered near £340.23, which is the point estimate from the original data. Most observations in this distribution lie between £0 and £408.1.</p>
</div>
<div class="guidedpractice">
<p>Using Figure <a href="inference-one-mean.html#fig:flatsbssd">17.4</a>, find <em>and interpret</em> a 90% confidence interval for the population standard deviation for three bedroom flat prices in Edinburgh.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;By looking at the percentile values in Figure &lt;a href="inference-one-mean.html#fig:flatsbssd"&gt;17.4&lt;/a&gt;, the middle 90% of the bootstrap standard deviations are given by the 5&lt;sup&gt;th&lt;/sup&gt; percentile (£153.9) and 95&lt;sup&gt;th&lt;/sup&gt; percentile (£385.6). That is, we are 90% confident that the true standard deviation of rent prices is between £153.9 and £385.6; or that, on average, rent prices tend to be somewhere between £153.9 and £385.6 away from the mean rent price. Note, the problem was set up as 90% to indicate that there was not a need for a high level of confidence (such a 95% or 99%). A lower degree of confidence increases potential for error, but it also produces a more narrow interval.&lt;/p&gt;'><sup>152</sup></a></p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:flatsbssd"></span>
<img src="17-numerical-one-mean_files/figure-html/flatsbssd-1.png" alt="The original Edinburgh data is bootstrapped 1,000 times. The histogram provides a sense for the variability of the standard deviation of the rent values from sample to sample." width="90%"><p class="caption">
Figure 17.4: The original Edinburgh data is bootstrapped 1,000 times. The histogram provides a sense for the variability of the standard deviation of the rent values from sample to sample.
</p>
</div>
</div>
<div id="bootstrapping-is-not-a-solution-to-small-sample-sizes" class="section level3" number="17.1.4">
<h3>
<span class="header-section-number">17.1.4</span> Bootstrapping is not a solution to small sample sizes!<a class="anchor" aria-label="anchor" href="#bootstrapping-is-not-a-solution-to-small-sample-sizes"><i class="fas fa-link"></i></a>
</h3>
<p>The example presented above is done for a sample with only five observations.
As with analysis techniques that build on mathematical models, bootstrapping works best when a large random sample has been taken from the population.
Bootstrapping is a method for capturing the variability of a statistic when the mathematical model is unknown — it is not a method for navigating small samples.
As you might guess, the larger the random sample, the more accurately that sample will represent the target population.</p>
</div>
</div>
<div id="one-mean-null-boot" class="section level2" number="17.2">
<h2>
<span class="header-section-number">17.2</span> Shifted bootstrap test for <span class="math inline">\(H_0: \mu = \mu_0\)</span><a class="anchor" aria-label="anchor" href="#one-mean-null-boot"><i class="fas fa-link"></i></a>
</h2>
<p>We can also use bootstrapping to conduct a simulation-based test of the null hypothesis that the population mean is equal to a specified value, <span class="math inline">\(\mu_0\)</span>, called the null value. In this case, we first <strong>shift</strong> each value in the data set so that the sample distribution is centered at <span class="math inline">\(\mu_0\)</span>. Then, we bootstrap from the <strong>shifted data</strong> in order to generate a null distribution of sample means. Consider the following example.</p>
<p>In 1851, Carl Wunderlich, a German physician, measured body temperatures of around 25,000 adults and found that the average body temperature was 98.6<span class="math inline">\(^{\circ}\)</span>F, which we’ve believed ever since. However, a recent study conducted at Stanford University suggests that the average body temperature may actually be lower than 98.6<span class="math inline">\(^{\circ}\)</span>F.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Protsiv, Ley,Lankester, Hastie, Parsonnet (2020). Decreasing human body temperature in the United States since the Industrial Revolution. eLife 2020;9:e49555, DOI: 10.7554/eLife.49555. &lt;a href="https://elifesciences.org/articles/49555"&gt;https://elifesciences.org/articles/49555&lt;/a&gt;&lt;/p&gt;'><sup>153</sup></a></p>
<div id="observed-data-5" class="section level3" number="17.2.1">
<h3>
<span class="header-section-number">17.2.1</span> Observed data<a class="anchor" aria-label="anchor" href="#observed-data-5"><i class="fas fa-link"></i></a>
</h3>
<p>Curious if average body temperature has decreased since 1851, you decided to collect data on a random sample of twenty Montana State University students. The mean body temperature in your sample is <span class="math inline">\(\bar{x}\)</span> = 97.47<span class="math inline">\(^{\circ}\)</span>F, and the standard deviation is <span class="math inline">\(s\)</span> = 0.35<span class="math inline">\(^{\circ}\)</span>F. A dot plot of the data is shown in Figure <a href="inference-one-mean.html#fig:body-temp-hist">17.5</a>, with summary statistics displayed below.</p>
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">favstats</span><span class="op">(</span><span class="va">temperatures</span><span class="op">)</span></span></code></pre></div>
<pre><code>#&gt;   min   Q1 median   Q3  max mean    sd  n missing
#&gt;  96.7 97.3   97.5 97.7 98.1 97.5 0.353 20       0</code></pre>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:body-temp-hist"></span>
<img src="17-numerical-one-mean_files/figure-html/body-temp-hist-1.png" alt="Distribution of body temperatures in a random sample of twenty Montana State University students." width="75%"><p class="caption">
Figure 17.5: Distribution of body temperatures in a random sample of twenty Montana State University students.
</p>
</div>
</div>
<div id="shifted-bootstrapped-null-distribution" class="section level3" number="17.2.2">
<h3>
<span class="header-section-number">17.2.2</span> Shifted bootstrapped null distribution<a class="anchor" aria-label="anchor" href="#shifted-bootstrapped-null-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>We would like to test the set of hypotheses <span class="math inline">\(H_0: \mu = 98.6\)</span> versus <span class="math inline">\(H_A: \mu &lt; 98.6\)</span>, where <span class="math inline">\(\mu\)</span> is the true mean body temperature among all adults (in degrees F). If we were to simulate sample mean body temperatures under <span class="math inline">\(H_0\)</span>, we would expect the null distribution to be centered at <span class="math inline">\(\mu_0\)</span> = 98.6<span class="math inline">\(^\circ\)</span>F. However, if we bootstrap sample means from our observed sample, the bootstrap distribution will be centered at the sample mean body temperature <span class="math inline">\(\bar{x}\)</span> = 97.5<span class="math inline">\(^\circ\)</span>F.</p>
<p>To use bootstrapping to generate a null distribution of sample means, we first have to <strong>shift the data</strong> to be centered at the null value. We do this by adding <span class="math inline">\(\mu_0 - \bar{x} = 98.6 - 97.5 = 1.1^\circ\)</span>F to each body temperature in the sample. This process is displayed in Figure <a href="inference-one-mean.html#fig:shift-boot-dat">17.6</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:shift-boot-dat"></span>
<img src="17-numerical-one-mean_files/figure-html/shift-boot-dat-1.png" alt="Distribution of body temperatures in a random sample of twenty Montana State University students (blue) and the shifted body temperatures (red), found by adding 1.1 degree F to each original body temperature." width="75%"><p class="caption">
Figure 17.6: Distribution of body temperatures in a random sample of twenty Montana State University students (blue) and the shifted body temperatures (red), found by adding 1.1 degree F to each original body temperature.
</p>
</div>
<p>A bootstrapped null distribution generated from sampling 20 shifted temperatures, with replacement, from the shifted data 1,000 times is shown in Figure <a href="inference-one-mean.html#fig:shifted-boot-null">17.7</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:shifted-boot-null"></span>
<img src="17-numerical-one-mean_files/figure-html/shifted-boot-null-1.png" alt="Bootstrapped null distribution of sample mean temperatures assuming the true mean temperature is 98.6 degrees F." width="75%"><p class="caption">
Figure 17.7: Bootstrapped null distribution of sample mean temperatures assuming the true mean temperature is 98.6 degrees F.
</p>
</div>
<div class="onebox">
<p><strong>Shifted bootstrap null distribution for a sample mean.</strong></p>
<p>To simulate a null distribution of sample means under the null hypothesis <span class="math inline">\(H_0: \mu = \mu_0\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Add <span class="math inline">\(\mu_0 - \bar{x}\)</span> to each value in the original sample:
<span class="math display">\[
x_1 + \mu_0 - \bar{x}, \hspace{2.5mm} x_2 + \mu_0 - \bar{x}, \hspace{2.5mm}  x_3 + \mu_0 - \bar{x}, \hspace{2.5mm}  \ldots, \hspace{2.5mm}  x_n + \mu_0 - \bar{x}.
  \]</span>
Note that if <span class="math inline">\(\bar{x}\)</span> is larger than <span class="math inline">\(\mu\)</span>, then the quantity <span class="math inline">\(\mu_0 - \bar{x}\)</span> will be negative, and you will be <em>subtracting</em> the distance between <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\bar{x}\)</span> from each value.</li>
<li>Generate 1000s of bootstrap resamples from this shifted distribution, plotting the shifted bootstrap sample mean each time.</li>
</ol>
</div>
<p>To calculate the p-value, since <span class="math inline">\(H_A: \mu &lt; 98.6\)</span>, we find the proportion of simulated sample means that were less than or equal to our original sample mean, <span class="math inline">\(\bar{x}\)</span> = 97.47. As shown in Figure <a href="inference-one-mean.html#fig:shifted-boot-null">17.7</a>, none of our simulated sample means were 97.5<span class="math inline">\(^\circ\)</span>F or lower, giving us very strong evidence that the true mean body temperature among all Montana State University students is less than the commonly accepted 98.6<span class="math inline">\(^\circ\)</span>F average temperature.</p>
</div>
</div>
<div id="one-mean-math" class="section level2" number="17.3">
<h2>
<span class="header-section-number">17.3</span> Theory-based inferential methods for <span class="math inline">\(\mu\)</span><a class="anchor" aria-label="anchor" href="#one-mean-math"><i class="fas fa-link"></i></a>
</h2>
<p>As with the sample proportion, the variability of the sample mean is well described by the mathematical theory given by the Central Limit Theorem. Similar to how we can model the behavior of the sample proportion <span class="math inline">\(\hat{p}\)</span> using a normal distribution, the sample mean <span class="math inline">\(\bar{x}\)</span> can also be modeled using
a normal distribution when certain conditions are met.
However, because of missing information about the inherent variability in the population, a <span class="math inline">\(t\)</span>-distribution is used in place of the standard normal when performing hypothesis test or confidence interval analyses.</p>
<div class="onebox">
<p><strong>Central Limit Theorem for the sample mean.</strong><br>
When we collect a sufficiently large sample of
<span class="math inline">\(n\)</span> independent observations from a population with
mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>,
the sampling distribution of <span class="math inline">\(\bar{x}\)</span> will be nearly
normal with
<span class="math display">\[\begin{align*}
  &amp;\text{Mean}=\mu
  &amp;&amp;\text{Standard Deviation }(SD) = \frac{\sigma}{\sqrt{n}}
  \end{align*}\]</span></p>
</div>
<p>Before diving into confidence intervals and hypothesis
tests using <span class="math inline">\(\bar{x}\)</span>, we first need to cover two topics:</p>
<ul>
<li>When we modeled <span class="math inline">\(\hat{p}\)</span> using the normal distribution,
certain conditions had to be satisfied.
The conditions for working with <span class="math inline">\(\bar{x}\)</span>
are a little more complex, and below, we will discuss
how to check conditions for inference using a mathematical model.</li>
<li>The standard deviation of the sample mean is dependent on the population
standard deviation, <span class="math inline">\(\sigma\)</span>.
However, we rarely know <span class="math inline">\(\sigma\)</span>, and instead
we must estimate it.
Because this estimation is itself imperfect,
we use a new distribution called the
<strong><span class="math inline">\(t\)</span>-distribution</strong>
to fix this problem.</li>
</ul>
<div id="evaluating-the-two-conditions-required-for-modeling-barx-using-theory-based-methods" class="section level3" number="17.3.1">
<h3>
<span class="header-section-number">17.3.1</span> Evaluating the two conditions required for modeling <span class="math inline">\(\bar{x}\)</span> using theory-based methods<a class="anchor" aria-label="anchor" href="#evaluating-the-two-conditions-required-for-modeling-barx-using-theory-based-methods"><i class="fas fa-link"></i></a>
</h3>
<p>There are two conditions required to apply the
Central Limit Theorem
for a sample mean <span class="math inline">\(\bar{x}\)</span>.
When the sample observations
are independent and the sample size is sufficiently
large, the normal model will describe the variability in sample means quite well; when the observations violate the conditions, the normal model can be inaccurate.</p>
<div class="onebox">
<p><strong>Conditions for the modeling
<span class="math inline">\(\bar{x}\)</span> using theory-based methods.</strong></p>
<p>The sampling distribution for <span class="math inline">\(\bar{x}\)</span> based on
a sample of size <span class="math inline">\(n\)</span> from a population with a true
mean <span class="math inline">\(\mu\)</span> and true standard deviation <span class="math inline">\(\sigma\)</span> can be modeled
using a normal distribution when:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Independence.</strong> The sample observations must be independent,
The most common way to satisfy this condition is
when the sample is a simple random sample from the
population.
If the data come from a random process,
analogous to rolling a die,
this would also satisfy the independence condition.</p></li>
<li>
<p><strong>Normality.</strong> When a sample is small,
we also require that the sample observations
come from a normally distributed population.
We can relax this condition more and more
for larger and larger sample sizes.
This condition is obviously vague,
making it difficult to evaluate,
so next we introduce a couple rules of thumb
to make checking this condition easier.</p>
<p>When these conditions are satisfied, then the sampling
distribution of <span class="math inline">\(\bar{x}\)</span> is approximately normal with mean
<span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span>.</p>
</li>
</ol>
</div>
<div class="importantbox">
<p><strong>General rule: how to perform the normality check.</strong></p>
<p>There is no perfect way to check the normality condition,
so instead we use two general rules:</p>
<ul>
<li>
<span class="math inline">\(\mathbf{n &lt; 30}\)</span>: If the sample size <span class="math inline">\(n\)</span> is less than 30
and there are no clear outliers in the data,
then we typically assume the data come from
a nearly normal distribution to satisfy the
condition.<br>
</li>
<li>
<span class="math inline">\(\mathbf{n \geq 30}\)</span>: If the sample size <span class="math inline">\(n\)</span> is at least 30
and there are no <em>particularly extreme</em> outliers,
then we typically assume the sampling distribution
of <span class="math inline">\(\bar{x}\)</span> is nearly normal, even if the underlying
distribution of individual observations is not.</li>
<li>
<span class="math inline">\(\mathbf{n &gt; 100}\)</span>: If the sample size <span class="math inline">\(n\)</span> is at least 100
(regardless of the presence of skew or outliers),
we typically assume the sampling distribution
of <span class="math inline">\(\bar{x}\)</span> is nearly normal, even if the underlying
distribution of individual observations is not.</li>
</ul>
</div>
<p>In this first course in statistics, you aren’t expected
to develop perfect judgement on the normality condition.
However, you are expected to be able to handle
clear cut cases based on the rules of thumb.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;More
nuanced guidelines would consider further relaxing
the &lt;em&gt;particularly extreme outlier&lt;/em&gt; check when the
sample size is very large.
However, we’ll leave further discussion here to a future course.&lt;/p&gt;"><sup>154</sup></a></p>
<div class="example">
<p><span id="exm:unlabeled-div-1" class="example"><strong>Example 17.1  </strong></span>Consider the following two plots
that come from simple random samples from
different populations.
Their sample sizes are <span class="math inline">\(n_1 = 15\)</span> and <span class="math inline">\(n_2 = 50\)</span>.</p>
<p>Are the independence and normality conditions met
in each case?</p>
<hr>
<p>Each sample is from a simple random sample of its
respective population, so the independence condition
is satisfied.
Let’s next check the normality condition for
each using the rule of thumb.</p>
<p>The first sample has fewer than 30 observations,
so we are watching for any clear outliers.
None are present; while there is a small gap in the
histogram on the right, this gap is small and
20% of the observations in this small sample
are represented in that far right bar of the histogram,
so we can hardly call these clear outliers.
With no clear outliers, the normality condition
is reasonably met.</p>
<p>The second sample has a sample size greater than 30 and
includes an outlier that appears to be roughly 5 times
further from the center of the distribution than the
next furthest observation.
This is an example of a particularly extreme outlier,
so the normality condition would not be satisfied.</p>
</div>
<p><img src="17-numerical-one-mean_files/figure-html/outliersandsscondition-1.png" width="90%" style="display: block; margin: auto;"><img src="17-numerical-one-mean_files/figure-html/outliersandsscondition-2.png" width="90%" style="display: block; margin: auto;"></p>
<p>In practice, it’s typical to also do a mental check to evaluate
whether we have reason to believe the underlying population
would have moderate skew (if <span class="math inline">\(n &lt; 30\)</span>)
or have particularly extreme outliers <span class="math inline">\((n \geq 30)\)</span>
beyond what we observe in the data.
For example, consider the number of followers
for each individual account on Twitter,
and then imagine this distribution.
The large majority of accounts have built up
a couple thousand followers or fewer,
while a relatively tiny fraction have amassed
tens of millions of followers,
meaning the distribution is extremely skewed.
When we know the data come from such an extremely
skewed distribution,
it takes some effort to understand what sample
size is large enough for the normality condition
to be satisfied.</p>
<p></p>
</div>
<div id="introducing-the-t-distribution" class="section level3" number="17.3.2">
<h3>
<span class="header-section-number">17.3.2</span> Introducing the <span class="math inline">\(t\)</span>-distribution<a class="anchor" aria-label="anchor" href="#introducing-the-t-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>
</p>
<p>In practice, we cannot directly calculate the standard deviation
for <span class="math inline">\(\bar{x}\)</span> since we do not know the population standard
deviation, <span class="math inline">\(\sigma\)</span>.
We encountered a similar issue when computing the standard
error for a sample proportion, which relied on the population
proportion, <span class="math inline">\(\pi\)</span>.
Our solution in the proportion context was to use sample
value in place
of the population value to calculate a standard error.
We’ll employ a similar strategy to compute the standard
error of <span class="math inline">\(\bar{x}\)</span>, using the sample
standard deviation <span class="math inline">\(s\)</span> in place of <span class="math inline">\(\sigma\)</span>:
<span class="math display">\[\begin{align*}
SE(\bar{x}) = \frac{s}{\sqrt{n}} \approx SD(\bar{x}) = \frac{\sigma}{\sqrt{n}}.
\end{align*}\]</span>
The standard error of <span class="math inline">\(\bar{x}\)</span> provides an estimate of the standard deviation of <span class="math inline">\(\bar{x}\)</span>. This strategy tends to work well when we have
a lot of data and can estimate <span class="math inline">\(\sigma\)</span> using <span class="math inline">\(s\)</span> accurately.
However, the estimate is less precise with smaller samples,
and this leads to problems when using the normal
distribution to model <span class="math inline">\(\bar{x}\)</span> if we do not know <span class="math inline">\(\sigma\)</span>.</p>
<p>We’ll find it useful to use a new distribution for
inference calculations called the <strong><span class="math inline">\(t\)</span>-distribution</strong>.
A <span class="math inline">\(t\)</span>-distribution, shown as a solid line in
Figure <a href="inference-one-mean.html#fig:tDistCompareToNormalDist">17.8</a>, has a bell shape.
However, its tails are thicker than the normal distribution’s,
meaning observations are more likely to fall beyond two
standard deviations from the mean than under the normal
distribution.</p>
<p>The extra thick tails of the <span class="math inline">\(t\)</span>-distribution are exactly
the correction needed to resolve the problem (due to extra variability of the test statistic) of using <span class="math inline">\(s\)</span>
in place of <span class="math inline">\(\sigma\)</span> in the <span class="math inline">\(SE(\bar{x})\)</span> calculation.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:tDistCompareToNormalDist"></span>
<img src="17-numerical-one-mean_files/figure-html/tDistCompareToNormalDist-1.png" alt="Comparison of a $t$-distribution and a normal distribution." width="90%"><p class="caption">
Figure 17.8: Comparison of a <span class="math inline">\(t\)</span>-distribution and a normal distribution.
</p>
</div>
<p>The <span class="math inline">\(t\)</span>-distribution is always centered at zero and
has a single parameter: degrees of freedom (<span class="math inline">\(df\)</span>).
The <strong>degrees of freedom</strong>
describes the precise form of the bell-shaped <span class="math inline">\(t\)</span>-distribution.
Several <span class="math inline">\(t\)</span>-distributions are shown in
Figure <a href="inference-one-mean.html#fig:tDistConvergeToNormalDist">17.9</a>
in comparison to the normal distribution.</p>
<p>For inference with a single mean, we’ll use a <span class="math inline">\(t\)</span>-distribution
with <span class="math inline">\(df = n - 1\)</span> to model the sample mean
when the sample size is <span class="math inline">\(n\)</span>.
That is, when we have more observations,
the degrees of freedom will be larger and
the <span class="math inline">\(t\)</span>-distribution will look more like the
standard normal distribution;
when the degrees of freedom is about 30 or more,
the <span class="math inline">\(t\)</span>-distribution is nearly indistinguishable
from the normal distribution.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:tDistConvergeToNormalDist"></span>
<img src="17-numerical-one-mean_files/figure-html/tDistConvergeToNormalDist-1.png" alt="The larger the degrees of freedom, the more closely the $t$-distribution resembles the standard normal distribution." width="90%"><p class="caption">
Figure 17.9: The larger the degrees of freedom, the more closely the <span class="math inline">\(t\)</span>-distribution resembles the standard normal distribution.
</p>
</div>
<div class="onebox">
<p><strong>Degrees of freedom: <span class="math inline">\(df\)</span>.</strong></p>
<p>The degrees of freedom describes the shape of the
<span class="math inline">\(t\)</span>-distribution.
The larger the degrees of freedom, the more closely
the distribution approximates the normal model.</p>
<p>When modeling <span class="math inline">\(\bar{x}\)</span> using the <span class="math inline">\(t\)</span>-distribution,
use <span class="math inline">\(df = n - 1\)</span>.</p>
</div>
<p>The <span class="math inline">\(t\)</span>-distribution allows us greater flexibility than
the normal distribution when analyzing numerical data.
In practice, it’s common to use statistical software,
such as R, Python, or SAS for these analyses.
In R, the function used for calculating probabilities under a <span class="math inline">\(t\)</span>-distribution is <code><a href="https://rdrr.io/r/stats/TDist.html">pt()</a></code> (which should seem similar to the previous R function <code><a href="https://rdrr.io/r/stats/Normal.html">pnorm()</a></code>).
Don’t forget that with the <span class="math inline">\(t\)</span>-distribution, the degrees of freedom must always be specified!</p>
<!--
Alternatively, a graphing calculator or a
\termsub{$\pmb{t}$-table}{t-table@$t$-table} may be used;
the $t$-table is similar to the normal distribution table,
and it may be found in Appendix \ref{tDistributionTable},
which includes usage instructions and examples
for those who wish to use this option.
-->
<p>For the examples and guided practices below, use R to find the answers. We recommend trying the problems so as to get a sense for how the <span class="math inline">\(t\)</span>-distribution can vary in width depending on the degrees of freedom, and to confirm your working
understanding of the <span class="math inline">\(t\)</span>-distribution.</p>
<div class="example">
<p><span id="exm:unlabeled-div-2" class="example"><strong>Example 17.2  </strong></span>What proportion of the <span class="math inline">\(t\)</span>-distribution
with 18 degrees of freedom falls below -2.10?</p>
<hr>
<p>Just like a normal probability problem, we first draw
the picture in Figure <a href="inference-one-mean.html#fig:tDistDF18LeftTail2Point10">17.10</a>
and shade the area below -2.10.</p>
<p>Using statistical software, we can obtain a precise
value: 0.0250.</p>
</div>
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># using pt() to find probability under the $t$-distribution</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">pt</a></span><span class="op">(</span><span class="op">-</span><span class="fl">2.10</span>, df <span class="op">=</span> <span class="fl">18</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.025</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:tDistDF18LeftTail2Point10"></span>
<img src="17-numerical-one-mean_files/figure-html/tDistDF18LeftTail2Point10-1.png" alt="The $t$-distribution with 18 degrees of freedom. The area below -2.10 has been shaded." width="90%"><p class="caption">
Figure 17.10: The <span class="math inline">\(t\)</span>-distribution with 18 degrees of freedom. The area below -2.10 has been shaded.
</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-3" class="example"><strong>Example 17.3  </strong></span>A <span class="math inline">\(t\)</span>-distribution with 20 degrees of freedom
is shown in the top panel of
Figure <a href="inference-one-mean.html#fig:tDistDF20RightTail1Point65">17.11</a>.
Estimate the proportion of the distribution falling
above 1.65.</p>
<hr>
<p>Note that with 20 degrees of freedom, the <span class="math inline">\(t\)</span>-distribution is relatively close to the normal distribution.
With a normal distribution, this would correspond to
about 0.05, so we should expect the <span class="math inline">\(t\)</span>-distribution
to give us a value in this neighborhood.
Using statistical software: 0.0573.</p>
</div>
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># using pt() to find probability under the $t$-distribution</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">pt</a></span><span class="op">(</span><span class="fl">1.65</span>, df <span class="op">=</span> <span class="fl">20</span>, lower.tail<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.0573</span></span>
<span><span class="co"># or</span></span>
<span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">pt</a></span><span class="op">(</span><span class="fl">1.65</span>, df <span class="op">=</span> <span class="fl">20</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.0573</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:tDistDF20RightTail1Point65"></span>
<img src="17-numerical-one-mean_files/figure-html/tDistDF20RightTail1Point65-1.png" alt="Top: The $t$-distribution with 20 degrees of freedom, with the area above 1.65 shaded. Bottom: The $t$-distribution with 2 degrees of freedom, with the area further than 3 units from 0 shaded." width="90%"><img src="17-numerical-one-mean_files/figure-html/tDistDF20RightTail1Point65-2.png" alt="Top: The $t$-distribution with 20 degrees of freedom, with the area above 1.65 shaded. Bottom: The $t$-distribution with 2 degrees of freedom, with the area further than 3 units from 0 shaded." width="90%"><p class="caption">
Figure 17.11: Top: The <span class="math inline">\(t\)</span>-distribution with 20 degrees of freedom, with the area above 1.65 shaded. Bottom: The <span class="math inline">\(t\)</span>-distribution with 2 degrees of freedom, with the area further than 3 units from 0 shaded.
</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-4" class="example"><strong>Example 17.4  </strong></span>A <span class="math inline">\(t\)</span>-distribution with 2 degrees of freedom
is shown in the bottom panel of
Figure <a href="inference-one-mean.html#fig:tDistDF20RightTail1Point65">17.11</a>.
Estimate the proportion of the distribution falling more
than 3 units from the mean (above or below).</p>
<hr>
<p>With so few degrees of freedom, the <span class="math inline">\(t\)</span>-distribution will
give a more notably different value than the normal
distribution.
Under a normal distribution, the area would be about
0.003 using the 68-95-99.7 rule.
For a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df = 2\)</span>, the area in both
tails beyond 3 units totals 0.0955.
This area is dramatically different than what
we obtain from the normal distribution.</p>
</div>
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># using pt() to find probability under the $t$-distribution</span></span>
<span><span class="fl">2</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">pt</a></span><span class="op">(</span><span class="op">-</span><span class="fl">3</span>, df <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.0955</span></span></code></pre></div>
<div class="guidedpractice">
<p>What proportion of the <span class="math inline">\(t\)</span>-distribution with 19 degrees
of freedom falls above -1.79 units?<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;We want to find the shaded area &lt;em&gt;above&lt;/em&gt; -1.79 (we leave the picture to you). The lower tail area has an area of 0.0447, so the upper area would have an area of &lt;span class="math inline"&gt;\(1 - 0.0447 = 0.9553\)&lt;/span&gt;.&lt;/p&gt;'><sup>155</sup></a></p>
</div>
<p>
</p>
</div>
<div id="one-sample-t-confidence-intervals" class="section level3" number="17.3.3">
<h3>
<span class="header-section-number">17.3.3</span> One sample <span class="math inline">\(t\)</span>-confidence intervals<a class="anchor" aria-label="anchor" href="#one-sample-t-confidence-intervals"><i class="fas fa-link"></i></a>
</h3>
<p></p>
<p>Let’s get our first taste of applying the <span class="math inline">\(t\)</span>-distribution
in the context of an example about the mercury content
of dolphin muscle.
Elevated mercury concentrations are an important problem
for both dolphins
and other animals, like humans, who occasionally eat them.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:rissosDolphin"></span>
<img src="06/figures/rissosDolphin.jpg" alt="A Risso's dolphin. Photo by Mike Baird, www.bairdphotos.com." width="75%"><p class="caption">
Figure 17.12: A Risso’s dolphin. Photo by Mike Baird, www.bairdphotos.com.
</p>
</div>
<div id="observed-data-6" class="section level4 unnumbered">
<h4>Observed data<a class="anchor" aria-label="anchor" href="#observed-data-6"><i class="fas fa-link"></i></a>
</h4>
<p>We will identify a confidence interval for the average mercury content in dolphin muscle using a sample of 19 Risso’s dolphins from the Taiji area in Japan. The data are summarized in Table <a href="inference-one-mean.html#tab:summaryStatsOfHgInMuscleOfRissosDolphins">17.1</a>. The minimum and maximum observed values can be used to evaluate whether or not there are clear outliers.</p>
<div class="inline-table"><table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:summaryStatsOfHgInMuscleOfRissosDolphins">Table 17.1: </span>Summary of mercury content in the muscle of 19 Risso’s dolphins from the Taiji area. Measurements are in micrograms of mercury per wet gram
of muscle (<span class="math inline">\(\mu\)</span>g/wet g).
</caption>
<thead><tr>
<th style="text-align:right;">
<span class="math inline">\(n\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\bar{x}\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(s\)</span>
</th>
<th style="text-align:right;">
minimum
</th>
<th style="text-align:right;">
maximum
</th>
</tr></thead>
<tbody><tr>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
4.4
</td>
<td style="text-align:right;">
2.3
</td>
<td style="text-align:right;">
1.7
</td>
<td style="text-align:right;">
9.2
</td>
</tr></tbody>
</table></div>
<div class="example">
<p><span id="exm:unlabeled-div-5" class="example"><strong>Example 17.5  </strong></span>Are the independence and
normality conditions satisfied for this data set?</p>
<hr>
<p>The observations are a simple random sample,
therefore independence is reasonable.
The summary statistics in
Table <a href="inference-one-mean.html#tab:summaryStatsOfHgInMuscleOfRissosDolphins">17.1</a>
do not suggest any clear outliers, with
all observations within 2.5 standard deviations
of the mean.
Based on this evidence, the normality condition
seems reasonable.</p>
</div>
<p>In the normal model, we used <span class="math inline">\(z^{\star}\)</span> and the standard error to determine the width of a confidence interval. We revise the confidence interval formula slightly when using the <span class="math inline">\(t\)</span>-distribution:
<span class="math display">\[\begin{align*}
&amp;\text{point estimate} \ \pm\  t^{\star}_{df} \times SE(\text{point estimate})
&amp;&amp;\to
&amp;&amp;\bar{x} \ \pm\  t^{\star}_{df} \times \frac{s}{\sqrt{n}},
\end{align*}\]</span>
where <span class="math inline">\(df = n - 1\)</span> when computing a one-sample <span class="math inline">\(t\)</span>-interval.</p>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example 17.6  </strong></span>Using the summary statistics in
Table <a href="inference-one-mean.html#tab:summaryStatsOfHgInMuscleOfRissosDolphins">17.1</a>,
compute the standard error for the average
mercury content in the <span class="math inline">\(n = 19\)</span> dolphins.</p>
<hr>
<p>We plug in <span class="math inline">\(s\)</span> and <span class="math inline">\(n\)</span> into the formula:
<span class="math inline">\(SE(\bar{x})  = s / \sqrt{n}  = 2.3 / \sqrt{19}  = 0.528\)</span>.</p>
</div>
<p>The value <span class="math inline">\(t^{\star}_{df}\)</span> is a cutoff we obtain based on the
confidence level and the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df\)</span> degrees
of freedom.
That cutoff is found in the same way as with a normal
distribution: we find <span class="math inline">\(t^{\star}_{df}\)</span> such that
the fraction of the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df\)</span> degrees
of freedom within a distance <span class="math inline">\(t^{\star}_{df}\)</span>
of 0 matches the confidence level of interest.</p>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Example 17.7  </strong></span>When <span class="math inline">\(n = 19\)</span>, what is the appropriate
degrees of freedom?
Find <span class="math inline">\(t^{\star}_{df}\)</span> for this degrees of freedom
and the confidence level of 95%.</p>
<hr>
<p>The degrees of freedom is easy to calculate:
<span class="math inline">\(df = n - 1 = 18\)</span>.</p>
<p>Using statistical software, we find the cutoff where
the upper tail is equal to 2.5%:
<span class="math inline">\(t^{\star}_{18} =\)</span> 2.10.
The area below -2.10 will also be equal to 2.5%.
That is, 95% of the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df = 18\)</span>
lies within 2.10 units of 0.</p>
</div>
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># use qt() to find the t-cutoff (with 95% in the middle)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">0.025</span>, df <span class="op">=</span> <span class="fl">18</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] -2.1</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">0.975</span>, df <span class="op">=</span> <span class="fl">18</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 2.1</span></span></code></pre></div>
<div class="example">
<p><span id="exm:unlabeled-div-8" class="example"><strong>Example 17.8  </strong></span>Compute and interpret the 95% confidence interval
for the average mercury content in Risso’s dolphins.</p>
<hr>
<p>We can construct the confidence interval as
<span class="math display">\[\begin{align*}
  \bar{x} \ \pm\  t^{\star}_{18} \times SE(\bar{x})
    \quad \to \quad 4.4 \ \pm\  2.10 \times 0.528
    \quad \to \quad (3.29, 5.51)
  \end{align*}\]</span>
We are 95% confident the average mercury content of muscles
in the population of Risso’s dolphins is between 3.29 and 5.51 <span class="math inline">\(\mu\)</span>g/wet gram,
which is considered extremely high.</p>
</div>
<p></p>
<div class="onebox">
<p><strong>Finding a <span class="math inline">\(t\)</span>-confidence interval for a population mean, <span class="math inline">\(\mu\)</span>.</strong></p>
<p>Based on a sample of <span class="math inline">\(n\)</span> independent and nearly normal
observations, a confidence interval for the population
mean is
<span class="math display">\[\begin{align*}
  &amp;\text{point estimate} \ \pm\  t^{\star}_{df} \times SE(\text{point estimate})
  &amp;&amp;\to
  &amp;&amp;\bar{x} \ \pm\  t^{\star}_{df} \times \frac{s}{\sqrt{n}}
  \end{align*}\]</span>
where <span class="math inline">\(\bar{x}\)</span> is the sample mean, <span class="math inline">\(t^{\star}_{df}\)</span>
corresponds to the confidence level and degrees of freedom
<span class="math inline">\(df\)</span>, and <span class="math inline">\(SE\)</span> is the standard error as estimated by
the sample.</p>
</div>
<div class="guidedpractice">
<p>The FDA’s webpage provides some data on mercury content of fish.
Based on a sample of 15 croaker white fish (Pacific), a sample mean and standard deviation were computed as 0.287 and 0.069 ppm (parts per million), respectively.
The 15 observations ranged from 0.18 to 0.41 ppm. We will assume these observations are independent.
Based on the summary statistics of the data, do you have any objections to the normality condition of the individual observations?<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;The sample size is under 30, so we check for obvious outliers: since all observations are within 2 standard deviations of the mean, there are no such clear outliers.&lt;/p&gt;"><sup>156</sup></a></p>
</div>
<p></p>
<div class="example">
<p><span id="exm:unlabeled-div-9" class="example"><strong>Example 17.9  </strong></span>Calculate the standard error of
<span class="math inline">\(\bar{x}\)</span> using the data summaries in the previous Guided Practice. If we are to use the <span class="math inline">\(t\)</span>-distribution to create a
90% confidence interval for the actual mean of the
mercury content, identify the degrees of freedom
and <span class="math inline">\(t^{\star}_{df}\)</span>.</p>
<hr>
<p>The standard error: <span class="math inline">\(SE(\bar{x}) = \frac{0.069}{\sqrt{15}} = 0.0178\)</span>.</p>
<p>Degrees of freedom: <span class="math inline">\(df = n - 1 = 14\)</span>.</p>
<p>Since the goal is a 90% confidence interval,
we choose <span class="math inline">\(t_{14}^{\star}\)</span> so that the two-tail area
is 0.1:
<span class="math inline">\(t^{\star}_{14} = 1.76\)</span>.</p>
</div>
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># use qt() to find the t-cutoff (with 90% in the middle)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">0.05</span>, df <span class="op">=</span> <span class="fl">14</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] -1.76</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">qt</a></span><span class="op">(</span><span class="fl">0.95</span>, df <span class="op">=</span> <span class="fl">14</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1.76</span></span></code></pre></div>
<!--
\begin{important}{Confidence interval for a single mean}
  Once you've determined a one-mean confidence interval
  would be helpful for an application,
  there are four steps to constructing the interval:
  \begin{description}
  \item[Prepare.]
      Identify $\bar{x}$, $s$, $n$, and determine what
      confidence level you wish to use.
  \item[Check.]
      Verify the conditions to ensure $\bar{x}$
      is nearly normal.
  \item[Calculate.]
      If the conditions hold, compute $SE$,
      find $t_{df}^{\star}$, and construct the interval.
  \item[Conclude.]
      Interpret the confidence interval in the context
      of the problem.
  \end{description}
\end{important}
-->
<div class="guidedpractice">
<p>Using the information and results of the previous Guided Practice and Example, compute a 90% confidence interval for the average mercury content of croaker white fish (Pacific).<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;span class="math inline"&gt;\(\bar{x} \ \pm\ t^{\star}_{14} \times SE(\bar{x})  \ \to\  0.287 \ \pm\  1.76 \times 0.0178  \ \to\ (0.256, 0.318)\)&lt;/span&gt;.
We are 90% confident that the average mercury content
of croaker white fish (Pacific) is between 0.256 and 0.318 ppm.&lt;/p&gt;'><sup>157</sup></a></p>
</div>
<div class="guidedpractice">
<p>The 90% confidence interval from the previous
Guided Practice is 0.256 ppm to 0.318 ppm.
Can we say that 90% of croaker white fish (Pacific)
have mercury levels between 0.256 and 0.318 ppm?<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;No, a confidence interval only provides a range
of plausible values for a population parameter,
in this case the population &lt;em&gt;mean&lt;/em&gt;.
It does not describe what we might observe
for &lt;em&gt;individual&lt;/em&gt; observations.&lt;/p&gt;"><sup>158</sup></a></p>
</div>
<p></p>
</div>
</div>
<div id="one-sample-t-tests" class="section level3" number="17.3.4">
<h3>
<span class="header-section-number">17.3.4</span> One sample <span class="math inline">\(t\)</span>-tests<a class="anchor" aria-label="anchor" href="#one-sample-t-tests"><i class="fas fa-link"></i></a>
</h3>
<p>Now that we’ve used the <span class="math inline">\(t\)</span>-distribution for making a confidence
intervals for a mean, let’s speed on through to
hypothesis tests for the mean.</p>
<div class="onebox">
<p><strong>The test statistic for assessing a single mean is a T.</strong></p>
<p>The <strong>T score</strong> is a ratio of how the sample mean differs from the hypothesized mean as compared to how the observations vary.</p>
<p><span class="math display">\[\begin{align*}
T = \frac{\bar{x} - \mbox{null value}}{s/\sqrt{n}}
\end{align*}\]</span></p>
<p>When the null hypothesis is true and the conditions are met, T has a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df = n - 1\)</span>.</p>
<p>Conditions:</p>
<ul>
<li>independently observed data<br>
</li>
<li>large enough sample to satisfy normality<br>
</li>
</ul>
</div>
<!--
\newcommand{\cherryblossomn}{100}
\newcommand{\cherryblossommean}{97.32}
\newcommand{\cherryblossomnull}{93.29}
\newcommand{\cherryblossomsd}{16.98}
\newcommand{\cherryblossomse}{1.70}
\newcommand{\cherryblossomz}{2.37}
-->
<div class="guidedpractice">
<p>Compare the T score — the standardized sample mean — to the Z score — the standardized sample proportion — presented in Section <a href="inference-one-prop.html#theory-prop">14.3</a>. Why do we use a “Z” when standardizing proportions, but a “T” when standardizing means?<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;The letter “Z” is typically used when its distribution follows a standard normal distribution. We use “T” when the test statistic follows a &lt;span class="math inline"&gt;\(t\)&lt;/span&gt;-distribution.&lt;/p&gt;'><sup>159</sup></a></p>
</div>
<p>Is the typical US runner getting faster or slower over time? We consider this question in the context of the Cherry Blossom Race, which is a 10-mile race in Washington, DC each spring.</p>
<p>The average time for all runners who finished the Cherry Blossom Race in 2006 was 93.29 minutes (93 minutes and about 17 seconds). We want to determine using data from 100 participants in the 2017 Cherry Blossom Race whether runners in this race are getting faster or slower, versus the other possibility that there has been no change.</p>
<div class="guidedpractice">
<p>What are appropriate hypotheses for this context?<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;&lt;span class="math inline"&gt;\(H_0\)&lt;/span&gt;: The average 10-mile run time was the same for 2006 and 2017. &lt;span class="math inline"&gt;\(\mu = 93.29\)&lt;/span&gt; minutes. &lt;span class="math inline"&gt;\(H_A\)&lt;/span&gt;: The average 10-mile run time for 2017 was &lt;em&gt;different&lt;/em&gt; than that of 2006. &lt;span class="math inline"&gt;\(\mu \neq 93.29\)&lt;/span&gt; minutes.&lt;/p&gt;'><sup>160</sup></a></p>
</div>
<div class="guidedpractice">
<p>The data come from a simple random sample of all participants, so the observations are independent.
<img src="17-numerical-one-mean_files/figure-html/unnamed-chunk-13-1.png" width="90%" style="display: block; margin: auto;">
A histogram of the race times is given to evaluate if we can move forward with a t-test. Should we be worried about the normality condition?<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;With a sample of 100, we should only be concerned if there is are particularly extreme outliers. The histogram of the data doesn’t show any outliers of concern (and arguably, no outliers at all).&lt;/p&gt;"><sup>161</sup></a></p>
</div>
<p>When completing a hypothesis test for the one-sample mean,
the process is nearly identical to completing a hypothesis
test for a single proportion.
First, we find the Z score using the observed value,
null value, and standard error;
however, we call it a <strong>T score</strong> since we use
a <span class="math inline">\(t\)</span>-distribution for calculating the tail area.
Then we find the p-value using the same ideas we used
previously: find the area under the <span class="math inline">\(t\)</span>-distribution as or more extreme than our T score.</p>
<div class="example">
<p><span id="exm:unlabeled-div-10" class="example"><strong>Example 17.10  </strong></span>With both the independence
and normality conditions satisfied,
we can proceed with a hypothesis test using
the <span class="math inline">\(t\)</span>-distribution.
The sample mean and sample standard deviation
of the sample
of 100 runners from the
2017 Cherry Blossom Race
are 97.32 and 16.98 minutes,
respectively.
Recall that the sample size is 100
and the average run time in 2006 was 93.29 minutes.
Find the test statistic and p-value.
What is your conclusion?</p>
<hr>
<p>The hypotheses, found in a previous Guided Practice, are:</p>
<p><span class="math inline">\(H_0: \mu = 93.29\)</span> minutes</p>
<p><span class="math inline">\(H_A: \mu \neq 93.29\)</span> minutes</p>
<p>To find the test statistic (T score),
we first must determine the standard error:
<span class="math display">\[\begin{align*}
  SE(\bar{x})
    = 16.98 / \sqrt{100}
    = 1.70
  \end{align*}\]</span>
Now we can compute the T score
using the sample mean (97.32),
null value (98.29), and <span class="math inline">\(SE\)</span>:
<span class="math display">\[\begin{align*}
  T
    = \frac{97.32 - 93.29}{1.70}
    = 2.37
  \end{align*}\]</span>
For <span class="math inline">\(df = 100 - 1 = 99\)</span>,
we can determine using statistical software
that the area under a <span class="math inline">\(t\)</span>-distribution with 99 <span class="math inline">\(df\)</span> that is above
our observed T score of 2.37 is 0.01 (see below),
which we double to get the p-value: 0.02.</p>
<p>Because the p-value is small,
the data provide strong evidence that the average
run time for the Cherry Blossom Run in 2017 is different
than the 2006 average.</p>
</div>
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># using pt() to find the p-value</span></span>
<span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/TDist.html">pt</a></span><span class="op">(</span><span class="fl">2.37</span>, df <span class="op">=</span> <span class="fl">99</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.00986</span></span></code></pre></div>
<div class="protip">
<p><strong>When using a <span class="math inline">\(t\)</span>-distribution, we use a T score (similar to a Z score).</strong></p>
<p>To help us remember to use the <span class="math inline">\(t\)</span>-distribution,
we use a <span class="math inline">\(T\)</span> to represent the test statistic,
and we often call this a T score.
The Z score and T score are computed in the exact same way
and are conceptually identical:
each represents how many standard errors the observed value
is from the null value.</p>
</div>
</div>
</div>
<div id="chp17-review" class="section level2" number="17.4">
<h2>
<span class="header-section-number">17.4</span> Chapter review<a class="anchor" aria-label="anchor" href="#chp17-review"><i class="fas fa-link"></i></a>
</h2>
<div id="summary-12" class="section level3 unnumbered">
<h3>Summary<a class="anchor" aria-label="anchor" href="#summary-12"><i class="fas fa-link"></i></a>
</h3>
<div class="underconstruction">
<p>TODO</p>
</div>
</div>
<div id="terms-13" class="section level3 unnumbered">
<h3>Terms<a class="anchor" aria-label="anchor" href="#terms-13"><i class="fas fa-link"></i></a>
</h3>
<p>We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as <strong>bolded text</strong>.</p>
<div class="inline-table"><table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;"><tbody>
<tr>
<td style="text-align:left;">
bootstrapping
</td>
<td style="text-align:left;">
degrees of freedom
</td>
<td style="text-align:left;">
T score
</td>
</tr>
<tr>
<td style="text-align:left;">
Central Limit Theorem
</td>
<td style="text-align:left;">
point estimate
</td>
<td style="text-align:left;">
t-distribution
</td>
</tr>
</tbody></table></div>
</div>
<div id="key-ideas-12" class="section level3 unnumbered">
<h3>Key ideas<a class="anchor" aria-label="anchor" href="#key-ideas-12"><i class="fas fa-link"></i></a>
</h3>
<div class="underconstruction">
<p>TODO</p>
</div>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="inference-categ-applications.html"><span class="header-section-number">16</span> Applications: Infer categorical</a></div>
<div class="next"><a href="inference-paired-means.html"><span class="header-section-number">18</span> Inference for comparing paired means</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#inference-one-mean"><span class="header-section-number">17</span> Inference for a single mean</a></li>
<li>
<a class="nav-link" href="#bootstrap-confidence-interval-for-mu"><span class="header-section-number">17.1</span> Bootstrap confidence interval for \(\mu\)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#observed-data-4"><span class="header-section-number">17.1.1</span> Observed data</a></li>
<li><a class="nav-link" href="#variability-of-the-statistic-3"><span class="header-section-number">17.1.2</span> Variability of the statistic</a></li>
<li><a class="nav-link" href="#bootstrap-percentile-confidence-interval-for-sigma-special-topic"><span class="header-section-number">17.1.3</span> Bootstrap percentile confidence interval for \(\sigma\) (special topic)</a></li>
<li><a class="nav-link" href="#bootstrapping-is-not-a-solution-to-small-sample-sizes"><span class="header-section-number">17.1.4</span> Bootstrapping is not a solution to small sample sizes!</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#one-mean-null-boot"><span class="header-section-number">17.2</span> Shifted bootstrap test for \(H_0: \mu = \mu_0\)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#observed-data-5"><span class="header-section-number">17.2.1</span> Observed data</a></li>
<li><a class="nav-link" href="#shifted-bootstrapped-null-distribution"><span class="header-section-number">17.2.2</span> Shifted bootstrapped null distribution</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#one-mean-math"><span class="header-section-number">17.3</span> Theory-based inferential methods for \(\mu\)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#evaluating-the-two-conditions-required-for-modeling-barx-using-theory-based-methods"><span class="header-section-number">17.3.1</span> Evaluating the two conditions required for modeling \(\bar{x}\) using theory-based methods</a></li>
<li><a class="nav-link" href="#introducing-the-t-distribution"><span class="header-section-number">17.3.2</span> Introducing the \(t\)-distribution</a></li>
<li><a class="nav-link" href="#one-sample-t-confidence-intervals"><span class="header-section-number">17.3.3</span> One sample \(t\)-confidence intervals</a></li>
<li><a class="nav-link" href="#one-sample-t-tests"><span class="header-section-number">17.3.4</span> One sample \(t\)-tests</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#chp17-review"><span class="header-section-number">17.4</span> Chapter review</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#summary-12">Summary</a></li>
<li><a class="nav-link" href="#terms-13">Terms</a></li>
<li><a class="nav-link" href="#key-ideas-12">Key ideas</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/MTstateIntroStats/IntroStatTextbook/blob/master/17-numerical-one-mean.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/MTstateIntroStats/IntroStatTextbook/edit/master/17-numerical-one-mean.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Montana State Introductory Statistics with R</strong>" was written by Nicole Carnegie, Stacey Hancock, Elijah Meyer, Jade Schmidt, Melinda Yager. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>

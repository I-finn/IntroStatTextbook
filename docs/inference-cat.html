<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Inference for categorical data | Montana State Introductory Statistics with R</title>
  <meta name="description" content="Open resources textbook for Stat 216 at Montana State University" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Inference for categorical data | Montana State Introductory Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Open resources textbook for Stat 216 at Montana State University" />
  <meta name="github-repo" content="MTstateIntroStats/IntroStatTextbook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Inference for categorical data | Montana State Introductory Statistics with R" />
  
  <meta name="twitter:description" content="Open resources textbook for Stat 216 at Montana State University" />
  

<meta name="author" content="Nicole Carnegie, Stacey Hancock, Elijah Meyer, Jade Schmidt, Melinda Yager" />


<meta name="date" content="2020-09-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mult-reg.html"/>
<link rel="next" href="inference-num.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/oistyle.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MSU Intro Stat with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#textbook-overview"><i class="fa fa-check"></i>Textbook overview</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#statistical-computing"><i class="fa fa-check"></i>Statistical computing</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#getting-rstudio"><i class="fa fa-check"></i>Getting RStudio</a></li>
<li><a href="index.html#installing-catstats">Installing <code>catstats</code></a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a>
<ul>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html#montana-state-university-authors"><i class="fa fa-check"></i>Montana State University Authors</a></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html#openintro-authors"><i class="fa fa-check"></i>OpenIntro Authors</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="copyright.html"><a href="copyright.html"><i class="fa fa-check"></i>Copyright</a></li>
<li class="chapter" data-level="1" data-path="intro-to-data.html"><a href="intro-to-data.html"><i class="fa fa-check"></i><b>1</b> Introduction to data</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro-to-data.html"><a href="intro-to-data.html#basic-stents-strokes"><i class="fa fa-check"></i><b>1.1</b> Case study: using stents to prevent strokes</a></li>
<li class="chapter" data-level="1.2" data-path="intro-to-data.html"><a href="intro-to-data.html#data-basics"><i class="fa fa-check"></i><b>1.2</b> Data basics</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="intro-to-data.html"><a href="intro-to-data.html#observations-variables-and-data-frames"><i class="fa fa-check"></i><b>1.2.1</b> Observations, variables, and data frames</a></li>
<li class="chapter" data-level="1.2.2" data-path="intro-to-data.html"><a href="intro-to-data.html#variable-types"><i class="fa fa-check"></i><b>1.2.2</b> Types of variables</a></li>
<li class="chapter" data-level="1.2.3" data-path="intro-to-data.html"><a href="intro-to-data.html#variable-relations"><i class="fa fa-check"></i><b>1.2.3</b> Relationships between variables</a></li>
<li class="chapter" data-level="1.2.4" data-path="intro-to-data.html"><a href="intro-to-data.html#explanatory-and-response-variables"><i class="fa fa-check"></i><b>1.2.4</b> Explanatory and response variables</a></li>
<li class="chapter" data-level="1.2.5" data-path="intro-to-data.html"><a href="intro-to-data.html#introducing-observational-studies-and-experiments"><i class="fa fa-check"></i><b>1.2.5</b> Introducing observational studies and experiments</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="intro-to-data.html"><a href="intro-to-data.html#sampling-principles-strategies"><i class="fa fa-check"></i><b>1.3</b> Sampling principles and strategies</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="intro-to-data.html"><a href="intro-to-data.html#populations-and-samples"><i class="fa fa-check"></i><b>1.3.1</b> Populations and samples</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro-to-data.html"><a href="intro-to-data.html#anecdotal-evidence"><i class="fa fa-check"></i><b>1.3.2</b> Anecdotal evidence</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro-to-data.html"><a href="intro-to-data.html#sampling-from-a-population"><i class="fa fa-check"></i><b>1.3.3</b> Sampling from a population</a></li>
<li class="chapter" data-level="1.3.4" data-path="intro-to-data.html"><a href="intro-to-data.html#samp-methods"><i class="fa fa-check"></i><b>1.3.4</b> Four sampling methods (special topic)</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro-to-data.html"><a href="intro-to-data.html#observational-studies"><i class="fa fa-check"></i><b>1.4</b> Observational studies</a></li>
<li class="chapter" data-level="1.5" data-path="intro-to-data.html"><a href="intro-to-data.html#experiments"><i class="fa fa-check"></i><b>1.5</b> Experiments</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="intro-to-data.html"><a href="intro-to-data.html#principles-of-experimental-design"><i class="fa fa-check"></i><b>1.5.1</b> Principles of experimental design</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro-to-data.html"><a href="intro-to-data.html#reducing-bias-human-experiments"><i class="fa fa-check"></i><b>1.5.2</b> Reducing bias in human experiments</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="intro-to-data.html"><a href="intro-to-data.html#scope-of-inference"><i class="fa fa-check"></i><b>1.6</b> Scope of inference</a></li>
<li class="chapter" data-level="1.7" data-path="intro-to-data.html"><a href="intro-to-data.html#data-in-r"><i class="fa fa-check"></i><b>1.7</b> Data in <code>R</code></a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="intro-to-data.html"><a href="intro-to-data.html#dataframes-in-r"><i class="fa fa-check"></i><b>1.7.1</b> Dataframes in <code>R</code></a></li>
<li class="chapter" data-level="1.7.2" data-path="intro-to-data.html"><a href="intro-to-data.html#datastruc"><i class="fa fa-check"></i><b>1.7.2</b> Tidy structure of data</a></li>
<li class="chapter" data-level="1.7.3" data-path="intro-to-data.html"><a href="intro-to-data.html#using-the-pipe-to-chain"><i class="fa fa-check"></i><b>1.7.3</b> Using the pipe to chain</a></li>
<li class="chapter" data-level="1.7.4" data-path="intro-to-data.html"><a href="intro-to-data.html#interactive-r-tutorials"><i class="fa fa-check"></i><b>1.7.4</b> Interactive R tutorials</a></li>
<li class="chapter" data-level="1.7.5" data-path="intro-to-data.html"><a href="intro-to-data.html#r-labs"><i class="fa fa-check"></i><b>1.7.5</b> R labs</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="intro-to-data.html"><a href="intro-to-data.html#chp1-review"><i class="fa fa-check"></i><b>1.8</b> Chapter 1 review</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>1.8.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>2</b> Exploratory data analysis</a>
<ul>
<li class="chapter" data-level="2.1" data-path="eda.html"><a href="eda.html#categorical-data"><i class="fa fa-check"></i><b>2.1</b> Exploring categorical data</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="eda.html"><a href="eda.html#contingency-tables-and-conditional-proportions"><i class="fa fa-check"></i><b>2.1.1</b> Contingency tables and conditional proportions</a></li>
<li class="chapter" data-level="2.1.2" data-path="eda.html"><a href="eda.html#bar-plots-and-mosaic-plots"><i class="fa fa-check"></i><b>2.1.2</b> Bar plots and mosaic plots</a></li>
<li class="chapter" data-level="2.1.3" data-path="eda.html"><a href="eda.html#why-not-pie-charts"><i class="fa fa-check"></i><b>2.1.3</b> Why not pie charts?</a></li>
<li class="chapter" data-level="2.1.4" data-path="eda.html"><a href="eda.html#simpsons-paradox"><i class="fa fa-check"></i><b>2.1.4</b> Simpson’s paradox</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="eda.html"><a href="eda.html#probability-with-tables"><i class="fa fa-check"></i><b>2.2</b> Probability with tables</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="eda.html"><a href="eda.html#defining-probability"><i class="fa fa-check"></i><b>2.2.1</b> Defining probability</a></li>
<li class="chapter" data-level="2.2.2" data-path="eda.html"><a href="eda.html#finding-probabilities-with-tables"><i class="fa fa-check"></i><b>2.2.2</b> Finding probabilities with tables</a></li>
<li class="chapter" data-level="2.2.3" data-path="eda.html"><a href="eda.html#probability-notation"><i class="fa fa-check"></i><b>2.2.3</b> Probability notation</a></li>
<li class="chapter" data-level="2.2.4" data-path="eda.html"><a href="eda.html#diagnostic-testing"><i class="fa fa-check"></i><b>2.2.4</b> Diagnostic testing</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="eda.html"><a href="eda.html#quantitative-data"><i class="fa fa-check"></i><b>2.3</b> Exploring quantitative data</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="eda.html"><a href="eda.html#scatterplots"><i class="fa fa-check"></i><b>2.3.1</b> Scatterplots for paired data</a></li>
<li class="chapter" data-level="2.3.2" data-path="eda.html"><a href="eda.html#dotplots"><i class="fa fa-check"></i><b>2.3.2</b> Dot plots and the mean</a></li>
<li class="chapter" data-level="2.3.3" data-path="eda.html"><a href="eda.html#histograms"><i class="fa fa-check"></i><b>2.3.3</b> Histograms and shape</a></li>
<li class="chapter" data-level="2.3.4" data-path="eda.html"><a href="eda.html#variance-sd"><i class="fa fa-check"></i><b>2.3.4</b> Variance and standard deviation</a></li>
<li class="chapter" data-level="2.3.5" data-path="eda.html"><a href="eda.html#box-plots-quartiles-and-the-median"><i class="fa fa-check"></i><b>2.3.5</b> Box plots, quartiles, and the median</a></li>
<li class="chapter" data-level="2.3.6" data-path="eda.html"><a href="eda.html#describing-and-comparing-quantitative-distributions"><i class="fa fa-check"></i><b>2.3.6</b> Describing and comparing quantitative distributions</a></li>
<li class="chapter" data-level="2.3.7" data-path="eda.html"><a href="eda.html#robust-statistics"><i class="fa fa-check"></i><b>2.3.7</b> Robust statistics</a></li>
<li class="chapter" data-level="2.3.8" data-path="eda.html"><a href="eda.html#transforming-data-special-topic"><i class="fa fa-check"></i><b>2.3.8</b> Transforming data (special topic)</a></li>
<li class="chapter" data-level="2.3.9" data-path="eda.html"><a href="eda.html#mapping-data-special-topic"><i class="fa fa-check"></i><b>2.3.9</b> Mapping data (special topic)</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="eda.html"><a href="eda.html#r-exploratory-data-analysis"><i class="fa fa-check"></i><b>2.4</b> <code>R</code>: Exploratory data analysis</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="intro-to-data.html"><a href="intro-to-data.html#interactive-r-tutorials"><i class="fa fa-check"></i><b>2.4.1</b> Interactive R tutorials</a></li>
<li class="chapter" data-level="2.4.2" data-path="intro-to-data.html"><a href="intro-to-data.html#r-labs"><i class="fa fa-check"></i><b>2.4.2</b> R labs</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="eda.html"><a href="eda.html#chp2-review"><i class="fa fa-check"></i><b>2.5</b> Chapter 2 review</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="eda.html"><a href="eda.html#notation-summary"><i class="fa fa-check"></i><b>2.5.1</b> Notation summary</a></li>
<li class="chapter" data-level="2.5.2" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>2.5.2</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="cor-reg.html"><a href="cor-reg.html"><i class="fa fa-check"></i><b>3</b> Correlation and regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="cor-reg.html"><a href="cor-reg.html#fit-line-res-cor"><i class="fa fa-check"></i><b>3.1</b> Fitting a line, residuals, and correlation</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="cor-reg.html"><a href="cor-reg.html#fitting-a-line-to-data"><i class="fa fa-check"></i><b>3.1.1</b> Fitting a line to data</a></li>
<li class="chapter" data-level="3.1.2" data-path="cor-reg.html"><a href="cor-reg.html#using-linear-regression-to-predict-possum-head-lengths"><i class="fa fa-check"></i><b>3.1.2</b> Using linear regression to predict possum head lengths</a></li>
<li class="chapter" data-level="3.1.3" data-path="cor-reg.html"><a href="cor-reg.html#residuals"><i class="fa fa-check"></i><b>3.1.3</b> Residuals</a></li>
<li class="chapter" data-level="3.1.4" data-path="cor-reg.html"><a href="cor-reg.html#describing-linear-relationships-with-correlation"><i class="fa fa-check"></i><b>3.1.4</b> Describing linear relationships with correlation</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="cor-reg.html"><a href="cor-reg.html#least-squares-regression"><i class="fa fa-check"></i><b>3.2</b> Least squares regression</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="cor-reg.html"><a href="cor-reg.html#gift-aid-for-freshman-at-elmhurst-college"><i class="fa fa-check"></i><b>3.2.1</b> Gift aid for freshman at Elmhurst College</a></li>
<li class="chapter" data-level="3.2.2" data-path="cor-reg.html"><a href="cor-reg.html#an-objective-measure-for-finding-the-best-line"><i class="fa fa-check"></i><b>3.2.2</b> An objective measure for finding the best line</a></li>
<li class="chapter" data-level="3.2.3" data-path="cor-reg.html"><a href="cor-reg.html#finding-and-interpreting-the-least-squares-line"><i class="fa fa-check"></i><b>3.2.3</b> Finding and interpreting the least squares line</a></li>
<li class="chapter" data-level="3.2.4" data-path="cor-reg.html"><a href="cor-reg.html#extrapolation-is-treacherous"><i class="fa fa-check"></i><b>3.2.4</b> Extrapolation is treacherous</a></li>
<li class="chapter" data-level="3.2.5" data-path="cor-reg.html"><a href="cor-reg.html#describing-the-strength-of-a-fit"><i class="fa fa-check"></i><b>3.2.5</b> Describing the strength of a fit</a></li>
<li class="chapter" data-level="3.2.6" data-path="cor-reg.html"><a href="cor-reg.html#categprical-predictor-two-levels"><i class="fa fa-check"></i><b>3.2.6</b> Categorical predictors with two levels (special topic)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="cor-reg.html"><a href="cor-reg.html#outliers-in-regression"><i class="fa fa-check"></i><b>3.3</b> Outliers in linear regression</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="cor-reg.html"><a href="cor-reg.html#types-of-outliers"><i class="fa fa-check"></i><b>3.3.1</b> Types of outliers</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="cor-reg.html"><a href="cor-reg.html#r-correlation-and-regression"><i class="fa fa-check"></i><b>3.4</b> <code>R</code>: Correlation and regression</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="cor-reg.html"><a href="cor-reg.html#intro-linear-models-r-tutorial"><i class="fa fa-check"></i><b>3.4.1</b> Interactive R tutorials</a></li>
<li class="chapter" data-level="3.4.2" data-path="intro-to-data.html"><a href="intro-to-data.html#r-labs"><i class="fa fa-check"></i><b>3.4.2</b> R labs</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="cor-reg.html"><a href="cor-reg.html#chp3-review"><i class="fa fa-check"></i><b>3.5</b> Chapter review</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>3.5.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mult-reg.html"><a href="mult-reg.html"><i class="fa fa-check"></i><b>4</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="mult-reg.html"><a href="mult-reg.html#num-vs.-whatever---mlr"><i class="fa fa-check"></i><b>4.1</b> Num vs. whatever - MLR</a></li>
<li class="chapter" data-level="4.2" data-path="mult-reg.html"><a href="mult-reg.html#parallel-slopes"><i class="fa fa-check"></i><b>4.2</b> Parallel slopes</a></li>
<li class="chapter" data-level="4.3" data-path="mult-reg.html"><a href="mult-reg.html#hint-at-interaction-planes-and-parallel-planes-but-not-quantify"><i class="fa fa-check"></i><b>4.3</b> Hint at interaction, planes, and parallel planes but not quantify</a></li>
<li class="chapter" data-level="4.4" data-path="mult-reg.html"><a href="mult-reg.html#r-multiple-and-logistic-regression"><i class="fa fa-check"></i><b>4.4</b> <code>R</code>: Multiple and logistic regression</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="intro-to-data.html"><a href="intro-to-data.html#interactive-r-tutorials"><i class="fa fa-check"></i><b>4.4.1</b> Interactive R tutorials</a></li>
<li class="chapter" data-level="4.4.2" data-path="intro-to-data.html"><a href="intro-to-data.html#r-labs"><i class="fa fa-check"></i><b>4.4.2</b> R labs</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="mult-reg.html"><a href="mult-reg.html#chp4-review"><i class="fa fa-check"></i><b>4.5</b> Chapter 4 review</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>4.5.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inference-cat.html"><a href="inference-cat.html"><i class="fa fa-check"></i><b>5</b> Inference for categorical data</a>
<ul>
<li class="chapter" data-level="5.1" data-path="inference-cat.html"><a href="inference-cat.html#inf-foundations"><i class="fa fa-check"></i><b>5.1</b> Foundations of inference</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="inference-cat.html"><a href="inference-cat.html#Martian"><i class="fa fa-check"></i><b>5.1.1</b> Motivating example: Martian alphabet</a></li>
<li class="chapter" data-level="5.1.2" data-path="inference-cat.html"><a href="inference-cat.html#var-stat"><i class="fa fa-check"></i><b>5.1.2</b> Variability in a statistic</a></li>
<li class="chapter" data-level="5.1.3" data-path="inference-cat.html"><a href="inference-cat.html#HypothesisTesting"><i class="fa fa-check"></i><b>5.1.3</b> Hypothesis tests</a></li>
<li class="chapter" data-level="5.1.4" data-path="inference-cat.html"><a href="inference-cat.html#ConfidenceIntervals"><i class="fa fa-check"></i><b>5.1.4</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="inference-cat.html"><a href="inference-cat.html#normal"><i class="fa fa-check"></i><b>5.2</b> The normal distribution</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="inference-cat.html"><a href="inference-cat.html#normal-distribution-model"><i class="fa fa-check"></i><b>5.2.1</b> Normal distribution model</a></li>
<li class="chapter" data-level="5.2.2" data-path="inference-cat.html"><a href="inference-cat.html#standardizing-with-z-scores"><i class="fa fa-check"></i><b>5.2.2</b> Standardizing with Z-scores</a></li>
<li class="chapter" data-level="5.2.3" data-path="inference-cat.html"><a href="inference-cat.html#normal-probability-calculations-in-r"><i class="fa fa-check"></i><b>5.2.3</b> Normal probability calculations in <code>R</code></a></li>
<li class="chapter" data-level="5.2.4" data-path="inference-cat.html"><a href="inference-cat.html#normal-probability-examples"><i class="fa fa-check"></i><b>5.2.4</b> Normal probability examples</a></li>
<li class="chapter" data-level="5.2.5" data-path="inference-cat.html"><a href="inference-cat.html#rule"><i class="fa fa-check"></i><b>5.2.5</b> 68-95-99.7 rule</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inference-cat.html"><a href="inference-cat.html#single-prop"><i class="fa fa-check"></i><b>5.3</b> One proportion</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="inference-cat.html"><a href="inference-cat.html#one-prop-null-boot"><i class="fa fa-check"></i><b>5.3.1</b> Simulation-based test for <span class="math inline">\(H_0: \pi = \pi_0\)</span></a></li>
<li class="chapter" data-level="5.3.2" data-path="inference-cat.html"><a href="inference-cat.html#boot-ci-prop"><i class="fa fa-check"></i><b>5.3.2</b> Bootstrap confidence interval for <span class="math inline">\(\pi\)</span></a></li>
<li class="chapter" data-level="5.3.3" data-path="inference-cat.html"><a href="inference-cat.html#theory-prop"><i class="fa fa-check"></i><b>5.3.3</b> Theory-based methods for <span class="math inline">\(\pi\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="inference-cat.html"><a href="inference-cat.html#diff-two-prop"><i class="fa fa-check"></i><b>5.4</b> Difference of two proportions</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="inference-cat.html"><a href="inference-cat.html#two-prop-errors"><i class="fa fa-check"></i><b>5.4.1</b> Randomization test for <span class="math inline">\(H_0: \pi_1 - \pi_2 = 0\)</span></a></li>
<li class="chapter" data-level="5.4.2" data-path="inference-cat.html"><a href="inference-cat.html#two-prop-boot-ci"><i class="fa fa-check"></i><b>5.4.2</b> Bootstrap confidence interval for <span class="math inline">\(\pi_1 - \pi_2\)</span></a></li>
<li class="chapter" data-level="5.4.3" data-path="inference-cat.html"><a href="inference-cat.html#math-2prop"><i class="fa fa-check"></i><b>5.4.3</b> Theory-based methods for <span class="math inline">\(\pi_1 - \pi_2\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="inference-cat.html"><a href="inference-cat.html#power"><i class="fa fa-check"></i><b>5.5</b> Power, Errors, and Practical Importance</a></li>
<li class="chapter" data-level="5.6" data-path="inference-cat.html"><a href="inference-cat.html#summary-of-z-procedures"><i class="fa fa-check"></i><b>5.6</b> Summary of Z-procedures</a></li>
<li class="chapter" data-level="5.7" data-path="inference-cat.html"><a href="inference-cat.html#r-inference-for-categorical-data"><i class="fa fa-check"></i><b>5.7</b> <code>R</code>: Inference for categorical data</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="intro-to-data.html"><a href="intro-to-data.html#interactive-r-tutorials"><i class="fa fa-check"></i><b>5.7.1</b> Interactive R tutorials</a></li>
<li class="chapter" data-level="5.7.2" data-path="intro-to-data.html"><a href="intro-to-data.html#r-labs"><i class="fa fa-check"></i><b>5.7.2</b> R labs</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="inference-cat.html"><a href="inference-cat.html#chp5-review"><i class="fa fa-check"></i><b>5.8</b> Chapter 5 review</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>5.8.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inference-num.html"><a href="inference-num.html"><i class="fa fa-check"></i><b>6</b> Inference for quantitative data</a>
<ul>
<li class="chapter" data-level="6.1" data-path="inference-num.html"><a href="inference-num.html#one-mean"><i class="fa fa-check"></i><b>6.1</b> One mean</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="inference-num.html"><a href="inference-num.html#bootstrap-confidence-interval-for-mu"><i class="fa fa-check"></i><b>6.1.1</b> Bootstrap confidence interval for <span class="math inline">\(\mu\)</span></a></li>
<li class="chapter" data-level="6.1.2" data-path="inference-num.html"><a href="inference-num.html#one-mean-math"><i class="fa fa-check"></i><b>6.1.2</b> Theory-based inferential methods for <span class="math inline">\(\bar{x}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="inference-num.html"><a href="inference-num.html#paired-data"><i class="fa fa-check"></i><b>6.2</b> Paired mean difference</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="inference-num.html"><a href="inference-num.html#randomization-test-for-h_0-mu_d-0"><i class="fa fa-check"></i><b>6.2.1</b> Randomization test for <span class="math inline">\(H_0: \mu_d = 0\)</span></a></li>
<li class="chapter" data-level="6.2.2" data-path="inference-num.html"><a href="inference-num.html#bootstrap-confidence-interval-for-mu_d"><i class="fa fa-check"></i><b>6.2.2</b> Bootstrap confidence interval for <span class="math inline">\(\mu_d\)</span></a></li>
<li class="chapter" data-level="6.2.3" data-path="inference-num.html"><a href="inference-num.html#mathematical-model"><i class="fa fa-check"></i><b>6.2.3</b> Mathematical model</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="inference-num.html"><a href="inference-num.html#differenceOfTwoMeans"><i class="fa fa-check"></i><b>6.3</b> Difference of two means</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="inference-num.html"><a href="inference-num.html#rand2mean"><i class="fa fa-check"></i><b>6.3.1</b> Randomization test for <span class="math inline">\(H_0: \mu_1 - \mu_2 = 0\)</span></a></li>
<li class="chapter" data-level="6.3.2" data-path="inference-num.html"><a href="inference-num.html#bootstrap-confidence-interval-for-mu_1---mu_2"><i class="fa fa-check"></i><b>6.3.2</b> Bootstrap confidence interval for <span class="math inline">\(\mu_1 - \mu_2\)</span></a></li>
<li class="chapter" data-level="6.3.3" data-path="inference-num.html"><a href="inference-num.html#mathematical-model-1"><i class="fa fa-check"></i><b>6.3.3</b> Mathematical model</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="inference-num.html"><a href="inference-num.html#summary-of-t-procedures"><i class="fa fa-check"></i><b>6.4</b> Summary of t-procedures</a></li>
<li class="chapter" data-level="6.5" data-path="inference-num.html"><a href="inference-num.html#r-inference-for-quantitative-data"><i class="fa fa-check"></i><b>6.5</b> <code>R</code>: Inference for quantitative data</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="intro-to-data.html"><a href="intro-to-data.html#interactive-r-tutorials"><i class="fa fa-check"></i><b>6.5.1</b> Interactive R tutorials</a></li>
<li class="chapter" data-level="6.5.2" data-path="intro-to-data.html"><a href="intro-to-data.html#r-labs"><i class="fa fa-check"></i><b>6.5.2</b> R labs</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="inference-num.html"><a href="inference-num.html#chp6-review"><i class="fa fa-check"></i><b>6.6</b> Chapter 6 review</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>6.6.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="inference-reg.html"><a href="inference-reg.html"><i class="fa fa-check"></i><b>7</b> Inference for regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="inference-reg.html"><a href="inference-reg.html#r-inference-for-regression"><i class="fa fa-check"></i><b>7.1</b> <code>R</code>: Inference for regression</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="intro-to-data.html"><a href="intro-to-data.html#interactive-r-tutorials"><i class="fa fa-check"></i><b>7.1.1</b> Interactive R tutorials</a></li>
<li class="chapter" data-level="7.1.2" data-path="intro-to-data.html"><a href="intro-to-data.html#r-labs"><i class="fa fa-check"></i><b>7.1.2</b> R labs</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="inference-reg.html"><a href="inference-reg.html#chp7-review"><i class="fa fa-check"></i><b>7.2</b> Chapter 7 review</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>7.2.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="case-studies.html"><a href="case-studies.html"><i class="fa fa-check"></i><b>8</b> Appendix: Case studies</a></li>
<li class="chapter" data-level="9" data-path="activities.html"><a href="activities.html"><i class="fa fa-check"></i><b>9</b> Appendix: Activities</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Montana State Introductory Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inference-cat" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Inference for categorical data</h1>

<div class="chapterintro">
<p>Statistical inference is primarily concerned with understanding and quantifying the <em>uncertainty</em> of parameter estimates—that is, how variable is a sample statistic
from sample to sample?
While the equations and details change depending on the setting, the foundations for inference are the same throughout all of statistics. We will begin this chapter with a discussion of the foundations of inference, and introduce the two primary vehicles of inference: the <strong>hypothesis test</strong> and <strong>confidence interval</strong>.</p>
<p>The rest of this chapter focuses on statistical inference for categorical data. The two data structures we detail are:</p>
<ul>
<li>one binary variable, summarized using a single proportion, and</li>
<li>two binary variables, summarized using a difference (or ratio) of two proportions.</li>
</ul>
We will also introduce a new important mathematical model, the <strong>normal distribution</strong> (as the foundation for the <span class="math inline">\(z\)</span>-test).
</div>
<p>Throughout the book so far, you have worked with data in a variety of contexts.
You have learned how to summarize and visualize the data as well as how to model multiple variables at the same time.
Sometimes the data set at hand represents the entire research question.
But more often than not, the data have been collected to answer a research question about a larger group of which the data are a (hopefully) representative subset.</p>
<p>You may agree that there is almost always variability in data (one data set will not be identical to a second data set even if they are both collected from the same population using the same methods).
However, quantifying the variability in the data is neither obvious nor easy to do (<strong>how</strong> different is one data set from another?).</p>

<div class="example">
<p>Suppose your professor splits the students in class into two groups: students on the left and students on the right. If <span class="math inline">\(\hat{p}_{_L}\)</span> and <span class="math inline">\(\hat{p}_{_R}\)</span> represent the proportion of students who own an Apple product on the left and right, respectively, would you be surprised if <span class="math inline">\(\hat{p}_{_L}\)</span> did not <em>exactly</em> equal <span class="math inline">\(\hat{p}_{_R}\)</span>?</p>
<hr />
While the proportions would probably be close to each other, it would be unusual for them to be exactly the same. We would probably observe a small difference due to <em>chance</em>.
</div>

<div class="guidedpractice">
If we don’t think the side of the room a person sits on in class is related to whether the person owns an Apple product, what assumption are we making about the relationship between these two variables?
(Reminder: for these Guided Practice questions, you can check your answer in the footnote.)<a href="#fn58" class="footnote-ref" id="fnref58"><sup>58</sup></a>
</div>
<p>Studying randomness of this form is a key focus of statistics.
Throughout this chapter, and those that follow, we provide two different approaches for quantifying the variability inherent in data: simulation-based methods and theory-based methods (mathematical models).
Using the methods provided in this and future chapters, we will be able to draw conclusions beyond the data set at hand to research questions about larger populations.</p>
<div id="inf-foundations" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Foundations of inference</h2>
<p>Given results seen in a sample, the process of determining what we can <em>infer</em> to the
population based on sample results is called <strong>statistical inference</strong>. Statistical inferential methods enable us to understand and quantify the <em>uncertainty</em> of our sample results. Statistical inference helps us answer two questions about the population:</p>
<ol style="list-style-type: decimal">
<li>How strong is the <em>evidence</em> of an effect?</li>
<li>How <em>large</em> is the effect?</li>
</ol>
<p>The first question is answered through a <strong>hypothesis test</strong>, while the second is addressed with a <strong>confidence interval</strong>.</p>
<p>Statistical inference is the practice of making decisions and conclusions from data in the context of uncertainty.
Errors do occur, just like rare events, and the data set at hand might lead us to the wrong conclusion.
While a given data set may not always lead us to a correct conclusion, statistical inference gives us tools to control and evaluate how often these errors occur.</p>
<div id="Martian" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Motivating example: Martian alphabet</h3>
<p>How well can humans distinguish one “Martian” letter from another? The Figure <a href="inference-cat.html#fig:kiki-bumba">5.1</a>
displays two Martian letters—one is Kiki and the another is Bumba. Which do you think
is Kiki and which do you think is Bumba?<a href="#fn59" class="footnote-ref" id="fnref59"><sup>59</sup></a></p>
<div class="figure" style="text-align: center"><span id="fig:kiki-bumba"></span>
<img src="05/images/bumBa-KiKi.png" alt="Two Martian letters: Bumba and Kiki. Do you think the letter Bumba is on the left or the right?^[Bumba is the Martian letter on the left!]" width="75%" />
<p class="caption">
Figure 5.1: Two Martian letters: Bumba and Kiki. Do you think the letter Bumba is on the left or the right?<a href="#fn60" class="footnote-ref" id="fnref60"><sup>60</sup></a>
</p>
</div>
<p>This same image and question were presented to an introductory statistics class of
38 students. In that class, 34 students correctly identified Bumba as the Martian letter on the left. Assuming we can’t read Martian, is this result surprising?</p>
<p>One of two possibilities occurred:</p>
<ol style="list-style-type: decimal">
<li><em>We can’t read Martian, and these results just occurred by chance.</em></li>
<li><em>We can read Martian, and these results reflect this ability.</em></li>
</ol>
<p>To decide between these two possibilities, we could calculate the probability
of observing such results in a randomly selected sample of 38 students, under
the assumption that students were just guessing. If this probability is <em>very low</em>,
we’d have reason to reject the first possibility in favor of the second.
We can calculate this probability using one of two methods:</p>
<ul>
<li><strong>Simulation-based method</strong>: simulate lots of samples (Classes) of 38 students under the assumption that
students are just guessing, then calculate the proportion of these
simulated samples where we saw 34 or more students guessing correctly, or</li>
<li><strong>Theory-based method</strong>: develop a mathematical model for the sample proportion in this
scenario and use the model to calculate the probability.</li>
</ul>

<div class="guidedpractice">
How could you use a coin or cards to simulate the guesses of one sample of 38 students who cannot read Martian?<a href="#fn61" class="footnote-ref" id="fnref61"><sup>61</sup></a>
</div>
<p>For this situation—since “just guessing” means you have a 50% chance of guessing correctly—we could simulate a sample of 38 students’ guesses by flipping a coin 38 times and counting the number of times it lands on heads. Using a computer to repeat this process 1,000 times, we create the dot plot in Figure <a href="inference-cat.html#fig:MartianDotPlot">5.2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:MartianDotPlot"></span>
<img src="05-inference-cat_files/figure-html/MartianDotPlot-1.png" alt="A dot plot of 1,000 sample proportions; each calculated by flipping a coin 38 times and calculating the proportion of times the coin landed on heads. None of the 1,000 simulations had sample proportion of at least 89%, which was the proportion observed in the study." width="70%" />
<p class="caption">
Figure 5.2: A dot plot of 1,000 sample proportions; each calculated by flipping a coin 38 times and calculating the proportion of times the coin landed on heads. None of the 1,000 simulations had sample proportion of at least 89%, which was the proportion observed in the study.
</p>
</div>
<p>None of our simulated samples produce 34 of 38 correct guesses! That is, if students were just guessing, it is nearly impossible to observe 34 or more correct guesses in a sample of 38 students. Given this low probability, the more plausible possibility is 2. <em>We can read Martian, and these results reflect this ability.</em> We’ve just completed our first hypothesis test!</p>
<p>Now, obviously no one can read Martian, so a more realistic possibility is that humans tend to choose Bumba on the left more often than the right—there is a greater than 50% chance of choosing Bumba as the letter on the left. Even though we may think we’re guessing just by chance, we have a preference for Bumba on the left. It turns out that the explanation for this preference is called <em>synesthesia</em>, a tendency for humans to correlate sharp sounding noises (e.g., Kiki) with sharp looking images.<a href="#fn62" class="footnote-ref" id="fnref62"><sup>62</sup></a></p>
<p>But wait—we’re not done! We have evidence that humans tend to prefer Bumba on the left, but by how much? To answer this, we need a confidence interval—an interval of plausible values for the true probability humans will select Bumba as the left letter. The width of this interval is determined by how variable sample proportions are from sample to sample. It turns out, there is a mathematical model for this variability that we will explore later in this chapter. For now, let’s take the standard deviation from our simulated sample proportions as an estimate for this variability: 0.08. Since the simulated distribution of proportions is bell-shaped, we know about 95% of sample proportions should fall within two standard deviations of the true proportion, so we can add and subtract this <strong>margin of error</strong> to our sample proportion to calculate an approximate 95% confidence interval<a href="#fn63" class="footnote-ref" id="fnref63"><sup>63</sup></a>:
<span class="math display">\[
\frac{34}{38} \pm 2\times 0.08 = 0.89 \pm 0.16 = (0.73, 1)
\]</span>
Thus, based on this data, we are 95% confident that the probability a human guesses Bumba on the left is somewhere between 73% and 100%.</p>
</div>
<div id="var-stat" class="section level3" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Variability in a statistic</h3>
<p>There are two approaches to modeling how a statistic, such as a sample proportion, may vary from sample to sample.
In the <a href="inference-cat.html#Martian">Martian alphabet example</a>, we used a <strong>simulation-based approach</strong> to model
this variability, using the standard deviation of the simulated distribution of sample proportions as a quantitative measure of this sampling variability. Simulation-based methods include the randomization tests and bootstrapping methods we will use in this textbook. We can also use a <strong>theory-based approach</strong>—one which makes use of
mathematical modeling—and involves the normal and <span class="math inline">\(t\)</span> probability distributions.</p>
<p>All of the theory-based methods discussed in this book work (under certain conditions) because of a very important theorem in Statistics called the <strong>Central Limit Theorem</strong>.</p>

<div class="onebox">
<p><strong>Central Limit Theorem.</strong></p>
For large sample sizes, the <strong>sampling distribution</strong> of a sample proportion (or sample mean) will appear to follow a bell-shaped curve called the <em>normal distribution</em>.
</div>
<p>An example of a perfect normal distribution is shown in Figure <a href="inference-cat.html#fig:simpleNormal">5.3</a>.
While the mean (center) and standard deviation (variability) may change for different scenarios, the general shape remains roughly intact.</p>
<div class="figure" style="text-align: center"><span id="fig:simpleNormal"></span>
<img src="05-inference-cat_files/figure-html/simpleNormal-1.png" alt="A normal curve." width="70%" />
<p class="caption">
Figure 5.3: A normal curve.
</p>
</div>
<p>Recall from Chapter <a href="eda.html#eda">2</a> that a <em>distribution</em> of a variable is a description of the possible values it takes and how frequently each value occurs. In a <strong>sampling distribution</strong>, our “variable” is a sample statistic, and the sampling distribution is a description of the possible values a sample statistic takes and how frequently each value occurs when looking across many many possible samples. It is quite amazing that something like a sample proportion, summarizing a categorical variable, will have a bell-shaped sampling distribution if we sample large enough samples!</p>
<p>Theory-based methods also give us mathematical expressions for the
standard deviation of a sampling distribution. For instance,
if the true population proportion is <span class="math inline">\(\pi\)</span>, then the standard deviation
of the sampling distribution of sample proportions—how far away we would expect a sample proportion to be away from the population proportion—is
<span class="math display">\[
SD(\hat{p}) = \sqrt{\frac{\pi(1-\pi)}{n}}.
\]</span>
Typically, values of parameters such as <span class="math inline">\(\pi\)</span> are unknown, so we are unable to calculate these standard deviations. In this case, we substitute our “best guess” for <span class="math inline">\(\pi\)</span> in the formulas, either from a hypothesis or from a point estimate.</p>

<div class="onebox">
<p><strong>Standard error.</strong></p>
<p>The <strong>standard deviation</strong> of a sampling distribution for a statistic represents how far away we would expect the statistic to land from the parameter.</p>
Since the formulas for these standard deviations depend on unknown parameters, we substitute our “best guess” for <span class="math inline">\(\pi\)</span> in the formulas, either from a hypothesis or from a point estimate. The resulting <em>estimated</em> standard deviation is called the <strong>standard error</strong> of the statistic.
</div>
</div>
<div id="HypothesisTesting" class="section level3" number="5.1.3">
<h3><span class="header-section-number">5.1.3</span> Hypothesis tests</h3>
<p>In the <a href="inference-cat.html#Martian">Martian alphabet example</a>, we utilized a <strong>hypothesis test</strong>, which is a formal technique for evaluating two competing possibilities.
Each hypothesis test involves a <strong>null hypothesis</strong>, which represents either a skeptical perspective or a perspective of no difference or no effect, and an <strong>alternative hypothesis</strong>, which represents a new perspective such as the possibility that there has been a change or that there is a treatment effect in an experiment. The alternative hypothesis is usually the reason the scientists set out to do the research in the first place.</p>

<div class="onebox">
<p><strong>Null and alternative hypotheses.</strong></p>
<p>When we observe an effect in a sample, we would like to determine
if this observed effect represents
an actual effect in the population, or whether it was simply due to
chance.
We label these two competing claims, <span class="math inline">\(H_0\)</span> and <span class="math inline">\(H_A\)</span>,
which are spoken as “H-nought” and “H_A”</p>
The <strong>null hypothesis (<span class="math inline">\(H_0\)</span>)</strong> often represents either a skeptical perspective or a claim to be tested. The <strong>alternative hypothesis (<span class="math inline">\(H_A\)</span>)</strong> represents an alternative claim under consideration and is often represented by a range of possible values for the parameter of interest.
</div>

<div class="guidedpractice">
In the Martian alphabet example, which of the two competing possibilities was the null hypothesis? the alternative hypothesis?<a href="#fn64" class="footnote-ref" id="fnref64"><sup>64</sup></a>
</div>
<p>The hypothesis testing framework is a very general tool, and we often use it without a second thought.
If a person makes a somewhat unbelievable claim, we are initially skeptical.
However, if there is sufficient evidence that supports the claim, we set aside our skepticism.
The hallmarks of hypothesis testing are also found in the US court system.</p>
<div id="the-us-court-system" class="section level4 unnumbered">
<h4>The US court system</h4>

<div class="example">
<p>A US court considers two possible claims about a defendant: they are either innocent or guilty. If we set these claims up in a hypothesis framework, which would be the null hypothesis and which the alternative?</p>
<hr />
The jury considers whether the evidence is so convincing (strong) that there is no reasonable doubt regarding the person’s guilt.
That is, the skeptical perspective (null hypothesis) is that the person is innocent until evidence is presented that convinces the jury that the person is guilty (alternative hypothesis). Analogously, in a hypothesis test, we assume the null hypothesis until evidence is presented that convinces us the alternative hypothesis is true.
</div>
<p>Jurors examine the evidence to see whether it convincingly shows a defendant is guilty.
Notice that if a jury finds a defendant <em>not guilty</em>, this does not necessarily mean the jury is confident in the person’s innocence.
They are simply not convinced of the alternative that the person is guilty.</p>
<p>This is also the case with hypothesis testing: <em>even if we fail to reject the null hypothesis, we typically do not accept the null hypothesis as truth</em>.
Failing to find strong evidence for the alternative hypothesis is not equivalent to providing evidence that the null hypothesis is true.</p>
</div>
<div id="p-value" class="section level4 unnumbered">
<h4>p-value</h4>
<p>In the <a href="inference-cat.html#Martian">Martian alphabet example</a>, we performed a simulation-based hypothesis test of the hypotheses:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: The chance a human chooses Bumba on the left is 50%.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: Humans have a preference for choosing Bumba on the left.</p></li>
</ul>
<p>The research question—can humans read Martian?—was framed in the context of these hypotheses.</p>
<p>The null hypothesis (<span class="math inline">\(H_0\)</span>) was a perspective of no effect (no ability to read Martian).
The student data provided a point estimate of 89.5% (<span class="math inline">\(34/38 \times 100\)</span>%) for the true probability of choosing Bumba on the left.
We determined that observing such a sample proportion from chance alone (assuming <span class="math inline">\(H_0\)</span>) would be rare—it would only happen in less than 1 out of 1000 samples. When results
like these are inconsistent with <span class="math inline">\(H_0\)</span>, we reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_A\)</span>.
Here, we concluded that humans have a preference for choosing Bumba on the left.</p>
<p>The less than 1-in-1000 chance is what we call a <strong>p-value</strong>, which is a probability quantifying the strength of the evidence against the null hypothesis and in favor of the alternative.</p>

<div class="onebox">
<p><strong>p-value.</strong></p>
The <strong>p-value</strong> is the probability of observing data at least as favorable to the alternative hypothesis as our current data set, if the null hypothesis were true.
We typically use a summary statistic of the data, such as a proportion or difference in proportions, to help compute the p-value and evaluate the hypotheses.
This summary value that is used to compute the p-value is often called the <strong>test statistic</strong>.
</div>

<div class="protip">
<p>When interpreting a p-value, remember that the definition of a p-value has three components. It is a (1) probability. What it is the probability of? It is the probability of (2) our observed sample statistic or one more extreme. Assuming what? It is the probability of our observed sample statistic or one more extreme, (3) assuming the null hypothesis is true:</p>
<ol style="list-style-type: decimal">
<li>probability</li>
<li>data<a href="#fn65" class="footnote-ref" id="fnref65"><sup>65</sup></a></li>
<li>null hypothesis
</div></li>
</ol>

<div class="example">
<p>What was the test statistic in the <a href="inference-cat.html#Martian">Martian alphabet example</a>?</p>
<hr />
The test statistic in the the Martian alphabet example was the sample proportion, <span class="math inline">\(\frac{34}{38} = 0.895\)</span> (or 89.5%). This is also the <strong>point estimate</strong> of the true probability that humans would choose Bumba on the left.
</div>
<p>Since the p-value is a probability, its value will always be between 0 and 1. The closer the p-value is to 0, the stronger the evidence we have <em>against the null hypothesis</em>. Why? A small p-value means that our data are <em>unlikely</em> to occur, <em>if</em> the null hypothesis is true. We take that to mean that the null hypothesis isn’t a plausible assumption, and we reject it. This process mimics the scientific method—it is easier to disprove a theory than prove it. If scientists want to find evidence that a new drug reduces the risk of stroke, then they assume it <em>doesn’t</em> reduce the risk of stroke and then show that the observed data are so unlikely to occur that the more plausible explanation is that the drug works.</p>
Think of p-values as a continuum of strength of evidence against the null, from 0 (extremely strong evidence) to 1 (no evidence). Beyond around 10%, the data provide no evidence against the null hypothesis. Be careful not to equate this with evidence for the null hypothesis, which is incorrect. <em>The absence of evidence is not evidence of absence.</em>
<div class="figure" style="text-align: center"><span id="fig:pval-continuum"></span>
<img src="05/figures/soe_gradient.png" alt="Strength of evidence against the null for a continuum of p-values. Once the p-value is beyond around 0.10, the data provide no evidence against the null hypothesis." width="100%" />
<p class="caption">
Figure 5.4: Strength of evidence against the null for a continuum of p-values. Once the p-value is beyond around 0.10, the data provide no evidence against the null hypothesis.
</p>
</div>
<!-- You may use Table \@ref(tab:pvalue-continuum) as a general guide, but remember that there are no hard and fast cutoffs on this scale---the strength of evidence against the null with a p-value of 0.049 is the same as with a p-value of 0.051.  -->
<!-- ```{r pvalue-continuum} -->
<!-- pval_table <- tribble( -->
<!--   ~variable,    ~col1,  -->
<!-- "p-value < 0.01",  "very strong", -->
<!-- "0.01 < p-value < 0.05", "strong", -->
<!-- "0.05 < p-value < 0.10", "moderate", -->
<!-- "p-value > 0.10", "little to no evidence", -->
<!-- ) -->
<!-- pval_table %>% -->
<!--  kable(caption = "The p-value as a continuum of strength of evidence against the null--a general guide.",  -->
<!--     col.names = c("p-value range", "Strength of evidence against $H_0$")) %>% -->
<!--  kable_styling() -->
<!-- ``` -->
<p>Regardless of the data structure or analysis method, the hypothesis testing framework always follows the same steps—only the details for how we model randomness in the data change.</p>

<div class="onebox">
<p><strong>General steps of a hypothesis test.</strong> Every hypothesis test follows these same general steps:</p>
<ol style="list-style-type: decimal">
<li>Frame the research question in terms of hypotheses.</li>
<li>Collect and summarize data using a test statistic.</li>
<li>Assume the null hypothesis is true, and simulate or mathematically model a null distribution for the test statistic.</li>
<li>Compare the observed test statistic to the null distribution to calculate a p-value.</li>
<li>Make a conclusion based on the p-value, and write a conclusion in context, in plain language, and in terms of the alternative hypothesis.
</div></li>
</ol>
</div>
<div id="decisions-and-statistical-significance" class="section level4 unnumbered">
<h4>Decisions and statistical significance</h4>
<p>In some cases, a <strong>decision</strong> to the hypothesis test is needed, with the two possible decisions as follows:</p>
<ul>
<li>Reject the null hypothesis</li>
<li>Fail to reject the null hypothesis</li>
</ul>

<div class="guidedpractice">
For which values of the p-value should you “reject” a null hypothesis? “fail to reject” a null hypothesis?<a href="#fn66" class="footnote-ref" id="fnref66"><sup>66</sup></a>
</div>
<p>In order to decide between these two options, we need a previously set threshold for our p-value: when the p-value is less than a previously set threshold, we reject <span class="math inline">\(H_0\)</span>; otherwise, we fail to reject <span class="math inline">\(H_0\)</span>. This threshold is called the <strong>significance level</strong>, and when the p-value is less than the significance level, we say the results are <strong>statistically significant</strong>. This means the data provide such strong evidence against <span class="math inline">\(H_0\)</span> that we reject the null hypothesis in favor of the alternative hypothesis.
The significance level, often represented by <span class="math inline">\(\alpha\)</span> (the Greek letter <em>alpha</em>), is typically set to <span class="math inline">\(\alpha = 0.05\)</span>, but can vary depending on the field or the application and the real-life consequences of an incorrect decision.
Using a significance level of <span class="math inline">\(\alpha = 0.05\)</span> in the Martian alphabet study, we can say that the data provided statistically significant evidence against the null hypothesis.</p>

<div class="onebox">
<p><strong>Statistical significance.</strong></p>
We say that the data provide <strong>statistically significant</strong> evidence against the null hypothesis if the p-value is less than some reference value called the <strong>significance level</strong>, denoted by <span class="math inline">\(\alpha\)</span>.
</div>

<div class="onebox">
<p><strong>What’s so special about 0.05?</strong></p>
We often use a threshold of 0.05 to determine whether a result is statistically significant.
But why 0.05?
Maybe we should use a bigger number, or maybe a smaller number.
If you’re a little puzzled, that probably means you’re reading with a critical eye—good job!
The <em>OpenIntro</em> authors have a video to help clarify <em>why 0.05</em>:
<center>
<a href="https://www.openintro.org/book/stat/why05/">https://www.openintro.org/book/stat/why05/</a>
</center>
<br>
Sometimes it’s also a good idea to deviate from the standard.
We’ll discuss when to choose a threshold different than 0.05 in Section <a href="inference-cat.html#two-prop-errors">5.4.1</a>.
</div>
<p>Statistical significance has been a hot topic in the news, related to the “reproducibility crisis” in some scientific fields. We encourage you to read more about the debate on the use of p-values and statistical significance. A good place to start would be the <em>Nature</em> article, “<a href="https://www.nature.com/articles/d41586-019-00857-9">Scientists rise up against statistical significance</a>,” from March 20, 2019.</p>
</div>
</div>
<div id="ConfidenceIntervals" class="section level3" number="5.1.4">
<h3><span class="header-section-number">5.1.4</span> Confidence intervals</h3>
<p>A point estimate provides a single plausible value for a parameter.
However, a point estimate is rarely perfect—usually there is some error in the estimate.
In addition to supplying a point estimate of a parameter, a next logical step would be to provide a plausible <em>range of values</em> for the parameter.</p>
<p>A plausible range of values for the population parameter is called a <strong>confidence interval</strong>.
Using only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net.
We can throw a spear where we saw a fish, but we will probably miss.
On the other hand, if we toss a net in that area, we have a good chance of catching the fish.</p>
<p>If we report a point estimate, we probably will not hit the exact population parameter.
On the other hand, if we report a range of plausible values—a confidence interval—we have a good shot at capturing the parameter.</p>
<p>This reasoning also explains why we can never prove a null hypothesis. Sample statistics will vary from sample to sample. While we can quantify this uncertainty (e.g., we are 95% sure the statistic is within 0.15 of the parameter), we can never be certain that the parameter is an exact value. For example, suppose you want to test whether a coin is a fair coin, i.e., <span class="math inline">\(H_0: \pi = 0.50\)</span> versus <span class="math inline">\(H_0: \pi \neq 0.50\)</span>, so you toss the coin 10 times to collect data. In those 10 tosses, 6 land on heads and 4 land on tails, resulting in a p-value of 0.754<a href="#fn67" class="footnote-ref" id="fnref67"><sup>67</sup></a>. We don’t have enough evidence to show that the coin is biased, but surely we wouldn’t say we just proved the coin is fair!</p>

<div class="importantbox">
There are only <em>two</em> possible decisions in a hypothesis test: (1) reject <span class="math inline">\(H_0\)</span>, or (2) fail to reject <span class="math inline">\(H_0\)</span>. Since one can never prove a null hypothesis—we can only disprove<a href="#fn68" class="footnote-ref" id="fnref68"><sup>68</sup></a> it—we never have the ability to “accept the null.” You may have seen this phrase in other textbooks or articles, but it is incorrect.
</div>

<div class="guidedpractice">
If we want to be very certain we capture the population parameter, should we use a wider interval or a smaller interval?<a href="#fn69" class="footnote-ref" id="fnref69"><sup>69</sup></a>
</div>
<p>We will explore both simulation-based methods (bootstrapping) and theory-based methods for creating confidence intervals in this text. Though the details change with different scenarios, theory-based confidence intervals will always take the form:
<span class="math display">\[
\mbox{statistic} \pm (\mbox{multiplier}) \times (\mbox{standard error of the statistic})
\]</span>
The statistic is our best guess for the value of the parameter, so it makes sense to build the confidence interval around that value. The standard error, which is a measure of the uncertainty associated with the statistic, provides a guide for how large we should make the confidence interval. The multiplier is determined by how confident we’d like to be, and tells us how many standard errors we need to add and subtract from the statistic. The amount we add and subtract from the statistic is called the <strong>margin of error</strong>.</p>

<div class="onebox">
<p><strong>General form of a confidence interval.</strong></p>
The general form of a <strong>theory-based confidence interval</strong> for an unknown parameter is
<span class="math display">\[
\mbox{statistic} \pm (\mbox{multiplier}) \times (\mbox{standard error of the statistic})
\]</span>
The amount we add and subtract to the statistic to calculate the confidence interval is called the <strong>margin of error</strong>.
<span class="math display">\[
\mbox{margin of error} = (\mbox{multiplier}) \times (\mbox{standard error of the statistic})
\]</span>
</div>
<p>In Section <a href="inference-cat.html#theory-prop">5.3.3</a> we will discuss different percentages for the confidence interval (e.g., 90% confidence interval or 99% confidence interval). Section <a href="inference-cat.html#two-prop-boot-ci">5.4.2</a> provides a longer discussion on what “95% confidence” actually means.</p>
</div>
</div>
<div id="normal" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> The normal distribution</h2>

<div class="todo">
Update examples in this section to use R rather than table.
</div>
<p></p>
<p>Among all the distributions we see in statistics, one is overwhelmingly the most common.
The symmetric, unimodal, bell curve is ubiquitous throughout statistics.
It is so common that people know it as a variety of names including the <strong>normal curve</strong>, <strong>normal model</strong>, or <strong>normal distribution</strong>.<a href="#fn70" class="footnote-ref" id="fnref70"><sup>70</sup></a>
Under certain conditions, sample proportions, sample means, and sample differences can be modeled using the normal distribution—the basis for our theory-based inference methods.
Additionally, some variables such as SAT scores and heights of US adult males closely follow the normal distribution.</p>

<div class="onebox">
<p><strong>Normal distribution facts.</strong></p>
Many summary statistics and variables are nearly normal, but none are exactly normal.
Thus the normal distribution, while not perfect for any single problem, is very useful for a variety of problems.
We will use it in data exploration and to solve important problems in statistics.
</div>
<p>In this section, we will discuss the normal distribution in the context of data to become more familiar with normal distribution techniques.</p>
<div id="normal-distribution-model" class="section level3" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Normal distribution model</h3>
<p>The normal distribution always describes a symmetric, unimodal, bell-shaped curve.
However, normal curves can look different depending on the details of the model.
Specifically, the normal model can be adjusted using two parameters: mean and standard deviation.
As you can probably guess, changing the mean shifts the bell curve to the left or right, while changing the standard deviation stretches or constricts the curve.
Figure <a href="inference-cat.html#fig:twoSampleNormals">5.5</a> shows the normal distribution with mean <span class="math inline">\(0\)</span> and standard deviation <span class="math inline">\(1\)</span> (which is commonly referred to as the <strong>standard normal distribution</strong>) on top.
A normal distribution with mean <span class="math inline">\(19\)</span> and standard deviation <span class="math inline">\(4\)</span> is shown on the bottom. Figure <a href="inference-cat.html#fig:twoSampleNormalsStacked">5.6</a> shows the same two normal distributions on the same axis.</p>
<div class="figure" style="text-align: center"><span id="fig:twoSampleNormals"></span>
<img src="05-inference-cat_files/figure-html/twoSampleNormals-1.png" alt="Both curves represent the normal distribution, however, they differ in their center and spread. The normal distribution with mean 0 and standard deviation 1 is called the **standard normal distribution**." width="70%" /><img src="05-inference-cat_files/figure-html/twoSampleNormals-2.png" alt="Both curves represent the normal distribution, however, they differ in their center and spread. The normal distribution with mean 0 and standard deviation 1 is called the **standard normal distribution**." width="70%" />
<p class="caption">
Figure 5.5: Both curves represent the normal distribution, however, they differ in their center and spread. The normal distribution with mean 0 and standard deviation 1 is called the <strong>standard normal distribution</strong>.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:twoSampleNormalsStacked"></span>
<img src="05-inference-cat_files/figure-html/twoSampleNormalsStacked-1.png" alt="The two normal models shown above and now plotted together on the same scale." width="70%" />
<p class="caption">
Figure 5.6: The two normal models shown above and now plotted together on the same scale.
</p>
</div>
<p>If a normal distribution has mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>, we may write the distribution as <span class="math inline">\(N(\mu, \sigma)\)</span>. The two distributions in Figure <a href="inference-cat.html#fig:twoSampleNormalsStacked">5.6</a> can be written as
<span class="math display">\[\begin{align*}
N(\mu=0,\sigma=1)\quad\text{and}\quad N(\mu=19,\sigma=4)
\end{align*}\]</span>
Because the mean and standard deviation describe a normal distribution exactly, they are called the distribution’s <strong>parameters</strong>.</p>

<div class="guidedpractice">
Write down the short-hand for a normal distribution with (a) mean 5 and standard deviation 3, (b) mean -100 and standard deviation 10, and (c) mean 2 and standard deviation 9.<a href="#fn71" class="footnote-ref" id="fnref71"><sup>71</sup></a>
</div>
</div>
<div id="standardizing-with-z-scores" class="section level3" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> Standardizing with Z-scores</h3>

<div class="guidedpractice">
Table <a href="inference-cat.html#tab:satACTstats">5.1</a> shows the mean and standard deviation for total scores on the SAT and ACT. The distribution of SAT and ACT scores are both nearly normal. Suppose Ann scored 1800 on her SAT and Tom scored 24 on his ACT. Who performed better?<a href="#fn72" class="footnote-ref" id="fnref72"><sup>72</sup></a>
</div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:satACTstats">Table 5.1: </span>Mean and standard deviation for the SAT and ACT.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
SAT
</th>
<th style="text-align:left;">
ACT
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Mean
</td>
<td style="text-align:left;">
1500
</td>
<td style="text-align:left;">
21
</td>
</tr>
<tr>
<td style="text-align:left;">
SD
</td>
<td style="text-align:left;">
300
</td>
<td style="text-align:left;">
5
</td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span id="fig:satActNormals"></span>
<img src="05-inference-cat_files/figure-html/satActNormals-1.png" alt="Ann's and Tom's scores shown with the distributions of SAT and ACT scores." width="70%" />
<p class="caption">
Figure 5.7: Ann’s and Tom’s scores shown with the distributions of SAT and ACT scores.
</p>
</div>
<p>The solution to the previous example relies on a standardization technique called a Z-score, a method most commonly employed for nearly normal observations (but that may be used with any distribution). The <strong>Z-score</strong> of an observation is defined as the number of standard deviations it falls above or below the mean.
If the observation is one standard deviation above the mean, its Z-score is 1. If it is 1.5 standard deviations <em>below</em> the mean, then its Z-score is -1.5.
If <span class="math inline">\(x\)</span> is an observation from a distribution <span class="math inline">\(N(\mu, \sigma)\)</span>, we define the Z-score mathematically as</p>
<p><span class="math display">\[\begin{eqnarray*}
Z = \frac{x-\mu}{\sigma}
\end{eqnarray*}\]</span>
Using <span class="math inline">\(\mu_{SAT}=1500\)</span>, <span class="math inline">\(\sigma_{SAT}=300\)</span>, and <span class="math inline">\(x_{Ann}=1800\)</span>, we find Ann’s Z-score:
<span class="math display">\[\begin{eqnarray*}
Z_{Ann} = \frac{x_{Ann} - \mu_{SAT}}{\sigma_{SAT}} = \frac{1800-1500}{300} = 1
\end{eqnarray*}\]</span></p>

<div class="onebox">
<p><strong>The Z-score.</strong></p>
The Z-score of an observation is the number of standard deviations it falls above or below the mean.
We compute the Z-score for an observation <span class="math inline">\(x\)</span> that follows a distribution with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> by first subtracting its mean, then dividing by its standard deviation:
<span class="math display">\[\begin{eqnarray*}
Z = \frac{x-\mu}{\sigma}
\end{eqnarray*}\]</span>
</div>

<div class="guidedpractice">
Use Tom’s ACT score, 24, along with the ACT mean and standard deviation to compute his Z-score.<a href="#fn73" class="footnote-ref" id="fnref73"><sup>73</sup></a>
</div>
<p>Observations above the mean always have positive Z-scores while those below the mean have negative Z-scores.
If an observation is equal to the mean (e.g., SAT score of 1500), then the Z-score is <span class="math inline">\(0\)</span>.</p>

<div class="guidedpractice">
Let <span class="math inline">\(X\)</span> represent a random variable from <span class="math inline">\(N(\mu=3, \sigma=2)\)</span>, and suppose we observe <span class="math inline">\(x=5.19\)</span>. (a) Find the Z-score of <span class="math inline">\(x\)</span>. (b) Use the Z-score to determine how many standard deviations above or below the mean <span class="math inline">\(x\)</span> falls.<a href="#fn74" class="footnote-ref" id="fnref74"><sup>74</sup></a>
</div>

<div class="guidedpractice">
Head lengths of brushtail possums follow a nearly normal distribution with mean 92.6 mm and standard deviation 3.6 mm. Compute the Z-scores for possums with head lengths of 95.4 mm and 85.8 mm.<a href="#fn75" class="footnote-ref" id="fnref75"><sup>75</sup></a>
</div>
<p>We can use Z-scores to roughly identify which observations are more unusual than others.
One observation <span class="math inline">\(x_1\)</span> is said to be more unusual than another observation <span class="math inline">\(x_2\)</span> if the absolute value of its Z-score is larger than the absolute value of the other observation’s Z-score: <span class="math inline">\(|Z_1| &gt; |Z_2|\)</span>.
This technique is especially insightful when a distribution is symmetric.</p>

<div class="guidedpractice">
Which of the two brushtail possum observations in the previous guided practice is more <em>unusual</em>?<a href="#fn76" class="footnote-ref" id="fnref76"><sup>76</sup></a>
</div>
</div>
<div id="normal-probability-calculations-in-r" class="section level3" number="5.2.3">
<h3><span class="header-section-number">5.2.3</span> Normal probability calculations in <code>R</code></h3>

<div class="example">
<p>Ann from the SAT Guided Practice earned a score of 1800 on her SAT with a corresponding <span class="math inline">\(Z=1\)</span>. She would like to know what percentile she falls in among all SAT test-takers.</p>
<hr />
Ann’s <strong>percentile</strong> is the percentage of people who earned a lower SAT score than Ann. We shade the area representing those individuals in Figure <a href="inference-cat.html#fig:satBelow1800">5.8</a>. The total area under the normal curve is always equal to 1, and the proportion of people who scored below Ann on the SAT is equal to the <em>area</em> shaded in Figure <a href="inference-cat.html#fig:satBelow1800">5.8</a>: 0.8413. In other words, Ann is in the <span class="math inline">\(84^{th}\)</span> percentile of SAT takers.
</div>
<div class="figure" style="text-align: center"><span id="fig:satBelow1800"></span>
<img src="05-inference-cat_files/figure-html/satBelow1800-1.png" alt="The normal model for SAT scores, shading the area of those individuals who scored below Ann." width="70%" />
<p class="caption">
Figure 5.8: The normal model for SAT scores, shading the area of those individuals who scored below Ann.
</p>
</div>
<p>We can use the normal model to find percentiles or probabilities. In <code>R</code>, the function
to calculate normal probabilities is <code>pnorm()</code>. The <code>normTail()</code> function is available in the <code>openintro</code> R package and will draw the associated curve if it is helpful. In the code below, we find the percentile of <span class="math inline">\(Z=0.43\)</span> is 0.6664, or the <span class="math inline">\(66.64^{th}\)</span> percentile.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="inference-cat.html#cb15-1" aria-hidden="true"></a><span class="kw">pnorm</span>(<span class="fl">0.43</span>, <span class="dt">m =</span> <span class="dv">0</span>, <span class="dt">s =</span> <span class="dv">1</span>)</span>
<span id="cb15-2"><a href="inference-cat.html#cb15-2" aria-hidden="true"></a><span class="co">#&gt; [1] 0.666</span></span>
<span id="cb15-3"><a href="inference-cat.html#cb15-3" aria-hidden="true"></a>openintro<span class="op">::</span><span class="kw">normTail</span>(<span class="fl">0.43</span>, <span class="dt">m =</span> <span class="dv">0</span>, <span class="dt">s =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="05-inference-cat_files/figure-html/unnamed-chunk-40-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We can also find the Z-score associated with a percentile.
For example, to identify Z for the <span class="math inline">\(80^{th}\)</span> percentile, we use <code>qnorm()</code> which identifies the <strong>quantile</strong> for a given percentage. The quantile represents the cutoff value. (To remember the function <code>qnorm()</code> as providing a cutoff, notice that both <code>qnorm()</code> and “cutoff” start with the sound “kuh”. To remember the <code>pnorm()</code> function as providing a probability from a given cutoff, notice that both <code>pnorm()</code> and probability start with the sound “puh”.)
We determine the Z-score for the <span class="math inline">\(80^{th}\)</span> percentile using <code>qnorm()</code>: 0.84.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="inference-cat.html#cb16-1" aria-hidden="true"></a><span class="kw">qnorm</span>(<span class="fl">0.80</span>, <span class="dt">m =</span> <span class="dv">0</span>, <span class="dt">s =</span> <span class="dv">1</span>)</span>
<span id="cb16-2"><a href="inference-cat.html#cb16-2" aria-hidden="true"></a><span class="co">#&gt; [1] 0.842</span></span>
<span id="cb16-3"><a href="inference-cat.html#cb16-3" aria-hidden="true"></a>openintro<span class="op">::</span><span class="kw">normTail</span>(<span class="fl">0.80</span>, <span class="dt">m =</span> <span class="dv">0</span>, <span class="dt">s =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="05-inference-cat_files/figure-html/unnamed-chunk-41-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We can use these functions with other normal distributions than the standard normal distribution by specifying the mean as the argument for <code>m</code> and the standard deviation as the argument for <code>s</code>. Here we determine the proportion of ACT test takers who scored worse than Tom on the ACT: 0.73.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="inference-cat.html#cb17-1" aria-hidden="true"></a><span class="kw">pnorm</span>(<span class="dv">24</span>, <span class="dt">m =</span> <span class="dv">21</span>, <span class="dt">s =</span> <span class="dv">5</span>)</span>
<span id="cb17-2"><a href="inference-cat.html#cb17-2" aria-hidden="true"></a><span class="co">#&gt; [1] 0.726</span></span>
<span id="cb17-3"><a href="inference-cat.html#cb17-3" aria-hidden="true"></a>openintro<span class="op">::</span><span class="kw">normTail</span>(<span class="dv">24</span>, <span class="dt">m =</span> <span class="dv">21</span>, <span class="dt">s =</span> <span class="dv">5</span>)</span></code></pre></div>
<p><img src="05-inference-cat_files/figure-html/unnamed-chunk-42-1.png" width="70%" style="display: block; margin: auto;" /></p>

<div class="guidedpractice">
Determine the proportion of SAT test takers who scored better than Ann on the SAT.<a href="#fn77" class="footnote-ref" id="fnref77"><sup>77</sup></a>
</div>
</div>
<div id="normal-probability-examples" class="section level3" number="5.2.4">
<h3><span class="header-section-number">5.2.4</span> Normal probability examples</h3>
<p>Cumulative SAT scores are approximated well by a normal model, <span class="math inline">\(N(\mu=1500, \sigma=300)\)</span>.</p>

<div class="example">
<p>Shannon is a randomly selected SAT taker, and nothing is known about Shannon’s SAT aptitude. What is the probability that Shannon scores at least 1630 on her SATs?</p>
<hr />
<p>First, always draw and label a picture of the normal distribution. (Drawings need not be exact to be useful.) We are interested in the chance she scores above 1630, so we shade the upper tail. See the normal curve below.</p>
<p>The picture shows the mean and the values at 2 standard deviations above and below the mean. The simplest way to find the shaded area under the curve makes use of the Z-score of the cutoff value. With <span class="math inline">\(\mu=1500\)</span>, <span class="math inline">\(\sigma=300\)</span>, and the cutoff value <span class="math inline">\(x=1630\)</span>, the Z-score is computed as
<span class="math display">\[\begin{eqnarray*}
Z = \frac{x - \mu}{\sigma} = \frac{1630 - 1500}{300} = \frac{130}{300} = 0.43
\end{eqnarray*}\]</span>
We use software to find the percentile of <span class="math inline">\(Z=0.43\)</span>, which yields 0.6664. However, the percentile describes those who had a Z-score <em>lower</em> than 0.43. To find the area <em>above</em> <span class="math inline">\(Z=0.43\)</span>, we compute one minus the area of the lower tail, as seen below.</p>
The probability Shannon scores at least 1630 on the SAT is 0.3336.
</div>
<p><img src="05-inference-cat_files/figure-html/satAbove1630-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p><img src="05-inference-cat_files/figure-html/subtractingArea-1.png" width="70%" style="display: block; margin: auto;" /></p>

<div class="protip">
<p><strong>Always draw a picture first, and find the Z-score second.</strong></p>
<p>For any normal probability situation, <em>always always always</em> draw and label the normal curve and shade the area of interest first.
The picture will provide an estimate of the probability.</p>
After drawing a figure to represent the situation, identify the Z-score for the observation of interest.
</div>

<div class="guidedpractice">
If the probability of Shannon scoring at least 1630 is 0.3336, then what is the probability she scores less than 1630? Draw the normal curve representing this exercise, shading the lower region instead of the upper one.<a href="#fn78" class="footnote-ref" id="fnref78"><sup>78</sup></a>
</div>

<div class="example">
<p>Edward earned a 1400 on his SAT. What is his percentile?</p>
<hr />
<p>First, a picture is needed. Edward’s percentile is the proportion of people who do not get as high as a 1400. These are the scores to the left of 1400.</p>
Identifying the mean <span class="math inline">\(\mu=1500\)</span>, the standard deviation <span class="math inline">\(\sigma=300\)</span>, and the cutoff for the tail area <span class="math inline">\(x=1400\)</span> makes it easy to compute the Z-score:
<span class="math display">\[\begin{eqnarray*}
Z = \frac{x - \mu}{\sigma} = \frac{1400 - 1500}{300} = -0.3333
\end{eqnarray*}\]</span>
Using the <code>pnorm()</code> function (either <code>pnorm(-1/3)</code> or <code>pnorm(1400, m=1500, s=300)</code> will give the desired result), the desired probability is <span class="math inline">\(0.3694\)</span>. Edward is at the <span class="math inline">\(37^{th}\)</span> percentile.
</div>
<p><img src="05-inference-cat_files/figure-html/satBelow1400-1.png" width="70%" style="display: block; margin: auto;" /></p>

<div class="guidedpractice">
Use the results of the previous example to compute the proportion of SAT takers who did better than Edward. Also draw a new picture.<a href="#fn79" class="footnote-ref" id="fnref79"><sup>79</sup></a>
</div>

<div class="protip">
<p><strong>Areas to the right.</strong></p>
The <code>pnorm()</code> function (and the normal probability table in most books) gives the area to the left. If you would like the area to the right, first find the area to the left and then subtract this amount from one. In <code>R</code>, you can also do this by setting the <code>lower.tail</code> argument to <code>FALSE</code>.
</div>

<div class="guidedpractice">
Stuart earned an SAT score of 2100. Draw a picture for each part. (a) What is his percentile? (b) What percent of SAT takers did better than Stuart?<a href="#fn80" class="footnote-ref" id="fnref80"><sup>80</sup></a>
</div>
<p>Based on a sample of 100 men,<a href="#fn81" class="footnote-ref" id="fnref81"><sup>81</sup></a> the heights of male adults between the ages 20 and 62 in the US is nearly normal with mean 70.0’’ and standard deviation 3.3’’.</p>

<div class="guidedpractice">
Mike is 5’7’’ and Jim is 6’4’’. (a) What is Mike’s height percentile? (b) What is Jim’s height percentile? Also draw one picture for each part.<a href="#fn82" class="footnote-ref" id="fnref82"><sup>82</sup></a>
</div>
<p>The last several problems have focused on finding the probability or percentile for a particular observation.
What if you would like to know the observation corresponding to a particular percentile?</p>

<div class="example">
<p>Erik’s height is at the <span class="math inline">\(40^{th}\)</span> percentile. How tall is he?</p>
<hr />
<p>As always, first draw the picture (see below).</p>
<p>In this case, the lower tail probability is known (0.40), which can be shaded on the diagram. We want to find the observation that corresponds to this value. As a first step in this direction, we determine the Z-score associated with the <span class="math inline">\(40^{th}\)</span> percentile.</p>
<p>Because the percentile is below 50%, we know <span class="math inline">\(Z\)</span> will be negative. Looking in the negative part of the normal probability table, we search for the probability <em>inside</em> the table closest to 0.4000. We find that 0.4000 falls in row <span class="math inline">\(-0.2\)</span> and between columns <span class="math inline">\(0.05\)</span> and <span class="math inline">\(0.06\)</span>. Since it falls closer to <span class="math inline">\(0.05\)</span>, we take this one: <span class="math inline">\(Z=-0.25\)</span>.</p>
Knowing <span class="math inline">\(Z_{Erik}=-0.25\)</span> and the population parameters <span class="math inline">\(\mu=70\)</span> and <span class="math inline">\(\sigma=3.3\)</span> inches, the Z-score formula can be set up to determine Erik’s unknown height, labeled <span class="math inline">\(x_{Erik}\)</span>:
<span class="math display">\[\begin{eqnarray*}
-0.25 = Z_{Erik} = \frac{x_{Erik} - \mu}{\sigma} = \frac{x_{Erik} - 70}{3.3}
\end{eqnarray*}\]</span>
Solving for <span class="math inline">\(x_{Erik}\)</span> yields the height 69.18 inches. That is, Erik is about 5’9’’ (this is notation for 5-feet, 9-inches).
</div>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="inference-cat.html#cb18-1" aria-hidden="true"></a><span class="kw">qnorm</span>(<span class="fl">0.4</span>, <span class="dt">m =</span> <span class="dv">0</span>, <span class="dt">s =</span> <span class="dv">1</span>)</span>
<span id="cb18-2"><a href="inference-cat.html#cb18-2" aria-hidden="true"></a><span class="co">#&gt; [1] -0.253</span></span></code></pre></div>
<p><img src="05-inference-cat_files/figure-html/height40Perc-1.png" width="70%" style="display: block; margin: auto;" /></p>

<div class="example">
<p>What is the adult male height at the <span class="math inline">\(82^{nd}\)</span> percentile?</p>
<hr />
<p>Again, we draw the figure first (see below).</p>
Next, we want to find the Z-score at the <span class="math inline">\(82^{nd}\)</span> percentile, which will be a positive value. Using <code>qnorm()</code>, the <span class="math inline">\(82^{nd}\)</span> percentile corresponds to <span class="math inline">\(Z=0.92\)</span>. Finally, the height <span class="math inline">\(x\)</span> is found using the Z-score formula with the known mean <span class="math inline">\(\mu\)</span>, standard deviation <span class="math inline">\(\sigma\)</span>, and Z-score <span class="math inline">\(Z=0.92\)</span>:
<span class="math display">\[\begin{eqnarray*}
0.92 = Z = \frac{x-\mu}{\sigma} = \frac{x - 70}{3.3}
\end{eqnarray*}\]</span>
This yields 73.04 inches or about 6’1’’ as the height at the <span class="math inline">\(82^{nd}\)</span> percentile.
</div>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="inference-cat.html#cb19-1" aria-hidden="true"></a><span class="kw">qnorm</span>(<span class="fl">0.82</span>, <span class="dt">m =</span> <span class="dv">0</span>, <span class="dt">s =</span> <span class="dv">1</span>)</span>
<span id="cb19-2"><a href="inference-cat.html#cb19-2" aria-hidden="true"></a><span class="co">#&gt; [1] 0.915</span></span></code></pre></div>
<p><img src="05-inference-cat_files/figure-html/height82Perc-1.png" width="70%" style="display: block; margin: auto;" /></p>

<div class="guidedpractice">
<ol style="list-style-type: lower-alpha">
<li>What is the <span class="math inline">\(95^{th}\)</span> percentile for SAT scores?<br />
</li>
<li>What is the <span class="math inline">\(97.5^{th}\)</span> percentile of the male heights? As always with normal probability problems, first draw a picture.<a href="#fn83" class="footnote-ref" id="fnref83"><sup>83</sup></a>
</div></li>
</ol>

<div class="guidedpractice">
<ol style="list-style-type: lower-alpha">
<li>What is the probability that a randomly selected male adult is at least 6’2’’ (74 inches)?<br />
</li>
<li>What is the probability that a male adult is shorter than 5’9’’ (69 inches)?<a href="#fn84" class="footnote-ref" id="fnref84"><sup>84</sup></a>
</div></li>
</ol>

<div class="example">
<p>What is the probability that a random adult male is between 5’9’’ and 6’2’’?</p>
<hr />
<p>These heights correspond to 69 inches and 74 inches. First, draw the figure. The area of interest is no longer an upper or lower tail.</p>
<p>The total area under the curve is 1. If we find the area of the two tails that are not shaded (from the previous Guided Practice, these areas are <span class="math inline">\(0.3821\)</span> and <span class="math inline">\(0.1131\)</span>), then we can find the middle area:</p>
That is, the probability of being between 5’9’’ and 6’2’’ is 0.5048.
</div>
<p><img src="05-inference-cat_files/figure-html/unnamed-chunk-59-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p><img src="05-inference-cat_files/figure-html/unnamed-chunk-60-1.png" width="70%" style="display: block; margin: auto;" /></p>

<div class="guidedpractice">
What percent of SAT takers get between 1500 and 2000?<a href="#fn85" class="footnote-ref" id="fnref85"><sup>85</sup></a>
</div>

<div class="guidedpractice">
What percent of adult males are between 5’5’’ and 5’7’’?<a href="#fn86" class="footnote-ref" id="fnref86"><sup>86</sup></a>
</div>
</div>
<div id="rule" class="section level3" number="5.2.5">
<h3><span class="header-section-number">5.2.5</span> 68-95-99.7 rule</h3>
<p>Here, we present a useful general rule for the probability of falling within 1, 2, and 3 standard deviations of the mean in the normal distribution. The rule will be useful in a wide range of practical settings, especially when trying to make a quick estimate without a calculator or Z table.</p>
<div class="figure" style="text-align: center"><span id="fig:er6895997"></span>
<img src="05-inference-cat_files/figure-html/er6895997-1.png" alt="Probabilities for falling within 1, 2, and 3 standard deviations of the mean in a normal distribution." width="70%" />
<p class="caption">
Figure 5.9: Probabilities for falling within 1, 2, and 3 standard deviations of the mean in a normal distribution.
</p>
</div>

<div class="guidedpractice">
Use <code>pnorm()</code> to confirm that about 68%, 95%, and 99.7% of observations fall within 1, 2, and 3, standard deviations of the mean in the normal distribution, respectively. For instance, first find the area that falls between <span class="math inline">\(Z=-1\)</span> and <span class="math inline">\(Z=1\)</span>, which should have an area of about 0.68. Similarly there should be an area of about 0.95 between <span class="math inline">\(Z=-2\)</span> and <span class="math inline">\(Z=2\)</span>.<a href="#fn87" class="footnote-ref" id="fnref87"><sup>87</sup></a>
</div>
<p>It is possible for a normal random variable to fall 4, 5, or even more standard deviations from the mean. However, these occurrences are very rare if the data are nearly normal. The probability of being further than 4 standard deviations from the mean is about 1-in-30,000. For 5 and 6 standard deviations, it is about 1-in-3.5 million and 1-in-1 billion, respectively.</p>

<div class="guidedpractice">
SAT scores closely follow the normal model with mean <span class="math inline">\(\mu = 1500\)</span> and standard deviation <span class="math inline">\(\sigma = 300\)</span>.<br />
(a) About what percent of test takers score 900 to 2100?<br />
(b) What percent score between 1500 and 2100?<a href="#fn88" class="footnote-ref" id="fnref88"><sup>88</sup></a>
</div>
</div>
</div>
<div id="single-prop" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> One proportion</h2>

<div class="onebox">
<p><strong>Notation</strong>.</p>
<ul>
<li><span class="math inline">\(n\)</span> = sample size (number of observational units in the data set)</li>
<li><span class="math inline">\(\hat{p}\)</span> = sample proportion (number of “successes” divided by the sample size)</li>
<li><span class="math inline">\(\pi\)</span> = population proportion<a href="#fn89" class="footnote-ref" id="fnref89"><sup>89</sup></a>
</div></li>
</ul>
<p>A single proportion is used to summarize data when we measured a single categorical variable on each observational unit—the single variable is measured as either a success or failure (e.g., “surgical complication” vs. “no surgical complication”)<a href="#fn90" class="footnote-ref" id="fnref90"><sup>90</sup></a>.</p>
<div id="one-prop-null-boot" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Simulation-based test for <span class="math inline">\(H_0: \pi = \pi_0\)</span></h3>
<p>
In Section <a href="inference-cat.html#HypothesisTesting">5.1.3</a>, we introduced the general steps of a hypothesis test:</p>

<div class="onebox">
<p><strong>General steps of a hypothesis test.</strong> Every hypothesis test follows these same general steps:</p>
<ol style="list-style-type: decimal">
<li>Frame the research question in terms of hypotheses.</li>
<li>Collect and summarize data using a test statistic.</li>
<li>Assume the null hypothesis is true, and simulate or mathematically model a null distribution for the test statistic.</li>
<li>Compare the observed test statistic to the null distribution to calculate a p-value.</li>
<li>Make a conclusion based on the p-value, and write a conclusion in context, in plain language, and in terms of the alternative hypothesis.
</div></li>
</ol>

<div class="example">
<p>People providing an organ for donation sometimes seek the help of a special medical consultant.
These consultants assist the patient in all aspects of the surgery, with the goal of reducing the possibility of complications during the medical procedure and recovery.
Patients might choose a consultant based in part on the historical complication rate of the consultant’s clients.</p>
<p>One consultant tried to attract patients by noting the average complication rate for liver donor surgeries in the US is about 10%, but her clients have had only 3 complications in the 62 liver donor surgeries she has facilitated.
She claims this is strong evidence that her work meaningfully contributes to reducing complications (and therefore she should be hired!).</p>
<p>Using these data, is it possible to assess the consultant’s claim that her work meaningfully contributes to reducing complications?</p>
<hr />
<p>No. The claim is that there is a causal connection, but the data are observational, so we must be on the lookout for confounding variables.
For example, maybe patients who can afford a medical consultant can afford better medical care, which can also lead to a lower complication rate.</p>
While it is not possible to assess the causal claim, it is still possible to understand the consultant’s true rate of complications.
</div>
<div id="steps-1-and-2-hypotheses-and-test-statistic" class="section level4 unnumbered">
<h4>Steps 1 and 2: Hypotheses and test statistic</h4>
<p>Regardless of if we use simulation-based methods or theory-based methods, the first two steps of a hypothesis test start out the same: setting up hypotheses and summarizing data with a test statistic.
We will let <span class="math inline">\(\pi\)</span> represent the true complication rate for liver donors working with this consultant. This “true” complication probability is called the <strong>parameter</strong> of interest<a href="#fn91" class="footnote-ref" id="fnref91"><sup>91</sup></a>.)
The sample proportion for the complication rate is 3 complications divided by the 62 surgeries the consultant has worked on: <span class="math inline">\(\hat{p} = 3/62 = 0.048\)</span>. Since this value is estimated from sample data, it is called a <strong>statistic</strong>. The statistic <span class="math inline">\(\hat{p}\)</span> is also our point estimate, or “best guess,” for <span class="math inline">\(\pi\)</span>, and we will use is as our <strong>test statistic</strong>.</p>

<div class="onebox">
<p><strong>Parameters and statistics.</strong></p>
<p>A <strong>parameter</strong> is the “true” value of interest.
We typically estimate the parameter using a <strong>statistic</strong> from a sample of data. When a statistic is used as an estimate of a parameter, it is called a <strong>point estimate</strong>.</p>
<p>For example, we estimate the probability <span class="math inline">\(\pi\)</span> of a complication for a client of the medical consultant by examining the past complications rates of her clients:</p>
<span class="math display">\[\hat{p} = 3 / 62 = 0.048\qquad\text{is used to estimate}\qquad \pi\]</span>
</div>

<div class="protip">
<p>Summary measures that summarize a sample of data, such as <span class="math inline">\(\hat{p}\)</span>, are called <strong>statistics</strong>. Numbers that summarize an entire population, such as <span class="math inline">\(\pi\)</span>, are called <strong>parameters</strong>. You can remember
this distinction by looking at the first letter of each term:</p>
<blockquote>
<p><strong><em>S</em></strong>tatistics summarize <strong><em>S</em></strong>amples.<br />
<strong><em>P</em></strong>arameters summarize <strong><em>P</em></strong>opulations.</p>
</blockquote>
We typically use Roman letters to symbolize statistics (e.g., <span class="math inline">\(\bar{x}\)</span>, <span class="math inline">\(\hat{p}\)</span>), and Greek letters to symbolize parameters (e.g., <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\pi\)</span>).
Since we rarely can measure the entire population, and thus rarely know
the actual parameter values, we like to say, “We don’t know Greek,
and we don’t know parameters!”
</div>

<div class="example">
<p>Write out hypotheses in both plain and statistical language to test for the association between the consultant’s work and the true complication rate, <span class="math inline">\(\pi\)</span>, for the consultant’s clients.</p>
<hr />
<p>In words:</p>
<blockquote>
<p><span class="math inline">\(H_0\)</span>: There is no association between the consultant’s contributions and the clients’ complication rate.<br />
<span class="math inline">\(H_A\)</span>: Patients who work with the consultant tend to have a complication rate lower than 10%.</p>
</blockquote>
<p>In statistical language:</p>
<blockquote>
<span class="math inline">\(H_0: \pi=0.10\)</span><br />
<span class="math inline">\(H_A: \pi&lt;0.10\)</span>
</div>
</blockquote>
</div>
<div id="steps-3-and-4-null-distribution-and-p-value" class="section level4 unnumbered">
<h4>Steps 3 and 4: Null distribution and p-value</h4>
<p>To assess these hypotheses, we need to evaluate the possibility of getting a sample proportion as far below the null value, <span class="math inline">\(0.10\)</span>, as what was observed (<span class="math inline">\(0.048\)</span>), <em>if the null hypothesis were true</em>.</p>

<div class="onebox">
<p><strong>Null value of a hypothesis test.</strong></p>
The <strong>null value</strong> is the reference value for the parameter in <span class="math inline">\(H_0\)</span>, and it is sometimes represented with the parameter’s label with a subscript 0 (or “null”), e.g., <span class="math inline">\(\pi_0\)</span> (just like <span class="math inline">\(H_0\)</span>).
</div>
<p>The deviation of the sample statistic from the null hypothesized parameter is usually quantified with a p-value<a href="#fn92" class="footnote-ref" id="fnref92"><sup>92</sup></a>. The p-value is computed based on the null distribution, which is the distribution of the test statistic if the null hypothesis is true. Supposing the null hypothesis is true, we can compute the p-value by identifying the chance of observing a test statistic that favors the alternative hypothesis at least as strongly as the observed test statistic.</p>

<div class="onebox">
<p><strong>Null distribution.</strong></p>
<p>The <strong>null distribution</strong> of a test statistic is the sampling distribution of that statistic <em>under the assumption of the null hypothesis</em>. It describes how that statistic would vary from sample to sample, if the null hypothesis were true.</p>
The null distribution can be estimated through simulation (simulation-based methods), as in this section,
or can be modeled by a mathematical function (theory-based methods),
as in Section <a href="inference-cat.html#theory-prop">5.3.3</a>.
</div>
<p>We want to identify the sampling distribution of the test statistic (<span class="math inline">\(\hat{p}\)</span>) if the null hypothesis was true. In other words, we want to see how the sample proportion changes due to chance alone. Then we plan to use this information to decide whether there is enough evidence to reject the null hypothesis.</p>
<p>Under the null hypothesis, 10% of liver donors have complications during or after surgery. Suppose this rate was really no different for the consultant’s clients (for <em>all</em> the consultant’s clients, not just the 62 previously measured). If this was the case, we could <em>simulate</em> 62 clients to get a sample proportion for the complication rate from the null distribution.</p>
<p>This is a similar scenario to the one we encountered in Section <a href="inference-cat.html#Martian">5.1.1</a>, with one important difference—the null value is 0.10, not 0.50. Thus, a flipping a coin to simulate whether a client had complications would not be simulating under the correct null hypothesis.</p>

<div class="guidedpractice">
What physical object could you use to simulate a random sample of 62 clients who had a 10% chance of complications? How would you use this object?<a href="#fn93" class="footnote-ref" id="fnref93"><sup>93</sup></a>
</div>
<p>Assuming the true complication rate for the consultant’s clients is 10%, each client can be simulated using a bag of marbles with 10% red marbles and 90% white marbles.
Sampling a marble from the bag (with 10% red marbles) is one way of simulating whether a patient has a complication <em>if the true complication rate is 10%</em> for the data. If we select 62 marbles and then compute the proportion of patients with complications in the simulation, <span class="math inline">\(\hat{p}_{sim}\)</span>, then the resulting sample proportion is calculated exactly from a sample from the null distribution.</p>
<p>An undergraduate student was paid $2 to complete this simulation. There were 5 simulated cases with a complication and 57 simulated cases without a complication, i.e., <span class="math inline">\(\hat{p}_{sim} = 5/62 = 0.081\)</span>.</p>

<div class="example">
<p>Is this one simulation enough to determine whether or not we should reject the null hypothesis?</p>
<hr />
No. To assess the hypotheses, we need to see a distribution of many <span class="math inline">\(\hat{p}_{sim}\)</span>, not just a <em>single</em> draw from this sampling distribution.
</div>
<p>One simulation isn’t enough to get a sense of the null distribution; many simulation studies are needed. Roughly 10,000 seems sufficient. However, paying someone to simulate 10,000 studies by hand is a waste of time and money. Instead, simulations are typically programmed into a computer, which is much more efficient.</p>
<p>Figure <a href="inference-cat.html#fig:nullDistForPHatIfLiverTransplantConsultantIsNotHelpful">5.10</a> shows the results of 10,000 simulated studies. The proportions that are equal to or less than <span class="math inline">\(\hat{p}=0.048\)</span> are shaded. The shaded areas represent sample proportions under the null distribution that provide at least as much evidence as <span class="math inline">\(\hat{p}\)</span> favoring the alternative hypothesis. There were 1222 simulated sample proportions with <span class="math inline">\(\hat{p}_{sim} \leq 0.048\)</span>. We use these to construct the null distribution’s left-tail area and find the p-value:
<span class="math display">\[\begin{align}
\text{left tail area }\label{estOfPValueBasedOnSimulatedNullForSingleProportion}
	&amp;= \frac{\text{Number of observed simulations with }\hat{p}_{sim}\leq\text{ 0.048}}{10000}
\end{align}\]</span>
Of the 10,000 simulated <span class="math inline">\(\hat{p}_{sim}\)</span>, 1222 were equal to or smaller than <span class="math inline">\(\hat{p}\)</span>. Since the hypothesis test is one-sided, the estimated p-value is equal to this tail area: 0.1222.</p>
<div class="figure" style="text-align: center"><span id="fig:nullDistForPHatIfLiverTransplantConsultantIsNotHelpful"></span>
<img src="05-inference-cat_files/figure-html/nullDistForPHatIfLiverTransplantConsultantIsNotHelpful-1.png" alt="The null distribution for $\hat{p}$, created from 10,000 simulated studies. The left tail, representing the p-value for the hypothesis test, contains 12.22% of the simulations." width="70%" />
<p class="caption">
Figure 5.10: The null distribution for <span class="math inline">\(\hat{p}\)</span>, created from 10,000 simulated studies. The left tail, representing the p-value for the hypothesis test, contains 12.22% of the simulations.
</p>
</div>
</div>
<div id="step-5-conclusion" class="section level4 unnumbered">
<h4>Step 5: Conclusion</h4>

<div class="guidedpractice">
Because the estimated p-value is 0.1222, which is not small, we have little to no evidence against the null hypothesis. Explain what this means in plain language in the context of the problem.<a href="#fn94" class="footnote-ref" id="fnref94"><sup>94</sup></a>
</div>
<p></p>

<div class="guidedpractice">
Does the conclusion in the previous Guided Practice imply there is no real association between the surgical consultant’s work and the risk of complications? Explain.<a href="#fn95" class="footnote-ref" id="fnref95"><sup>95</sup></a>
</div>
<!--
Add this to an Appendix someday!

#### Generating the exact null distribution and p-value  {-} {#exactNullDistributionUsingBinomialModel}

The number of successes in $n$ independent cases can be described using the binomial model, which was introduced in Section \ref{binomialModel}. Recall that the probability of observing exactly $k$ successes is given by
\begin{align} \label{binomialEquationShownForFindingNullDistributionInSmallSamplePropTest}
P(k\text{ successes}) = {n\choose k} p^{k}(1-p)^{n-k} = \frac{n!}{k!(n-k)!} p^{k}(1-p)^{n-k}
\end{align}
where $p$ is the true probability of success. The expression ${n\choose k}$ is read as \emph{$n$ choose $k$}, and the exclamation points represent factorials. For instance, $3!$ is equal to $3\times 2\times 1=6$, $4!$ is equal to $4\times 3\times 2\times 1 = 24$, and so on (see Section \ref{binomialModel}).

The tail area of the null distribution is computed by adding up the probability in Equation \eqref{binomialEquationShownForFindingNullDistributionInSmallSamplePropTest} for each $k$ that provides at least as strong of evidence favoring the alternative hypothesis as the data. If the hypothesis test is one-sided, then the p-value is represented by a single tail area. If the test is two-sided, compute the single tail area and double it to get the p-value, just as we have done in the past.

\begin{example}{Compute the exact p-value to check the consultant's claim that her clients' complication rate is below 105.}
Exactly $k=3$ complications were observed in the $n=62$ cases cited by the consultant. Since we are testing against the 10% national average, our null hypothesis is $p=0.10$. We can compute the p-value by adding up the cases where there are 3 or fewer complications:
\begin{align*}
\text{p-value}
	&= \sum_{j=0}^{3} {n\choose j} p^{j}(1-p)^{n-j} \\
	&= \sum_{j=0}^{3} {62\choose j} 0.1^{j}(1-0.1)^{62-j} \\
	&= {62\choose 0} 0.1^{0}(1-0.1)^{62-0} +
		{62\choose 1} 0.1^{1}(1-0.1)^{62-1} \\
	& \qquad + {62\choose 2} 0.1^{2}(1-0.1)^{62-2} +
		{62\choose 3} 0.1^{3}(1-0.1)^{62-3} \\
	&= 0.0015 + 0.0100 + 0.0340 + 0.0755 \\
	&= 0.1210
\end{align*}
This exact p-value is very close to the p-value based on the simulations (0.1222), and we come to the same conclusion. We do not reject the null hypothesis, and there is not statistically significant evidence to support the association.

If it were plotted, the exact null distribution would look almost identical to the simulated null distribution shown in Figure \ref{nullDistForPHatIfLiverTransplantConsultantIsNotHelpful} on page \pageref{nullDistForPHatIfLiverTransplantConsultantIsNotHelpful}.
\end{example}

-->
</div>
</div>
<div id="boot-ci-prop" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Bootstrap confidence interval for <span class="math inline">\(\pi\)</span></h3>
<p>A confidence interval provides a range of
plausible values for the parameter <span class="math inline">\(\pi\)</span>.
If the goal is to produce a range of possible values for a population value, then in an ideal world, we would sample data from the population again and recompute the sample proportion.
Then we could do it again.
And again.
And so on until we have a good sense of the variability of our original estimate.
The ideal world where sampling data is free or extremely cheap is almost never the case, and taking repeated samples from a population is usually impossible.
So, instead of using a “resample from the population” approach, bootstrapping uses a “resample from the sample” approach.</p>
<p></p>

<div class="example">
<p>Let’s revisit our medical consultant example from Section <a href="inference-cat.html#one-prop-null-boot">5.3.1</a>. This consultant tried to attract patients by noting the average complication rate for liver donor surgeries in the US is about 10%, but her clients have had only 3 complications in the 62 liver donor surgeries she has facilitated. This data, however, did not provide sufficient evidence that the consultant’s complication rate was less than 10%, since the p-value was approximately 0.122. Does this mean we can conclude that the consultant’s complication rate was equal to 10%?</p>
<hr />
No! Though our decision was to fail to reject the null hypothesis, this does not mean we have evidence <em>for</em> the null hypothesis—we cannot “accept” the null. The sample proportion was <span class="math inline">\(\hat{p} = 3/62 = 0.048\)</span>, which is our point estimate—or “best guess”—of <span class="math inline">\(\pi\)</span>. It wouldn’t make sense that a sample complication rate of 4.8% gives us evidence that the true complication rate was exactly 10%. It`s plausible that the true complication rate is 10%, but there are a range of plausible values for <span class="math inline">\(\pi\)</span>. In this section, we will use a simulation-based method called <strong>bootstrapping</strong> to generate this range of plausible values for <span class="math inline">\(\pi\)</span> using the observed data.
</div>
<p>In the medical consultant case study, the parameter is <span class="math inline">\(\pi\)</span>, the true probability of a complication for a client of the medical consultant.
There is no reason to believe that <span class="math inline">\(\pi\)</span> is exactly <span class="math inline">\(\hat{p} = 3/62\)</span>, but there is also no reason to believe that <span class="math inline">\(\pi\)</span> is particularly far from <span class="math inline">\(\hat{p} = 3/62\)</span>.
By sampling with replacement from the data set (a process called <strong>bootstrapping</strong>), the variability of the possible <span class="math inline">\(\hat{p}\)</span> values can be approximated, which will allow us to generate a range of plausible values for <span class="math inline">\(\pi\)</span>, i.e., a confidence interval.</p>
<p>Most of the inferential procedures covered in this text are grounded in quantifying how one data set would differ from another when they are both taken from the same population.
It doesn’t make sense to take repeated samples from the same population because if you have the means to take more samples, a larger sample size will benefit you more than the exact same sample twice.
Instead, we measure how the samples behave under an estimate of the population. Figure <a href="inference-cat.html#fig:boot1">5.11</a> shows how an unknown original population of red and white marbles can be estimated by using multiple copies of a sample of seven marbles.</p>
<div class="figure" style="text-align: center"><span id="fig:boot1"></span>
<img src="05/figures/boot1prop1.png" alt="An unknown population of red and white marbles. The estimated population on the right is many copies of the observed sample." width="75%" />
<p class="caption">
Figure 5.11: An unknown population of red and white marbles. The estimated population on the right is many copies of the observed sample.
</p>
</div>
<p>By taking repeated samples from the estimated population, the variability from sample to sample can be observed. In Figure <a href="inference-cat.html#fig:boot2">5.12</a> the repeated bootstrap samples are obviously different both from each other, from the original sample, and from the original population.
Recall that the bootstrap samples were taken from the same (estimated) population, and so the differences are due entirely to natural variability in the sampling procedure.</p>
<div class="figure" style="text-align: center"><span id="fig:boot2"></span>
<img src="05/figures/boot1prop2.png" alt="Selecting $k$ random samples from the estimated population created from copies of the observed sample." width="75%" />
<p class="caption">
Figure 5.12: Selecting <span class="math inline">\(k\)</span> random samples from the estimated population created from copies of the observed sample.
</p>
</div>
<p>By summarizing each of the bootstrap samples (here, using the sample proportion), we see, directly, the variability of the sample proportion of red marbles, <span class="math inline">\(\hat{p}\)</span>, from sample to sample.
The distribution of bootstrapped <span class="math inline">\(\hat{p}\)</span>’s for the example scenario is shown in Figure <a href="inference-cat.html#fig:boot3">5.13</a>, and the bootstrap distribution for the medical consultant data is shown in Figure <a href="inference-cat.html#fig:MedConsBSSim">5.14</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:boot3"></span>
<img src="05/figures/boot1prop3.png" alt="Calculate the sample proportion of red marbles in each bootstrap resample, then plot these simulated sample proportions in a dot plot. The dot plot of sample proportion provides us a sense of how sample proportions would vary from sample to sample if we could take many samples from our original population." width="50%" /><img src="05-inference-cat_files/figure-html/boot3-2.png" alt="Calculate the sample proportion of red marbles in each bootstrap resample, then plot these simulated sample proportions in a dot plot. The dot plot of sample proportion provides us a sense of how sample proportions would vary from sample to sample if we could take many samples from our original population." width="50%" />
<p class="caption">
Figure 5.13: Calculate the sample proportion of red marbles in each bootstrap resample, then plot these simulated sample proportions in a dot plot. The dot plot of sample proportion provides us a sense of how sample proportions would vary from sample to sample if we could take many samples from our original population.
</p>
</div>
<p>It turns out that in practice, it is very difficult for computers to work with an infinite population (with the same proportional breakdown as in the sample).
However, there is a physical and computational model which produces an equivalent bootstrap distribution of the sample proportion in a computationally efficient manner.
Consider the observed data to be a bag of marbles 3 of which are red and 4 of which are white. By drawing the marbles out of the bag <em>with replacement</em>, we depict the same sampling <strong>process</strong> as was done with the infinitely large estimated population.
Note that when sampling the original observations with replacement, a particular marble may end up in the new sample one time, multiple times, or not at all.</p>

<div class="onebox">
<p><strong>Bootstrapping<a href="#fn96" class="footnote-ref" id="fnref96"><sup>96</sup></a> from one sample</strong>.</p>
<ol style="list-style-type: decimal">
<li>Take a random sample of size <span class="math inline">\(n\)</span> from the original sample, <em>with replacement</em>. This is called a <strong>bootstrapped resample</strong>.</li>
<li>Record the sample proportion (or statistic of interest) from the boostrapped resample. This is called a <strong>bootstrapped statistic</strong>.</li>
<li>Repeat steps (1) and (2) 1000s of times to create a distribution of bootstrapped statistics.
</div></li>
</ol>
<p>If we apply the bootstrap sampling process to the medical consultant example, we consider each client to be one of the marbles in the bag.
There will be 59 white marbles (no complication) and 3 red marbles (complication).
If we 62 choose marbles out of the bag (one at a time), replacing each chosen marble after its color is recorded, and compute the proportion of simulated patients with complications, <span class="math inline">\(\hat{p}_{bs}\)</span>, then this “bootstrap” proportion represents a single simulated proportion from the “resample from the sample” approach.</p>

<div class="guidedpractice">
In a simulation of 62 patients conduced by sampling with replacement from the original sample, about how many would we expect to have had a complication?<a href="#fn97" class="footnote-ref" id="fnref97"><sup>97</sup></a>
</div>
<p>One simulated bootstrap resample isn’t enough to get a sense of the variability from one bootstrap proportion to another bootstrap proportion, so we repeated the simulation 10,000 times using a computer.
Figure <a href="inference-cat.html#fig:MedConsBSSim">5.14</a> shows the distribution from the 10,000 bootstrap simulations.
The bootstrapped proportions vary from about zero to 0.15. By taking the range of the middle 95% of this distribution, we can construct a <strong>95% bootstrapped confidence interval</strong> for <span class="math inline">\(\pi\)</span>. The 2.5<sup>th</sup> percentile is 0, and the 97.5<sup>th</sup> percentile is 0.113, so the middle 95% of the distribution is the range (0, 0.113).
The variability in the bootstrapped proportions leads us to believe that the true risk of complication (the parameter, <span class="math inline">\(\pi\)</span>) is somewhere between 0 and 11.3%.</p>
<div class="figure" style="text-align: center"><span id="fig:MedConsBSSim"></span>
<img src="05-inference-cat_files/figure-html/MedConsBSSim-1.png" alt="The original medical consultant data is bootstrapped 10,000 times. Each simulation creates a sample from the original data where the probability of a complication is $\hat{p} = 3/62$. The bootstrap 2.5 percentile proportion is 0 and the 97.5 percentile is 0.113. The result is: we are confident that, in the population, the true probability of a complication is between 0% and 11.3%." width="70%" />
<p class="caption">
Figure 5.14: The original medical consultant data is bootstrapped 10,000 times. Each simulation creates a sample from the original data where the probability of a complication is <span class="math inline">\(\hat{p} = 3/62\)</span>. The bootstrap 2.5 percentile proportion is 0 and the 97.5 percentile is 0.113. The result is: we are confident that, in the population, the true probability of a complication is between 0% and 11.3%.
</p>
</div>

<div class="example">
<p>The original claim was that the consultant’s true rate of complication was under the national rate of 10%. Does the interval estimate of 0 to 11.3% for the true probability of complication indicate that the surgical consultant has a lower rate of complications than the national average?
Explain.</p>
<hr />
No. Because the interval overlaps 10%, it might be that the consultant’s work is associated with a lower risk of complciations, or it might be that the consulant’s work is associated with a higher risk (i.e., greater than 10%) of complications! Additionally, as previously mentioned, because this is an observational study, even if an association can be measured, there is no evidence that the consultant’s work is the cause of the complication rate (being higher or lower).
</div>
<!--
%However, we currently don't have enough data to say whether the corresponding complication rate is any different than 0.10.
-->
<p></p>
</div>
<div id="theory-prop" class="section level3" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> Theory-based methods for <span class="math inline">\(\pi\)</span></h3>
<p>In Section <a href="inference-cat.html#var-stat">5.1.2</a>, we introduced the normal distribution and showed how it can be used as a mathematical model to describe the variability of a sample mean or sample proportion as a result of the Central Limit Theorem. We explored the normal distribution
further in Section <a href="inference-cat.html#normal">5.2</a>. Theory-based hypothesis tests and confidence intervals for proportions use the normal distribution to calculate the p-value and to determine the width of the confidence interval.</p>
<div id="variability-of-hatp" class="section level4 unnumbered">
<h4>Variability of <span class="math inline">\(\hat{p}\)</span></h4>
<p>There are conditions under which a sample proportion <span class="math inline">\(\hat{p}\)</span> is well-modeled using a normal distribution.
When the sample observations
are independent and the sample size is sufficiently
large, the normal model will describe the variability quite well; when the observations violate the conditions, the normal model can be inaccurate.</p>

<div class="onebox">
<p><strong>Conditions for the sampling distribution of
<span class="math inline">\(\hat{p}\)</span> to be normal.</strong></p>
<p>The sampling distribution for <span class="math inline">\(\hat{p}\)</span> based on
a sample of size <span class="math inline">\(n\)</span> from a population with a true
proportion <span class="math inline">\(\pi\)</span> can be modeled
using a normal distribution when:</p>
<ol style="list-style-type: decimal">
<li><em>Independence</em>. The sample’s observations are independent,
e.g., are from a simple random sample.</li>
<li><em>Success-failure condition</em>. We expected to see at least 10 successes and
10 failures in the sample, i.e., <span class="math inline">\(n\pi\geq10\)</span> and
<span class="math inline">\(n(1-\pi)\geq10\)</span>.</li>
</ol>
When these conditions are satisfied, then the sampling
distribution of <span class="math inline">\(\hat{p}\)</span> is approximately normal with mean
<span class="math inline">\(\pi\)</span> and standard deviation <span class="math inline">\(\sqrt{\frac{\ \pi(1-\pi)\ }{n}}\)</span>.
</div>
<p>
</p>

<div class="tipbox">
The success-failure condition listed above is only necessary for the sampling distribution of <span class="math inline">\(\hat{p}\)</span> to be approximately normal. The mean of the sampling distribution of <span class="math inline">\(\hat{p}\)</span> is <span class="math inline">\(\pi\)</span>, and the standard deviation is <span class="math inline">\(\sqrt{\frac{\ \pi(1-\pi)\ }{n}}\)</span>, regardless of the sample size.
</div>
<p>Typically we don’t know the true proportion <span class="math inline">\(\pi\)</span>,
so we substitute some value to check the success-failure condition
and to estimate the standard deviation of the sampling distribution of <span class="math inline">\(\hat{p}\)</span>.
The independence condition is a more nuanced requirement.
When it isn’t met, it is important to understand how and why
it isn’t met.
For example, <em>there exist no statistical methods available to truly correct the inherent biases of data from a convenience sample.</em>
On the other hand, if we took a cluster random sample
(see Section <a href="intro-to-data.html#samp-methods">1.3.4</a>), the observations wouldn’t be independent, but suitable statistical methods are available for analyzing the data (but they are beyond the scope of even most second or third courses
in statistics)<a href="#fn98" class="footnote-ref" id="fnref98"><sup>98</sup></a>.</p>

<div class="example">
<p>In the examples based on large sample theory, we modeled <span class="math inline">\(\hat{p}\)</span> using the normal distribution. Why is this not appropriate for the study on the medical consultant?</p>
<hr />
The independence assumption may be reasonable if each of the surgeries is from a different surgical team. However, the success-failure condition is not satisfied. Under the null hypothesis, we would anticipate seeing <span class="math inline">\(62\times 0.10=6.2\)</span> complications, not the 10 required for the normal approximation.
</div>
<p>Since theory-based methods cannot be used on the medical consultant example, we’ll turn to another example to demonstrate these methods, where conditions for approximating the distribution of <span class="math inline">\(\hat{p}\)</span> by a normal distribution are met.</p>
</div>
<div id="hypothesis-test-for-h_0-pi-pi_0" class="section level4 unnumbered">
<h4>Hypothesis test for <span class="math inline">\(H_0: \pi = \pi_0\)</span></h4>
<p>One possible regulation for payday lenders is that they
would be required to do a credit check and evaluate debt
payments against the borrower’s finances.
We would like to know: would borrowers support this form
of regulation?</p>

<div class="example">
<p>Set up hypotheses to evaluate whether borrowers
have a majority support for this
type of regulation. We take “majority” to mean
greater than 50% of the population.</p>
<hr />
<p>In words,</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: there is not majority support for the regulation</li>
<li><span class="math inline">\(H_A\)</span>: the majority of borrowers support the regulation</li>
</ul>
<p>In statistical notation,</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: <span class="math inline">\(\pi = 0.50\)</span></li>
<li><span class="math inline">\(H_A\)</span>: <span class="math inline">\(\pi &gt; 0.50\)</span>,</li>
</ul>
where <span class="math inline">\(\pi\)</span> represents the proportion of <em>all</em> payday loan borrowers that would support the regulation.
</div>

<div class="tipbox">
<p>Note that the null hypothesis above was stated as <span class="math inline">\(H_0: \pi = 0.50\)</span>, even though saying there is “not majority support” would imply <span class="math inline">\(\pi \leq 0.50\)</span>. Indeed, some textbooks would
write <span class="math inline">\(H_0: \pi \leq 0.50\)</span> in this case, and it is not an incorrect statement. However,
when calculating the p-value, we need to assume a particular value for <span class="math inline">\(\pi\)</span> under
the null hypothesis, so in this textbook, our null hypothesis will always be of the form:</p>
<span class="math display">\[
H_0: \mbox{ parameter } = \mbox{ null value}
\]</span>
</div>
<p>To apply the normal distribution to model the null distribution, the independence
and success-failure conditions must be satisfied.
In a hypothesis test, the success-failure condition is
checked using the null proportion:
we verify <span class="math inline">\(np_0\)</span> and <span class="math inline">\(n(1-p_0)\)</span> are at least 10,
where <span class="math inline">\(p_0\)</span> is the null value.</p>

<div class="guidedpractice">
Do payday loan borrowers support a regulation
that would
require lenders to pull their credit report
and evaluate their debt payments?
From a random sample of 826 borrowers,
51% said they would support such
a regulation.
Is it reasonable use a normal distribution to model <span class="math inline">\(\hat{p}\)</span>
for a hypothesis test here?<a href="#fn99" class="footnote-ref" id="fnref99"><sup>99</sup></a>
</div>

<div class="example">
<p>Continuing the previous Example,
evaluate whether the poll on lending regulations provides convincing evidence
that a majority of payday loan borrowers support
a new regulation that would
require lenders to pull credit reports
and evaluate debt payments.</p>
<hr />
<p>With hypotheses already set up and conditions checked,
we can move onto calculations.
The standard error in the context of a one-proportion
hypothesis test is computed using the null value, <span class="math inline">\(p_0\)</span>:
<span class="math display">\[\begin{align*}
  SE = \sqrt{\frac{p_0 (1 - p_0)}{n}}
      = \sqrt{\frac{0.5 (1 - 0.5)}{826}}
      = 0.017
  \end{align*}\]</span>
A picture of the normal model for the null distribution of sample proportions
in this scenario is shown below in Figure <a href="inference-cat.html#fig:paydayCC-norm-pvalue">5.15</a>,
with the p-value represented by the shaded region.
Note that this null distribution is centered at 0.50, the null value,
and has standard deviation 0.017.</p>
<p>Under <span class="math inline">\(H_0\)</span>, the probability of observing <span class="math inline">\(\hat{p} = 0.51\)</span> or higher
is 0.278, the area above
0.51 on the null distribution.</p>
<p>With a p-value of 0.278, the poll does not provide convincing evidence that
a majority of payday loan borrowers support regulations around credit checks and evaluation of
debt payments.</p>
You’ll note that this conclusion is somewhat unsatisfactory
because there is no conclusion, as is the case with larger p-values.
That is, there is no resolution one way or the other about public opinion.
We cannot claim that exactly 50% of people support the regulation, but we cannot claim a majority support it either.
</div>
<div class="figure" style="text-align: center"><span id="fig:paydayCC-norm-pvalue"></span>
<img src="05-inference-cat_files/figure-html/paydayCC-norm-pvalue-1.png" alt="Approximate sampling distribution of $\hat{p}$ across all possible samples assuming $\pi = 0.50$. The shaded area represents the p-value corresponding to an observed sample proportion of 0.51." width="70%" />
<p class="caption">
Figure 5.15: Approximate sampling distribution of <span class="math inline">\(\hat{p}\)</span> across all possible samples assuming <span class="math inline">\(\pi = 0.50\)</span>. The shaded area represents the p-value corresponding to an observed sample proportion of 0.51.
</p>
</div>
<p>Often, with theory-based methods, we use a <strong>standardized statistic</strong> rather than
the original statistic as our test statistic. A standardized statistic is computed by subtracting the mean of the null distribution from the original statistic, then dividing by the standard error:
<span class="math display">\[
\mbox{standardized statistic} = \frac{\mbox{observed statistic} - \mbox{null value}}{\mbox{null standard error}}
\]</span>
The <strong>null standard error</strong> of the observed statistic is its estimated standard deviation assuming the null hypothesis is true. We can interpret the standardized statistic as <em>the number of standard errors our observed statistic is above (if positive) or below (if negative) the null value</em>. When we are modeling the null distribution with a normal
distribution, this standardized statistic is called <span class="math inline">\(Z\)</span>, since it is the Z-score of the sample proportion.</p>

<div class="onebox">
<p><strong>Standardized sample proportion.</strong></p>
The <strong>standardized statistic</strong> for theory-based methods for one proportion is
<span class="math display">\[
Z = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}}
\]</span>
where <span class="math inline">\(p_0\)</span> is the null value. The denominator, <span class="math inline">\(\sqrt{\frac{p_0(1-p_0)}{n}}\)</span>, is called the <strong>null standard error</strong> of the sample proportion.
</div>
<p>With the standardized statistic as our test statistic, we can find
the p-value as the area under a standard normal distribution at or more extreme
than our observed <span class="math inline">\(Z\)</span> value.</p>

<div class="example">
<p>Do payday loan borrowers support a regulation that would require lenders to pull their credit report and evaluate their debt payments? From a random sample of 826 borrowers, 51% said they would support such a regulation. We set up hypotheses and checked conditions previously. Now calculate and interpret the standardized statistic, then use the standard normal distribution to calculate the approximate p-value.</p>
<hr />
<p>Our sample proportion is <span class="math inline">\(\hat{p} = 0.51\)</span>. Since our null value is <span class="math inline">\(p_0 = 0.50\)</span>,<br />
the null standard error is
<span class="math display">\[\begin{align*}
  SE = \sqrt{\frac{p_0 (1 - p_0)}{n}}
      = \sqrt{\frac{0.5 (1 - 0.5)}{826}}
      = 0.017
  \end{align*}\]</span></p>
<p>The standardized statistic is
<span class="math display">\[\begin{align*}
Z = \frac{0.51 - 0.50}{0.017} = 0.57
\end{align*}\]</span></p>
<p>Interpreting this value, we can say that our sample proportion of 0.51 was only 0.57 standard errors above the null value of 0.50.</p>
Shown in Figure <a href="inference-cat.html#fig:paydayCC-stdnorm-pvalue">5.16</a>, the p-value is the area above <span class="math inline">\(Z = 0.57\)</span> on a standard normal distribution—0.278—the same p-value we would obtain by finding the area above <span class="math inline">\(\hat{p} = 0.51\)</span> on a normal distribution with mean 0.50 and standard deviation 0.017, as in Figure <a href="inference-cat.html#fig:paydayCC-norm-pvalue">5.15</a>.
</div>
<div class="figure" style="text-align: center"><span id="fig:paydayCC-stdnorm-pvalue"></span>
<img src="05-inference-cat_files/figure-html/paydayCC-stdnorm-pvalue-1.png" alt="Approximate sampling distribution of $Z$ across all possible samples assuming $\pi = 0.50$. The shaded area represents the p-value corresponding to an observed standardized statistic of 0.57. Compare to Figure @
ef(fig:paydatCC-norm-pvalue)" width="70%" />
<p class="caption">
Figure 5.16: Approximate sampling distribution of <span class="math inline">\(Z\)</span> across all possible samples assuming <span class="math inline">\(\pi = 0.50\)</span>. The shaded area represents the p-value corresponding to an observed standardized statistic of 0.57. Compare to Figure @
ef(fig:paydatCC-norm-pvalue)
</p>
</div>

<div class="onebox">
<p><strong>Theory-based hypothesis test for a proportion: one-sample <span class="math inline">\(Z\)</span>-test.</strong></p>
<ol style="list-style-type: decimal">
<li>Frame the research question in terms of hypotheses.</li>
<li>Using the null value, <span class="math inline">\(p_0\)</span>, verify the conditions for using the normal distribution to approximate the null distribution.</li>
<li>Calculate the test statistic:
<span class="math display">\[
 Z = \frac{\hat{p} - p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}}
 \]</span></li>
<li>Use the test statistic and the standard normal distribution to calculate the p-value.</li>
<li>Make a conclusion based on the p-value, and write a conclusion in context, in plain language, and in terms of the alternative hypothesis.
</div></li>
</ol>
<!-- Move this paragraph to the right place: -->
<p>Regardless of the statistical method chosen, the p-value is always derived by analyzing the null distribution of the test statistic. The normal model poorly approximates the null distribution for <span class="math inline">\(\hat{p}\)</span> when the success-failure condition is not satisfied. As a substitute, we can generate the null distribution using simulated sample proportions and use this distribution to compute the tail area, i.e., the p-value.
Neither the p-value approximated by the normal distribution nor the simulated p-value are exact, because the normal distribution and simulated null distribution themselves are not exact, only a close approximation.
An exact p-value can be generated using the binomial distribution, but that method will not be covered in this text.</p>
<p></p>
</div>
<div id="confidence-interval-for-pi" class="section level4 unnumbered">
<h4>Confidence interval for <span class="math inline">\(\pi\)</span></h4>
<p></p>
<p>A confidence interval provides a range of
plausible values for the parameter <span class="math inline">\(\pi\)</span>.
A point estimate is our best guess for the value of the parameter, so it makes sense to build the confidence interval around that value. The standard error, which is a measure of the uncertainty associated with the point estimate, provides a guide for how large we should make the confidence interval. When <span class="math inline">\(\hat{p}\)</span> can be modeled using a
normal distribution, the 68-95-99.7 rule tells us that, in general, 95% of observations are within 2 standard errors of the mean. Here, we use the value 1.96 to be slightly more precise. The confidence interval
for <span class="math inline">\(\pi\)</span> then takes the form
<span class="math display">\[\begin{align*}
\hat{p} \pm z^{\star} \times SE.
\end{align*}\]</span></p>
<p>We have seen <span class="math inline">\(\hat{p}\)</span> to be the sample proportion. The value <span class="math inline">\(z^{\star}\)</span> comes from a standard normal distribution and is determined by the chosen confidence level. The value of the standard error, <span class="math inline">\(SE\)</span>, approximates how far we would expect the sample proportion to fall from <span class="math inline">\(\pi\)</span>, and depends heavily on the sample size.</p>

<div class="onebox">
<p><strong>Standard error of one proportion, <span class="math inline">\(\hat{p}\)</span>.</strong></p>
<p>When the conditions are met so that the distribution fo <span class="math inline">\(\hat{p}\)</span> is nearly normal, the <strong>variability</strong> of a single proportion, <span class="math inline">\(\hat{p}\)</span> is well described by its standard deviation:</p>
<p><span class="math display">\[SD(\hat{p}) = \sqrt{\frac{\pi(1-\pi)}{n}}\]</span></p>
<p>Note that we almost never know the true value of <span class="math inline">\(\pi\)</span>, but we can substitute our best guess of <span class="math inline">\(\pi\)</span> to obtain an approximate standard deviation, called the <strong>standard error</strong> of <span class="math inline">\(\hat{p}\)</span>:</p>
<p><span class="math display">\[SD(\hat{p}) \approx \hspace{3mm} SE(\hat{p}) = \sqrt{\frac{(\mbox{best guess of }\pi)(1 - \mbox{best guess of }\pi)}{n}}\]</span></p>
For hypothesis testing, we often use <span class="math inline">\(p_0\)</span> as the best guess of <span class="math inline">\(\pi\)</span>, as seen in Section <a href="inference-cat.html#theory-prop">5.3.3</a>. For confidence intervals, we typically use <span class="math inline">\(\hat{p}\)</span> as the best guess of <span class="math inline">\(\pi\)</span>.
</div>
<p></p>
<!--
\newcommand{\paydayN}{826}
\newcommand{\paydayNHalf}{413}
\newcommand{\paydayRegPerc}{70\%}
\newcommand{\paydayRegProp}{0.70}
\newcommand{\paydayRegSE}{0.016}
\newcommand{\paydayRegSEPerc}{1.6\%}
\newcommand{\paydayRegLower}{0.669}
\newcommand{\paydayRegUpper}{0.731}
\newcommand{\paydayRegLowerPerc}{66.9\%}
\newcommand{\paydayRegUpperPerc}{73.1\%}
% https://www.pewtrusts.org/-/media/assets/2017/04/payday-loan-customers-want-more-protections-methodology.pdf

did search and replace for each term above.  for example 826 for 826

-->

<div class="guidedpractice">
<p>Consider taking many polls of registered voters (i.e., random samples) of size 300 and asking them if they support legalized marijuana.
It is suspected that about 2/3 of all voters support legalized marijuana.
To understand how the sample proportion (<span class="math inline">\(\hat{p}\)</span>) would vary across the samples, calculate the standard error of <span class="math inline">\(\hat{p}\)</span>.<a href="#fn100" class="footnote-ref" id="fnref100"><sup>100</sup></a></p>
</div>

<div class="example">
<p>A simple random sample of 826
payday loan borrowers was surveyed to better
understand their interests around regulation and costs.
70% of the responses supported new
regulations on payday lenders.</p>
<ol style="list-style-type: decimal">
<li><p>Is it reasonable to model the variability of <span class="math inline">\(\hat{p}\)</span> from sample to sample
using a normal distribution?</p></li>
<li><p>Calculate the standard error of <span class="math inline">\(\hat{p}\)</span>.</p></li>
<li><p>Construct a 95% confidence interval for <span class="math inline">\(\pi\)</span>,
the proportion of <em>all</em> payday borrowers who support increased
regulation for payday lenders.</p></li>
</ol>
<hr />
<ol style="list-style-type: decimal">
<li><p>The data are a random sample, so the observations are
independent and representative of the population of
interest.</p>
<p>We also must check the success-failure condition,
which we do using <span class="math inline">\(\hat{p}\)</span> in place
of <span class="math inline">\(\pi\)</span> when computing a confidence interval:
<span class="math display">\[\begin{align*}
\text{Support: }
  n \hat{p} &amp;
      = 826 \times 0.70
  \approx 578
&amp;\text{Not: }
  n (1 - \hat{p}) &amp;
    = 826 \times (1 - 0.70)
  \approx 248
\end{align*}\]</span>
Since both values are at least 10, we can use the normal
distribution to model the sampling distribution of <span class="math inline">\(\hat{p}\)</span>.</p></li>
<li><p>Because <span class="math inline">\(\pi\)</span> is unknown and the standard error is for
a confidence interval, use <span class="math inline">\(\hat{p}\)</span> as our best guess of <span class="math inline">\(\pi\)</span>
in the formula.</p>
<p><span class="math inline">\(SE = \sqrt{\frac{0.70 (1 - 0.70)}  {826}} = 0.016\)</span>.</p></li>
<li><p>Using
the point estimate <span class="math inline">\(0.70\)</span>,
<span class="math inline">\(z^{\star} = 1.96\)</span> for a 95% confidence interval,
and
the standard error <span class="math inline">\(SE = 0.016\)</span> from the previous
Guided Practice,
the confidence interval is
<span class="math display">\[\begin{eqnarray*}
  \text{point estimate} \ \pm\ z^{\star} \times SE
   \quad\to\quad
   0.70 \ \pm\ 1.96 \times 0.016
   \quad\to\quad
   (0.669, 0.731)
  \end{eqnarray*}\]</span>
We are 95% confident that the true proportion of
payday borrowers who supported regulation at the time
of the poll was between 0.669 and
0.731.</p>
</div></li>
</ol>

<div class="onebox">
<p><strong>Constructing a confidence interval for a single proportion.</strong></p>
<p>There are four steps to constructing a confidence
interval for <span class="math inline">\(p\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Check independence and the success-failure condition
using <span class="math inline">\(\hat{p}\)</span>.
If the conditions are met, the sampling distribution
of <span class="math inline">\(\hat{p}\)</span> may be well-approximated by the normal model.</li>
<li>Construct the standard error:
<span class="math display">\[
 SE(\hat{p}) = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
  \]</span></li>
<li>Use statistical software to find the multiplier <span class="math inline">\(z^{\star}\)</span> corresponding to the confidence level.</li>
<li>Apply the general confidence interval formula <span class="math inline">\(\mbox{statistic} \pm (\mbox{multiplier}) \times SE\)</span>:
<span class="math display">\[
 \hat{p} \pm z^{\star}\times \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
  \]</span>
</div></li>
</ol>
</div>
<div id="zstar-and-the-confidence-level" class="section level4 unnumbered">
<h4><span class="math inline">\(z^{\star}\)</span> and the confidence level</h4>
<p></p>
<p>Suppose we want to consider confidence intervals where the confidence level is somewhat higher than 95%: perhaps we would like a confidence level of 99%. Think back to the analogy about trying to catch a fish: if we want to be more sure that we will catch the fish, we should use a wider net. To create a 99% confidence level, we must also widen our 95% interval. On the other hand, if we want an interval with lower confidence, such as 90%, we could make our original 95% interval slightly slimmer.</p>
<p>The 95% confidence interval structure provides guidance in how to make intervals with new confidence levels. Below is a general 95% confidence interval for a parameter whose point estimate has a nearly normal distribution:
<span class="math display">\[\begin{eqnarray}
\text{point estimate}\ \pm\ 1.96\times SE
\end{eqnarray}\]</span>
There are three components to this interval: the point estimate, “1.96”, and the standard error. The choice of <span class="math inline">\(1.96\times SE\)</span> was based on capturing 95% of the sampling distribution of statistics since the point estimate is within 1.96 standard errors of the true parameter about 95% of the time. The choice of 1.96 corresponds to a 95% <strong>confidence level</strong>.</p>

<div class="guidedpractice">
If <span class="math inline">\(X\)</span> is a normally distributed random variable, how often will <span class="math inline">\(X\)</span> be within 2.58 standard deviations of the mean?<a href="#fn101" class="footnote-ref" id="fnref101"><sup>101</sup></a>
</div>
<div class="figure" style="text-align: center"><span id="fig:choosingZForCI"></span>
<img src="05-inference-cat_files/figure-html/choosingZForCI-1.png" alt="The area between -$z^{\star}$ and $z^{\star}$ increases as $|z^{\star}|$ becomes larger. If the confidence level is 99%, we choose $z^{\star}$ such that 99% of the normal curve is between -$z^{\star}$ and $z^{\star}$, which corresponds to 0.5% in the lower tail and 0.5% in the upper tail: $z^{\star}=2.58$." width="70%" />
<p class="caption">
Figure 5.17: The area between -<span class="math inline">\(z^{\star}\)</span> and <span class="math inline">\(z^{\star}\)</span> increases as <span class="math inline">\(|z^{\star}|\)</span> becomes larger. If the confidence level is 99%, we choose <span class="math inline">\(z^{\star}\)</span> such that 99% of the normal curve is between -<span class="math inline">\(z^{\star}\)</span> and <span class="math inline">\(z^{\star}\)</span>, which corresponds to 0.5% in the lower tail and 0.5% in the upper tail: <span class="math inline">\(z^{\star}=2.58\)</span>.
</p>
</div>
<p></p>
<p>To create a 99% confidence interval, change 1.96 in the 95% confidence interval formula to be 2.58. The previous Guided Practice highlights that 99% of the time a normal random variable will be within 2.58 standard deviations of its mean. This approach—using the Z-scores in the normal model to compute confidence levels—is appropriate when the point estimate is associated with a normal distribution and we can properly compute the standard error. Thus, the formula for a 99% confidence interval is:</p>
<p><span class="math display">\[\begin{eqnarray*}
\text{point estimate}\ \pm\ 2.58\times SE
\end{eqnarray*}\]</span></p>
<!--
label for previous equation?
\label{99PercCIForMean}
\label{99PercCIForNormalPointEstimate}

%\Comment{I don't know where the equation number above gets referenced. Might drop the equation number.}
-->
<p>The normal approximation is crucial to the precision of the <span class="math inline">\(z^\star\)</span> confidence intervals. When the normal model is not a good fit, we will use alternative distributions that better characterize the sampling distribution or we will use bootstrapping procedures.</p>

<div class="guidedpractice">
Create a 99% confidence interval for the impact of the stent on the risk of stroke using the data from Section <a href="intro-to-data.html#basic-stents-strokes">1.1</a>. The point estimate is 0.090, and the standard error is <span class="math inline">\(SE = 0.028\)</span>. It has been verified for you that the point estimate can reasonably be modeled by a normal distribution.<a href="#fn102" class="footnote-ref" id="fnref102"><sup>102</sup></a>
</div>

<div class="onebox">
<p><strong>Theory-based <span class="math inline">\((1-\alpha)\times 100\)</span>% confidence interval.</strong></p>
If the statistic follows the normal model with standard error <span class="math inline">\(SE\)</span>, then a confidence interval for the population parameter is
<span class="math display">\[\begin{eqnarray*}
\text{statistic}\ \pm\ z^{\star} \times SE
\end{eqnarray*}\]</span>
where <span class="math inline">\(z^{\star}\)</span> corresponds to the confidence level selected: the middle <span class="math inline">\((1-\alpha)\times 100\)</span>% of a standard normal distribution lies between <span class="math inline">\(-z^{\star}\)</span> and <span class="math inline">\(z^{\star}\)</span>.
</div>
</div>
<div id="using-r-to-find-zstar" class="section level4 unnumbered">
<h4>Using <code>R</code> to find <span class="math inline">\(z^{\star}\)</span></h4>
<p>Figure <a href="inference-cat.html#fig:choosingZForCI">5.17</a> provides a picture of how to identify <span class="math inline">\(z^{\star}\)</span> based on a confidence level. We select <span class="math inline">\(z^{\star}\)</span> so that the area between -<span class="math inline">\(z^{\star}\)</span> and <span class="math inline">\(z^{\star}\)</span> in the normal model corresponds to the confidence level. In <code>R</code>, you can find <span class="math inline">\(z^{\star}\)</span> using the <code>qnorm()</code> function:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="inference-cat.html#cb20-1" aria-hidden="true"></a><span class="co"># z* for 90% --&gt; alpha = 0.15 --&gt; need 5% on each size:</span></span>
<span id="cb20-2"><a href="inference-cat.html#cb20-2" aria-hidden="true"></a><span class="kw">qnorm</span>(.<span class="dv">90</span> <span class="op">+</span><span class="st"> </span><span class="fl">.05</span>)</span>
<span id="cb20-3"><a href="inference-cat.html#cb20-3" aria-hidden="true"></a><span class="co">#&gt; [1] 1.645</span></span>
<span id="cb20-4"><a href="inference-cat.html#cb20-4" aria-hidden="true"></a></span>
<span id="cb20-5"><a href="inference-cat.html#cb20-5" aria-hidden="true"></a><span class="co"># z* for 95% --&gt; alpha = 0.05 --&gt; need 2.5% on each size:</span></span>
<span id="cb20-6"><a href="inference-cat.html#cb20-6" aria-hidden="true"></a><span class="kw">qnorm</span>(.<span class="dv">95</span> <span class="op">+</span><span class="st"> </span><span class="fl">.025</span>)</span>
<span id="cb20-7"><a href="inference-cat.html#cb20-7" aria-hidden="true"></a><span class="co">#&gt; [1] 1.96</span></span>
<span id="cb20-8"><a href="inference-cat.html#cb20-8" aria-hidden="true"></a></span>
<span id="cb20-9"><a href="inference-cat.html#cb20-9" aria-hidden="true"></a><span class="co"># z* for 99% --&gt; alpha = 0.01 --&gt; need .5% on each size:</span></span>
<span id="cb20-10"><a href="inference-cat.html#cb20-10" aria-hidden="true"></a><span class="kw">qnorm</span>(.<span class="dv">99</span> <span class="op">+</span><span class="st"> </span><span class="fl">.005</span>)</span>
<span id="cb20-11"><a href="inference-cat.html#cb20-11" aria-hidden="true"></a><span class="co">#&gt; [1] 2.576</span></span></code></pre></div>

<div class="guidedpractice">
Previously, we found that implanting a stent in the brain of a patient at risk for a stroke <em>increased</em> the risk of a stroke. The study estimated a 9% increase in the number of patients who had a stroke, and the standard error of this estimate was about <span class="math inline">\(SE = 2.8%\)</span>. Compute a 90% confidence interval for the effect.<a href="#fn103" class="footnote-ref" id="fnref103"><sup>103</sup></a>
</div>
<!--
#### Choosing a sample size when estimating a proportion {-}

\BeginKnitrBlock{onebox}<div class="onebox">**Margin of error.**

In a confidence interval, $z^{\star}\times SE$ is called the **margin of error**\index{margin of error}.</div>\EndKnitrBlock{onebox}


\index{margin of error|(}

When collecting data, we choose a sample size suitable
for the purpose of the study. 
You might agree that the following interval estimate would not be particularly useful: a 95% confidence interval for the proportion of liver transplants with complications is between 0 and 1.
Often times "suitable for the study" means choosing a sample size large
enough that the **margin of error**\index{margin of error} --
which is the part we add and subtract from the point
estimate in a confidence interval --
is sufficiently small that the result is useful.
For example, our task might be to find a sample size
$n$ so that the sample proportion is within $\pm 0.04$
of the actual proportion in a 95% confidence interval.

<!--
% For example, the margin of error for a point estimate using 95% confidence can be written as $1.96\times SE$. We set up a general equation to represent the problem:
%\begin{align*}
%ME = z^{\star} \times SE \leq m
%\end{align*}
%where $ME$ represented the actual margin of error and $z^{\star}$ was chosen to correspond to the confidence level. The standard error formula is specified to correspond to the particular setting. For instance, in the case of means, the standard error was given as $\sigma / \sqrt{n}$. In the case of a single proportion, we use $\sqrt{p(1-p) / n\ }$ for the standard error.


\index{data!Student football stadium|(}

\BeginKnitrBlock{example}<div class="example">A university newspaper is conducting
    a survey to determine what fraction of students
    support a $200 per year increase in fees to pay
    for a new football stadium.
    How big of a sample is required to ensure the
    margin of error is smaller than 0.04 using a
    95% confidence level?

---
      
  The margin of error for a sample proportion is
  \begin{align*}
  z^{\star} \sqrt{\frac{p (1 - p)}{n}}
  \end{align*}
  Our goal is to find the smallest sample size $n$
  so that this margin of error is smaller than $0.04$.
  For a 95% confidence level, the value $z^{\star}$
  corresponds to 1.96:
  \begin{align*}
  1.96\times \sqrt{\frac{p(1-p)}{n}} \ < \ 0.04
  \end{align*}
  There are two unknowns in the equation: $p$ and $n$.
  If we have an estimate of $p$, perhaps from a prior
  survey, we could enter in that value and solve for $n$.
  If we have no such estimate, we must use some other
  value for $p$.
  It turns out that the margin of error is largest
  when $p$ is 0.5, so we typically use this
  \emph{worst case value} if no estimate of the
  proportion is available:
  \begin{align*}
	1.96\times \sqrt{\frac{0.5(1-0.5)}{n}} &\ < \ 0.04 \\
	1.96^2\times \frac{0.5(1-0.5)}{n} &\ < \ 0.04^2 \\
	1.96^2\times \frac{0.5(1-0.5)}{0.04^2} &\ < \ n \\
	600.25 &\ < \  n
  \end{align*}
  We would need over 600.25 participants, which means
  we need 601 participants or more, to ensure the
  sample proportion is within 0.04 of the true proportion
  with 95% confidence.</div>\EndKnitrBlock{example}

\index{data!Student football stadium|)}

When an estimate of the proportion is available, we use it in place of the worst case proportion value, 0.5.


\index{data!Tire failure rate|(}

\BeginKnitrBlock{todo}<div class="todo">not sure why the footnote didn't go into the footnote</div>\EndKnitrBlock{todo}


\BeginKnitrBlock{guidedpractice}<div class="guidedpractice">A manager is about to oversee the mass
production of a new tire model in her factory,
and she would like to estimate what proportion of
these tires will be rejected through quality control.
The quality control team has monitored the last three
tire models produced by the factory,
failing 1.7% of tires in the first model,
6.2% of the second model,
and 1.3% of the third model.
The manager would like to examine enough tires
to estimate the failure rate of the new tire model
to within about 1% with a 90% confidence level.
There are three different failure rates to choose from.
Perform the sample size computation for each separately,
and identify three sample sizes to consider.^[For a 90% confidence interval, $z^{\star} = 1.65$,
  and since an estimate of the proportion 0.017 is available,
  we'll use it in the margin of error formula:
  \begin{align*}
  1.65\times \sqrt{\frac{0.017(1-0.017)}{n}} &\ < \ 0.01
    \qquad\to\qquad
      \frac{0.017(1-0.017)}{n} \ < \ 
          \left(\frac{0.01}{1.65}\right)^2
    \qquad\to\qquad
      454.96 \ < \ n
  \end{align*}
  For sample size calculations, we always round up,
  so the first tire model suggests 455 tires would
  be sufficient.

  A similar computation can be accomplished using 0.062
  and 0.013 for $p$, and you should verify that using these
  proportions results in minimum sample sizes of 1584
  and 350 tires, respectively.]</div>\EndKnitrBlock{guidedpractice}

\BeginKnitrBlock{example}<div class="example">The sample sizes vary widely in the previous
    Guided Practice.
    Which of the three would you suggest using?
    What would influence your choice?

---
      
  We could examine which of the old models is most
  like the new model, then choose the corresponding sample
  size.
  Or if two of the previous estimates are based on small
  samples while the other is based on a larger sample,
  we might consider the value corresponding to the larger
  sample.
  There are also other reasonable approaches.

  Also observe that the success-failure
  condition would need to be checked in the final sample.
  For instance, if we sampled $n = 1584$ tires and found
  a failure rate of 0.5%, the normal approximation would
  not be reasonable, and we would require more advanced
  statistical methods for creating the confidence interval.</div>\EndKnitrBlock{example}

\index{data!Tire failure rate|)}
\index{data!Payday regulation poll|(}

\BeginKnitrBlock{guidedpractice}<div class="guidedpractice">Suppose we want to continually track the support
of payday borrowers for regulation on lenders,
where we would conduct a new poll every month.
Running such frequent polls is expensive, so we decide
a wider margin of error of 5% for each individual survey
would be acceptable.
Based on a previous sample of borrowers where
70% supported some form of regulation,
how big should our monthly sample be for a margin
of error of 0.04 with 95% confidence?^[We complete the same computations as before,
   except now we use $0.70$ instead of $0.5$
   for $p$:
   \begin{align*}
   1.96\times \sqrt{\frac{p(1-p)}{n}}
       \approx 1.96\times
           \sqrt{\frac{0.70(1-0.70)}
               {n}}
       &\leq 0.05
     \qquad\to\qquad
       n \geq 322.7
  \end{align*}
  A sample size of 323 or more would be reasonable.
  (Reminder: always round up for sample size calculations!)
  Given that we plan to track this poll over time,
  we also may want to periodically repeat these calculations
  to ensure that we're being thoughtful in our sample
  size recommendations in case the baseline rate fluctuates.]</div>\EndKnitrBlock{guidedpractice}

\index{data!Payday regulation poll|)}
\index{margin of error|)}

{\input{ch_inference_for_props/TeX/inference_for_a_single_proportion.tex}}
-->
</div>
<div id="violating-conditions" class="section level4 unnumbered">
<h4>Violating conditions</h4>
<p>We’ve spent a lot of time discussing conditions for when
<span class="math inline">\(\hat{p}\)</span> can be reasonably modeled by a normal distribution.
What happens when the success-failure condition fails?
What about when the independence condition fails?
In either case, the general ideas of confidence intervals
and hypothesis tests remain the same, but the strategy
or technique used to generate the interval or p-value
change.</p>
<p>When the success-failure condition isn’t met
for a hypothesis test, we can simulate the null distribution
of <span class="math inline">\(\hat{p}\)</span> using the null value, <span class="math inline">\(p_0\)</span>, as seen in Section <a href="inference-cat.html#one-prop-null-boot">5.3.1</a>. Unfortunately, methods for dealing with observations which are
not independent are outside the scope of this book.</p>

<div class="todo">
Add tappers and listeners case study example from old Ch. 5 here.
</div>
</div>
</div>
</div>
<div id="diff-two-prop" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Difference of two proportions</h2>

<div class="onebox">
<p><strong>Notation.</strong></p>
<ul>
<li><span class="math inline">\(n_1\)</span>, <span class="math inline">\(n_2\)</span> = sample sizes of two independent samples</li>
<li><span class="math inline">\(\hat{p}_1\)</span>, <span class="math inline">\(\hat{p}_2\)</span> = sample proportions of two independent samples</li>
<li><span class="math inline">\(\pi_1\)</span>, <span class="math inline">\(\pi_2\)</span> = population proportions of two independent samples
</div></li>
</ul>
<p>We now extend the methods from Section <a href="inference-cat.html#single-prop">5.3</a> to apply confidence intervals and hypothesis tests to differences in population proportions that come from two groups: <span class="math inline">\(\pi_1 - \pi_2\)</span>.</p>
<p>In our investigations, we’ll identify a reasonable
point estimate of <span class="math inline">\(\pi_1 - \pi_2\)</span> based on the sample,
and you may have already guessed its form:
<span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span>.

We’ll look at statistical inference for a difference in proportions
in two ways: simulation-based methods through a randomization test
and bootstrap confidence interval, and theory-based methods through
a two sample <span class="math inline">\(z\)</span>-test and <span class="math inline">\(z\)</span>-interval.</p>
<div id="two-prop-errors" class="section level3" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> Randomization test for <span class="math inline">\(H_0: \pi_1 - \pi_2 = 0\)</span></h3>
<p>As you learned in Chapter <a href="intro-to-data.html#intro-to-data">1</a>, a <strong>randomized experiment</strong> is done to assess whether or not one variable (the <strong>explanatory</strong> variable) causes changes in a second variable (the <strong>response</strong> variable).
Every data set has some variability in it, so to decide whether the variability in the data is due to (1) the causal mechanism (the randomized explanatory variable in the experiment) or instead (2) natural variability inherent to the data, we set up a sham randomized experiment as a comparison.
That is, we assume that each observational unit would have gotten the exact same response value regardless of the treatment level.
By reassigning the treatments many many times, we can compare the actual experiment to the sham experiment. If the actual experiment has more extreme results than any of the sham experiments, we are led to believe that it is the explanatory variable which is causing the result and not inherent data variability.
Using a few different studies, let’s look more carefully at this idea of a <strong>randomization test</strong>.</p>
<div id="caseStudyGenderDiscrimination" class="section level4" number="5.4.1.1">
<h4><span class="header-section-number">5.4.1.1</span> Gender discrimination</h4>
<p></p>
<p>We consider a study investigating gender discrimination in the 1970s, which is set in the context of personnel decisions within a bank.<a href="#fn104" class="footnote-ref" id="fnref104"><sup>104</sup></a> The research question we hope to answer is, “Are females discriminated against in promotion decisions made by male managers?”</p>
</div>
<div id="observed-data" class="section level4 unnumbered">
<h4>Observed data</h4>
<p>The participants in this study were 48 male bank supervisors attending a management institute at the University of North Carolina in 1972.
They were asked to assume the role of the personnel director of a bank and were given a personnel file to judge whether the person should be promoted to a branch manager position.
The files given to the participants were identical, except that half of them indicated the candidate was male and the other half indicated the candidate was female.
These files were randomly assigned to the subjects.</p>

<div class="guidedpractice">
Is this an observational study or an experiment? How does the type of study impact what can be inferred from the results?<a href="#fn105" class="footnote-ref" id="fnref105"><sup>105</sup></a>
</div>
<p>For each supervisor we recorded the gender associated with the assigned file and the promotion decision.
Using the results of the study summarized in Table <a href="inference-cat.html#tab:discriminationResults">5.2</a>, we would like to evaluate if females are unfairly discriminated against in promotion decisions.
In this study, a smaller proportion of females are promoted than males (0.583 versus 0.875), but it is unclear whether the difference provides <em>convincing evidence</em> that females are unfairly discriminated against.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:discriminationResults">Table 5.2: </span>Summary results for the gender discrimination study.
</caption>
<thead>
<tr>
<th style="border-bottom:hidden" colspan="1">
</th>
<th style="border-bottom:hidden" colspan="1">
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
<code>gender</code>
</div>
</th>
<th style="border-bottom:hidden" colspan="1">
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
male
</th>
<th style="text-align:left;">
female
</th>
<th style="text-align:left;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
promoted
</td>
<td style="text-align:left;">
21
</td>
<td style="text-align:left;">
14
</td>
<td style="text-align:left;">
35
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>decision</code>
</td>
<td style="text-align:left;">
not promoted
</td>
<td style="text-align:left;">
3
</td>
<td style="text-align:left;">
10
</td>
<td style="text-align:left;">
13
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
24
</td>
<td style="text-align:left;">
24
</td>
<td style="text-align:left;">
48
</td>
</tr>
</tbody>
</table>
<p>The data are visualized in Figure <a href="inference-cat.html#fig:genderrand1">5.18</a>. Note that the promoted decision is colored in red (promoted) and white(not promoted). Additionally, the observations are broken up into the male and female groups.</p>
<div class="figure" style="text-align: center"><span id="fig:genderrand1"></span>
<img src="05/figures/genderrand1b.png" alt="The gender descrimination study can be thought of as 48 red and black cards." width="50%" />
<p class="caption">
Figure 5.18: The gender descrimination study can be thought of as 48 red and black cards.
</p>
</div>

<div class="example">
<p>Statisticians are sometimes called upon to evaluate the strength of evidence.
When looking at the rates of promotion for males and females in this study, why might we be tempted to immediately conclude that females are being discriminated against?</p>
<hr />
<p>The large difference in promotion rates (58.3% for females versus 87.5% for males) suggest there might be discrimination against women in promotion decisions.
However, we cannot yet be sure if the observed difference represents discrimination or is just from random chance.
Generally there is a little bit of fluctuation in sample data, and we wouldn’t expect the sample proportions to be <em>exactly</em> equal, even if the truth was that the promotion decisions were independent of gender.</p>
Additionally, the researchers used a <strong>convenience sample</strong>—48 male bank supervisors attending a management institute—so we will need to think carefully about to which population we can generalize these results.
</div>
<p>The previous example is a reminder that the observed outcomes in the sample may not perfectly reflect the true relationships between variables in the underlying population.
Table <a href="inference-cat.html#tab:discriminationResults">5.2</a> shows there were 7 fewer promotions in the female group than in the male group, a difference in promotion rates of 29.2%:
<span class="math display">\[
\hat{p}_M - \hat{p}_F = \frac{21}{24} - \frac{14}{24} = 0.292. 
\]</span>
This point estimate of the true difference is large, but the sample size for the study is small, making it unclear if this observed difference represents discrimination or whether it is simply due to chance.
These two competing claims are our null and alternative hypotheses:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: <strong>Null hypothesis</strong>. The variables <code>gender</code> and <code>decision</code> are independent. They have no relationship, and the observed difference between the proportion of males and females who were promoted, 29.2%, was due to chance.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: <strong>Alternative hypothesis</strong>. The variables <code>gender</code> and <code>decision</code> are <em>not</em> independent. The difference in promotion rates of 29.2% was not due to chance, and equally qualified females are less likely to be promoted than males.</p></li>
</ul>
<p>In statistical notation:</p>
<ul>
<li><p><span class="math inline">\(H_0: \pi_M - \pi_F = 0\)</span></p></li>
<li><p><span class="math inline">\(H_A: \pi_M - \pi_F &gt; 0\)</span></p></li>
</ul>
<p>What would it mean if the null hypothesis, which says the variables <code>gender</code> and <code>decision</code> are unrelated, is true?
It would mean each banker would decide whether to promote the candidate without regard to the gender indicated on the file.
That is, the difference in the promotion percentages would be due to the way the files were randomly divided to the bankers, and the randomization just happened to give rise to a relatively large difference of 29.2%.</p>
<p>Consider the alternative hypothesis: bankers were influenced by which gender was listed on the personnel file.
If this was true, and especially if this influence was substantial, we would expect to see some difference in the promotion rates of male and female candidates.
If this gender bias was against females, we would expect a smaller fraction of promotion recommendations for female personnel files relative to the male files.</p>
<p>We will choose between these two competing claims by assessing if the data conflict so much with <span class="math inline">\(H_0\)</span> that the null hypothesis cannot be deemed reasonable.
If this is the case, and the data support <span class="math inline">\(H_A\)</span>, then we will reject the notion of independence and conclude that these data provide strong evidence of discrimination.</p>
</div>
<div id="variability-of-the-statistic" class="section level4 unnumbered">
<h4>Variability of the statistic</h4>
<p>Table <a href="inference-cat.html#tab:discriminationResults">5.2</a> shows that 35 bank supervisors recommended promotion and 13 did not.
Now, suppose the bankers’ decisions were independent of gender.
Then, if we conducted the experiment again with a different random assignment of gender to the files, differences in promotion rates would be based only on random fluctuation.
We can actually perform this <strong>randomization</strong>, which simulates what would have happened if the bankers’ decisions had been independent of gender but we had distributed the file genders differently.<a href="#fn106" class="footnote-ref" id="fnref106"><sup>106</sup></a></p>
<p>In this <strong>simulation</strong>, we thoroughly shuffle 48 personnel files, 35 labeled <code>promoted</code> and 13 labeled <code>not promoted</code>, and we deal these files into two stacks.
Note that by keeping 35 promoted and 13 not promoted, we are assuming that 35 of the bank managers would have promoted the individual whose content is contained in the file (<strong>independent</strong> of gender).
We will deal 24 files into the first stack, which will represent the 24 “female” files.
The second stack will also have 24 files, and it will represent the 24 “male” files.
Figure <a href="inference-cat.html#fig:genderrand3">5.19</a> highlights both the shuffle and the reallocation to the sham gender groups.</p>
<div class="figure" style="text-align: center"><span id="fig:genderrand3"></span>
<img src="05/figures/genderrand3b.png" alt="The gender descrimination data is shuffled and reallocated to the gender groups." width="80%" />
<p class="caption">
Figure 5.19: The gender descrimination data is shuffled and reallocated to the gender groups.
</p>
</div>
<p>Then, as we did with the original data, we tabulate the results and determine the fraction of <code>male</code> and <code>female</code> who were promoted.</p>
<p>Since the randomization of files in this simulation is independent of the promotion decisions, any difference in the two promotion rates is entirely due to chance.
Table <a href="inference-cat.html#tab:discriminationRand1">5.3</a> show the results of one such simulation.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:discriminationRand1">Table 5.3: </span>Simulation results, where the difference in promotion rates between <code>male</code> and <code>female</code> is purely due to chance.
</caption>
<thead>
<tr>
<th style="border-bottom:hidden" colspan="1">
</th>
<th style="border-bottom:hidden" colspan="1">
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
<code>gender</code>
</div>
</th>
<th style="border-bottom:hidden" colspan="1">
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
male
</th>
<th style="text-align:left;">
female
</th>
<th style="text-align:left;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
promoted
</td>
<td style="text-align:left;">
18
</td>
<td style="text-align:left;">
17
</td>
<td style="text-align:left;">
35
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>decision</code>
</td>
<td style="text-align:left;">
not promoted
</td>
<td style="text-align:left;">
6
</td>
<td style="text-align:left;">
7
</td>
<td style="text-align:left;">
13
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
24
</td>
<td style="text-align:left;">
24
</td>
<td style="text-align:left;">
48
</td>
</tr>
</tbody>
</table>

<div class="guidedpractice">
What is the difference in promotion rates between the two simulated groups in Table <a href="inference-cat.html#tab:discriminationRand1">5.3</a> ?
How does this compare to the observed difference 29.2% from the actual study?<a href="#fn107" class="footnote-ref" id="fnref107"><sup>107</sup></a>
</div>
<p>Figure <a href="inference-cat.html#fig:genderrand4">5.20</a> shows that the difference in promotion rates is much larger in the original data than it is in the simulated groups (0.292 &gt;&gt;&gt; 0.042).
The quantity of interest throughout this case study has been the difference in promotion rates.
This summary value is the <strong>statistic</strong> of interest (or often the <strong>test statistic</strong>).</p>
<div class="figure" style="text-align: center"><span id="fig:genderrand4"></span>
<img src="05/figures/genderrand4c.png" alt="We summarize the randomized data to produce one estimate of the difference in proportions given no gender discrimination." width="100%" />
<p class="caption">
Figure 5.20: We summarize the randomized data to produce one estimate of the difference in proportions given no gender discrimination.
</p>
</div>
</div>
<div id="observed-statistic-vs.-null-statistics" class="section level4 unnumbered">
<h4>Observed statistic vs. null statistics</h4>
<p>We computed one possible sample difference in proportions under the null hypothesis in the Guided Practice above, which represents one difference due to chance.
While in this first simulation, we physically dealt out files, it is much more efficient to perform this simulation using a computer.
Repeating the simulation on a computer, we get another difference due to chance: -0.042.
And another: 0.208.
And so on until we repeat the simulation enough times that we have a good idea of what represents the <em>distribution of differences in sample proportions from chance alone</em>.
Figure <a href="inference-cat.html#fig:discRandDotPlot">5.21</a> shows a plot of the differences found from 100 simulations, where each dot represents a simulated difference between the proportions of male and female files recommended for promotion.</p>
<div class="figure" style="text-align: center"><span id="fig:discRandDotPlot"></span>
<img src="05-inference-cat_files/figure-html/discRandDotPlot-1.png" alt="A dot plot of differences from 100 simulations produced under the null hypothesis, $H_0$, where `gender_simulated` and `decision` are independent. Two of the 100 simulations had a difference of at least 29.2%, the difference observed in the study, and are shown as solid red dots." width="70%" />
<p class="caption">
Figure 5.21: A dot plot of differences from 100 simulations produced under the null hypothesis, <span class="math inline">\(H_0\)</span>, where <code>gender_simulated</code> and <code>decision</code> are independent. Two of the 100 simulations had a difference of at least 29.2%, the difference observed in the study, and are shown as solid red dots.
</p>
</div>
<p>Note that the distribution of these simulated differences in proportions is centered around 0.
Because we simulated differences in a way that made no distinction between men and women, this makes sense: we should expect differences from chance alone to fall around zero with some random fluctuation for each simulation.</p>

<div class="example">
<p>How often would you observe a difference of at least 29.2% (0.292) according to Figure <a href="inference-cat.html#fig:discRandDotPlot">5.21</a>?
Often, sometimes, rarely, or never?</p>
<hr />
It appears that a difference of at least 29.2% due to chance alone would only happen about 2% of the time according to Figure <a href="inference-cat.html#fig:discRandDotPlot">5.21</a>.
Such a low probability indicates that observing such a large difference from chance is rare.
</div>
<p>The difference of 29.2% is a rare event if there really is no impact from listing gender in the candidates’ files, which provides us with two possible interpretations of the study results:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: <strong>Null hypothesis</strong>. Gender has no effect on promotion decision, and we observed a difference that is so large that it would only happen rarely.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: <strong>Alternative hypothesis</strong>. Gender has an effect on promotion decision, and what we observed was actually due to equally qualified women being discriminated against in promotion decisions, which explains the large difference of 29.2%.</p></li>
</ul>
<p>When we conduct formal studies, we reject a null position (the idea that the data are a result of chance only) if the data strongly conflict with that null position.<a href="#fn108" class="footnote-ref" id="fnref108"><sup>108</sup></a>
In our analysis, we determined that there was only a <span class="math inline">\(\approx\)</span> 2% probability of obtaining a sample where <span class="math inline">\(\geq\)</span> 29.2% more males than females get promoted by chance alone, so we conclude that the data provide strong evidence of gender discrimination against women by the supervisors.</p>

<div class="guidedpractice">
What statistical term is given to the 2% probability of obtaining a sample where <span class="math inline">\(\geq\)</span> 29.2% more males than females get promoted by chance alone?<a href="#fn109" class="footnote-ref" id="fnref109"><sup>109</sup></a>
</div>
</div>
<div id="scope-of-inference" class="section level4 unnumbered">
<h4>Scope of inference</h4>
<p>Since the study was a randomized experiment, we can conclude that the effect was due to gender discrimination—the gender of the application <em>caused</em> the lower rate of promotion. However, since this study was a convenience sample, we can only generalize this result to individuals similar to those in the study. Thus, we have evidence of gender discrimination, but only among male bank supervisors attending a management institute at the University of North Carolina in 1972 that are similar to those in the study.</p>
<p></p>
</div>
<div id="caseStudyOpportunityCost" class="section level4" number="5.4.1.2">
<h4><span class="header-section-number">5.4.1.2</span> Opportunity cost</h4>
<p>How rational and consistent is the behavior of the typical American college student?
In this section, we’ll explore whether college student consumers always consider the following: money not spent now can be spent later.</p>
<p>In particular, we are interested in whether reminding students about this well-known fact about money causes them to be a little thriftier.
A skeptic might think that such a reminder would have no impact.
We can summarize the two different perspectives using the null and alternative hypothesis framework.</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: <strong>Null hypothesis</strong>. Reminding students that they can save money for later purchases will not have any impact on students’ spending decisions.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: <strong>Alternative hypothesis</strong>. Reminding students that they can save money for later purchases will reduce the chance they will continue with a purchase.</p></li>
</ul>

<div class="guidedpractice">
How could you design a randomized experiment to test these two hypotheses?<a href="#fn110" class="footnote-ref" id="fnref110"><sup>110</sup></a>
</div>
<p>In statistical notation, we can define parameters <span class="math inline">\(\pi_{ctrl}\)</span> = the probability a student under a control condition (not reminding them that they can save money for later purchases) refrains from making a purchase, and <span class="math inline">\(\pi_{trmt}\)</span> = the probability a student under a treatment condition (reminding them that they can save money for later purchases) refrains from makes a purchase. Our hypotheses are then</p>
<ul>
<li><p><span class="math inline">\(H_0: \pi_{trmt} - \pi_{ctrl} = 0\)</span></p></li>
<li><p><span class="math inline">\(H_A: \pi_{trmt} - \pi_{ctrl} &gt; 0\)</span></p></li>
</ul>
<p>In this section, we’ll explore an experiment conducted by researchers that investigates this very question for students at a university in the southwestern United States.<a href="#fn111" class="footnote-ref" id="fnref111"><sup>111</sup></a></p>
</div>
<div id="observed-data-1" class="section level4 unnumbered">
<h4>Observed data</h4>
<!--Shane Frederick of Yale School of Management and his collaborators conducted an experiment exploring the rational behavior of consumers. 

% Suppose when a person is about to spend money, we simply reminded them that they could spend the money on something else. Would it have any impact on the likelihood that they would continue with the purchase?
%What would you do in this situation? Please circle one of the options below.

-->
<p>One-hundred and fifty students were recruited for the study, and each was given the following statement:</p>
<blockquote>
<p>Imagine that you have been saving some extra money on the side to make some purchases, and on your most recent visit to the video store you come across a special sale on a new video. This video is one with your favorite actor or actress, and your favorite type of movie (such as a comedy, drama, thriller, etc.). This particular video that you are considering is one you have been thinking about buying for a long time. It is available for a special sale price of $14.99.</p>
</blockquote>
<blockquote>
<p>What would you do in this situation? Please circle one of the options below.</p>
</blockquote>
<p>Half of the 150 students were randomized into a control group and were given the following two options:</p>
<blockquote>
<ol style="list-style-type: upper-alpha">
<li>Buy this entertaining video.</li>
</ol>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: upper-alpha">
<li>Not buy this entertaining video.</li>
</ol>
</blockquote>
<p>The remaining 75 students were placed in the treatment group, and they saw a slightly modified option (B):</p>
<blockquote>
<ol style="list-style-type: upper-alpha">
<li>Buy this entertaining video.</li>
</ol>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: upper-alpha">
<li>Not buy this entertaining video. Keep the $14.99 for other purchases.</li>
</ol>
</blockquote>
<p>Would the extra statement reminding students of an obvious fact impact the purchasing decision?
Table <a href="inference-cat.html#tab:OpportunityCostTable">5.4</a> summarizes the study results.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:OpportunityCostTable">Table 5.4: </span>Summary of student choices in the opportunity cost study.
</caption>
<thead>
<tr>
<th style="text-align:left;">
variable
</th>
<th style="text-align:left;">
col1
</th>
<th style="text-align:left;">
col2
</th>
<th style="text-align:left;">
col3
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
control group
</td>
<td style="text-align:left;">
treatment group
</td>
<td style="text-align:left;">
Total
</td>
</tr>
<tr>
<td style="text-align:left;">
buy DVD
</td>
<td style="text-align:left;">
56
</td>
<td style="text-align:left;">
19
</td>
<td style="text-align:left;">
75
</td>
</tr>
<tr>
<td style="text-align:left;">
not buy DVD
</td>
<td style="text-align:left;">
41
</td>
<td style="text-align:left;">
34
</td>
<td style="text-align:left;">
75
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
97
</td>
<td style="text-align:left;">
53
</td>
<td style="text-align:left;">
150
</td>
</tr>
</tbody>
</table>
<!--
%150 participants were asked whether they would buy a DVD under a particular circumstance. Participants in the control group were given two options, and participants in the treatment group were given the same options, except in the *not buy* option they were reminded that not spending the money meant the money could be used for a later purchase. This table summarizes the results from the study.}
-->
<p>It might be a little easier to review the results using row proportions, specifically considering the proportion of participants in each group who said they would buy or not buy the DVD.
These summaries are given in Table <a href="inference-cat.html#tab:OpportunityCostTableRowProp">5.5</a>, and a segmented bar plot is provided in Figure <a href="inference-cat.html#fig:OpportunityCostBarplot">5.22</a>.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:OpportunityCostTableRowProp">Table 5.5: </span>The data above are now summarized using column proportions. Column proportions are particularly useful here since we can view the proportion of <em>buy</em> and <em>not buy</em> decisions in each group, and across the whole sample.
</caption>
<thead>
<tr>
<th style="text-align:left;">
variable
</th>
<th style="text-align:left;">
col1
</th>
<th style="text-align:left;">
col2
</th>
<th style="text-align:left;">
col3
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
control group
</td>
<td style="text-align:left;">
treatment group
</td>
<td style="text-align:left;">
Total
</td>
</tr>
<tr>
<td style="text-align:left;">
buy DVD
</td>
<td style="text-align:left;">
0.747
</td>
<td style="text-align:left;">
0.547
</td>
<td style="text-align:left;">
0.647
</td>
</tr>
<tr>
<td style="text-align:left;">
not buy DVD
</td>
<td style="text-align:left;">
0.253
</td>
<td style="text-align:left;">
0.453
</td>
<td style="text-align:left;">
0.353
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
1.00
</td>
<td style="text-align:left;">
1.00
</td>
<td style="text-align:left;">
1.00
</td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span id="fig:OpportunityCostBarplot"></span>
<img src="05-inference-cat_files/figure-html/OpportunityCostBarplot-1.png" alt="Segmented bar plot comparing the proportion who bought and did not buy the DVD between the control and treatment groups." width="70%" />
<p class="caption">
Figure 5.22: Segmented bar plot comparing the proportion who bought and did not buy the DVD between the control and treatment groups.
</p>
</div>
<p>We will define a <strong>success</strong> in this study as a student who chooses not to buy the DVD.<a href="#fn112" class="footnote-ref" id="fnref112"><sup>112</sup></a>
Then, the value of interest is the change in DVD purchase rates that results by reminding students that not spending money now means they can spend the money later.</p>
<!--
%A first look at the data suggests that reminding students that not spending money means they can spend the money later has an impact. 
-->
<p>We can construct a point estimate for this difference as
<span class="math display">\[\begin{align*}
\hat{p}_{trmt} - \hat{p}_{ctrl}
 = \frac{34}{75} - \frac{19}{75}
 = 0.453 - 0.253
 = 0.200
\end{align*}\]</span>
The proportion of students who chose not to buy the DVD was 20% higher in the treatment group than the control group.
However, is this result <strong>statistically significant</strong>? In other words, is a 20% difference between the two groups so prominent that it is unlikely to have occurred from chance alone?</p>
</div>
<div id="variability-of-the-statistic-1" class="section level4 unnumbered">
<h4>Variability of the statistic</h4>
<p>The primary goal in this data analysis is to understand what sort of differences we might see if the null hypothesis were true, i.e., the treatment had no effect on students.
For this, we’ll use the same procedure we applied in Section <a href="inference-cat.html#caseStudyGenderDiscrimination">5.4.1.1</a>: randomization.</p>
<p>Let’s think about the data in the context of the hypotheses.
If the null hypothesis (<span class="math inline">\(H_0\)</span>) was true and the treatment had no impact on student decisions, then the observed difference between the two groups of 20% could be attributed entirely to chance.
If, on the other hand, the alternative hypothesis (<span class="math inline">\(H_A\)</span>) is true, then the difference indicates that reminding students about saving for later purchases actually impacts their buying decisions.</p>
</div>
<div id="observed-statistic-vs.-null-statistics-1" class="section level4 unnumbered">
<h4>Observed statistic vs. null statistics</h4>
<p>Just like with the gender discrimination study, we can perform a statistical analysis.
Using the same randomization technique from the last section, let’s see what happens when we simulate the experiment under the scenario where there is no effect from the treatment.</p>
<p>While we would in reality do this simulation on a computer, it might be useful to think about how we would go about carrying out the simulation without a computer.
We start with 150 index cards and label each card to indicate the distribution of our response variable: <code>decision</code>.
That is, 53 cards will be labeled “not buy DVD” to represent the 53 students who opted not to buy, and 97 will be labeled “buy DVD” for the other 97 students.
Then we shuffle these cards thoroughly and divide them into two stacks of size 75, representing the simulated treatment and control groups.
Any observed difference between the proportions of “not buy DVD” cards (what we earlier defined as <em>success</em>) can be attributed entirely to chance.</p>

<div class="example">
<p>If we are randomly assigning the cards into the simulated treatment and control groups, how many “not buy DVD” cards would we expect to end up with in each simulated group?
What would be the expected difference between the proportions of “not buy DVD” cards in each group?</p>
<hr />
Since the simulated groups are of equal size, we would expect <span class="math inline">\(53 / 2 = 26.5\)</span>, i.e., 26 or 27, “not buy DVD” cards in each simulated group, yielding a simulated point estimate of 0% . However, due to random fluctuations, we might actually observe a number a little above or below 26 and 27.
</div>
<!--
%We'll take the students and randomize them into two new groups, simulated-control and simulated-treatment groups, and then we'll look at the difference in the two groups. 
-->
<p>The results of a single randomization from chance alone is shown in Table <a href="inference-cat.html#tab:OpportunityCostTableSimulated">5.6</a>.
From this table, we can compute a difference that occurred from chance alone:
<span class="math display">\[\begin{align*}
\hat{p}_{trmt, simulated} - \hat{p}_{ctrl, simulated}
 = \frac{24}{75} - \frac{29}{75}
 = 0.32 - 0.387
 = - 0.067
\end{align*}\]</span>
<!--
%This difference of -6.7% is entirely due to chance.
--></p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:OpportunityCostTableSimulated">Table 5.6: </span>Summary of student choices against their simulated groups. The group assignment had no connection to the student decisions, so any difference between the two groups is due to chance.
</caption>
<thead>
<tr>
<th style="text-align:left;">
variable
</th>
<th style="text-align:left;">
col1
</th>
<th style="text-align:left;">
col2
</th>
<th style="text-align:left;">
col3
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
control group
</td>
<td style="text-align:left;">
treatment group
</td>
<td style="text-align:left;">
Total
</td>
</tr>
<tr>
<td style="text-align:left;">
buy DVD
</td>
<td style="text-align:left;">
46
</td>
<td style="text-align:left;">
51
</td>
<td style="text-align:left;">
97
</td>
</tr>
<tr>
<td style="text-align:left;">
not buy DVD
</td>
<td style="text-align:left;">
29
</td>
<td style="text-align:left;">
24
</td>
<td style="text-align:left;">
53
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
75
</td>
<td style="text-align:left;">
75
</td>
<td style="text-align:left;">
150
</td>
</tr>
</tbody>
</table>
<p>Just one simulation will not be enough to get a sense of what sorts of differences would happen from chance alone.
We’ll simulate another set of simulated groups and compute the new difference: 0.013.
And again: 0.067.
And again: -0.173.
We’ll do this 1,000 times. The results are summarized in a dot plot in Figure <a href="inference-cat.html#fig:OpportunityCostDiffsDotPlot">5.23</a>, where each point represents a simulation.
Since there are so many points, it is more convenient to summarize the results in a histogram such as the one in Figure <a href="inference-cat.html#fig:OpportunityCostDiffs">5.24</a>, where the height of each histogram bar represents the fraction of observations in that group.</p>
<div class="figure" style="text-align: center"><span id="fig:OpportunityCostDiffsDotPlot"></span>
<img src="05-inference-cat_files/figure-html/OpportunityCostDiffsDotPlot-1.png" alt="A stacked dot plot of 1,000 chance differences produced under the null hypothesis, $H_0$. Six of the 1,000 simulations had a difference of at least 20% , which was the difference observed in the study." width="70%" />
<p class="caption">
Figure 5.23: A stacked dot plot of 1,000 chance differences produced under the null hypothesis, <span class="math inline">\(H_0\)</span>. Six of the 1,000 simulations had a difference of at least 20% , which was the difference observed in the study.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:OpportunityCostDiffs"></span>
<img src="05-inference-cat_files/figure-html/OpportunityCostDiffs-1.png" alt="A histogram of 1,000 chance differences produced under the null hypothesis, $H_0$. Histograms like this one are a more convenient representation of data or results when there are a large number of observations." width="70%" />
<p class="caption">
Figure 5.24: A histogram of 1,000 chance differences produced under the null hypothesis, <span class="math inline">\(H_0\)</span>. Histograms like this one are a more convenient representation of data or results when there are a large number of observations.
</p>
</div>
<p>If there was no treatment effect, then we’d only observe a difference of at least +20% about 0.6% of the time, or about 1-in-150 times.
That is really rare!
Instead, we will conclude the data provide strong evidence there is a treatment effect: reminding students before a purchase that they could instead spend the money later on something else lowers the chance that they will continue with the purchase.
Notice that we are able to make a causal statement for this study since the study is an experiment.</p>
</div>
<div id="scope-of-inference-1" class="section level4 unnumbered">
<h4>Scope of inference</h4>
<p>Since the study was a randomized experiment, we can conclude that the effect was due to the reminder about saving money for other purchases—the reminder <em>caused</em> the lower rate of purchase. However, since this study used a volunteer sample (students were “recruited”), we can only generalize this result to individuals similar to those in the study. Thus, we have evidence that reminding students that they can save money for later purchases will reduce the chance they will continue with a purchase, but only among students are similar to those in the study.</p>
</div>
<div id="caseStudyMalaria" class="section level4" number="5.4.1.3">
<h4><span class="header-section-number">5.4.1.3</span> Malaria vaccine</h4>
</div>
<div id="observed-data-2" class="section level4 unnumbered">
<h4>Observed data</h4>
<p>We consider a study on a new malaria vaccine
called PfSPZ.
In this study, volunteer patients were randomized
into one of two experiment groups:
14 patients received an experimental vaccine
and 6 patients received a placebo vaccine.
Nineteen weeks later, all 20 patients were exposed
to a drug-sensitive malaria virus strain;
the motivation of using a drug-sensitive strain
of virus here is for ethical considerations,
allowing any infections to be treated effectively.
The results are summarized in
Table <a href="inference-cat.html#tab:malaria-vaccine-20-exp-summary">5.7</a>,
where 9 of the 14 treatment patients remained free
of signs of infection while all of the 6 patients
in the control group patients showed some baseline
signs of infection.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:malaria-vaccine-20-exp-summary">Table 5.7: </span>Summary results for the malaria vaccine experiment.
</caption>
<thead>
<tr>
<th style="border-bottom:hidden" colspan="1">
</th>
<th style="border-bottom:hidden" colspan="1">
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
<code>treatment</code>
</div>
</th>
<th style="border-bottom:hidden" colspan="1">
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
vaccine
</th>
<th style="text-align:left;">
placebo
</th>
<th style="text-align:left;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
infection
</td>
<td style="text-align:left;">
5
</td>
<td style="text-align:left;">
6
</td>
<td style="text-align:left;">
11
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>no infection</code>
</td>
<td style="text-align:left;">
placebo
</td>
<td style="text-align:left;">
9
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
9
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
14
</td>
<td style="text-align:left;">
6
</td>
<td style="text-align:left;">
20
</td>
</tr>
</tbody>
</table>

<div class="guidedpractice">
Is this an observational study or an experiment?
What implications does the study type have on what can
be inferred from the results?<a href="#fn113" class="footnote-ref" id="fnref113"><sup>113</sup></a>
</div>
<p>In this study, a smaller proportion of patients
who received the vaccine showed signs of an infection
(35.7% versus 100%).
However, the sample is very small,
and it is unclear whether the difference provides
<em>convincing evidence</em> that the vaccine is
effective. To determine this, we need to perform
statistical inference.</p>
<p>Instead of using the <em>difference in proportion</em> infected as our summary measure,
let’s use the <em>relative risk</em> of infection for this case study. Thus,
the parameter of interest is <span class="math inline">\(\pi_{Vac} / \pi_{Pla}\)</span>, and our point estimate
of this parameter is</p>
<p><span class="math display">\[
\frac{\hat{p}_{Vac}}{\hat{p}_{Pla}} = \frac{5/14}{6/6} = 0.357.
\]</span></p>
<p>Converting this to a percent decrease<a href="#fn114" class="footnote-ref" id="fnref114"><sup>114</sup></a>, we
see that the patients in the vaccine group had a 64.3% reduced risk of infection
compared to the placebo group.<a href="#fn115" class="footnote-ref" id="fnref115"><sup>115</sup></a></p>
<p>In terms of relative risk, our null and alternative hypotheses are</p>
<p><strong>Independence model</strong> <span class="math inline">\(H_0: \dfrac{\pi_{Vac}}{\pi_{Pla}} = 1\)</span>
<br></p>
<p><strong>Alternative model</strong> <span class="math inline">\(H_a: \dfrac{\pi_{Vac}}{\pi_{Pla}} &lt; 1\)</span></p>

<div class="tipbox">
Whether we write our hypotheses in terms of a difference in proportions or
a ratio of proportions (relative risk), the hypotheses still have the same interpretation.
For example, the three null hypotheses <span class="math inline">\(H_0: \pi_{Vac} = \pi_{Pla}\)</span>, <span class="math inline">\(H_0: \pi_{Vac} - \pi_{Pla} = 0\)</span>, and <span class="math inline">\(H_0: \pi_{Vac}/\pi_{Pla} = 1\)</span>, are all algebraicly equivalent.
</div>
<p>What would it mean if the independence model,
which says the vaccine had no influence on the
rate of infection, is true?
It would mean 11 patients were going to
develop an infection <em>no matter which group
they were randomized into</em>,
and 9 patients would not develop an infection
<em>no matter which group they were randomized
into</em>.
That is, if the vaccine did not affect the rate
of infection, the difference in the infection rates
was due to chance alone in how the patients were
randomized.</p>
<p>Now consider the alternative model:
infection rates were influenced by whether a patient
received the vaccine or not.
If this was true, and especially if this influence
was substantial, we would expect to see some difference
in the infection rates of patients in the groups.</p>
<p>We choose between these two competing claims
by assessing if the data conflict so much with
<span class="math inline">\(H_0\)</span> that the independence model cannot be deemed
reasonable.
If this is the case, and the data support <span class="math inline">\(H_A\)</span>,
then we will reject the notion of independence
and conclude the vaccine is effective.</p>
</div>
<div id="variability-of-the-statistic-2" class="section level4 unnumbered">
<h4>Variability of the statistic</h4>
<p>We’re going to implement simulation,
where we will pretend we know that the malaria
vaccine being tested does  work.
Ultimately, we want to understand if the large
difference we observed is common in these
simulations.
If it is common, then maybe the difference
we observed was purely due to chance.
If it is very uncommon, then the possibility
that the vaccine was helpful seems more plausible.</p>
<p>We can again randomize the responses (<code>infection</code> or <code>no infection</code>) to the treatment conditions under the null hypothesis of independence, but this time, we’ll compute sample relative risks with each simulated sample.</p>

<div class="guidedpractice">
How could you use cards to re-randomize one sample into groups? Remember, in this hypothetical world, we believe each patient that got an infection was going to get it regardless of which group they were in, and we would like to see what happens if we randomly assign these patients to the treatment
and control groups again.<a href="#fn116" class="footnote-ref" id="fnref116"><sup>116</sup></a>
</div>
<p>Figure <a href="inference-cat.html#fig:malaria-rand-dot-plot">5.25</a> shows a histogram
of the relative risks found from 1,000 randomization simulations,
where each dot represents a simulated relative risk of infection (treatment rate divided by control rate).</p>
<div class="figure" style="text-align: center"><span id="fig:malaria-rand-dot-plot"></span>
<img src="05-inference-cat_files/figure-html/malaria-rand-dot-plot-1.png" alt="A histogram of relative risks of infection from 1,000 simulations produced under the independence model $H_0$, where in these simulations infections are unaffected by the vaccine. Seventeen of the 1,000 simulations (shaded in red) had a relative risk of at most 0.357, the relative risk observed in the study." width="70%" />
<p class="caption">
Figure 5.25: A histogram of relative risks of infection from 1,000 simulations produced under the independence model <span class="math inline">\(H_0\)</span>, where in these simulations infections are unaffected by the vaccine. Seventeen of the 1,000 simulations (shaded in red) had a relative risk of at most 0.357, the relative risk observed in the study.
</p>
</div>
</div>
<div id="observed-statistic-vs-null-statistics" class="section level4 unnumbered">
<h4>Observed statistic vs null statistics</h4>
<p>Note that the distribution of these simulated differences
is centered around 1.
We simulated the relative risks assuming that the independence
model was true, and under this condition,
we expect the difference to be near one with some random
fluctuation, where <em>near</em> is pretty generous in this
case since the sample sizes are so small in this study.</p>

<div class="example">
<p>How often would you observe a sample relative risk
of at most 0.357 (at least a 64.3% reduction in risk on vaccine)
according to Figure <a href="inference-cat.html#fig:malaria-rand-dot-plot">5.25</a>?
Often, sometimes, rarely, or never?</p>
<hr />
It appears that a 64.3% reduction in risk due to chance alone would only
happen about 2% of the time according to
Figure <a href="inference-cat.html#fig:malaria-rand-dot-plot">5.25</a>.
Such a low probability indicates a rare event.
</div>
<p>Based on the simulations, we have two options:</p>
<ol style="list-style-type: decimal">
<li><p>We conclude that the study results do not provide
strong evidence against the independence model.
That is, we do not have sufficiently strong evidence
to conclude the vaccine had an effect in this
clinical setting.</p></li>
<li><p>We conclude the evidence is sufficiently strong
to reject <span class="math inline">\(H_0\)</span> and assert that the vaccine was useful.
When we conduct formal studies, usually we reject the
notion that we just happened to observe a rare
event.<a href="#fn117" class="footnote-ref" id="fnref117"><sup>117</sup></a></p></li>
</ol>
<p>In this case, we reject the independence model in favor
of the alternative.
That is, we are concluding the data provide strong evidence
that the vaccine provides some protection against malaria
in this clinical setting.</p>
<p></p>
<p>Statistical inference is built
on evaluating whether such differences are due to chance.
In statistical inference, data scientists evaluate which
model is most reasonable given the data.
Errors do occur, just like rare events, and we might choose
the wrong model.
While we do not always choose correctly, statistical
inference gives us tools to control and evaluate how
often these errors occur.</p>
</div>
<div id="types-of-errors" class="section level4" number="5.4.1.4">
<h4><span class="header-section-number">5.4.1.4</span> Decision errors</h4>
<p></p>
<p>Hypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free.
Similarly, data can point to the wrong conclusion.
However, what distinguishes statistical hypothesis tests from a court system is that our framework allows us to quantify and control how often the data lead us to the incorrect conclusion.</p>
<p>In a hypothesis test, there are two competing hypotheses: the null and the alternative.
We make a statement about which one might be true, but we might choose incorrectly. There are four possible scenarios in a hypothesis test, which are summarized in Table <a href="inference-cat.html#tab:fourHTScenarios">5.8</a>.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:fourHTScenarios">Table 5.8: </span>Four different scenarios for hypothesis tests.
</caption>
<thead>
<tr>
<th style="border-bottom:hidden" colspan="1">
</th>
<th style="border-bottom:hidden" colspan="1">
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
<strong>Test conclusion</strong>
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Fail to reject <span class="math inline">\(H_0\)</span>
</td>
<td style="text-align:left;">
Reject <span class="math inline">\(H_0\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
<span class="math inline">\(H_0\)</span> true
</td>
<td style="text-align:left;">
good decision
</td>
<td style="text-align:left;">
Type 1 Error
</td>
</tr>
<tr>
<td style="text-align:left;">
<strong>Truth</strong>
</td>
<td style="text-align:left;">
<span class="math inline">\(H_A\)</span> true
</td>
<td style="text-align:left;">
Type 2 Error
</td>
<td style="text-align:left;">
good decision
</td>
</tr>
</tbody>
</table>
<p>A <strong>Type 1 Error</strong> is rejecting the null hypothesis when <span class="math inline">\(H_0\)</span> is actually true.
Since we rejected the null hypothesis in the <a href="caseStudyGenderDiscrimination">gender discrimination</a>, <a href="caseStudyOpportunityCost">opportunity cost</a>, and <a href="caseStudyMalaria">malaria</a> studies, it is possible that we made a Type 1 Error in one, two, or all three of those studies.</p>
<p>A <strong>Type 2 Error</strong> is failing to reject the null hypothesis when the alternative is actually true. Since we failed to reject the null hypothesis in the <a href="one-prop-null-boot">medical consultant</a> study, it is possible that we made a Type 2 Error in that study.</p>

<div class="example">
<p>In a US court, the defendant is either innocent (<span class="math inline">\(H_0\)</span>) or guilty (<span class="math inline">\(H_A\)</span>).
What does a Type 1 Error represent in this context?
What does a Type 2 Error represent?
Table <a href="inference-cat.html#tab:fourHTScenarios">5.8</a> may be useful.</p>
<hr />
If the court makes a Type 1 Error, this means the defendant is innocent (<span class="math inline">\(H_0\)</span> true) but wrongly convicted.
A Type 2 Error means the court failed to reject <span class="math inline">\(H_0\)</span> (i.e., failed to convict the person) when they were in fact guilty (<span class="math inline">\(H_A\)</span> true).
</div>

<div class="guidedpractice">
Consider the opportunity cost study where we concluded students were less likely to make a DVD purchase if they were reminded that money not spent now could be spent later. What would a Type 1 Error represent in this context?<a href="#fn118" class="footnote-ref" id="fnref118"><sup>118</sup></a>
</div>

<div class="example">
<p>How could we reduce the Type 1 Error rate in US courts?
What influence would this have on the Type 2 Error rate?</p>
<hr />
To lower the Type 1 Error rate, we might raise our standard for conviction from “beyond a reasonable doubt” to “beyond a conceivable doubt” so fewer people would be wrongly convicted. However, this would also make it more difficult to convict the people who are actually guilty, so we would make more Type 2 Errors.
</div>

<div class="guidedpractice">
How could we reduce the Type 2 Error rate in US courts?
What influence would this have on the Type 1 Error rate?<a href="#fn119" class="footnote-ref" id="fnref119"><sup>119</sup></a>
</div>
<p></p>
<p>The example and guided practice above provide an important lesson: if we reduce how often we make one type of error, we generally make more of the other type.</p>
<!--
%Hypothesis testing is built around rejecting or failing to reject the null hypothesis. That is, we do not reject $H_0$ unless the data provide strong evidence against it. But what precisely does *strong evidence* mean? As a general rule of thumb, for those cases where the null hypothesis is actually true, we do not want to incorrectly reject $H_0$ more than 5% of the time. This corresponds to our default significance level of $\alpha = 0.05$, which we use as a comparison with the p-value. In the next section, we discuss the appropriateness of different significance levels.
-->
</div>
<div id="choosing-a-significance-level" class="section level4 unnumbered">
<h4>Choosing a significance level</h4>
<p>
</p>
<p>Choosing a significance level for a test is important in many contexts, and the traditional level is 0.05.
However, it is sometimes helpful to adjust the significance level based on the application.
We may select a level that is smaller or larger than 0.05 depending on the consequences of any conclusions reached from the test.</p>
<p>If making a Type 1 Error is dangerous or especially costly, we should choose a small significance level (e.g., 0.01 or 0.001).
If we want to be very cautious about rejecting the null hypothesis, we demand very strong evidence favoring the alternative <span class="math inline">\(H_A\)</span> before we would reject <span class="math inline">\(H_0\)</span>.</p>
<p>If a Type 2 Error is relatively more dangerous or much more costly than a Type 1 Error, then we should choose a higher significance level (e.g., 0.10).
Here we want to be cautious about failing to reject <span class="math inline">\(H_0\)</span> when the null is actually false.</p>

<div class="important">
<p><strong>Significance levels should reflect consequences of errors.</strong></p>
The significance level selected for a test should reflect the real-world consequences associated with making a Type 1 or Type 2 Error.
</div>
</div>
<div id="two-sided-tests" class="section level4" number="5.4.1.5">
<h4><span class="header-section-number">5.4.1.5</span> Two-sided hypotheses</h4>
<!--
%_________________
%\section[Case study: CPR and blood thinner (randomization)]{Case study: blood thinner and CPR\\(randomization)}
-->
<p></p>
<p>In in the <a href="caseStudyGenderDiscrimination">gender discrimination</a> and <a href="caseStudyOpportunityCost">opportunity cost</a> studies, we explored whether women were discriminated against and whether a simple trick could make students a little thriftier.
In these two case studies, we’ve actually ignored some possibilities:</p>
<ul>
<li>What if <em>men</em> are actually discriminated against?</li>
<li>What if the money trick actually makes students <em>spend more</em>?</li>
</ul>
<p>These possibilities weren’t considered in our original hypotheses or analyses.
The disregard of the extra alternatives may have seemed natural since the data pointed in the directions in which we framed the problems. However, there are two dangers if we ignore possibilities that disagree with our data or that conflict with our world view:</p>
<ol style="list-style-type: decimal">
<li><p>Framing an alternative hypothesis simply to match the direction that the data point will generally inflate the Type 1 Error rate. After all the work we’ve done (and will continue to do) to rigorously control the error rates in hypothesis tests, careless construction of the alternative hypotheses can disrupt that hard work.</p></li>
<li><p>If we only use alternative hypotheses that agree with our worldview, then we’re going to be subjecting ourselves to <strong>confirmation bias</strong>, which means we are looking for data that supports our ideas. That’s not very scientific, and we can do better!</p></li>
</ol>
<p>The original hypotheses we’ve seen are called <strong>one-sided hypothesis tests</strong> because they only explored one direction of possibilities.
Such hypotheses are appropriate when we are exclusively interested in the single direction, but usually we want to consider all possibilities.
To do so, let’s learn about <strong>two-sided hypothesis tests</strong> in the context of a new study that examines the impact of using blood thinners on patients who have undergone CPR.</p>
</div>
<div id="cpr-and-blood-thinner" class="section level4 unnumbered">
<h4>CPR and blood thinner</h4>
<p></p>
<p>Cardiopulmonary resuscitation (CPR) is a procedure used on individuals suffering a heart attack when other emergency resources are unavailable.
This procedure is helpful in providing some blood circulation to keep a person alive, but CPR chest compressions can also cause internal injuries. Internal bleeding and other injuries that can result from CPR complicate additional treatment efforts.
For instance, blood thinners may be used to help release a clot that is causing the heart attack once a patient arrives in the hospital. However, blood thinners negatively affect internal injuries.</p>
<p>Here we consider an experiment with patients who underwent CPR for a heart attack and were subsequently admitted to a hospital.<a href="#fn120" class="footnote-ref" id="fnref120"><sup>120</sup></a>
Each patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group).
The outcome variable of interest was whether the patient survived for at least 24 hours.</p>

<div class="example">
<p>Form hypotheses for this study in plain and statistical language.
Let <span class="math inline">\(\pi_c\)</span> represent the true survival rate of people who do not receive a blood thinner (corresponding to the control group) and <span class="math inline">\(\pi_t\)</span> represent the true survival rate for people receiving a blood thinner (corresponding to the treatment group).</p>
<hr />
<p>We want to understand whether blood thinners are helpful or harmful.
We’ll consider both of these possibilities using a two-sided hypothesis test.</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: Blood thinners do not have an overall survival effect, i.e., <span class="math inline">\(\pi_t - \pi_c = 0\)</span>.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: Blood thinners have an impact on survival, either positive or negative, but not zero, i.e., <span class="math inline">\(\pi_t - \pi_c \neq 0\)</span>.</p></li>
</ul>
<p>Note that if we had done a one-sided hypothesis test, the resulting hypotheses would have been:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: Blood thinners do not have a positive overall survival effect, i.e., <span class="math inline">\(\pi_t - \pi_c = 0\)</span>.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: Blood thinners have a positive impact on survival, i.e., <span class="math inline">\(\pi_t - \pi_c &gt; 0\)</span>.</p></li>
</ul>
</div>
<p>There were 50 patients in the experiment who did not receive a blood thinner and 40 patients who did.
The study results are shown in Table <a href="inference-cat.html#tab:resultsForCPRStudyInSmallSampleSection">5.9</a>.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:resultsForCPRStudyInSmallSampleSection">Table 5.9: </span>Results for the CPR study. Patients in the treatment group were given a blood thinner, and patients in the control group were not.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Treatment
</th>
<th style="text-align:left;">
Control
</th>
<th style="text-align:left;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Survived
</td>
<td style="text-align:left;">
14
</td>
<td style="text-align:left;">
11
</td>
<td style="text-align:left;">
25
</td>
</tr>
<tr>
<td style="text-align:left;">
Died
</td>
<td style="text-align:left;">
26
</td>
<td style="text-align:left;">
39
</td>
<td style="text-align:left;">
65
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
40
</td>
<td style="text-align:left;">
50
</td>
<td style="text-align:left;">
90
</td>
</tr>
</tbody>
</table>

<div class="guidedpractice">
What is the observed survival rate in the control group?
And in the treatment group?
Also, provide a point estimate of the difference in survival proportions of the two groups (<span class="math inline">\(\hat{p}_t - \hat{p}_c\)</span>) and the relative “risk” of survival (<span class="math inline">\(\hat{p}_t/\hat{p}_c\)</span>).<a href="#fn121" class="footnote-ref" id="fnref121"><sup>121</sup></a>
</div>
<p>According to the point estimate, for patients who have undergone CPR outside of the hospital, an additional 13% of these patients survive when they are treated with blood thinners. Interpreting the relative risk, patients in this sample who had undergone CPR outside of the hospital had a 59% higher survival rate when they were treated with blood thinners.
However, we wonder if this difference could be easily explainable by chance.</p>
<p>As we did in our past studies this chapter, we will simulate what type of differences we might see from chance alone under the null hypothesis.
By randomly assigning “simulated treatment” and “simulated control” stickers to the patients’ files, we get a new grouping.
If we repeat this simulation 10,000 times, we can build a <strong>null distribution</strong> of the differences in sample proportions shown in Figure <a href="inference-cat.html#fig:CPR-study-right-tail">5.26</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:CPR-study-right-tail"></span>
<img src="05-inference-cat_files/figure-html/CPR-study-right-tail-1.png" alt="Null distribution of the point estimate for the difference in proportions, $\hat{p}_t - \hat{p}_c$. The shaded right tail shows observations that are at least as large as the observed difference, 0.13." width="70%" />
<p class="caption">
Figure 5.26: Null distribution of the point estimate for the difference in proportions, <span class="math inline">\(\hat{p}_t - \hat{p}_c\)</span>. The shaded right tail shows observations that are at least as large as the observed difference, 0.13.
</p>
</div>
<p>The right tail area is 0.131. (Note: it is only a coincidence that we also have <span class="math inline">\(\hat{p}_t - \hat{p}_c=0.13\)</span>.)
However, contrary to how we calculated the p-value in previous studies, the p-value of this test is not 0.131!</p>
<p>The p-value is defined as the chance we observe a result <em>at least as favorable to the alternative hypothesis as the result</em> (i.e., the difference) we observe.
In this case, any differences less than or equal to -0.13 would also provide equally strong evidence favoring the alternative hypothesis as a difference of +0.13 did.
A difference of -0.13 would correspond to the survival rate in the <em>control group</em> being 0.13 higher than the treatment group.<a href="#fn122" class="footnote-ref" id="fnref122"><sup>122</sup></a>
In Figure <a href="inference-cat.html#fig:CPR-study-p-value">5.27</a> we’ve also shaded these differences in the left tail of the distribution.
These two shaded tails provide a visual representation of the p-value for a two-sided test.</p>
<!--
%There is something different in this study than in the past studies: in this study, we are particularly interested in whether blood thinners increase *or* decrease the risk of death in patients who undergo CPR before arriving at the hospital.\footnote{Realistically, we probably are interested in either direction in the past studies as well, and so we should have used the approach we now discuss in this section. However, for simplicity and the sake of not introducing too many concepts at once, we skipped over these details in earlier sections.} For example, there are chance differences of $\hat{p}_t - \hat{p}_c = -0.14$, that would have been stronger evidence against the null hypothesis as our observed difference of +0.13. Likewise, anything less than or equal -0.13 would provide as much evidence against the null hypothesis as +0.13, and for this reason, we must count both tails towards the p-value, as shown in Figure \ref{CPR-study-p-value}.
-->
<div class="figure" style="text-align: center"><span id="fig:CPR-study-p-value"></span>
<img src="05-inference-cat_files/figure-html/CPR-study-p-value-1.png" alt="Null distribution of the point estimate for the difference in proportions, $\hat{p}_t - \hat{p}_c$. All values that are at least as extreme as +0.13 but in either direction away from 0 are shaded." width="70%" />
<p class="caption">
Figure 5.27: Null distribution of the point estimate for the difference in proportions, <span class="math inline">\(\hat{p}_t - \hat{p}_c\)</span>. All values that are at least as extreme as +0.13 but in either direction away from 0 are shaded.
</p>
</div>
<p>For a two-sided test, since the null distribution is symmetric, take the single tail (in this case, 0.131) and double it to get the p-value: 0.262.
With this large p-value, we do not find statistically significant evidence that the blood thinner has any influence on survival of patients who undergo CPR prior to arriving at the hospital.</p>
<!--%Once again, we can discuss the causal conclusion since this is an experiment.
-->
<p></p>

<div class="onebox">
<p><strong>Default to a two-sided test.</strong></p>
We want to be rigorous and keep an open mind when we analyze data and evidence.
Use a one-sided hypothesis test only if you truly have interest in only one direction.
</div>

<div class="onebox">
<p><strong>Computing a p-value for a two-sided test.</strong></p>
If your null distribution is symmetric, first compute the p-value for one tail of the distribution, then double that value to get the two-sided p-value.
That’s it!<a href="#fn123" class="footnote-ref" id="fnref123"><sup>123</sup></a>
</div>

<div class="example">
<p>Consider the situation of the medical consultant.
Now that you know about one-sided and two-sided tests, which type of test do you think is more appropriate?</p>
<hr />
The setting has been framed in the context of the consultant being helpful (which is what led us to a one-sided test originally), but what if the consultant actually performed <em>worse</em> than the average?
Would we care?
More than ever!
Since it turns out that we care about a finding in either direction, we should run a two-sided test.
The p-value for the two-sided test is double that of the one-sided test, here the simulated p-value would be 0.2444.
</div>
<p>Generally, to find a two-sided p-value we double the single tail area, which remains a reasonable approach even when the sampling distribution is asymmetric.
However, the approach can result in p-values larger than 1 when the point estimate is very near the mean in the null distribution; in such cases, we write that the p-value is 1.
Also, very large p-values computed in this way (e.g., 0.85), may also be slightly inflated.
Typically, we do not worry too much about the precision of very large p-values because they lead to the same analysis conclusion, even if the value is slightly off.</p>
</div>
<div id="controlling-the-type-1-error-rate" class="section level4 unnumbered">
<h4>Controlling the Type 1 Error rate</h4>
<p>Now that we understand the difference between one-sided and two-sided tests, we must recognize when to use each type of test.
Because of the result of increased error rates, it is never okay to change two-sided tests to one-sided tests after observing the data.
We explore the consequences of ignoring this advice in the next example.</p>

<div class="example">
<p>Using <span class="math inline">\(\alpha=0.05\)</span>, we show that freely switching from two-sided tests to one-sided tests will lead us to make twice as many Type 1 Errors as intended.</p>
<hr />
<p>Suppose we are interested in finding any difference from 0.
We’ve created a smooth-looking <strong>null distribution</strong> representing differences due to chance in Figure <a href="inference-cat.html#fig:type1ErrorDoublingExampleFigure">5.28</a>.</p>
<p>Suppose the sample difference was larger than 0.
Then if we can flip to a one-sided test, we would use <span class="math inline">\(H_A\)</span>: difference <span class="math inline">\(&gt; 0\)</span>.
Now if we obtain any observation in the upper 5% of the distribution, we would reject <span class="math inline">\(H_0\)</span> since the p-value would just be a the single tail.
Thus, if the null hypothesis is true, we incorrectly reject the null hypothesis about 5% of the time when the sample mean is above the null value, as shown in Figure <a href="inference-cat.html#fig:type1ErrorDoublingExampleFigure">5.28</a>.</p>
<p>Suppose the sample difference was smaller than 0.
Then if we change to a one-sided test, we would use <span class="math inline">\(H_A\)</span>: difference <span class="math inline">\(&lt; 0\)</span>.
If the observed difference falls in the lower 5% of the figure, we would reject <span class="math inline">\(H_0\)</span>.
That is, if the null hypothesis is true, then we would observe this situation about 5% of the time.</p>
By examining these two scenarios, we can determine that we will make a Type 1 Error <span class="math inline">\(5\%+5\%=10\%\)</span> of the time if we are allowed to swap to the “best” one-sided test for the data.
This is twice the error rate we prescribed with our significance level: <span class="math inline">\(\alpha=0.05\)</span> (!).
</div>
<div class="figure" style="text-align: center"><span id="fig:type1ErrorDoublingExampleFigure"></span>
<img src="05-inference-cat_files/figure-html/type1ErrorDoublingExampleFigure-1.png" alt="The shaded regions represent areas where we would reject $H_0$ under the bad practices considered in when $\alpha = 0.05$." width="70%" />
<p class="caption">
Figure 5.28: The shaded regions represent areas where we would reject <span class="math inline">\(H_0\)</span> under the bad practices considered in when <span class="math inline">\(\alpha = 0.05\)</span>.
</p>
</div>

<div class="importantbox">
<p><strong>Hypothesis tests should be set up <em>before</em> seeing the data.</strong></p>
After observing data, it is tempting to turn a two-sided test into a one-sided test.
Avoid this temptation.
Hypotheses should be set up <em>before</em> observing the data.
</div>
<!--
%\Comment{Should we scrap this subsection and example and just leave the caution box? Downside: weakens item 1 near the start of Section \@ref(IntroducingTwoSidedHypotheses).}
-->
<p></p>
</div>
</div>
<div id="two-prop-boot-ci" class="section level3" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> Bootstrap confidence interval for <span class="math inline">\(\pi_1 - \pi_2\)</span></h3>
<p>In Section <a href="inference-cat.html#two-prop-errors">5.4.1</a>, we worked with the randomization distribution to understand the distribution of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> when the null hypothesis <span class="math inline">\(H_0: \pi_1 - \pi_2 = 0\)</span> is true.
Now, through bootstrapping, we study the variability of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> without the null assumption.</p>
<div id="observed-data-3" class="section level4 unnumbered">
<h4>Observed data</h4>
<p>Reconsider the CPR data from Section <a href="inference-cat.html#two-prop-errors">5.4.1</a> which is provided in Table <a href="inference-cat.html#tab:resultsForCPRStudyInSmallSampleSection">5.9</a>. The experiment consisted of two treatments on patients who underwent CPR for a heart attack and were subsequently admitted to a hospital. Each patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group).
The outcome variable of interest was whether the patient survived for at least 24 hours.</p>
<p>Again, we use the difference in sample proportions as the observed statistic of interest. Here, the value of the statistic is: <span class="math inline">\(\hat{p}_t - \hat{p}_c = 0.35 - 0.22 = 0.13\)</span>.</p>
</div>
<div id="variability-of-the-statistic-3" class="section level4 unnumbered">
<h4>Variability of the statistic</h4>
<p>The bootstrap method applied to two samples is an extension of the method described in Section <a href="inference-cat.html#boot-ci-prop">5.3.2</a>. Now, we have two samples, so each sample estimates the population from which they came. In the CPR setting, the <code>treatment</code> sample estimates the population of all individuals who have gotten (or will get) the treatment; the <code>control</code> sample estimate the population of all individuals who do not get the treatment and are controls. Figure <a href="inference-cat.html#fig:boot2proppops">5.29</a> extends Figure <a href="inference-cat.html#fig:boot1">5.11</a> to show the bootstrapping process from two samples simultaneously.</p>
<div class="figure" style="text-align: center"><span id="fig:boot2proppops"></span>
<img src="05/figures/boot2proppops.png" alt="Creating two estimated populations from two different samples from different populations." width="100%" />
<p class="caption">
Figure 5.29: Creating two estimated populations from two different samples from different populations.
</p>
</div>
<p>As before, once the population is estimated, we can randomly resample observations to create bootstrap samples, as seen in Figure <a href="inference-cat.html#fig:boot2propresamps">5.30</a>. Computationally, each bootstrap resample
is created by randomly sampling with replacement from the original sample.</p>
<div class="figure" style="text-align: center"><span id="fig:boot2propresamps"></span>
<img src="05/figures/boot2propresamps.png" alt="Bootstrapped resamples from two separate estimated populations." width="100%" />
<p class="caption">
Figure 5.30: Bootstrapped resamples from two separate estimated populations.
</p>
</div>
<p>The variability of the statistic (the difference in sample proportions) can be calculated by taking one treatment bootstrap sample and one control bootstrap sample and calculating the difference of the bootstrap survival proportions. Figure @(boot2samp2) displays one bootstrap resample from each of the estimated populations, with the difference in sample proportions calculated between the treatment bootstrap sample and the control bootstrap sample.</p>
<div class="figure" style="text-align: center"><span id="fig:boot2samp2"></span>
<img src="05/figures/boot2prop3.png" alt="The bootstrap resample on the left is from the first estimated population; the one on the right from the second. In this case, the value of the simulated bootstrap statistic would be $\hat{p}_1 - \hat{p}_2 = \frac{2}{7}-\frac{1}{7}$." width="75%" />
<p class="caption">
Figure 5.31: The bootstrap resample on the left is from the first estimated population; the one on the right from the second. In this case, the value of the simulated bootstrap statistic would be <span class="math inline">\(\hat{p}_1 - \hat{p}_2 = \frac{2}{7}-\frac{1}{7}\)</span>.
</p>
</div>
<p>As always, the variability of the difference in proportions can only be estimated by repeated simulations, in this case, repeated bootstrap samples. Figure <a href="inference-cat.html#fig:boot2samp3">5.32</a> shows multiple bootstrap differences calculated for each of the repeated bootstrap samples.</p>
<div class="figure" style="text-align: center"><span id="fig:boot2samp3"></span>
<img src="05/figures/boot2prop2.png" alt="in this graph, some kind of connection between each of the two sides" width="75%" />
<p class="caption">
Figure 5.32: in this graph, some kind of connection between each of the two sides
</p>
</div>
<p>Repeated bootstrap simulations lead to a bootstrap sampling distribution of the statistic of interest, here the difference in sample proportions.
Figure <a href="inference-cat.html#fig:boot2samp1">5.33</a> visualizes the process in the toy example, and Figure <a href="inference-cat.html#fig:bootCPR1000">5.34</a> shows 1000 bootstrap differences in proportions for the CPR data.</p>
<div class="figure" style="text-align: center"><span id="fig:boot2samp1"></span>
<img src="05/figures/boot2prop1.png" alt="The process of repeatedly resampling from the estimated population (sampling with replacement from the original sample), computing a difference in sample proportions from each pair of samples, then plotting this distribution." width="100%" />
<p class="caption">
Figure 5.33: The process of repeatedly resampling from the estimated population (sampling with replacement from the original sample), computing a difference in sample proportions from each pair of samples, then plotting this distribution.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:bootCPR1000"></span>
<img src="05-inference-cat_files/figure-html/bootCPR1000-1.png" alt="A histogram of differences in proportions (treatment $-$ control) from 1000 bootstrap simulations using the CPR data." width="70%" />
<p class="caption">
Figure 5.34: A histogram of differences in proportions (treatment <span class="math inline">\(-\)</span> control) from 1000 bootstrap simulations using the CPR data.
</p>
</div>
</div>
<div id="percentile-vs.-se-bootstrap-confidence-intervals" class="section level4 unnumbered">
<h4>Percentile vs. SE bootstrap confidence intervals</h4>
<p>Figure <a href="inference-cat.html#fig:bootCPR1000">5.34</a> provides an estimate for the variability of the difference in survival proportions from sample to sample, The values in the histogram can be used in two different ways to create a confidence interval for the parameter of interest: <span class="math inline">\(\pi_1 - \pi_2\)</span>.</p>
<div id="percentile-bootstrap-interval" class="section level5 unnumbered">
<h5>Percentile bootstrap interval</h5>
<p>As in Section <a href="inference-cat.html#boot-ci-prop">5.3.2</a>, the bootstrap confidence interval can be calculated directly from the bootstrapped differences in Figure <a href="inference-cat.html#fig:bootCPR1000">5.34</a>. The interval created from the percentiles of the distribution is called the <strong>percentile interval</strong>.
Note that here we calculate the 90% confidence interval by finding the 5<sup>th</sup> and 95<sup>th</sup> percentile values from the bootstrapped differences.
The bootstrap 5 percentile proportion is -0.155 and the 95 percentile is 0.167.
The result is: we are 90% confident that, in the population, the true difference in probability of survival (treatment <span class="math inline">\(-\)</span> control) is between -0.155 and 0.167.
More clearly, we are 90% confident that the probability of survival for heart attack patients who underwent CPR on blood thinners is between 0.155 less to 0.167 more than that for patients who were not given blood thinners. The interval shows that we do not have much definitive evidence of the affect of blood thinners, one way or another.</p>
<div class="figure" style="text-align: center"><span id="fig:bootCPR1000CI"></span>
<img src="05-inference-cat_files/figure-html/bootCPR1000CI-1.png" alt="The CPR data is bootstrapped 1000 times. Each simulation creates a sample from the original data where the probability of survival in the treatment group is $\hat{p}_{t}  = 14/40$ and the probability of survival in the control group is $\hat{p}_{c} = 11/50$. " width="70%" />
<p class="caption">
Figure 5.35: The CPR data is bootstrapped 1000 times. Each simulation creates a sample from the original data where the probability of survival in the treatment group is <span class="math inline">\(\hat{p}_{t} = 14/40\)</span> and the probability of survival in the control group is <span class="math inline">\(\hat{p}_{c} = 11/50\)</span>.
</p>
</div>
</div>
<div id="se-bootstrap-interval" class="section level5 unnumbered">
<h5>SE bootstrap interval</h5>
<p>Alternatively, we can use the variability in the bootstrapped differences to calculate a standard error of the difference.
The resulting interval is called the <strong>SE interval</strong>.
Section <a href="inference-cat.html#math-2prop">5.4.3</a> details the mathematical model for the standard error of the difference in sample proportions, but the bootstrap distribution typically does an excellent job of estimating the variability.</p>
<p><span class="math display">\[SE(\hat{p}_t - \hat{p}_c) \approx SD(\hat{p}_{bs,t} - \hat{p}_{bs,c}) = 0.0975\]</span></p>
<p>The variability of the bootstrapped difference in proportions was calculated in <code>R</code> using the <code>sd()</code> function, but any statistical software will calculate the standard deviation of the differences, here, the exact quantity we hope to approximate.</p>
<p>Note that we do not know know the true distribution of <span class="math inline">\(\hat{p}_t - \hat{p}_c\)</span>, so we will use a rough approximation to find a confidence interval for <span class="math inline">\(\pi_t - \pi_c\)</span>. As seen in the bootstrap histograms, the shape of the distribution is roughly symmetric and bell-shaped. So for a rough approximation, we will apply the 68-95-99.7 rule which tells us that 95% of observed differences should be roughly no farther than 2 SE from the true parameter difference. An approximate 95% confidence interval for <span class="math inline">\(\pi_t - \pi_c\)</span> is given by:</p>
<p><span class="math display">\[\begin{align*}
\hat{p}_t - \hat{p}_c \pm 2 \cdot SE \ \ \ \rightarrow \ \ \ 14/40 - 11/50 \pm 2 \cdot 0.0975 \ \ \  \rightarrow \ \ \  (-0.065, 0.325)
\end{align*}\]</span></p>
<p>We are 95% confident that the true value of <span class="math inline">\(\pi_t - \pi_c\)</span> is between -0.065 and 0.325. Again, the wide confidence interval that overlaps zero indicates that the study provides very little evidence about the effectiveness of blood thinners.</p>

<div class="important">
Since the multiplier “2” in the SE bootstrap interval comes from the 68-95-99.7 rule for normal distributions, these intervals are only valid when the bootstrap sampling distribution is approximately normal.
</div>
</div>
</div>
<div id="what-does-95-mean" class="section level4 unnumbered">
<h4>What does 95% mean?</h4>
<p>Recall that the goal of a confidence interval is to find a plausible range of values for a <em>parameter</em> of interest.
The estimated statistic is not the value of interest, but it is typically the best guess for the unknown parameter.
The confidence level (often 95%) is a number that takes a while to get used to. Surprisingly, the percentage doesn’t describe the data set at hand, it describes many possible data sets.
One way to understand a confidence interval is to think about all the confidence intervals that you have ever made or that you will ever make as a scientist, the confidence level describes <strong>those</strong> intervals.</p>
<p>Figure <a href="inference-cat.html#fig:ci25ints">5.36</a> demonstrates a hypothetical situation in which 25 different studies are performed on the exact same population (with the same goal of estimating the true parameter value of <span class="math inline">\(\pi_1 - \pi_2 = 0.47\)</span>).
The study at hand represents one point estimate (a dot) and a corresponding interval.
It is not possible to know whether the interval at hand is to the right of the unknown true parameter value (the black line) or to the left of that line.
It is also impossible to know whether the interval captures the true parameter (is blue) or doesn’t (is red).
If we are making 95% intervals, then 5% of the intervals we create over our lifetime will <em>not</em> capture the parameter of interest (e.g., will be red as in Figure <a href="inference-cat.html#fig:ci25ints">5.36</a> ).
What we know is that over our lifetimes as scientists, 95% of the intervals created and reported on will capture the parameter value of interest: thus the language “95% confident.”</p>
<div class="figure" style="text-align: center"><span id="fig:ci25ints"></span>
<img src="05-inference-cat_files/figure-html/ci25ints-1.png" alt="One hypothetical population, parameter value of: $\pi_1 - \pi_2 = 0.47$.  Twenty-five different studies all which led to a different point estimate, SE, and confidence interval.  The study at hand is one of the horizontal lines (hopefully a blue line!)." width="70%" />
<p class="caption">
Figure 5.36: One hypothetical population, parameter value of: <span class="math inline">\(\pi_1 - \pi_2 = 0.47\)</span>. Twenty-five different studies all which led to a different point estimate, SE, and confidence interval. The study at hand is one of the horizontal lines (hopefully a blue line!).
</p>
</div>
<p>The choice of 95% or 90% or even 99% as a confidence level is admittedly somewhat arbitrary; however, it is related to the logic we used when deciding that a p-value should be declared as significant if it is lower than 0.05 (or 0.10 or 0.01, respectively).
Indeed, one can show mathematically, that a 95% confidence interval and a two-sided hypothesis test at a cutoff of 0.05 will provide the same conclusion when the same data and mathematical tools are applied for the analysis.
A full derivation of the explicit connection between confidence intervals and hypothesis tests is beyond the scope of this text.</p>
</div>
</div>
<div id="math-2prop" class="section level3" number="5.4.3">
<h3><span class="header-section-number">5.4.3</span> Theory-based methods for <span class="math inline">\(\pi_1 - \pi_2\)</span></h3>
<div id="variability-of-hatp_1---hatp_2" class="section level4 unnumbered">
<h4>Variability of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span></h4>
<p>Like with <span class="math inline">\(\hat{p}\)</span>, the difference of two sample
proportions <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> can be modeled
using a normal distribution when certain conditions
are met.
First, we require a broader independence condition,
and secondly,
the success-failure condition must be met by both groups.</p>

<div class="onebox">
<p><strong>Conditions for the sampling distribution of <span class="math inline">\(\hat{p}_1 -\hat{p}_2\)</span> to be normal.</strong></p>
<p>The difference <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> can be modeled
using a normal distribution when</p>
<ol style="list-style-type: decimal">
<li><em>Independence</em> (extended). The data are independent within and between
the two groups. Generally this is satisfied if the data come from two independent random samples or if the data come from a randomized experiment.</li>
<li><em>Success-failure condition.</em>
The success-failure condition holds for both
groups, where we check successes and failures
in each group separately.</li>
</ol>
<p>When these conditions are satisfied,
then the sampling distribution of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> is
approximately normal with mean <span class="math inline">\(\pi_1 - \pi_2\)</span> and standard deviation</p>
<span class="math display">\[\begin{eqnarray*}
  SD(\hat{p}_1 - \hat{p}_2) = \sqrt{\frac{\pi_1(1-\pi_1)}{n_1} + \frac{\pi_2(1-\pi_2)}{n_2}}
  \end{eqnarray*}\]</span>
where <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(\pi_2\)</span> represent the population proportions,
and <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> represent the sample sizes.
</div>
<p></p>
<!--
SE reference above?
    \label{seForDiffOfProp}
-->

<div class="tipbox">
The success-failure condition listed above is only necessary for the sampling distribution of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> to be approximately normal. The mean of the sampling distribution of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> is <span class="math inline">\(\pi_1 - \pi_2\)</span>, and the standard deviation is <span class="math inline">\(\sqrt{\frac{\pi_1(1-\pi_1)}{n_1}+\frac{\pi_1(1-\pi_1)}{n_1}}\)</span>, regardless of the two sample sizes.
</div>
<p>As in the case of one proportion, we typically don’t know the true proportions <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(\pi_2\)</span>,
so we will substitute some value to check the success-failure condition
and to estimate the standard deviation of the sampling distribution of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span>.</p>
</div>
<div id="confidence-interval-for-pi_1---pi_2" class="section level4 unnumbered">
<h4>Confidence interval for <span class="math inline">\(\pi_1 - \pi_2\)</span></h4>
<p></p>

<div class="onebox">
<p><strong>Standard error of the difference in two proportions, <span class="math inline">\(\hat{p}_1 -\hat{p}_2\)</span>: confidence intervals.</strong></p>
<p>When computing a theory-based confidence interval for <span class="math inline">\(\pi_1 - \pi_2\)</span>, we substitute <span class="math inline">\(\hat{p}_1\)</span> for <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(\hat{p}_2\)</span> for <span class="math inline">\(\pi_2\)</span> in the expression for the standard deviation of the statistic, resulting in its standard error:</p>
<p><span class="math display">\[\begin{eqnarray*}
  SE(\hat{p}_1 -\hat{p}_2) = \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}
  \end{eqnarray*}\]</span></p>
This is the standard error formula we will use when computing confidence intervals for the difference in two proportions.
</div>
<p>If the conditions for the sampling distribution of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> to be normal are met, we can apply the generic confidence interval formula
for a difference of two proportions,
where we use <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> as the point
estimate and substitute the <span class="math inline">\(SE\)</span> formula above:
<span class="math display">\[\begin{align*}
&amp;\text{point estimate} \ \pm\  z^{\star} \times SE
&amp;&amp;\to
&amp;&amp;\hat{p}_1 - \hat{p}_2 \ \pm\ 
    z^{\star} \times
   \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}
\end{align*}\]</span></p>
<!--
We can also follow the same
Prepare, Check, Calculate, Conclude steps for
computing a confidence interval
or completing a hypothesis test.
The details change a little,
but the general approach remain the same.
Think about these steps when you apply statistical methods.
-->
<!--
\BeginKnitrBlock{onebox}<div class="onebox">**Confidence interval for a difference of two proportions**
  Once you've determined a confidence interval for the
  difference of two proportions would be helpful for an
  application, there are four steps to constructing the interval:

* **Prepare.**
      Identify the sample proportions and sample sizes
      for each of the two groups,
      determine what confidence level you wish to use.
* **Check.**
      Verify the conditions to ensure each sample
      proportion is nearly normal.
      The success-failure condition should be checked
      for each group.
* **Calculate.**
      If the conditions hold, compute $SE$,
      find $z^{\star}$, and construct the interval.
* **Conclude.**
      Interpret the confidence interval in the context
      of the problem.</div>\EndKnitrBlock{onebox}
-->

<div class="example">
<p>We reconsider the experiment for patients
who underwent cardiopulmonary resuscitation (CPR)
for a heart attack and were
subsequently admitted to a
hospital.
These patients were randomly divided into a treatment
group where they received a blood thinner or the control
group where they did not receive a blood thinner.
The outcome variable of interest was whether the
patients survived for at least 24 hours.
The results are shown in
Table <a href="inference-cat.html#tab:resultsForCPRStudyInSmallSampleSection">5.9</a>.
Check whether we can model the difference in
sample proportions using the normal distribution.</p>
<hr />
<p>We first check for independence:
since this is a randomized experiment,
this condition is satisfied.</p>
<p>Next, we check the success-failure condition for
each group.
We have at least 10 successes and 10 failures in
each experiment arm (11, 14, 39, 26),
so this condition is also satisfied.</p>
With both conditions satisfied,
the difference in sample proportions can be
reasonably modeled using a normal distribution
for these data.
</div>
<!--
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>(\#tab:resultsForCPRStudyInSmallSampleSectionDup)Results for the CPR study.
    Patients in the treatment group were given
    a blood thinner, and patients in the control
    group were not.</caption>
 <thead>
  <tr>
   <th style="text-align:left;">  </th>
   <th style="text-align:left;"> Survived </th>
   <th style="text-align:left;"> Died </th>
   <th style="text-align:left;"> Total </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> Control </td>
   <td style="text-align:left;"> 11 </td>
   <td style="text-align:left;"> 39 </td>
   <td style="text-align:left;"> 50 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Treatment </td>
   <td style="text-align:left;"> 14 </td>
   <td style="text-align:left;"> 26 </td>
   <td style="text-align:left;"> 40 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Total </td>
   <td style="text-align:left;"> 25 </td>
   <td style="text-align:left;"> 65 </td>
   <td style="text-align:left;"> 90 </td>
  </tr>
</tbody>
</table>
-->

<div class="example">
<p>Create and interpret a 90% confidence interval of the
difference for the survival rates in the CPR study.</p>
<hr />
<p>We’ll use <span class="math inline">\(\pi_t\)</span> for the true survival
rate in the treatment group and <span class="math inline">\(\pi_c\)</span> for the control
group. Our point estimate of <span class="math inline">\(\pi_t - \pi_c\)</span> is:
<span class="math display">\[\begin{align*}
  \hat{p}_{t} - \hat{p}_{c}
    = \frac{14}{40} - \frac{11}{50}
    = 0.35 - 0.22
    = 0.13
  \end{align*}\]</span>
We use the standard error formula previously provided.
As with the one-sample proportion case,
we use the sample estimates of each proportion
in the formula in the confidence interval context:
<span class="math display">\[\begin{align*}
  SE \approx \sqrt{\frac{0.35 (1 - 0.35)}{40} +
      \frac{0.22 (1 - 0.22)}{50}}
    = 0.095
  \end{align*}\]</span>
For a 90% confidence interval, we use <span class="math inline">\(z^{\star} = 1.65\)</span>:
<span class="math display">\[\begin{align*}
  \text{point estimate} \ \pm\ z^{\star} \times SE
    \quad \to \quad 0.13 \ \pm\ 1.65 \times  0.095
    \quad \to \quad (-0.027, 0.287)
  \end{align*}\]</span>
We are 90% confident that the survival probability for those
patients given blood thinners is between 0.027 lower to 0.287 higher
than that of patients not given blood thinners, among patients like
those in the study.
Because 0% is contained in the interval,
we do not have enough information to say
whether blood thinners help or harm
heart attack patients who have been admitted after
they have undergone CPR.</p>
Note, the problem was set up as 90% to indicate that there was not a need for a high level of confidence (such a 95% or 99%). A lower degree of confidence increases potential for error, but it also produces a more narrow interval.
</div>
<p></p>

<div class="guidedpractice">
<p>A 5-year experiment
was conducted to evaluate the effectiveness
of fish oils on reducing cardiovascular events,
where each subject was randomized into one of two
treatment groups.
We will consider heart attack outcomes in the patients listed in Table <a href="inference-cat.html#tab:fish-oil-data">5.10</a>.</p>
Create a 95% confidence interval for the effect of fish oils
on heart attacks for patients who are well-represented by
those in the study.
Also interpret the interval in the context of the
study.<a href="#fn124" class="footnote-ref" id="fnref124"><sup>124</sup></a>
</div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:fish-oil-data">Table 5.10: </span>Results for the study on n-3 fatty acid supplement and related health benefits.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
fish oil
</th>
<th style="text-align:right;">
placebo
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
heart attack
</td>
<td style="text-align:right;">
145
</td>
<td style="text-align:right;">
200
</td>
</tr>
<tr>
<td style="text-align:left;">
no event
</td>
<td style="text-align:right;">
12788
</td>
<td style="text-align:right;">
12738
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:right;">
12933
</td>
<td style="text-align:right;">
12938
</td>
</tr>
</tbody>
</table>
</div>
<div id="hypothesis-test-for-h_0-pi_1---pi_2-0" class="section level4 unnumbered">
<h4>Hypothesis test for <span class="math inline">\(H_0: \pi_1 - \pi_2 = 0\)</span></h4>
<p>
</p>
<p>A mammogram is an X-ray procedure used to check for
breast cancer.
Whether mammograms should be used is part of a
controversial discussion, and it’s the topic of our
next example where we learn about two proportion
hypothesis tests when <span class="math inline">\(H_0\)</span> is <span class="math inline">\(\pi_1 - \pi_2 = 0\)</span>
(or equivalently, <span class="math inline">\(\pi_1 = \pi_2\)</span>).</p>
<p>A 30-year study was conducted with nearly 90,000 female participants. During a 5-year screening period, each woman was randomized to one of two groups: in the first group, women received regular mammograms to screen for breast cancer, and in the second group, women received regular non-mammogram breast cancer exams. No intervention was made during the following 25 years of the study, and we’ll consider death resulting from breast cancer over the full 30-year period. Results from the study are summarized in Figure <a href="inference-cat.html#tab:mammogramStudySummaryTable">5.11</a>.</p>
<p>If mammograms are much more effective than non-mammogram breast cancer exams, then we would expect to see additional deaths from breast cancer in the control group. On the other hand, if mammograms are not as effective as regular breast cancer exams, we would expect to see an increase in breast cancer deaths in the mammogram group.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:mammogramStudySummaryTable">Table 5.11: </span>Summary results for breast cancer study.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Mammogram
</th>
<th style="text-align:left;">
Control
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Death from breast cancer?
</td>
<td style="text-align:left;">
Yes
</td>
<td style="text-align:left;">
500
</td>
<td style="text-align:left;">
505
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
No
</td>
<td style="text-align:left;">
44,425
</td>
<td style="text-align:left;">
44,405
</td>
</tr>
</tbody>
</table>

<div class="guidedpractice">
Is this study an experiment or an observational study?<a href="#fn125" class="footnote-ref" id="fnref125"><sup>125</sup></a>
</div>

<div class="guidedpractice">
Set up hypotheses to test whether there was a difference
in breast cancer deaths in the mammogram and control groups.<a href="#fn126" class="footnote-ref" id="fnref126"><sup>126</sup></a>
</div>
<p>The research question describing mammograms is set up to address specific hypotheses (in contrast to a confidence interval for a parameter). In order to fully take advantage of the hypothesis testing structure, we assess the randomness under the condition that the null hypothesis is true (as we always do for hypothesis testing).
Using the data from Table <a href="inference-cat.html#tab:mammogramStudySummaryTable">5.11</a>,
we will check the conditions for using a normal distribution to
analyze the results of the study using a hypothesis test.
The details for checking conditions are very similar to that of confidence intervals.
However, when the null hypothesis is that <span class="math inline">\(\pi_1 - \pi_2 = 0\)</span>,
we use a special proportion called the
<strong>pooled proportion</strong> to check the success-failure condition and when computing the standard error:
<span class="math display">\[\begin{align*}
\hat{p}_{\textit{pool}}
    &amp;= \frac
        {\text{# of patients who died from breast cancer in the entire study}}
        {\text{# of patients in the entire study}} \\
        &amp;\\
	&amp;= \frac{500 + 505}{500 + \text{44,425} + 505 + \text{44,405}} \\
	&amp;\\
	&amp;= 0.0112
\end{align*}\]</span>
This proportion is an estimate of the breast cancer death rate
across the entire study, and it’s our best estimate of the
death rates <span class="math inline">\(\pi_{mgm}\)</span> and <span class="math inline">\(\pi_{ctrl}\)</span>
<em>if the null hypothesis is true that <span class="math inline">\(\pi_{mgm} = \pi_{ctrl}\)</span></em>.</p>

<div class="onebox">
<p><strong>Use the pooled proportion when <span class="math inline">\(H_0\)</span> is <span class="math inline">\(\pi_1 - \pi_2 = 0\)</span>.</strong></p>
When the null hypothesis is that the proportions are equal,
use the pooled proportion (<span class="math inline">\(\hat{p}_{\textit{pool}}\)</span>)
to verify the
success-failure condition and estimate the standard error:
<span class="math display">\[\begin{eqnarray*}
  \hat{p}_{\textit{pool}}
    = \frac{\text{number of &quot;successes&quot;}}
      {\text{number of cases}}
    = \frac{\hat{p}_1 n_1 + \hat{p}_2 n_2}{n_1 + n_2}
  \end{eqnarray*}\]</span>
Here <span class="math inline">\(\hat{p}_1 n_1\)</span> represents the number of successes in
sample 1 since
<span class="math display">\[\begin{eqnarray*}
  \hat{p}_1
    = \frac{\text{number of successes in sample 1}}{n_1}
  \end{eqnarray*}\]</span>
Similarly, <span class="math inline">\(\hat{p}_2 n_2\)</span> represents the number
of successes in sample 2.
</div>

<div class="example">
<p>Is it reasonable to model the difference
in proportions using a normal distribution in this
study?</p>
<hr />
Because the patients are randomized, they can be treated
as independent, both within and between groups.
We also must check the success-failure condition for each group.
Under the null hypothesis, the proportions <span class="math inline">\(\pi_{mgm}\)</span>
and <span class="math inline">\(\pi_{ctrl}\)</span> are equal, so we check the success-failure
condition with our best estimate of these values under <span class="math inline">\(H_0\)</span>,
the pooled proportion from the two samples,
<span class="math inline">\(\hat{p}_{\textit{pool}} = 0.0112\)</span>:
<span class="math display">\[\begin{align*}
  \hat{p}_{\textit{pool}} \times n_{mgm}
      &amp;= 0.0112 \times \text{44,925} = 503 \\
   (1 - \hat{p}_{\textit{pool}}) \times n_{mgm}
      &amp;= 0.9888 \times \text{44,925} = \text{44,422} \\
  &amp; \\
  \hat{p}_{\textit{pool}} \times n_{ctrl}
      &amp;= 0.0112 \times \text{44,910} = 503 \\
   (1 - \hat{p}_{\textit{pool}}) \times n_{ctrl}
      &amp;= 0.9888 \times \text{44,910} = \text{44,407}
  \end{align*}\]</span>
The success-failure condition is satisfied since
all values are at least 10.
With both conditions satisfied, we can safely model
the difference in proportions using a normal
distribution.
</div>
<p>We used the pooled proportion to check the success-failure
condition<a href="#fn127" class="footnote-ref" id="fnref127"><sup>127</sup></a>. We next use it again in the standard error calculation.</p>

<div class="onebox">
<p><strong>Standard error of the difference in two proportions, <span class="math inline">\(\hat{p}_1 -\hat{p}_2\)</span>: hypothesis tests.</strong></p>
<p>When conducting a theory-based hypothesis test for <span class="math inline">\(H_0: \pi_1 - \pi_2 = 0\)</span>, we substitute the <strong>pooled sample proportion</strong>, <span class="math inline">\(\hat{p}_{pool}\)</span> in for both <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(\pi_2\)</span> in the expression for the standard deviation of the statistic, resulting in its standard error:</p>
<p><span class="math display">\[\begin{eqnarray*}
  SE(\hat{p}_1 -\hat{p}_2) = \sqrt{\frac{\hat{p}_{pool}(1-\hat{p}_{pool})}{n_1} + \frac{\hat{p}_{pool}(1-\hat{p}_{pool})}{n_2}} = \sqrt{\hat{p}_{pool}(1-\hat{p}_{pool})\left(\frac{1}{n_1} + \frac{1}{n_2}\right)}
  \end{eqnarray*}\]</span></p>
This is the standard error formula we will use when computing the test statistic for a hypothesis test of <span class="math inline">\(H_0: \pi_1 - \pi_2 = 0\)</span>.
</div>

<div class="example">
<p>Compute the point estimate of the difference
in breast cancer death rates in the two groups,
and use the pooled proportion
<span class="math inline">\(\hat{p}_{\textit{pool}} = 0.0112\)</span> to calculate
the standard error.</p>
<hr />
<p>The point estimate of the difference in breast cancer death
rates is
<span class="math display">\[\begin{align*}
  \hat{p}_{mgm} - \hat{p}_{ctrl}
    &amp;= \frac{500}{500 + 44,425} - \frac{505}{505 + 44,405} \\
  &amp; \\
    &amp;= 0.01113 - 0.01125 \\
  &amp; \\
    &amp;= -0.00012
  \end{align*}\]</span>
The breast cancer death rate in the mammogram group
was 0.00012 less than in the control group.</p>
Next, the standard error of <span class="math inline">\(\hat{p}_{mgm} - \hat{p}_{ctrl}\)</span> is calculated
<em>using the pooled proportion</em>, <span class="math inline">\(\hat{p}_{\textit{pool}}\)</span>:
<span class="math display">\[\begin{align*}
SE = \sqrt{
      \frac{\hat{p}_{\textit{pool}}(1-\hat{p}_{\textit{pool}})}
          {n_{mgm}}
      + \frac{\hat{p}_{\textit{pool}}(1-\hat{p}_{\textit{pool}})}
          {n_{ctrl}}
    }
	= 0.00070
\end{align*}\]</span>
</div>

<div class="example">
<p>Using the point estimate <span class="math inline">\(\hat{p}_{mgm} - \hat{p}_{ctrl} = -0.00012\)</span> and standard error <span class="math inline">\(SE = 0.00070\)</span>, calculate a p-value for the hypothesis test and write a conclusion.</p>
<hr />
<p>Just like in past tests, we first compute a test statistic and draw a picture:
<span class="math display">\[\begin{align*}
Z = \frac{\text{point estimate} - \text{null value}}{\mbox{Null }SE}
	= \frac{-0.00012 - 0}{0.00070}
	= -0.17
\end{align*}\]</span></p>
The lower tail area below -0.17 on a standard normal distribution is 0.4325, which we double to get the p-value: 0.8650 (see Figure <a href="inference-cat.html#fig:mamm-norm">5.37</a>). With this very large p-value, the difference in breast cancer death rates is reasonably explained by chance, and we have no significant evidence that mammograms either decrease or increase the risk of death by breast cancer compared to regular breast exams, among women similar to those in the study.
</div>
<div class="figure" style="text-align: center"><span id="fig:mamm-norm"></span>
<img src="05-inference-cat_files/figure-html/mamm-norm-1.png" alt="Standard normal distribution with the p-value shaded. The shaded area represents the probability of observing a difference in sample proportions of -0.17 or further away from zero, if the true proportions were equal." width="70%" />
<p class="caption">
Figure 5.37: Standard normal distribution with the p-value shaded. The shaded area represents the probability of observing a difference in sample proportions of -0.17 or further away from zero, if the true proportions were equal.
</p>
</div>
<p>Can we conclude that mammograms have no benefits or harm?
Here are a few considerations to keep in mind when reviewing
the mammogram study as well as any other medical study:</p>
<ul>
<li>We do not accept the null hypothesis. We can only say
we don’t have sufficient evidence to conclude that
mammograms reduce breast cancer deaths, and
we don’t have sufficient evidence to conclude that
mammograms increase breast cancer deaths.</li>
<li>If mammograms are helpful or harmful, the data
suggest the effect isn’t very large.</li>
<li>Are mammograms more or less expensive than
a non-mammogram breast exam?
If one option is much more expensive than the
other and doesn’t offer clear benefits,
then we should lean towards the less expensive
option.</li>
<li>The study’s authors also found that mammograms
led to over-diagnosis of breast cancer,
which means some breast cancers were found
(or thought to be found) but that these cancers
would not cause symptoms during patients’ lifetimes.
That is, something else would kill the patient
before breast cancer symptoms appeared.
This means some patients may have been treated
for breast cancer unnecessarily, and this
treatment is another cost to consider.
It is also important to recognize that
over-diagnosis can cause unnecessary physical
or emotional harm to patients.</li>
</ul>
<p>These considerations highlight the complexity around medical care and treatment recommendations. Experts and medical boards who study medical treatments use considerations like those above to provide their best recommendation based on the current evidence.</p>
<p>
</p>
<!--
\BeginKnitrBlock{onebox}<div class="onebox">**Hypothesis testing when ${H_0}$ is $\pi_1 - \pi_2 = 0$.**
  
  Once you've determined a hypothesis test for the difference
  of two proportions is the correct procedure, there are four
  steps to completing the test:

* **Prepare.**
      Identify the parameter of interest,
      list out hypotheses,
      identify the significance level,
      and compute summary statistics for each group.
* **Check.**
      Verify the conditions to ensure
      $\hat{p}_1 - \hat{p}_2$ is nearly normal under $H_0$.
      When the null hypothesis is that the difference is 0,
      use a pooled proportion to check the success-failure
      condition for each group.
* **Calculate.**
      If the conditions hold, compute the standard
      error, again using the pooled proportion,
      compute the Z-score, and identify the p-value.
* **Conclude.**
      Evaluate the hypothesis test by comparing the p-value
      to $\alpha$, and provide a conclusion in the context
      of the problem.</div>\EndKnitrBlock{onebox}
-->
</div>
</div>
</div>
<div id="power" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Power, Errors, and Practical Importance</h2>

<div class="todo">
<ul>
<li>power</li>
<li>what affects power</li>
<li>what affects width of CIs</li>
<li>statistical significance vs practical importance</li>
<li>effect size?
</div></li>
</ul>
<p><img src="05/images/whalberg.png" width="50%" style="display: block; margin: auto;" /></p>
</div>
<div id="summary-of-z-procedures" class="section level2" number="5.6">
<h2><span class="header-section-number">5.6</span> Summary of Z-procedures</h2>
<p>So far in this chapter, we have seen the normal distribution applied as the appropriate mathematical model in two distinct settings. Although the two data structures are different, their similarities and differences are worth pointing out. We provide Table <a href="inference-cat.html#tab:zcompare">5.12</a> partly as a mechanism for understanding <span class="math inline">\(z\)</span>-procedures and partly to highlight the extremely common usage of the normal distribution in practice.
You will often hear the following two <span class="math inline">\(z\)</span>-procedures referred to as a <strong>one sample <span class="math inline">\(z\)</span>-test</strong> (<span class="math inline">\(z\)</span>-interval) and <strong>two sample <span class="math inline">\(z\)</span>-test</strong> (<span class="math inline">\(z\)</span>-interval).</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:zcompare">Table 5.12: </span>Similarities of <span class="math inline">\(z\)</span>-methods across one sample and two independent samples analysis of a categorical response variable.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
one sample
</th>
<th style="text-align:left;">
two indep. samples
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
response variable
</td>
<td style="text-align:left;">
binary
</td>
<td style="text-align:left;">
binary
</td>
</tr>
<tr>
<td style="text-align:left;">
explanatory variable
</td>
<td style="text-align:left;">
none
</td>
<td style="text-align:left;">
binary
</td>
</tr>
<tr>
<td style="text-align:left;">
parameter of interest
</td>
<td style="text-align:left;">
proportion: <span class="math inline">\(\pi\)</span>
</td>
<td style="text-align:left;">
diff in props:<span class="math inline">\(\pi_1 - \pi_2\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
statistic of interest
</td>
<td style="text-align:left;">
proportion: <span class="math inline">\(\hat{p}\)</span>
</td>
<td style="text-align:left;">
diff in props: <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
null standard error
</td>
<td style="text-align:left;">
<span class="math inline">\(\sqrt{\frac{\pi_0(1-\pi_0)}{n}}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\sqrt{\hat{p}_{pool}(1-\hat{p}_{pool})\left(\frac{1}{n_1} + \frac{1}{n_2}\right)}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
standard error
</td>
<td style="text-align:left;">
<span class="math inline">\(\sqrt{\frac{\hat{p}(1-\hat{p})}{n}}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
conditions
</td>
<td style="text-align:left;">
<ol style="list-style-type: decimal">
<li>independence, 2. large samples (at least 10 successes and 10 failures)
</td>
<td style="text-align:left;">
<ol style="list-style-type: decimal">
<li>independence, 2. large samples (at least 10 successes and 10 failures in each sample)
</td>
</tr>
</tbody>
</table></li>
</ol></li>
</ol>
<p><strong>Hypothesis tests.</strong> When applying the normal distribution for a hypothesis test, we proceed as follows:</p>
<ol style="list-style-type: decimal">
<li>Write appropriate hypotheses.<br />
</li>
<li>Verify conditions for using the normal distribution.
<ul>
<li>One-sample: the observations must be independent, and you must have at least 10 successes and 10 failures.<br />
</li>
<li>For a difference of proportions: each sample must separately satisfy the one-sample conditions for the normal distribution, and the data in the groups must also be independent.</li>
</ul></li>
<li>Compute the statistic of interest, the null standard error, and the degrees of freedom. For <span class="math inline">\(df\)</span>, use <span class="math inline">\(n-1\)</span> for one sample, and for two samples use either statistical software or the smaller of <span class="math inline">\(n_1 - 1\)</span> and <span class="math inline">\(n_2 - 1\)</span>.<br />
</li>
<li>Compute the T-score using the general formula:
<span class="math display">\[
 T = \frac{\mbox{statistic} - \mbox{null value}}{\mbox{null standard error}}
 \]</span></li>
<li>Use the statistical software to find the p-value using the standard normal distribution:
<ul>
<li>Sign in <span class="math inline">\(H_a\)</span> is <span class="math inline">\(&lt;\)</span>: p-value = area below Z-score</li>
<li>Sign in <span class="math inline">\(H_a\)</span> is <span class="math inline">\(&gt;\)</span>: p-value = area above Z-score</li>
<li>Sign in <span class="math inline">\(H_a\)</span> is <span class="math inline">\(\neq\)</span>: p-value = 2 <span class="math inline">\(\times\)</span> area below <span class="math inline">\(-|\mbox{Z-score}|\)</span></li>
</ul></li>
<li>Make a conclusion based on the p-value, and write a conclusion in context, in plain language, and in terms of the alternative hypothesis.</li>
</ol>
<p><strong>Confidence intervals.</strong> Similarly, the following is how we generally compute a confidence interval using a normal distribution:</p>
<ol style="list-style-type: decimal">
<li>Verify conditions for using the normal distribution. (See above.)<br />
</li>
<li>Compute the statistic of interest, the standard error, and <span class="math inline">\(z^{\star}\)</span>.<br />
</li>
<li>Calculate the confidence interval using the general formula:
<span class="math display">\[
 \mbox{statistic} \pm\ z^{\star} SE.
 \]</span></li>
<li>Put the conclusions in context and in plain language so even non-data scientists can understand the results.</li>
</ol>
</div>
<div id="r-inference-for-categorical-data" class="section level2" number="5.7">
<h2><span class="header-section-number">5.7</span> <code>R</code>: Inference for categorical data</h2>

<div class="todo">
Section on doing inference for categorical data in R.
- Raw data to tables
- Simulation functions in catstats
- <code>prop.test</code>
</div>
<div id="interactive-r-tutorials" class="section level3" number="5.7.1">
<h3><span class="header-section-number">5.7.1</span> Interactive R tutorials</h3>
<p>Navigate the concepts you’ve learned in this chapter in R using the following self-paced tutorials.
All you need is your browser to get started!</p>
<div class="alltutorials">
<p><a href="https://openintrostat.github.io/ims-tutorials/05-introduction-to-statistical-inference/">Tutorial 5: Introduction to statistical inference</a></p>
</div>
<div class="singletutorial">
<p><a href="https://openintro.shinyapps.io/ims-05-introduction-to-statistical-inference-01/">Tutorial 5 - Lesson 1: Sampling variability</a></p>
</div>
<div class="singletutorial">
<p><a href="https://openintro.shinyapps.io/ims-05-introduction-to-statistical-inference-02/">Tutorial 5 - Lesson 2: Randomization test</a></p>
</div>
<div class="singletutorial">
<p><a href="https://openintro.shinyapps.io/ims-05-introduction-to-statistical-inference-03/">Tutorial 5 - Lesson 3: Errors in hypothesis testing</a></p>
</div>
<div class="singletutorial">
<p><a href="https://openintro.shinyapps.io/ims-05-introduction-to-statistical-inference-04/">Tutorial 5 - Lesson 4: Parameters and confidence intervals</a></p>
</div>
<div class="alltutorials">
<p><a href="https://openintrostat.github.io/ims-tutorials/06-inference-for-categorical-responses/">Tutorial 6: Inference for categorical responses</a></p>
</div>
<div class="singletutorial">
<p><a href="https://openintro.shinyapps.io/ims-06-inference-for-categorical-responses-01/">Tutorial 6 - Lesson 1: Inference for a single proportion</a></p>
</div>
<div class="singletutorial">
<p><a href="https://openintro.shinyapps.io/ims-06-inference-for-categorical-responses-02/">Tutorial 6 - Lesson 2: Hypothesis tests to compare proportions</a></p>
</div>
<p>You can also access the full list of tutorials supporting this book <a href="https://openintrostat.github.io/ims-tutorials/">here</a>.</p>
</div>
<div id="r-labs" class="section level3" number="5.7.2">
<h3><span class="header-section-number">5.7.2</span> R labs</h3>
<p>Further apply the concepts you’ve learned in this chapter in R with computational labs that walk you through a data analysis case study.</p>
<div class="singlelab">
<p><a href="https://openintro.shinyapps.io/sampling_distributions/">Sampling distributions - Does science benefit you?</a></p>
</div>
<div class="singlelab">
<p><a href="https://openintro.shinyapps.io/confidence_intervals/">Confidence intervals - Climate change</a></p>
</div>
<div class="singlelab">
<p><a href="https://openintro.shinyapps.io/inf_for_categorical_data/">Inference for categorical responses - Texting while driving</a></p>
</div>
<div class="alllabs">
<p><a href="http://openintrostat.github.io/oilabs-tidy/">Full list of labs supporting OpenIntro::Introduction to Modern Statistics</a></p>
</div>
</div>
</div>
<div id="chp5-review" class="section level2" number="5.8">
<h2><span class="header-section-number">5.8</span> Chapter 5 review</h2>
<div id="terms" class="section level3" number="5.8.1">
<h3><span class="header-section-number">5.8.1</span> Terms</h3>
<p>We introduced the following terms in the chapter.
If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions.
We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate.
However you should be able to easily spot them as <strong>bolded text</strong>.</p>
<table>
<tbody>
<tr>
<td style="text-align:left;">
alternative hypothesis
</td>
<td style="text-align:left;">
null distribution
</td>
<td style="text-align:left;">
pooled proportion
</td>
<td style="text-align:left;">
statistically significant
</td>
</tr>
<tr>
<td style="text-align:left;">
bootstrapping
</td>
<td style="text-align:left;">
null hypothesis
</td>
<td style="text-align:left;">
randomization
</td>
<td style="text-align:left;">
success
</td>
</tr>
<tr>
<td style="text-align:left;">
Central Limit Theorem
</td>
<td style="text-align:left;">
null value
</td>
<td style="text-align:left;">
sampling distribution
</td>
<td style="text-align:left;">
success-failure condition
</td>
</tr>
<tr>
<td style="text-align:left;">
confidence interval
</td>
<td style="text-align:left;">
one sample <span class="math inline">\(z\)</span>-test
</td>
<td style="text-align:left;">
SE interval
</td>
<td style="text-align:left;">
test statistic
</td>
</tr>
<tr>
<td style="text-align:left;">
confidence level
</td>
<td style="text-align:left;">
one-sided hypothesis test
</td>
<td style="text-align:left;">
simulation
</td>
<td style="text-align:left;">
two sample <span class="math inline">\(z\)</span>-test
</td>
</tr>
<tr>
<td style="text-align:left;">
confirmation bias
</td>
<td style="text-align:left;">
p-value
</td>
<td style="text-align:left;">
standard error
</td>
<td style="text-align:left;">
two-sided hypothesis test
</td>
</tr>
<tr>
<td style="text-align:left;">
hypothesis test
</td>
<td style="text-align:left;">
parameter
</td>
<td style="text-align:left;">
standard error for difference in proportions
</td>
<td style="text-align:left;">
Type 1 Error
</td>
</tr>
<tr>
<td style="text-align:left;">
margin of error
</td>
<td style="text-align:left;">
percentile
</td>
<td style="text-align:left;">
standard error of single proportion
</td>
<td style="text-align:left;">
Type 2 Error
</td>
</tr>
<tr>
<td style="text-align:left;">
normal curve
</td>
<td style="text-align:left;">
percentile interval
</td>
<td style="text-align:left;">
standard normal distribution
</td>
<td style="text-align:left;">
Z-score
</td>
</tr>
<tr>
<td style="text-align:left;">
normal distribution
</td>
<td style="text-align:left;">
permutation test
</td>
<td style="text-align:left;">
statistic
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
normal model
</td>
<td style="text-align:left;">
point estimate
</td>
<td style="text-align:left;">
statistical inference
</td>
<td style="text-align:left;">
</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="58">
<li id="fn58"><p>We would be assuming that these two variables are <strong>independent</strong>.<a href="inference-cat.html#fnref58" class="footnote-back">↩︎</a></p></li>
<li id="fn59"><p>If you are a STAT 216 student, you will recognize this from our first week’s in-class activity.<a href="inference-cat.html#fnref59" class="footnote-back">↩︎</a></p></li>
<li id="fn60"><p>Bumba is the Martian letter on the left!<a href="inference-cat.html#fnref60" class="footnote-back">↩︎</a></p></li>
<li id="fn61"><p>A fair coin has a 50% chance of landing on heads, which is the chance a student would guess Bumba correctly if they were just guessing. Thus, toss a coin 38 times with heads representing “guess correctly”; then calculate the proportion of tosses that landed on heads. Another option would be to 10 black cards and 10 red cards, letting red represent “guess correctly”. Shuffle the cards and draw one card, record if it is red or black, then replace the card and shuffle again. Do this 38 times and calculate the proportion of red cards observed.<a href="inference-cat.html#fnref61" class="footnote-back">↩︎</a></p></li>
<li id="fn62"><p>To explore this further, watch this <a href="https://www.ted.com/talks/vs_ramachandran_3_clues_to_understanding_your_brain">TED Talk</a> by neurologist Vilayanur Ramachandran (The synesthesia part begins at roughly 17:40 minutes).<a href="inference-cat.html#fnref62" class="footnote-back">↩︎</a></p></li>
<li id="fn63"><p>If you carry out the calculations, you’ll note that the upper bound is actually <span class="math inline">\(0.89 + 0.16 = 1.05\)</span>, but since a sample proportion cannot be greater than 1, we truncated the interval to 1.<a href="inference-cat.html#fnref63" class="footnote-back">↩︎</a></p></li>
<li id="fn64"><p>The first possibility (<em>We can’t read Martian, and these results just occurred by chance.</em>) was the null hypothesis; the second possibility (<em>We can read Martian, and these results reflect this ability.</em>) was the alternative hypothesis.<a href="inference-cat.html#fnref64" class="footnote-back">↩︎</a></p></li>
<li id="fn65"><p>Technically, the observed sample statistic or one more extreme in the direction of our alternative. But it is helpful to just remember this as “the data”.<a href="inference-cat.html#fnref65" class="footnote-back">↩︎</a></p></li>
<li id="fn66"><p>Since a smaller p-value gives you stronger evidence <em>against</em> the null hypothesis, we reject <span class="math inline">\(H_0\)</span> when the p-value is very small, and fail to reject <span class="math inline">\(H_0\)</span> when the p-value is not small.<a href="inference-cat.html#fnref66" class="footnote-back">↩︎</a></p></li>
<li id="fn67"><p>You will get more practice calculating p-values such as these in this Chapter.<a href="inference-cat.html#fnref67" class="footnote-back">↩︎</a></p></li>
<li id="fn68"><p>Since statistical methods are grounded in probability, technically we can only find strong evidence against a hypothesis, not disprove it.<a href="inference-cat.html#fnref68" class="footnote-back">↩︎</a></p></li>
<li id="fn69"><p>If we want to be more certain we will capture the fish, we might use a wider net. Likewise, we use a wider confidence interval if we want to be more certain that we capture the parameter.<a href="inference-cat.html#fnref69" class="footnote-back">↩︎</a></p></li>
<li id="fn70"><p>It is also introduced as the Gaussian distribution after Frederic Gauss, the first person to formalize its mathematical expression.<a href="inference-cat.html#fnref70" class="footnote-back">↩︎</a></p></li>
<li id="fn71"><p>(a) <span class="math inline">\(N(\mu=5,\sigma=3)\)</span>. (b) <span class="math inline">\(N(\mu=-100, \sigma=10)\)</span>. (c) <span class="math inline">\(N(\mu=2, \sigma=9)\)</span>.<a href="inference-cat.html#fnref71" class="footnote-back">↩︎</a></p></li>
<li id="fn72"><p>We use the standard deviation as a guide. Ann is 1 standard deviation above average on the SAT: <span class="math inline">\(1500 + 300=1800\)</span>. Tom is 0.6 standard deviations above the mean on the ACT: <span class="math inline">\(21+0.6\times 5=24\)</span>. In Figure <a href="inference-cat.html#fig:satActNormals">5.7</a>, we can see that Ann tends to do better with respect to everyone else than Tom did, so her score was better.<a href="inference-cat.html#fnref72" class="footnote-back">↩︎</a></p></li>
<li id="fn73"><p><span class="math inline">\(Z_{Tom} = \frac{x_{Tom} - \mu_{ACT}}{\sigma_{ACT}} = \frac{24 - 21}{5} = 0.6\)</span><a href="inference-cat.html#fnref73" class="footnote-back">↩︎</a></p></li>
<li id="fn74"><p>(a) Its Z-score is given by <span class="math inline">\(Z = \frac{x-\mu}{\sigma} = \frac{5.19 - 3}{2} = 2.19/2 = 1.095\)</span>. (b) The observation <span class="math inline">\(x\)</span> is 1.095 standard deviations <em>above</em> the mean. We know it must be above the mean since <span class="math inline">\(Z\)</span> is positive.<a href="inference-cat.html#fnref74" class="footnote-back">↩︎</a></p></li>
<li id="fn75"><p>For <span class="math inline">\(x_1=95.4\)</span> mm: <span class="math inline">\(Z_1 = \frac{x_1 - \mu}{\sigma} = \frac{95.4 - 92.6}{3.6} = 0.78\)</span>. For <span class="math inline">\(x_2=85.8\)</span> mm: <span class="math inline">\(Z_2 = \frac{85.8 - 92.6}{3.6} = -1.89\)</span>.<a href="inference-cat.html#fnref75" class="footnote-back">↩︎</a></p></li>
<li id="fn76"><p>Because the <em>absolute value</em> of Z-score for the second observation is larger than that of the first, the second observation has a more unusual head length.<a href="inference-cat.html#fnref76" class="footnote-back">↩︎</a></p></li>
<li id="fn77"><p>If 84% had lower scores than Ann, the number of people who had better scores must be 16%. (Generally ties are ignored when the normal model, or any other continuous distribution, is used.)<a href="inference-cat.html#fnref77" class="footnote-back">↩︎</a></p></li>
<li id="fn78"><p>We found the probability to be 0.6664. A picture for this exercise is represented by the shaded area below “0.6664”.<a href="inference-cat.html#fnref78" class="footnote-back">↩︎</a></p></li>
<li id="fn79"><p>If Edward did better than 37% of SAT takers, then about 63% must have done better than him.
<a href="inference-cat.html#fnref79" class="footnote-back">↩︎</a></p></li>
<li id="fn80"><p>Numerical answers: (a) 0.9772. (b) 0.0228.<a href="inference-cat.html#fnref80" class="footnote-back">↩︎</a></p></li>
<li id="fn81"><p>This sample was taken from the USDA Food Commodity Intake Database.<a href="inference-cat.html#fnref81" class="footnote-back">↩︎</a></p></li>
<li id="fn82"><p>First put the heights into inches: 67 and 76 inches. Figures are shown below. (a) <span class="math inline">\(Z_{Mike} = \frac{67 - 70}{3.3} = -0.91\ \to\ 0.1814\)</span>. (b) <span class="math inline">\(Z_{Jim} = \frac{76 - 70}{3.3} = 1.82\ \to\ 0.9656\)</span>. \<a href="inference-cat.html#fnref82" class="footnote-back">↩︎</a></p></li>
<li id="fn83"><p>Remember: draw a picture first, then find the Z-score. (We leave the pictures to you.) The Z-score can be found by using the percentiles and the normal probability table. (a) We look for 0.95 in the probability portion (middle part) of the normal probability table, which leads us to row 1.6 and (about) column 0.05, i.e., <span class="math inline">\(Z_{95}=1.65\)</span>. Knowing <span class="math inline">\(Z_{95}=1.65\)</span>, <span class="math inline">\(\mu = 1500\)</span>, and <span class="math inline">\(\sigma = 300\)</span>, we setup the Z-score formula: <span class="math inline">\(1.65 = \frac{x_{95} - 1500}{300}\)</span>. We solve for <span class="math inline">\(x_{95}\)</span>: <span class="math inline">\(x_{95} = 1995\)</span>. (b) Similarly, we find <span class="math inline">\(Z_{97.5} = 1.96\)</span>, again setup the Z-score formula for the heights, and calculate <span class="math inline">\(x_{97.5} = 76.5\)</span>.<a href="inference-cat.html#fnref83" class="footnote-back">↩︎</a></p></li>
<li id="fn84"><p>Numerical answers: (a) 0.1131. (b) 0.3821.<a href="inference-cat.html#fnref84" class="footnote-back">↩︎</a></p></li>
<li id="fn85"><p>This is an abbreviated solution. (Be sure to draw a figure!) First find the percent who get below 1500 and the percent that get above 2000: <span class="math inline">\(Z_{1500} = 0.00 \to 0.5000\)</span> (area below), <span class="math inline">\(Z_{2000} = 1.67 \to 0.0475\)</span> (area above). Final answer: <span class="math inline">\(1.0000-0.5000 - 0.0475 = 0.4525\)</span>.<a href="inference-cat.html#fnref85" class="footnote-back">↩︎</a></p></li>
<li id="fn86"><p>5’5’’ is 65 inches. 5’7’’ is 67 inches. Numerical solution: <span class="math inline">\(1.000 - 0.0649 - 0.8183 = 0.1168\)</span>, i.e., 11.68%.<a href="inference-cat.html#fnref86" class="footnote-back">↩︎</a></p></li>
<li id="fn87"><p>First draw the pictures. To find the area between <span class="math inline">\(Z=-1\)</span> and <span class="math inline">\(Z=1\)</span>, use <code>pnorm()</code> to determine the areas below <span class="math inline">\(Z=-1\)</span> and above <span class="math inline">\(Z=1\)</span>. Next verify the area between <span class="math inline">\(Z=-1\)</span> and <span class="math inline">\(Z=1\)</span> is about 0.68. Repeat this for <span class="math inline">\(Z=-2\)</span> to <span class="math inline">\(Z=2\)</span> and also for <span class="math inline">\(Z=-3\)</span> to <span class="math inline">\(Z=3\)</span>.<a href="inference-cat.html#fnref87" class="footnote-back">↩︎</a></p></li>
<li id="fn88"><p>(a) 900 and 2100 represent two standard deviations above and below the mean, which means about 95% of test takers will score between 900 and 2100. (b) Since the normal model is symmetric, then half of the test takers from part (a) (<span class="math inline">\(\frac{95\%}{2} = 47.5\%\)</span> of all test takers) will score 900 to 1500 while 47.5% score between 1500 and 2100.<a href="inference-cat.html#fnref88" class="footnote-back">↩︎</a></p></li>
<li id="fn89"><p>When you see <span class="math inline">\(\pi\)</span> in this textbook, it will always symbolize a (typically unknown) population proportion, not the value 3.14….<a href="inference-cat.html#fnref89" class="footnote-back">↩︎</a></p></li>
<li id="fn90"><p>The terms “success” and “failure” may not actually represent outcomes we view as successful or not, but it is the typical generic way to referring to the possible outcomes of a binary variable. The “success” is whatever we count when calculating our sample proportion.<a href="inference-cat.html#fnref90" class="footnote-back">↩︎</a></p></li>
<li id="fn91"><p>Parameters were first introduced in Section <a href="eda.html#dotplots">2.3.2</a><a href="inference-cat.html#fnref91" class="footnote-back">↩︎</a></p></li>
<li id="fn92"><p>Now would be a good time to review the definition of a p-value in Section <a href="inference-cat.html#HypothesisTesting">5.1.3</a>!<a href="inference-cat.html#fnref92" class="footnote-back">↩︎</a></p></li>
<li id="fn93"><p>One option would be to use a spinner with 10% shaded red, and the rest shaded green. Each spin of the spinner would represent one client. Spin the spinner 62 times and count the number of times the spinner lands on red. The proportion of times the spinner lands on red represents a simulated <span class="math inline">\(\hat{p}\)</span> under the assumption that <span class="math inline">\(\pi = 0.10\)</span>. Other objects include: a bag of marbles with 10% red marbles and 90% white marbles, or 10 cards where 1 is red and 9 are white. Sampling 62 times with replacement from these collections would simulate one sample of clients.<a href="inference-cat.html#fnref93" class="footnote-back">↩︎</a></p></li>
<li id="fn94"><p>There isn’t sufficiently strong evidence to support the claim that fewer than 10% of the consultant’s clients experience complications. That is, there isn’t sufficiently strong evidence to support an association between the consultant’s work and fewer surgery complications.<a href="inference-cat.html#fnref94" class="footnote-back">↩︎</a></p></li>
<li id="fn95"><p>No. It might be that the consultant’s work is associated with a reduction but that there isn’t enough data to convincingly show this connection.<a href="inference-cat.html#fnref95" class="footnote-back">↩︎</a></p></li>
<li id="fn96"><p>If you’re curious where the term “bootstrapping” comes from, it comes from the phrase “lift yourself up by your own bootstraps.” Lifting yourself up by your own bootstraps is analogous to creating more samples from the single original sample.<a href="inference-cat.html#fnref96" class="footnote-back">↩︎</a></p></li>
<li id="fn97"><p>Since in the original sample, 3 out of 62, or about 5% had complications, we could expect about 5% of the patients (6.2 on average) in the simulation will have a complication, though we will see a little variation from one simulation to the next.<a href="inference-cat.html#fnref97" class="footnote-back">↩︎</a></p></li>
<li id="fn98"><p>While this book is scoped to well-constrained statistical
problems, do remember that this is just the first
book in what is a large library of statistical methods that
are suitable for a very wide range of data and contexts.<a href="inference-cat.html#fnref98" class="footnote-back">↩︎</a></p></li>
<li id="fn99"><p>Independence holds since the poll
is based on a random sample.
The success-failure condition also holds,
which is checked
using the null value (<span class="math inline">\(p_0 = 0.5\)</span>) from <span class="math inline">\(H_0\)</span>:
<span class="math inline">\(np_0 = 826 \times 0.5 = 413\)</span>,
<span class="math inline">\(n(1 - p_0) = 826 \times 0.5 = 413\)</span>. Recall that here, the best guess for <span class="math inline">\(\pi\)</span> is <span class="math inline">\(p_0\)</span> which comes from the null hypothesis (because we assume the null hypothesis is true when performing the testing procedure steps). <span class="math inline">\(H_0\)</span>: there is not support for the regulation; <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\pi \leq 0.50\)</span>. <span class="math inline">\(H_A\)</span>: the majority of borrowers support the regulation; <span class="math inline">\(H_A\)</span>: <span class="math inline">\(\pi &gt; 0.50\)</span>.<a href="inference-cat.html#fnref99" class="footnote-back">↩︎</a></p></li>
<li id="fn100"><p>Because the <span class="math inline">\(\pi\)</span> is unknown but expected to be around 2/3, we will use 2/3 in place of <span class="math inline">\(\pi\)</span> in the formula for the standard deviation of <span class="math inline">\(\hat{p}\)</span> and calculate<br />
<span class="math inline">\(SE = \sqrt{\frac{2/3 (1 - 2/3)}  {300}} = 0.027\)</span>.<a href="inference-cat.html#fnref100" class="footnote-back">↩︎</a></p></li>
<li id="fn101"><p>This is equivalent to asking how often the <span class="math inline">\(Z\)</span> score will be larger than -2.58 but less than 2.58. (For a picture, see Figure <a href="inference-cat.html#fig:choosingZForCI">5.17</a>.) To determine this probability, look up -2.58 and 2.58 in the normal probability table (0.0049 and 0.9951). Thus, there is a <span class="math inline">\(0.9951-0.0049 \approx 0.99\)</span> probability that the unobserved random variable <span class="math inline">\(X\)</span> will be within 2.58 standard deviations of the mean.<a href="inference-cat.html#fnref101" class="footnote-back">↩︎</a></p></li>
<li id="fn102"><p>Since the necessary conditions for applying the normal model have already been checked for us, we can go straight to the construction of the confidence interval: <span class="math inline">\(\text{point estimate}\ \pm\ 2.58 \times SE \rightarrow (0.018, 0.162)\)</span>. We are 99% confident that implanting a stent in the brain of a patient who is at risk of stroke increases the risk of stroke within 30 days by a rate of 0.018 to 0.162 (assuming the patients are representative of the population).<a href="inference-cat.html#fnref102" class="footnote-back">↩︎</a></p></li>
<li id="fn103"><p>We must find <span class="math inline">\(z^{\star}\)</span> such that 90% of the distribution falls between -<span class="math inline">\(z^{\star}\)</span> and <span class="math inline">\(z^{\star}\)</span> in the standard normal model, <span class="math inline">\(N(\mu=0, \sigma=1)\)</span>. We can find -<span class="math inline">\(z^{\star}\)</span> from a standard normal distribution by looking for a lower tail of 5% (the other 5% is in the upper tail), thus <span class="math inline">\(z^{\star}=1.645\)</span>. The 90% confidence interval can then be computed as <span class="math inline">\(\text{point estimate}\ \pm\ 1.65\times SE \to (4.4\%, 13.6\%)\)</span>. (Note: The conditions for normality had earlier been confirmed for us.) That is, we are 90% confident that implanting a stent in a stroke patient’s brain increased the risk of stroke within 30 days by 4.4% to 13.6%.<a href="inference-cat.html#fnref103" class="footnote-back">↩︎</a></p></li>
<li id="fn104"><p>Rosen B and Jerdee T. 1974. “Influence of sex role stereotypes on personnel decisions.” Journal of Applied Psychology 59(1):9-14.<a href="inference-cat.html#fnref104" class="footnote-back">↩︎</a></p></li>
<li id="fn105"><p>The study is an experiment, as subjects were randomly assigned a “male” file or a “female” file (remember, all the files were actually identical in content). Since this is an experiment, the results can be used to evaluate a causal relationship between gender of a candidate and the promotion decision.<a href="inference-cat.html#fnref105" class="footnote-back">↩︎</a></p></li>
<li id="fn106"><p>The test procedure we employ in this section is sometimes referred to as a <strong>permutation test</strong>.<a href="inference-cat.html#fnref106" class="footnote-back">↩︎</a></p></li>
<li id="fn107"><p><span class="math inline">\(18/24 - 17/24=0.042\)</span> or about 4.2% in favor of the men.
This difference due to chance is much smaller than the difference observed in the actual groups.<a href="inference-cat.html#fnref107" class="footnote-back">↩︎</a></p></li>
<li id="fn108"><p>This reasoning does not generally extend to anecdotal observations.
Each of us observes incredibly rare events every day, events we could not possibly hope to predict.
However, in the non-rigorous setting of anecdotal evidence, almost anything may appear to be a rare event, so the idea of looking for rare events in day-to-day activities is treacherous.
For example, we might look at the lottery: there was only a 1 in 176 million chance that the Mega Millions numbers for the largest jackpot in history (March 30, 2012) would be (2, 4, 23, 38, 46) with a Mega ball of (23), but nonetheless those numbers came up!
However, no matter what numbers had turned up, they would have had the same incredibly rare odds.
That is, <em>any set of numbers we could have observed would ultimately be incredibly rare</em>.
This type of situation is typical of our daily lives: each possible event in itself seems incredibly rare, but if we consider every alternative, those outcomes are also incredibly rare.
We should be cautious not to misinterpret such anecdotal evidence.<a href="inference-cat.html#fnref108" class="footnote-back">↩︎</a></p></li>
<li id="fn109"><p>This probability of the results we see in our study, under the assumption of no discrimination, is the <strong>p-value</strong>.<a href="inference-cat.html#fnref109" class="footnote-back">↩︎</a></p></li>
<li id="fn110"><p>With a sample of students, randomly assign half of them to the control condition, and the other half to the treatment condition. For those in the control condition, present them with a situation where an item is on sale and ask if they would like to buy the item. For those in the treatment condition, present them with the same situation, but also remind them that they can save money for later purchases, then ask if they would like to buy the item. Compute and compare the proportions who refrained from purchasing the item in each group.<a href="inference-cat.html#fnref110" class="footnote-back">↩︎</a></p></li>
<li id="fn111"><p>Frederick S, Novemsky N, Wang J, Dhar R, Nowlis S. 2009. Opportunity Cost Neglect. Journal of Consumer Research 36: 553-561.<a href="inference-cat.html#fnref111" class="footnote-back">↩︎</a></p></li>
<li id="fn112"><p>Success is often defined in a study as the outcome of interest, and a “success” may or may not actually be a positive outcome. For example, researchers working on a study on HIV prevalence might define a “success” in the statistical sense as a patient who is HIV+. A more complete discussion of the term <strong>success</strong> will be given in Chapter <a href="inference-cat.html#inference-cat">5</a>.<a href="inference-cat.html#fnref112" class="footnote-back">↩︎</a></p></li>
<li id="fn113"><p>The
study is an experiment, as patients were randomly
assigned an experiment group.
Since this is an experiment, the results can be used
to evaluate a causal relationship between the malaria
vaccine and whether patients showed signs
of an infection.<a href="inference-cat.html#fnref113" class="footnote-back">↩︎</a></p></li>
<li id="fn114"><p><span class="math inline">\((0.357 - 1)\times 100\)</span>% = -64.3%<a href="inference-cat.html#fnref114" class="footnote-back">↩︎</a></p></li>
<li id="fn115"><p>With small sample sizes, researchers often
add 0.5 to each of the four cells prior to calculating the sample relative risk
in order to avoid dividing by zero. With this adjustment, the sample relative
risk is: <span class="math inline">\(\frac{5.5/15}{6.5/7} = 0.395\)</span>. We will use this adjustment
when simulating relative risks as well.<a href="inference-cat.html#fnref115" class="footnote-back">↩︎</a></p></li>
<li id="fn116"><p>1. Take 20 notecards to represent the 20 patients, where we write down “infection” on 11 cards and “no infection” on 9 cards.
2. Thoroughly shuffle the notecards and deal 14 into
a “vaccine” pile and 6 into a “placebo” pile. 3. Compute the proportion of “infection” cards in the “vaccine” pile and divide it by the proportion of “infection” cards in the “placebo” pile to get the simulated sample relative risk.<a href="inference-cat.html#fnref116" class="footnote-back">↩︎</a></p></li>
<li id="fn117"><p>This reasoning does not generally extend
to anecdotal observations.
Each of us observes incredibly rare events every day,
events we could not possibly hope to predict.
However, in the non-rigorous setting of anecdotal
evidence, almost anything may appear to be a rare event,
so the idea of looking for rare events in day-to-day
activities is treacherous.
For example, we might look at the lottery:
there was only a 1 in 292 million chance that the
Powerball numbers for the largest jackpot in history
(January 13th, 2016) would be (04, 08, 19, 27, 34)
with a Powerball of (10),
but nonetheless those numbers came up!
However, no matter what numbers had turned up,
they would have had the same incredibly rare odds.
That is, <em>any set of numbers we could have
observed would ultimately be incredibly rare</em>.
This type of situation is typical of our daily lives:
each possible event in itself seems incredibly rare,
but if we consider every alternative, those outcomes
are also incredibly rare.
We should be cautious not to misinterpret such
anecdotal evidence.<a href="inference-cat.html#fnref117" class="footnote-back">↩︎</a></p></li>
<li id="fn118"><p>Making a Type 1 Error in this context would mean that reminding students that money not spent now can be spent later does not affect their buying habits, despite the strong evidence (the data suggesting otherwise) found in the experiment. Notice that this does <em>not</em> necessarily mean something was wrong with the data or that we made a computational mistake. Sometimes data simply point us to the wrong conclusion, which is why scientific studies are often repeated to check initial findings.<a href="inference-cat.html#fnref118" class="footnote-back">↩︎</a></p></li>
<li id="fn119"><p>To lower the Type 2 Error rate, we want to convict more guilty people. We could lower the standards for conviction from “beyond a reasonable doubt” to “beyond a little doubt”. Lowering the bar for guilt will also result in more wrongful convictions, raising the Type 1 Error rate.<a href="inference-cat.html#fnref119" class="footnote-back">↩︎</a></p></li>
<li id="fn120"><p>B<span class="math inline">\(\ddot{\text{o}}\)</span>ttiger et al. “Efficacy and safety of thrombolytic therapy after initially unsuccessful cardiopulmonary resuscitation: a prospective clinical trial.” The Lancet, 2001.<a href="inference-cat.html#fnref120" class="footnote-back">↩︎</a></p></li>
<li id="fn121"><p>Observed control survival rate: <span class="math inline">\(\hat{p}_c = \frac{11}{50} = 0.22\)</span>.
Treatment survival rate: <span class="math inline">\(\hat{p}_t = \frac{14}{40} = 0.35\)</span>.
Observed difference: <span class="math inline">\(\hat{p}_t - \hat{p}_c = 0.35 - 0.22 = 0.13\)</span>.
Relative risk: <span class="math inline">\(\hat{p}_t/\hat{p}_c = 0.35/0.22 = 1.59\)</span><a href="inference-cat.html#fnref121" class="footnote-back">↩︎</a></p></li>
<li id="fn122"><p>Note that the relative risk in the opposite direction is not a <em>decrease</em> of 59%! When comparing control to treatment, the relative risk would be <span class="math inline">\(0.22/0.35 = 0.63\)</span>, or a decrease of 37%. These values differ because the quantity we’re comparing to (the “100%” quantity) differs.<a href="inference-cat.html#fnref122" class="footnote-back">↩︎</a></p></li>
<li id="fn123"><p>If the null distribution is not symmetric, then the computer will have to count the proportions in each tail separately, since the two tail proportions may differ.<a href="inference-cat.html#fnref123" class="footnote-back">↩︎</a></p></li>
<li id="fn124"><p>Because the patients were randomized, the subjects are independent, both within and between the two groups.
The success-failure condition is also met for both groups as all counts are at least 10.
This satisfies the conditions necessary to model the difference in proportions using a normal distribution.
Compute the sample proportions (<span class="math inline">\(\hat{p}_{\text{fish oil}} = 0.0112\)</span>, <span class="math inline">\(\hat{p}_{\text{placebo}} = 0.0155\)</span>), point estimate of the difference (<span class="math inline">\(0.0112 - 0.0155 = -0.0043\)</span>), and standard error <span class="math inline">\(SE = \sqrt{\frac{0.0112 \times 0.9888}{12933} + \frac{0.0155 \times 0.9845}{12938}} = 0.00145\)</span>.
Next, plug the values into the general formula for a confidence interval, where <span class="math inline">\(z^{\star} = 1.96\)</span> for a 95% confidence level: <span class="math inline">\(-0.0043 \pm 1.96 \times 0.00145 \rightarrow (-0.0071, -0.0015)\)</span>.
We are 95% confident that fish oils decreases
heart attacks by
0.15 to 0.71 percentage points
(off of a baseline of about 1.55%)
over a 5-year period for subjects who are similar
to those in the study.
Because the interval is entirely below 0, and the treatment was randomly assigned,
the data provide strong evidence
that fish oil supplements reduce heart attacks
in patients like those in the study.<a href="inference-cat.html#fnref124" class="footnote-back">↩︎</a></p></li>
<li id="fn125"><p>This is an experiment. Patients were randomized
to receive mammograms or a standard breast cancer exam.
We will be able to make causal conclusions based on this study.<a href="inference-cat.html#fnref125" class="footnote-back">↩︎</a></p></li>
<li id="fn126"><p><span class="math inline">\(H_0\)</span>: the breast cancer death rate for patients
screened using mammograms is the same as the breast cancer
death rate for patients in the control,
<span class="math inline">\(\pi_{mgm} - \pi_{ctrl} = 0\)</span>.<br />
<span class="math inline">\(H_A\)</span>: the breast cancer death rate for patients screened
using mammograms is different than the breast cancer death
rate for patients in the control,
<span class="math inline">\(\pi_{mgm} - \pi_{ctrl} \neq 0\)</span>.<a href="inference-cat.html#fnref126" class="footnote-back">↩︎</a></p></li>
<li id="fn127"><p>For an example of a two-proportion
hypothesis test that does not require the
success-failure condition to be met, see
Section <a href="inference-cat.html#two-prop-errors">5.4.1</a>.<a href="inference-cat.html#fnref127" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mult-reg.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inference-num.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/MTstateIntroStats/IntroStatTextbook/edit/master/05-inference-cat.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["stat216-textbook.pdf"],
"toc": {
"collapse": "subsection",
"toc_depth": 4,
"toc_float": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

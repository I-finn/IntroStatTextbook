<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Inference for categorical data | Montana State Introductory Statistics with R</title>
  <meta name="description" content="Open resources textbook for Stat 216 at Montana State University" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Inference for categorical data | Montana State Introductory Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Open resources textbook for Stat 216 at Montana State University" />
  <meta name="github-repo" content="MTstateIntroStats/IntroStatTextbook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Inference for categorical data | Montana State Introductory Statistics with R" />
  
  <meta name="twitter:description" content="Open resources textbook for Stat 216 at Montana State University" />
  

<meta name="author" content="Nicole Carnegie, Stacey Hancock, Elijah Meyer, Jade Schmidt, Melinda Yager" />


<meta name="date" content="2020-08-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mult-reg.html"/>
<link rel="next" href="inference-num.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MSU Intro Stat with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#textbook-overview"><i class="fa fa-check"></i>Textbook overview</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a>
<ul>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html#montana-state-university-authors"><i class="fa fa-check"></i>Montana State University Authors</a></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html#openintro-authors"><i class="fa fa-check"></i>OpenIntro Authors</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="copyright.html"><a href="copyright.html"><i class="fa fa-check"></i>Copyright</a></li>
<li class="chapter" data-level="1" data-path="intro-to-data.html"><a href="intro-to-data.html"><i class="fa fa-check"></i><b>1</b> Introduction to data</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro-to-data.html"><a href="intro-to-data.html#basic-stents-strokes"><i class="fa fa-check"></i><b>1.1</b> Case study: using stents to prevent strokes</a></li>
<li class="chapter" data-level="1.2" data-path="intro-to-data.html"><a href="intro-to-data.html#data-basics"><i class="fa fa-check"></i><b>1.2</b> Data basics</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="intro-to-data.html"><a href="intro-to-data.html#observations-variables-and-data-frames"><i class="fa fa-check"></i><b>1.2.1</b> Observations, variables, and data frames</a></li>
<li class="chapter" data-level="1.2.2" data-path="intro-to-data.html"><a href="intro-to-data.html#variable-types"><i class="fa fa-check"></i><b>1.2.2</b> Types of variables</a></li>
<li class="chapter" data-level="1.2.3" data-path="intro-to-data.html"><a href="intro-to-data.html#variable-relations"><i class="fa fa-check"></i><b>1.2.3</b> Relationships between variables</a></li>
<li class="chapter" data-level="1.2.4" data-path="intro-to-data.html"><a href="intro-to-data.html#explanatory-and-response-variables"><i class="fa fa-check"></i><b>1.2.4</b> Explanatory and response variables</a></li>
<li class="chapter" data-level="1.2.5" data-path="intro-to-data.html"><a href="intro-to-data.html#introducing-observational-studies-and-experiments"><i class="fa fa-check"></i><b>1.2.5</b> Introducing observational studies and experiments</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="intro-to-data.html"><a href="intro-to-data.html#sampling-principles-strategies"><i class="fa fa-check"></i><b>1.3</b> Sampling principles and strategies</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="intro-to-data.html"><a href="intro-to-data.html#populations-and-samples"><i class="fa fa-check"></i><b>1.3.1</b> Populations and samples</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro-to-data.html"><a href="intro-to-data.html#anecdotal-evidence"><i class="fa fa-check"></i><b>1.3.2</b> Anecdotal evidence</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro-to-data.html"><a href="intro-to-data.html#sampling-from-a-population"><i class="fa fa-check"></i><b>1.3.3</b> Sampling from a population</a></li>
<li class="chapter" data-level="1.3.4" data-path="intro-to-data.html"><a href="intro-to-data.html#four-sampling-methods-special-topic"><i class="fa fa-check"></i><b>1.3.4</b> Four sampling methods (special topic)</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro-to-data.html"><a href="intro-to-data.html#observational-studies"><i class="fa fa-check"></i><b>1.4</b> Observational studies</a></li>
<li class="chapter" data-level="1.5" data-path="intro-to-data.html"><a href="intro-to-data.html#experiments"><i class="fa fa-check"></i><b>1.5</b> Experiments</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="intro-to-data.html"><a href="intro-to-data.html#principles-of-experimental-design"><i class="fa fa-check"></i><b>1.5.1</b> Principles of experimental design</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro-to-data.html"><a href="intro-to-data.html#reducing-bias-human-experiments"><i class="fa fa-check"></i><b>1.5.2</b> Reducing bias in human experiments</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="intro-to-data.html"><a href="intro-to-data.html#scope-of-inference"><i class="fa fa-check"></i><b>1.6</b> Scope of inference</a></li>
<li class="chapter" data-level="1.7" data-path="intro-to-data.html"><a href="intro-to-data.html#data-in-r"><i class="fa fa-check"></i><b>1.7</b> Data in <code>R</code></a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="intro-to-data.html"><a href="intro-to-data.html#dataframes-in-r"><i class="fa fa-check"></i><b>1.7.1</b> Dataframes in <code>R</code></a></li>
<li class="chapter" data-level="1.7.2" data-path="intro-to-data.html"><a href="intro-to-data.html#datastruc"><i class="fa fa-check"></i><b>1.7.2</b> Tidy structure of data</a></li>
<li class="chapter" data-level="1.7.3" data-path="intro-to-data.html"><a href="intro-to-data.html#using-the-pipe-to-chain"><i class="fa fa-check"></i><b>1.7.3</b> Using the pipe to chain</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="intro-to-data.html"><a href="intro-to-data.html#chapter-review"><i class="fa fa-check"></i><b>1.8</b> Chapter review</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>1.8.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>2</b> Exploratory data analysis</a>
<ul>
<li class="chapter" data-level="2.1" data-path="eda.html"><a href="eda.html#categorical-data"><i class="fa fa-check"></i><b>2.1</b> Exploring categorical data</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="eda.html"><a href="eda.html#contingency-tables-and-conditional-proportions"><i class="fa fa-check"></i><b>2.1.1</b> Contingency tables and conditional proportions</a></li>
<li class="chapter" data-level="2.1.2" data-path="eda.html"><a href="eda.html#bar-plots-and-mosaic-plots"><i class="fa fa-check"></i><b>2.1.2</b> Bar plots and mosaic plots</a></li>
<li class="chapter" data-level="2.1.3" data-path="eda.html"><a href="eda.html#why-not-pie-charts"><i class="fa fa-check"></i><b>2.1.3</b> Why not pie charts?</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="eda.html"><a href="eda.html#quantitative-data"><i class="fa fa-check"></i><b>2.2</b> Exploring quantitative data</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="eda.html"><a href="eda.html#scatterplots"><i class="fa fa-check"></i><b>2.2.1</b> Scatterplots for paired data</a></li>
<li class="chapter" data-level="2.2.2" data-path="eda.html"><a href="eda.html#dotplots"><i class="fa fa-check"></i><b>2.2.2</b> Dot plots and the mean</a></li>
<li class="chapter" data-level="2.2.3" data-path="eda.html"><a href="eda.html#histograms"><i class="fa fa-check"></i><b>2.2.3</b> Histograms and shape</a></li>
<li class="chapter" data-level="2.2.4" data-path="eda.html"><a href="eda.html#variance-sd"><i class="fa fa-check"></i><b>2.2.4</b> Variance and standard deviation</a></li>
<li class="chapter" data-level="2.2.5" data-path="eda.html"><a href="eda.html#box-plots-quartiles-and-the-median"><i class="fa fa-check"></i><b>2.2.5</b> Box plots, quartiles, and the median</a></li>
<li class="chapter" data-level="2.2.6" data-path="eda.html"><a href="eda.html#describing-and-comparing-quantitative-distributions"><i class="fa fa-check"></i><b>2.2.6</b> Describing and comparing quantitative distributions</a></li>
<li class="chapter" data-level="2.2.7" data-path="eda.html"><a href="eda.html#robust-statistics"><i class="fa fa-check"></i><b>2.2.7</b> Robust statistics</a></li>
<li class="chapter" data-level="2.2.8" data-path="eda.html"><a href="eda.html#transforming-data-special-topic"><i class="fa fa-check"></i><b>2.2.8</b> Transforming data (special topic)</a></li>
<li class="chapter" data-level="2.2.9" data-path="eda.html"><a href="eda.html#mapping-data"><i class="fa fa-check"></i><b>2.2.9</b> Mapping data</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="eda.html"><a href="eda.html#exploratory-data-analysis-in-r"><i class="fa fa-check"></i><b>2.3</b> Exploratory data analysis in <code>R</code></a></li>
<li class="chapter" data-level="2.4" data-path="eda.html"><a href="eda.html#chp2-review"><i class="fa fa-check"></i><b>2.4</b> Chapter 2 review</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>2.4.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="cor-reg.html"><a href="cor-reg.html"><i class="fa fa-check"></i><b>3</b> Correlation and regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="cor-reg.html"><a href="cor-reg.html#chp3-review"><i class="fa fa-check"></i><b>3.1</b> Chapter 3 review</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>3.1.1</b> Terms</a></li>
<li class="chapter" data-level="3.1.2" data-path="cor-reg.html"><a href="cor-reg.html#chapter-exercises"><i class="fa fa-check"></i><b>3.1.2</b> Chapter exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mult-reg.html"><a href="mult-reg.html"><i class="fa fa-check"></i><b>4</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="mult-reg.html"><a href="mult-reg.html#num-vs.-whatever---mlr"><i class="fa fa-check"></i><b>4.1</b> Num vs. whatever - MLR</a></li>
<li class="chapter" data-level="4.2" data-path="mult-reg.html"><a href="mult-reg.html#parallel-slopes"><i class="fa fa-check"></i><b>4.2</b> Parallel slopes</a></li>
<li class="chapter" data-level="4.3" data-path="mult-reg.html"><a href="mult-reg.html#hint-at-interaction-planes-and-parallel-planes-but-not-quantify"><i class="fa fa-check"></i><b>4.3</b> Hint at interaction, planes, and parallel planes but not quantify</a></li>
<li class="chapter" data-level="4.4" data-path="mult-reg.html"><a href="mult-reg.html#chp4-review"><i class="fa fa-check"></i><b>4.4</b> Chapter 4 review</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>4.4.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inference-cat.html"><a href="inference-cat.html"><i class="fa fa-check"></i><b>5</b> Inference for categorical data</a>
<ul>
<li class="chapter" data-level="5.1" data-path="inference-cat.html"><a href="inference-cat.html#inf-foundations"><i class="fa fa-check"></i><b>5.1</b> Foundations of inference</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="inference-cat.html"><a href="inference-cat.html#Martian"><i class="fa fa-check"></i><b>5.1.1</b> Motivating example: Martian alphabet</a></li>
<li class="chapter" data-level="5.1.2" data-path="inference-cat.html"><a href="inference-cat.html#HypothesisTesting"><i class="fa fa-check"></i><b>5.1.2</b> Hypothesis tests</a></li>
<li class="chapter" data-level="5.1.3" data-path="inference-cat.html"><a href="inference-cat.html#ConfidenceIntervals"><i class="fa fa-check"></i><b>5.1.3</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="inference-cat.html"><a href="inference-cat.html#single-prop"><i class="fa fa-check"></i><b>5.2</b> One proportion</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="inference-cat.html"><a href="inference-cat.html#one-prop-null-boot"><i class="fa fa-check"></i><b>5.2.1</b> Bootstrap test for <span class="math inline">\(H_0: p = p_0\)</span></a></li>
<li class="chapter" data-level="5.2.2" data-path="inference-cat.html"><a href="inference-cat.html#one-prop-norm"><i class="fa fa-check"></i><b>5.2.2</b> Mathematical model</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inference-cat.html"><a href="inference-cat.html#diff-two-prop"><i class="fa fa-check"></i><b>5.3</b> Difference of two proportions</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="inference-cat.html"><a href="inference-cat.html#two-prop-errors"><i class="fa fa-check"></i><b>5.3.1</b> Randomization test for <span class="math inline">\(H_0: p_1 - p_2 = 0\)</span></a></li>
<li class="chapter" data-level="5.3.2" data-path="inference-cat.html"><a href="inference-cat.html#two-prop-boot-ci"><i class="fa fa-check"></i><b>5.3.2</b> Bootstrap confidence interval for <span class="math inline">\(p_1 - p_2\)</span></a></li>
<li class="chapter" data-level="5.3.3" data-path="inference-cat.html"><a href="inference-cat.html#math-2prop"><i class="fa fa-check"></i><b>5.3.3</b> Mathematical model</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="inference-cat.html"><a href="inference-cat.html#independence-in-two-way-tables"><i class="fa fa-check"></i><b>5.4</b> Independence in two-way tables</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="inference-cat.html"><a href="inference-cat.html#randomization-test-of-h_0-independence"><i class="fa fa-check"></i><b>5.4.1</b> Randomization test of <span class="math inline">\(H_0:\)</span> independence</a></li>
<li class="chapter" data-level="5.4.2" data-path="inference-cat.html"><a href="inference-cat.html#mathematical-model"><i class="fa fa-check"></i><b>5.4.2</b> Mathematical model</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="inference-cat.html"><a href="inference-cat.html#chp6-review"><i class="fa fa-check"></i><b>5.5</b> Chapter 6 review</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>5.5.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inference-num.html"><a href="inference-num.html"><i class="fa fa-check"></i><b>6</b> Inference for numerical data</a>
<ul>
<li class="chapter" data-level="6.1" data-path="inference-num.html"><a href="inference-num.html#one-mean"><i class="fa fa-check"></i><b>6.1</b> One mean</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="inference-num.html"><a href="inference-num.html#bootstrap-confidence-interval-for-mu"><i class="fa fa-check"></i><b>6.1.1</b> Bootstrap confidence interval for <span class="math inline">\(\mu\)</span></a></li>
<li class="chapter" data-level="6.1.2" data-path="inference-num.html"><a href="inference-num.html#one-mean-math"><i class="fa fa-check"></i><b>6.1.2</b> Mathematical model</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="inference-num.html"><a href="inference-num.html#paired-data"><i class="fa fa-check"></i><b>6.2</b> Paired difference</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="inference-num.html"><a href="inference-num.html#case-study"><i class="fa fa-check"></i><b>6.2.1</b> case study</a></li>
<li class="chapter" data-level="6.2.2" data-path="inference-num.html"><a href="inference-num.html#randomization-test-for-h_0-mu_d-0"><i class="fa fa-check"></i><b>6.2.2</b> Randomization test for <span class="math inline">\(H_0: \mu_d = 0\)</span></a></li>
<li class="chapter" data-level="6.2.3" data-path="inference-num.html"><a href="inference-num.html#bootstrap-confidence-interval-for-mu_d"><i class="fa fa-check"></i><b>6.2.3</b> Bootstrap confidence interval for <span class="math inline">\(\mu_d\)</span></a></li>
<li class="chapter" data-level="6.2.4" data-path="inference-cat.html"><a href="inference-cat.html#mathematical-model"><i class="fa fa-check"></i><b>6.2.4</b> Mathematical model</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="inference-num.html"><a href="inference-num.html#difference-of-two-means"><i class="fa fa-check"></i><b>6.3</b> Difference of two means</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="inference-num.html"><a href="inference-num.html#case-study-1"><i class="fa fa-check"></i><b>6.3.1</b> case study</a></li>
<li class="chapter" data-level="6.3.2" data-path="inference-num.html"><a href="inference-num.html#randomization-test-for-h_0-mu_1---mu_2-0"><i class="fa fa-check"></i><b>6.3.2</b> Randomization test for <span class="math inline">\(H_0: \mu_1 - \mu_2 = 0\)</span></a></li>
<li class="chapter" data-level="6.3.3" data-path="inference-num.html"><a href="inference-num.html#bootstrap-confidence-interval-for-mu_1---mu_2"><i class="fa fa-check"></i><b>6.3.3</b> Bootstrap confidence interval for <span class="math inline">\(\mu_1 - \mu_2\)</span></a></li>
<li class="chapter" data-level="6.3.4" data-path="inference-num.html"><a href="inference-num.html#mathematical-model-1"><i class="fa fa-check"></i><b>6.3.4</b> Mathematical model</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="inference-num.html"><a href="inference-num.html#chp7-review"><i class="fa fa-check"></i><b>6.4</b> Chapter 7 review</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>6.4.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="inference-reg.html"><a href="inference-reg.html"><i class="fa fa-check"></i><b>7</b> Inference for regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="inference-reg.html"><a href="inference-reg.html#chp8-review"><i class="fa fa-check"></i><b>7.1</b> Chapter 8 review</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>7.1.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="case-studies.html"><a href="case-studies.html"><i class="fa fa-check"></i><b>8</b> Case studies</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Montana State Introductory Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inference-cat" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Inference for categorical data</h1>

<div class="chapterintro">
<p>Statistical inference is primarily concerned with understanding and quantifying the <em>uncertainty</em> of parameter estimates—that is, how variable is a sample statistic
from sample to sample?
While the equations and details change depending on the setting, the foundations for inference are the same throughout all of statistics. We will begin this chapter with a discussion of the foundations of inference, and introduce the two primary vehicles of inference: the <strong>hypothesis test</strong> and <strong>confidence interval</strong>.</p>
Te rest of this chapter focuses statistical inference for categorical data. The two data structures we detail are one binary variable, summarized using a single proportion, and two binary variables, summarized using a difference (or ratio) of two proportions.
</div>
<p>Throughout the book so far, you have worked with data in a variety of contexts.
You have learned how to summarize and visualize the data as well as how to model multiple variables at the same time.
Sometimes the dataset at hand represents the entire research question.
But more often than not, the data have been collected to answer a research question about a larger group of which the data are a (hopefully) representative subset.</p>
<p>You may agree that there is almost always variability in data (one dataset will not be identical to a second dataset even if they are both collected from the same population using the same methods).
However, quantifying the variability in the data is neither obvious nor easy to do (<strong>how</strong> different is one dataset from another?).</p>

<div class="example">
<p>Suppose your professor splits the students in class into two groups: students on the left and students on the right. If <span class="math inline">\(\hat{p}_{_L}\)</span> and <span class="math inline">\(\hat{p}_{_R}\)</span> represent the proportion of students who own an Apple product on the left and right, respectively, would you be surprised if <span class="math inline">\(\hat{p}_{_L}\)</span> did not <em>exactly</em> equal <span class="math inline">\(\hat{p}_{_R}\)</span>?</p>
<hr />
While the proportions would probably be close to each other, it would be unusual for them to be exactly the same. We would probably observe a small difference due to <em>chance</em>.
</div>

<div class="guidedpractice">
If we don’t think the side of the room a person sits on in class is related to whether the person owns an Apple product, what assumption are we making about the relationship between these two variables?
(Reminder: for these Guided Practice questions, you can check your answer in the footnote.)<a href="#fn36" class="footnote-ref" id="fnref36"><sup>36</sup></a>
</div>
<p>Studying randomness of this form is a key focus of statistics.
Throughout this chapter, and those that follow, we provide two different approaches for quantifying the variability inherent in data: simulation-based methods and theory-based methods (mathematical models).
Using the methods provided in this and future chapters, we will be able to draw conclusions beyond the data set at hand to research questions about larger populations.</p>
<div id="inf-foundations" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Foundations of inference</h2>
<p>Given results seen in a sample, the process of determining what we can <em>infer</em> to the
population based on sample results is called <strong>statistical inference</strong>. Statistical inferential methods enable us to understand and quantify the <em>uncertainty</em> of our sample results. Statistical inference helps us answer two questions about the population:</p>
<ol style="list-style-type: decimal">
<li>How strong is the <em>evidence</em> of an effect?</li>
<li>How <em>large</em> is the effect?</li>
</ol>
<p>The first question is answered through a <strong>hypothesis test</strong>, while the second is addressed with a <strong>confidence interval</strong>.</p>
<div id="Martian" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Motivating example: Martian alphabet</h3>
<p>How well can humans distinguish one “Martian” letter from another? The Figure <a href="inference-cat.html#fig:kiki-bumba">5.1</a>
displays two Martians—one named Kiki and another named Bumba. Which do you think
is Kiki and which do you think is Bumba?<a href="#fn37" class="footnote-ref" id="fnref37"><sup>37</sup></a></p>
<div class="figure" style="text-align: center"><span id="fig:kiki-bumba"></span>
<img src="06/images/bumBa-KiKi.png" alt="Two Martians named Bumba and Kiki. Do you think Bumba is on the left or the right?^[Bumba is the Martian on the left!]" width="75%" />
<p class="caption">
Figure 5.1: Two Martians named Bumba and Kiki. Do you think Bumba is on the left or the right?<a href="#fn38" class="footnote-ref" id="fnref38"><sup>38</sup></a>
</p>
</div>
<p>This same image and question were presented to an introductory statistics class of
38 students. In that class, 34 students correctly identified Bumba as the Martian on the left. Assuming we can’t read Martian, is this result surprising?</p>
<p>One of two possibilities occurred:</p>
<ol style="list-style-type: decimal">
<li><em>We can’t read Martian, and these results just occurred by chance.</em></li>
<li><em>We can read Martian, and these results reflect this ability.</em></li>
</ol>
<p>To decide between these two possibilities, we could calculate the probability
of observing such results in a randomly selected sample of 38 students, under
the assumption that students were just guessing. If this probability is <em>very low</em>,
we’d have reason to reject the first possibility in favor of the second.
We can calculate this probability using one of two methods:</p>
<ul>
<li><strong>Simulation-based method</strong>: simulate lots of samples of 38 students under the assumption that
students are just guessing, then calculate the proportion of these
simulated samples where we saw 34 or more students guessing correctly, or</li>
<li><strong>Theory-based method</strong>: develop a mathematical model for the sample proportion in this
scenario and use the model to calculate the probability.</li>
</ul>

<div class="guidedpractice">
How could you use a coin or cards to simulate the guesses of one sample of 38 students who cannot read Martian?<a href="#fn39" class="footnote-ref" id="fnref39"><sup>39</sup></a>
</div>
<p>For this situation—since “just guessing” means you have a 50% chance of guessing correctly—we could simulate a sample of 38 students’ guesses by flipping a coin 38 times and counting the number of times it lands on heads. Using a computer to repeat this process 1,000 times, we create the dot plot in Figure <a href="inference-cat.html#fig:MartianDotPlot">5.2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:MartianDotPlot"></span>
<img src="06-inference-cat_files/figure-html/MartianDotPlot-1.png" alt="A dot plot of 1,000 sample proportions; each calculated by flipping a coin 38 times and calculating the proportion of times the coin landed on heads. None of the 1,000 simulations had sample proportion of at least 89%, which was the proportion observed in the study." width="70%" />
<p class="caption">
Figure 5.2: A dot plot of 1,000 sample proportions; each calculated by flipping a coin 38 times and calculating the proportion of times the coin landed on heads. None of the 1,000 simulations had sample proportion of at least 89%, which was the proportion observed in the study.
</p>
</div>
<p>None of our simulated samples produce 34 of 38 correct guesses! That is, if students were just guessing, it is nearly impossible to observe 34 or more correct guesses in a sample of 38 students. Given this low probability, the more plausible possibility is 2. <em>We can read Martian, and these results reflect this ability.</em> We’ve just completed our first hypothesis test!</p>
<p>Now, obviously no one can read Martian, so a more realistic possibility is that humans tend to choose Bumba on the left more often than the right—there is a greater than 50% chance of choosing Bumba on the left. Even though we may think we’re guessing just by chance, we have a preference for Bumba on the left. It turns out that the explanation for this preference is called <em>synesthesia</em>, a tendency for humans to correlate sharp sounding noises (e.g., Kiki) with sharp looking images.<a href="#fn40" class="footnote-ref" id="fnref40"><sup>40</sup></a></p>
<p>But wait—we’re not done! We have evidence that humans tend to prefer Bumba on the left, but by how much? To answer this, we need a confidence interval—an interval of plausible values for the true probability humans will choose Bumba on the left. The width of this interval is determined by how variable sample proportions are from sample to sample. It turns out, there is a mathematical model for this variability that we will explore later in this chapter. For now, let’s take the standard deviation from our simulated sample proportions as an estimate for this variability: 0.08. Since the simulated distribution of proportions is bell-shaped, we know about 95% of sample proportions should fall within two standard deviations of the true proportion, so we can add and subtract this <strong>margin of error</strong> to our sample proportion to calculate an approximate 95% confidence interval<a href="#fn41" class="footnote-ref" id="fnref41"><sup>41</sup></a>:
<span class="math display">\[
\frac{34}{38} \pm 2\times 0.08 = 0.89 \pm 0.16 = (0.73, 1)
\]</span>
Thus, based on this data, we are 95% confident that the probability a human guesses Bumba on the left is somewhere between 73% and 100%.</p>
</div>
<div id="HypothesisTesting" class="section level3" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Hypothesis tests</h3>
<p>In the <a href="inference-cat.html#Martian">Martian alphabet example</a>, we utilized a <strong>hypothesis test</strong>, which is a formal technique for evaluating two competing possibilities.
Each hypothesis test involves a <strong>null hypothesis</strong>, which represents either a skeptical perspective or a perspective of no difference or no effect, and an <strong>alternative hypothesis</strong>, which represents a new perspective such as the possibility that there has been a change or that there is a treatment effect in an experiment. The alternative hypothesis is usually the reason the scientists set out to do the research in the first place.</p>

<div class="onebox">
<p><strong>Null and alternative hypotheses.</strong></p>
The <strong>null hypothesis (<span class="math inline">\(H_0\)</span>)</strong> often represents either a skeptical perspective or a claim to be tested. The <strong>alternative hypothesis (<span class="math inline">\(H_A\)</span>)</strong> represents an alternative claim under consideration and is often represented by a range of possible values for the parameter of interest.
</div>

<div class="guidedpractice">
In the Martian alphabet example, which of the two competing possibilities was the null hypothesis? the alternative hypothesis?<a href="#fn42" class="footnote-ref" id="fnref42"><sup>42</sup></a>
</div>
<p>The hypothesis testing framework is a very general tool, and we often use it without a second thought.
If a person makes a somewhat unbelievable claim, we are initially skeptical.
However, if there is sufficient evidence that supports the claim, we set aside our skepticism.
The hallmarks of hypothesis testing are also found in the US court system.</p>
<div id="the-us-court-system" class="section level4 unnumbered">
<h4>The US court system</h4>

<div class="example">
<p>A US court considers two possible claims about a defendant: they are either innocent or guilty. If we set these claims up in a hypothesis framework, which would be the null hypothesis and which the alternative?</p>
<hr />
The jury considers whether the evidence is so convincing (strong) that there is no reasonable doubt regarding the person’s guilt.
That is, the skeptical perspective (null hypothesis) is that the person is innocent until evidence is presented that convinces the jury that the person is guilty (alternative hypothesis).
</div>
<p>Jurors examine the evidence to see whether it convincingly shows a defendant is guilty.
Notice that if a jury finds a defendant <em>not guilty</em>, this does not necessarily mean the jury is confident in the person’s innocence.
They are simply not convinced of the alternative that the person is guilty.</p>
<p>This is also the case with hypothesis testing: <em>even if we fail to reject the null hypothesis, we typically do not accept the null hypothesis as truth</em>.
Failing to find strong evidence for the alternative hypothesis is not equivalent to providing evidence that the null hypothesis is true.</p>
</div>
<div id="p-value-and-statistical-significance" class="section level4 unnumbered">
<h4>p-value and statistical significance</h4>
<p>In the <a href="inference-cat.html#Martian">Martian alphabet example</a>, we performed a simulation-based hypothesis test of the hypotheses:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: The chance a human chooses Bumba on the left is 50%.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: Humans have a preference for choosing Bumba on the left.</p></li>
</ul>
<p>The research question—can humans read Martian?—was framed in the context of these hypotheses.</p>
<p>The null hypothesis (<span class="math inline">\(H_0\)</span>) was a perspective of no effect (no ability to read Martian).
The student Martian guessing data provided a point estimate of 89.5% (<span class="math inline">\(34/38 \times 100\)</span>%) for the true probability of choosing Bumba on the left.
We determined that observing such a sample proportion from chance alone would be rare—it would only happen in less than 1 out of 1000 samples. When results
like these are inconsistent with <span class="math inline">\(H_0\)</span>, we reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_A\)</span>.
Here, we concluded there humans have a preference for choosing Bumba on the left.</p>
<p>The less than 1-in-1000 chance is what we call a <strong>p-value</strong>, which is a probability quantifying the strength of the evidence against the null hypothesis and in favor of the alternative.</p>

<div class="onebox">
<p><strong>p-value.</strong></p>
The <strong>p-value</strong> is the probability of observing data at least as favorable to the alternative hypothesis as our current data set, if the null hypothesis were true.
We typically use a summary statistic of the data, such as a proportion or difference in proportions, to help compute the p-value and evaluate the hypotheses.
This summary value that is used to compute the p-value is often called the <strong>test statistic</strong>.
</div>

<div class="protip">
<p>When interpreting a p-value, remember that the definition of a p-value has three components. It is a (1) probability. What it is the probability of? It is the probability of (2) our observed sample statistic or one more extreme. Assuming what? It is the probability of our observed sample statistic or one more extreme, (3) assuming the null hypothesis is true:</p>
<ul>
<li>probability</li>
<li>data<a href="#fn43" class="footnote-ref" id="fnref43"><sup>43</sup></a></li>
<li>null hypothesis
</div></li>
</ul>

<div class="example">
<p>What was the test statistic in the Martian alphabet example?</p>
<hr />
The test statistic in the the Martian alphabet example was the sample proportion, <span class="math inline">\(\frac{34}{38} = 0.895\)</span> (or 89.5%). This is also the <strong>point estimate</strong> of the true probability that humans would choose Bumba on the left.
</div>
<p>When the p-value is small, i.e., less than a previously set threshold, we say the results are <strong>statistically significant</strong>.
This means the data provide such strong evidence against <span class="math inline">\(H_0\)</span> that we reject the null hypothesis in favor of the alternative hypothesis.
The threshold, called the <strong>significance level</strong> and often represented by <span class="math inline">\(\alpha\)</span> (the Greek letter <em>alpha</em>), is typically set to <span class="math inline">\(\alpha = 0.05\)</span>, but can vary depending on the field or the application and the consequences of an incorrect decision.
Using a significance level of <span class="math inline">\(\alpha = 0.05\)</span> in the Martian alphabet study, we can say that the data provided statistically significant evidence against the null hypothesis.</p>

<div class="onebox">
<p><strong>Statistical significance.</strong></p>
We say that the data provide <strong>statistically significant</strong> evidence against the null hypothesis if the p-value is less than some reference value called the **significance level*, denoted by <span class="math inline">\(\alpha\)</span>.
</div>

<div class="onebox">
<p><strong>What’s so special about 0.05?</strong></p>
We often use a threshold of 0.05 to determine whether a result is statistically significant.
But why 0.05?
Maybe we should use a bigger number, or maybe a smaller number.
If you’re a little puzzled, that probably means you’re reading with a critical eye—good job!
The <span class="math inline">\(OpenIntro\)</span> authors have a video to help clarify <em>why 0.05</em>:
<center>
<a href="https://www.openintro.org/book/stat/why05/">https://www.openintro.org/book/stat/why05/</a>
</center>
<br>
Sometimes it’s also a good idea to deviate from the standard.
We’ll discuss when to choose a threshold different than 0.05 in Section ??.
</div>
</div>
</div>
<div id="ConfidenceIntervals" class="section level3" number="5.1.3">
<h3><span class="header-section-number">5.1.3</span> Confidence intervals</h3>
<p>A point estimate provides a single plausible value for a parameter.
However, a point estimate is rarely perfect—usually there is some error in the estimate.
In addition to supplying a point estimate of a parameter, a next logical step would be to provide a plausible <em>range of values</em> for the parameter.</p>
<p>A plausible range of values for the population parameter is called a <strong>confidence interval</strong>.
Using only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net.
We can throw a spear where we saw a fish, but we will probably miss.
On the other hand, if we toss a net in that area, we have a good chance of catching the fish.</p>
<p>If we report a point estimate, we probably will not hit the exact population parameter.
On the other hand, if we report a range of plausible values—a confidence interval—we have a good shot at capturing the parameter.</p>

<div class="guidedpractice">
If we want to be very certain we capture the population parameter, should we use a wider interval or a smaller interval?<a href="#fn44" class="footnote-ref" id="fnref44"><sup>44</sup></a>
</div>
<p>In Section ?? we will discuss different percentages for the confidence interval (e.g., 90% confidence interval or 99% confidence interval). Section ?? also provides a longer discussion on what “95% confidence” actually means.</p>
</div>
</div>
<div id="single-prop" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> One proportion</h2>
<p>We encountered inference methods for a single proportion
in Chapter <a href="#inference-foundations"><strong>??</strong></a>,
exploring point estimates, confidence intervals,
and hypothesis tests.
In this section, we’ll do a review of these topics
and also how to choose an appropriate sample size
when collecting data for single proportion contexts.</p>
<p>Note that there is only one variable being measured in a study which focuses on one proportion.
For each observational unit, the single variable is measured as either a success or failure (e.g., “surgical complication” vs. “no surgical complication”).
Because the nature of the research question at hand focuses on only a single variable, there is not a way to randomize the variable across a different (explanatory) variable.
For this reason, we will not use randomization as an analysis tool when focusing on a single proportion. Instead, we will apply bootstrapping techniques to test a given hypothesis, and we will also revisit the associated mathematical models.</p>
<div id="one-prop-null-boot" class="section level3" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Bootstrap test for <span class="math inline">\(H_0: p = p_0\)</span></h3>
<p>The bootstrap simulation concept when <span class="math inline">\(H_0\)</span> is true is similar to the ideas used
in the case studies presented in
Section <a href="#boot-ci"><strong>??</strong></a> where we bootstrapped without an assumption about <span class="math inline">\(H_0\)</span>. Because we will be testing a hypothesized value of <span class="math inline">\(p\)</span> (referred to as <span class="math inline">\(p_0\)</span>), the bootstrap simulation for hypothesis testing has a fantastic advantage that it can be used for any sample size (a huge benefit for small samples, a nice alternative for large samples).</p>
<p>We expand on the medical consultant example, see Section <a href="#sec-med-consult"><strong>??</strong></a>, where instead of finding an interval estimate for the true complication rate, we work to test a specific research claim.</p>
<div id="observed-data" class="section level5 unnumbered">
<h5>Observed data</h5>
<p>Recall the set-up for the example:</p>
<p>People providing an organ for donation sometimes seek the help of a special “medical consultant”. These consultants assist the patient in all aspects of the surgery, with the goal of reducing the possibility of complications during the medical procedure and recovery. Patients might choose a consultant based in part on the historical complication rate of the consultant’s clients. One consultant tried to attract patients by noting the average complication rate for liver donor surgeries in the US is about 10%, but her clients have only had 3 complications in the 62 liver donor surgeries she has facilitated. She claims this is strong evidence that her work meaningfully contributes to reducing complications (and therefore she should be hired!).</p>

<div class="example">
<p>Using the data, is it possible to assess the consultant’s claim that her complication rate is less than 10%?</p>
<hr />
<p>No. The claim is that there is a causal connection, but the data are observational. Patients who hire this medical consultant may have lower complication rates for other reasons.</p>
While it is not possible to assess this causal claim, it is still possible to test for an association using these data. For this question we ask, could the low complication rate of <span class="math inline">\(\hat{p} = 0.048\)</span> be due to chance?
</div>

<div class="guidedpractice">
Write out hypotheses in both plain and statistical language to test for the association between the consultant’s work and the true complication rate, <span class="math inline">\(p\)</span>, for the consultant’s clients.<a href="#fn45" class="footnote-ref" id="fnref45"><sup>45</sup></a>
</div>
<p>Because, as it turns out, the conditions of working with the normal distribution are not met (see Section <a href="inference-cat.html#one-prop-norm">5.2.2</a>), the uncertainty associated with the sample proportion should not be modeled using the normal distribution. However, we would still like to assess the hypotheses from the previous Guided Practice in absence of the normal framework. To do so, we need to evaluate the possibility of a sample value (<span class="math inline">\(\hat{p}\)</span>) as far below the null value, <span class="math inline">\(p_0=0.10\)</span> as what was observed. The deviation of the sample value from the hypothesized parameter is usually quantified with a p-value.</p>
<p>The p-value is computed based on the null distribution, which is the distribution of the test statistic if the null hypothesis is true. Supposing the null hypothesis is true, we can compute the p-value by identifying the chance of observing a test statistic that favors the alternative hypothesis at least as strongly as the observed test statistic. Here we will use a bootstrap simulation to measure the p-value.</p>
</div>
<div id="variability-of-the-statistic" class="section level5 unnumbered">
<h5>Variability of the statistic</h5>
<p>We want to identify the sampling distribution of the test statistic (<span class="math inline">\(\hat{p}\)</span>) if the null hypothesis was true. In other words, we want to see how the sample proportion changes due to chance alone. Then we plan to use this information to decide whether there is enough evidence to reject the null hypothesis.</p>
<p>Under the null hypothesis, 10% of liver donors have complications during or after surgery. Suppose this rate was really no different for the consultant’s clients (for <em>all</em> the consultant’s clients, not just the 62 previously measured). If this was the case, we could <em>simulate</em> 62 clients to get a sample proportion for the complication rate from the null distribution. Simulating observations using a hypothesized null parameter value is often called a <strong>parametric bootstrap simulation</strong>.</p>
<p>Similar to the process described in Section <a href="#boot-ci"><strong>??</strong></a>, each client can be simulated using a bag of marbles with 10% red marbles and 90% white marbles.
Sampling a marble from the bag (with 10% red marbles) is one way of simulating whether a patient has a complication <em>if the true complication rate is 10%</em> for the data. If we select 62 marbles and then compute the proportion of patients with complications in the simulation, <span class="math inline">\(\hat{p}_{sim}\)</span>, then the resulting sample proportion is exactly a sample from the null distribution.</p>
<p>An undergraduate student was paid $2 to complete this simulation. There were 5 simulated cases with a complication and 57 simulated cases without a complication, i.e., <span class="math inline">\(\hat{p}_{sim} = 5/62 = 0.081\)</span>.</p>

<div class="example">
<p>Is this one simulation enough to determine whether or not we should reject the null hypothesis?</p>
<hr />
No. To assess the hypotheses, we need to see a distribution of many <span class="math inline">\(\hat{p}_{sim}\)</span>, not just a <em>single</em> draw from this sampling distribution.
</div>
</div>
<div id="observed-statistic-vs.-null-statistics" class="section level4 unnumbered">
<h4>Observed statistic vs. null statistics</h4>
<p>One simulation isn’t enough to get a sense of the null distribution; many simulation studies are needed. Roughly 10,000 seems sufficient. However, paying someone to simulate 10,000 studies by hand is a waste of time and money. Instead, simulations are typically programmed into a computer, which is much more efficient.</p>
<p>Figure <a href="inference-cat.html#fig:nullDistForPHatIfLiverTransplantConsultantIsNotHelpful">5.3</a> shows the results of 10,000 simulated studies. The proportions that are equal to or less than <span class="math inline">\(\hat{p}=0.048\)</span> are shaded. The shaded areas represent sample proportions under the null distribution that provide at least as much evidence as <span class="math inline">\(\hat{p}\)</span> favoring the alternative hypothesis. There were 1222 simulated sample proportions with <span class="math inline">\(\hat{p}_{sim} \leq 0.048\)</span>. We use these to construct the null distribution’s left-tail area and find the p-value:
<span class="math display">\[\begin{align}
\text{left tail area }\label{estOfPValueBasedOnSimulatedNullForSingleProportion}
	&amp;= \frac{\text{Number of observed simulations with }\hat{p}_{sim}\leq\text{ 0.048}}{10000}
\end{align}\]</span>
Of the 10,000 simulated <span class="math inline">\(\hat{p}_{sim}\)</span>, 1222 were equal to or smaller than <span class="math inline">\(\hat{p}\)</span>. Since the hypothesis test is one-sided, the estimated p-value is equal to this tail area: 0.1222.</p>
<div class="figure" style="text-align: center"><span id="fig:nullDistForPHatIfLiverTransplantConsultantIsNotHelpful"></span>
<img src="06-inference-cat_files/figure-html/nullDistForPHatIfLiverTransplantConsultantIsNotHelpful-1.png" alt="The null distribution for $\hat{p}$, created from 10,000 simulated studies. The left tail, representing the p-value for the hypothesis test, contains 12.22% of the simulations." width="70%" />
<p class="caption">
Figure 5.3: The null distribution for <span class="math inline">\(\hat{p}\)</span>, created from 10,000 simulated studies. The left tail, representing the p-value for the hypothesis test, contains 12.22% of the simulations.
</p>
</div>

<div class="guidedpractice">
Because the estimated p-value is 0.1222, which is larger than the significance level 0.05, we do not reject the null hypothesis. Explain what this means in plain language in the context of the problem.<a href="#fn46" class="footnote-ref" id="fnref46"><sup>46</sup></a>
</div>
<p></p>

<div class="guidedpractice">
Does the conclusion in the previous Guided Practice imply there is no real association between the surgical consultant’s work and the risk of complications? Explain.<a href="#fn47" class="footnote-ref" id="fnref47"><sup>47</sup></a>
</div>

<div class="onebox">
<p><strong>Null distribution of <span class="math inline">\(\hat{p}\)</span> with bootstrap simulation</strong></p>
Regardless of the statistial method chosen, the p-value is always derived by analyzing the null distribution of the test statistic. The normal model poorly approximates the null distribution for <span class="math inline">\(\hat{p}\)</span> when the success-failure condition is not satisfied. As a substitute, we can generate the null distribution using simulated sample proportions and use this distribution to compute the tail area, i.e., the p-value.
</div>
<p>In the previous Guided Practice, the p-value is <em>estimated</em>.
It is not exact because the simulated null distribution itself is not exact, only a close approximation.
An exact p-value can be generated using the binomial distribution, but that method will not be covered in this text.</p>
<!--
#### Generating the exact null distribution and p-value  {-} {#exactNullDistributionUsingBinomialModel}

The number of successes in $n$ independent cases can be described using the binomial model, which was introduced in Section \ref{binomialModel}. Recall that the probability of observing exactly $k$ successes is given by
\begin{align} \label{binomialEquationShownForFindingNullDistributionInSmallSamplePropTest}
P(k\text{ successes}) = {n\choose k} p^{k}(1-p)^{n-k} = \frac{n!}{k!(n-k)!} p^{k}(1-p)^{n-k}
\end{align}
where $p$ is the true probability of success. The expression ${n\choose k}$ is read as \emph{$n$ choose $k$}, and the exclamation points represent factorials. For instance, $3!$ is equal to $3\times 2\times 1=6$, $4!$ is equal to $4\times 3\times 2\times 1 = 24$, and so on (see Section \ref{binomialModel}).

The tail area of the null distribution is computed by adding up the probability in Equation \eqref{binomialEquationShownForFindingNullDistributionInSmallSamplePropTest} for each $k$ that provides at least as strong of evidence favoring the alternative hypothesis as the data. If the hypothesis test is one-sided, then the p-value is represented by a single tail area. If the test is two-sided, compute the single tail area and double it to get the p-value, just as we have done in the past.

\begin{example}{Compute the exact p-value to check the consultant's claim that her clients' complication rate is below 105.}
Exactly $k=3$ complications were observed in the $n=62$ cases cited by the consultant. Since we are testing against the 10% national average, our null hypothesis is $p=0.10$. We can compute the p-value by adding up the cases where there are 3 or fewer complications:
\begin{align*}
\text{p-value}
	&= \sum_{j=0}^{3} {n\choose j} p^{j}(1-p)^{n-j} \\
	&= \sum_{j=0}^{3} {62\choose j} 0.1^{j}(1-0.1)^{62-j} \\
	&= {62\choose 0} 0.1^{0}(1-0.1)^{62-0} +
		{62\choose 1} 0.1^{1}(1-0.1)^{62-1} \\
	& \qquad + {62\choose 2} 0.1^{2}(1-0.1)^{62-2} +
		{62\choose 3} 0.1^{3}(1-0.1)^{62-3} \\
	&= 0.0015 + 0.0100 + 0.0340 + 0.0755 \\
	&= 0.1210
\end{align*}
This exact p-value is very close to the p-value based on the simulations (0.1222), and we come to the same conclusion. We do not reject the null hypothesis, and there is not statistically significant evidence to support the association.

If it were plotted, the exact null distribution would look almost identical to the simulated null distribution shown in Figure \ref{nullDistForPHatIfLiverTransplantConsultantIsNotHelpful} on page \pageref{nullDistForPHatIfLiverTransplantConsultantIsNotHelpful}.
\end{example}

-->
</div>
</div>
<div id="one-prop-norm" class="section level3" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> Mathematical model</h3>
<div id="conditions" class="section level4 unnumbered">
<h4>Conditions</h4>
<p>In Section <a href="#normalDist"><strong>??</strong></a>, we introduced the normal distribution and showed how it can be used as a mathematical model to describe the variability of a statistic.
There are conditions under which a sample proportion <span class="math inline">\(\hat{p}\)</span> is well modeled using a normal distribution.
When the sample observations
are independent and the sample size is sufficiently
large, the normal model will describe the variability quite well; when the observations violate the conditions, the normal model can be inaccurate</p>

<div class="onebox">
<p><strong>Sampling distribution of
<span class="math inline">\(\hat{p}\)</span></strong></p>
<p>The sampling distribution for <span class="math inline">\(\hat{p}\)</span> based on
a sample of size <span class="math inline">\(n\)</span> from a population with a true
proportion <span class="math inline">\(p\)</span> is nearly normal when:</p>
<ol style="list-style-type: decimal">
<li>The sample’s observations are independent,
e.g., are from a simple random sample.</li>
<li>We expected to see at least 10 successes and
10 failures in the sample, i.e., <span class="math inline">\(np\geq10\)</span> and
<span class="math inline">\(n(1-p)\geq10\)</span>.
This is called the <strong>success-failure condition</strong>.</li>
</ol>
When these conditions are met, then the sampling
distribution of <span class="math inline">\(\hat{p}\)</span> is nearly normal with mean
<span class="math inline">\(p\)</span> and standard error of <span class="math inline">\(\hat{p}\)</span> as <span class="math inline">\(SE = \sqrt{\frac{\ p(1-p)\ }{n}}\)</span>.
</div>
<p>
</p>
<p>Typically we don’t know the true proportion <span class="math inline">\(p\)</span>,
so we substitute some value to check conditions
and estimate the standard error.
For confidence intervals, the sample proportion
<span class="math inline">\(\hat{p}\)</span> is used to check the success-failure condition
and compute the standard error.
For hypothesis tests, typically the null value –
that is, the proportion claimed in the null hypothesis –
is used in place of <span class="math inline">\(p\)</span>.</p>
<p>The independence condition is a more nuanced requirement.
When it isn’t met, it is important to understand how and why
it isn’t met.
For example, there exist no statistical methods available to truly correct the inherent biases of data from a convenience sample.
On the other hand, if we took a cluster sample
(see Section <a href="#samp-methods"><strong>??</strong></a>), the observations wouldn’t be independent, but suitable statistical methods are available for analyzing the data (but they are beyond the scope of even most second or third courses
in statistic).</p>

<div class="example">
<p>In the examples based on large sample theory, we modeled <span class="math inline">\(\hat{p}\)</span> using the normal distribution. Why is this not appropriate for the case study on the medical consultant?</p>
<hr />
The independence assumption may be reasonable if each of the surgeries is from a different surgical team. However, the success-failure condition is not satisfied. Under the null hypothesis, we would anticipate seeing <span class="math inline">\(62\times 0.10=6.2\)</span> complications, not the 10 required for the normal approximation.
</div>
<p>While this book is scoped to well-constrained statistical
problems, do remember that this is just the first
book in what is a large library of statistical methods that
are suitable for a very wide range of data and contexts.</p>
</div>
<div id="confidence-interval-for-p" class="section level4 unnumbered">
<h4>Confidence interval for <span class="math inline">\(p\)</span></h4>
<p></p>
<p>A confidence interval provides a range of
plausible values for the parameter <span class="math inline">\(p\)</span>,
and when <span class="math inline">\(\hat{p}\)</span> can be modeled using a
normal distribution, the confidence interval
for <span class="math inline">\(p\)</span> takes the form
<span class="math display">\[\begin{align*}
\hat{p} \pm z^{\star} \times SE.
\end{align*}\]</span></p>
<p>We have seen <span class="math inline">\(\hat{p}\)</span> to be the sample proportion. The value <span class="math inline">\(z^{\star}\)</span> determines the confidence level (previously set to be 1.96) and will be discussed in detail in the examples following. The value of the standard error, <span class="math inline">\(SE\)</span>, depends heavily on the sample size.</p>

<div class="onebox">
<p><strong>Standard Error of one proportion, <span class="math inline">\(\hat{p}\)</span></strong></p>
<p>When the conditions are met so that the distribution fo <span class="math inline">\(\hat{p}\)</span> is nearly normal, the <strong>variability</strong> of a single proportion, <span class="math inline">\(\hat{p}\)</span> is well described by:</p>
<p><span class="math display">\[SE(\hat{p}) = \sqrt{\frac{p(1-p)}{n}}\]</span></p>
<p>Note that we almost never know the true value of <span class="math inline">\(p\)</span>. A more helpful formula to use is:</p>
<p><span class="math display">\[SE(\hat{p}) \approx \sqrt{\frac{(\mbox{best guess of }p)(1 - \mbox{best guess of }p)}{n}}\]</span></p>
For hypothesis testing, we often use <span class="math inline">\(p_0\)</span> as the best guess of <span class="math inline">\(p\)</span>. For confidence intervals, we typically use <span class="math inline">\(\hat{p}\)</span> as the best guess of <span class="math inline">\(p\)</span>.
</div>
<p></p>
<!--
\newcommand{\paydayN}{826}
\newcommand{\paydayNHalf}{413}
\newcommand{\paydayRegPerc}{70\%}
\newcommand{\paydayRegProp}{0.70}
\newcommand{\paydayRegSE}{0.016}
\newcommand{\paydayRegSEPerc}{1.6\%}
\newcommand{\paydayRegLower}{0.669}
\newcommand{\paydayRegUpper}{0.731}
\newcommand{\paydayRegLowerPerc}{66.9\%}
\newcommand{\paydayRegUpperPerc}{73.1\%}
% https://www.pewtrusts.org/-/media/assets/2017/04/payday-loan-customers-want-more-protections-methodology.pdf

did search and replace for each term above.  for example 826 for 826

-->

<div class="guidedpractice">
<p>Consider taking many polls of registered voters (i.e., random samples) of size 300 asking them if they support legalized marijuana.
It is suspected that about 2/3 of all voters support legalized marijuana.
To understand how the sample proportion (<span class="math inline">\(\hat{p}\)</span>) would vary across the samples, calculate the standard error of <span class="math inline">\(\hat{p}\)</span>.<a href="#fn48" class="footnote-ref" id="fnref48"><sup>48</sup></a></p>
</div>
<div id="variability-of-the-statistic-1" class="section level5 unnumbered">
<h5>Variability of the statistic</h5>

<div class="example">
<p>A simple random sample of 826
payday loan borrowers was surveyed to better
understand their interests around regulation and costs.
70% of the responses supported new
regulations on payday lenders.</p>
<ol style="list-style-type: decimal">
<li><p>Is it reasonable to model the variability of <span class="math inline">\(\hat{p}\)</span> from sample to sample
using a normal distribution?</p></li>
<li><p>Estimate the standard error of <span class="math inline">\(\hat{p}\)</span>.</p></li>
<li><p>Construct a 95% confidence interval for <span class="math inline">\(p\)</span>,
the proportion of payday borrowers who support increased
regulation for payday lenders.</p></li>
</ol>
<hr />
<ol style="list-style-type: decimal">
<li>The data are a random sample, so the observations are
independent and representative of the population of
interest.</li>
</ol>
<p>We also must check the success-failure condition,
which we do using <span class="math inline">\(\hat{p}\)</span> in place
of <span class="math inline">\(p\)</span> when computing a confidence interval:
<span class="math display">\[\begin{align*}
  \text{Support: }
      n p &amp;
          \approx 826 \times 0.70
      = 578
  &amp;\text{Not: }
      n (1 - p) &amp;
        \approx 826 \times (1 - 0.70)
      = 248
  \end{align*}\]</span>
Since both values are at least 10, we can use the normal
distribution to model <span class="math inline">\(\hat{p}\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li>Because <span class="math inline">\(p\)</span> is unknown and the standard error is for
a confidence interval, use <span class="math inline">\(\hat{p}\)</span> in place of <span class="math inline">\(p\)</span>
in the formula.</li>
</ol>
<p><span class="math inline">\(SE = \sqrt{\frac{p(1-p)}{n}} \approx  \sqrt{\frac{0.70 (1 - 0.70)}  {826}} = 0.016\)</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li>Using
the point estimate 0.70,
<span class="math inline">\(z^{\star} = 1.96\)</span> for a 95% confidence interval,
and
the standard error <span class="math inline">\(SE = 0.016\)</span> from the pervious
Guided Practice,
the confidence interval is
<span class="math display">\[\begin{eqnarray*}
  \text{point estimate} \ \pm\ z^{\star} \times SE
   \quad\to\quad
   0.70 \ \pm\ 1.96 \times 0.016
   \quad\to\quad
   (0.669, 0.731)
  \end{eqnarray*}\]</span>
We are 95% confident that the true proportion of
payday borrowers who supported regulation at the time
of the poll was between 0.669 and
0.731.
</div></li>
</ol>

<div class="onebox">
<p><strong>Constructing a confidence interval for a single proportion</strong></p>
<p>There are three steps to constructing a confidence
interval for <span class="math inline">\(p\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Check independence and the success-failure condition
using <span class="math inline">\(\hat{p}\)</span>.
If the conditions are met, the sampling distribution
of <span class="math inline">\(\hat{p}\)</span> may be well-approximated by the normal model.</li>
<li>Construct the standard error using <span class="math inline">\(\hat{p}\)</span>
in place of <span class="math inline">\(p\)</span> in the standard error formula.</li>
<li>Apply the general confidence interval formula.
</div></li>
</ol>
<p>For additional one-proportion confidence interval examples,
see Section <a href="inference-cat.html#ConfidenceIntervals">5.1.3</a>.</p>
</div>
</div>
<div id="changing-the-confidence-level" class="section level4 unnumbered">
<h4>Changing the confidence level</h4>
<p></p>
<p>Suppose we want to consider confidence intervals where the confidence level is somewhat higher than 95%: perhaps we would like a confidence level of 99%. Think back to the analogy about trying to catch a fish: if we want to be more sure that we will catch the fish, we should use a wider net. To create a 99% confidence level, we must also widen our 95% interval. On the other hand, if we want an interval with lower confidence, such as 90%, we could make our original 95% interval slightly slimmer.</p>
<p>The 95% confidence interval structure provides guidance in how to make intervals with new confidence levels. Below is a general 95% confidence interval for a point estimate that comes from a nearly normal distribution:
<span class="math display">\[\begin{eqnarray}
\text{point estimate}\ \pm\ 1.96\times SE
\end{eqnarray}\]</span>
There are three components to this interval: the point estimate, “1.96”, and the standard error. The choice of <span class="math inline">\(1.96\times SE\)</span> was based on capturing 95% of the data since the estimate is within 1.96 standard errors of the true value about 95% of the time. The choice of 1.96 corresponds to a 95% confidence level.</p>

<div class="guidedpractice">
If <span class="math inline">\(X\)</span> is a normally distributed random variable, how often will <span class="math inline">\(X\)</span> be within 2.58 standard deviations of the mean?<a href="#fn49" class="footnote-ref" id="fnref49"><sup>49</sup></a>
</div>
<div class="figure" style="text-align: center"><span id="fig:choosingZForCI"></span>
<img src="06-inference-cat_files/figure-html/choosingZForCI-1.png" alt="The area between -$z^{\star}$ and $z^{\star}$ increases as $|z^{\star}|$ becomes larger. If the confidence level is 99%, we choose $z^{\star}$ such that 99% of the normal curve is between -$z^{\star}$ and $z^{\star}$, which corresponds to 0.5% in the lower tail and 0.5% in the upper tail: $z^{\star}=2.58$." width="70%" />
<p class="caption">
Figure 5.4: The area between -<span class="math inline">\(z^{\star}\)</span> and <span class="math inline">\(z^{\star}\)</span> increases as <span class="math inline">\(|z^{\star}|\)</span> becomes larger. If the confidence level is 99%, we choose <span class="math inline">\(z^{\star}\)</span> such that 99% of the normal curve is between -<span class="math inline">\(z^{\star}\)</span> and <span class="math inline">\(z^{\star}\)</span>, which corresponds to 0.5% in the lower tail and 0.5% in the upper tail: <span class="math inline">\(z^{\star}=2.58\)</span>.
</p>
</div>
<p></p>
<p>To create a 99% confidence interval, change 1.96 in the 95% confidence interval formula to be <span class="math inline">\(2.58\)</span>. The previous Guided Practice highlights that 99% of the time a normal random variable will be within 2.58 standard deviations of its mean. This approach – using the Z scores in the normal model to compute confidence levels – is appropriate when the point estimate is associated with a normal distribution and we can properly compute the standard error. Thus, the formula for a 99% confidence interval is:</p>
<p><span class="math display">\[\begin{eqnarray*}
\text{point estimate}\ \pm\ 2.58\times SE
\end{eqnarray*}\]</span></p>
<!--
label for previous equation?
\label{99PercCIForMean}
\label{99PercCIForNormalPointEstimate}

%\Comment{I don't know where the equation number above gets referenced. Might drop the equation number.}
-->
<p>The normal approximation is crucial to the precision of the <span class="math inline">\(z^\star\)</span> confidence intervals (in contrast to the bootstrap confidence intervals). When the normal model is not a good fit, we will use alternative distributions that better characterize the sampling distribution or we will use bootstrapping procedures.</p>

<div class="guidedpractice">
Create a 99% confidence interval for the impact of the stent on the risk of stroke using the data from Section <a href="intro-to-data.html#basic-stents-strokes">1.1</a>. The point estimate is 0.090, and the standard error is <span class="math inline">\(SE = 0.028\)</span>. It has been verified for you that the point estimate can reasonably be modeled by a normal distribution.<a href="#fn50" class="footnote-ref" id="fnref50"><sup>50</sup></a>
</div>

<div class="onebox">
<p><strong>Mathematical model confidence interval for any confidence level.</strong></p>
If the point estimate follows the normal model with standard error <span class="math inline">\(SE\)</span>, then a confidence interval for the population parameter is
<span class="math display">\[\begin{eqnarray*}
\text{point estimate}\ \pm\ z^{\star} \times SE
\end{eqnarray*}\]</span>
where <span class="math inline">\(z^{\star}\)</span> corresponds to the confidence level selected.
</div>
<p>Figure <a href="inference-cat.html#fig:choosingZForCI">5.4</a> provides a picture of how to identify <span class="math inline">\(z^{\star}\)</span> based on a confidence level. We select <span class="math inline">\(z^{\star}\)</span> so that the area between -<span class="math inline">\(z^{\star}\)</span> and <span class="math inline">\(z^{\star}\)</span> in the normal model corresponds to the confidence level.</p>

<div class="guidedpractice">
Previously, we found that implanting a stent in the brain of a patient at risk for a stroke <em>increased</em> the risk of a stroke. The study estimated a 9% increase in the number of patients who had a stroke, and the standard error of this estimate was about <span class="math inline">\(SE = 2.8%\)</span>. Compute a 90% confidence interval for the effect.<a href="#fn51" class="footnote-ref" id="fnref51"><sup>51</sup></a>
</div>
</div>
<div id="hypothesis-test-for-h_0-p-p_0" class="section level4 unnumbered">
<h4>Hypothesis test for <span class="math inline">\(H_0: p = p_0\)</span></h4>
<!--
\label{htForPropSection}

\newcommand{\paydayCCPerc}{51\%}
\newcommand{\paydayCCProp}{0.51}
\newcommand{\paydayCCSE}{0.017}
\newcommand{\paydayCCSEPerc}{1.7\%}
\newcommand{\paydayCCZ}{0.59}
\newcommand{\paydayCCOneTail}{0.2776}
\newcommand{\paydayCCPvalue}{0.5552}
-->
<p>One possible regulation for payday lenders is that they
would be required to do a credit check and evaluate debt
payments against the borrower’s finances.
We would like to know: would borrowers support this form
of regulation?</p>
<!--
\label{paydayCC_hypotheses_gp}%
-->

<div class="guidedpractice">
Set up hypotheses to evaluate whether borrowers
have a majority support for this
type of regulation.<a href="#fn52" class="footnote-ref" id="fnref52"><sup>52</sup></a>
</div>
<p>To apply the normal distribution framework in the context
of a hypothesis test for a proportion, the independence
and success-failure conditions must be satisfied.
In a hypothesis test, the success-failure condition is
checked using the null proportion:
we verify <span class="math inline">\(np_0\)</span> and <span class="math inline">\(n(1-p_0)\)</span> are at least 10,
where <span class="math inline">\(p_0\)</span> is the null value.</p>
<!--
\label{paydayCC_conditions_gp}%
-->

<div class="guidedpractice">
Do payday loan borrowers support a regulation
that would
require lenders to pull their credit report
and evaluate their debt payments?
From a random sample of 826 borrowers,
51% said they would support such
a regulation.
Is it reasonable use a normal distribution to model <span class="math inline">\(\hat{p}\)</span>
for a hypothesis test here?<a href="#fn53" class="footnote-ref" id="fnref53"><sup>53</sup></a>
</div>

<div class="example">
<p>Using the hypotheses and data from the previous
Guided Practices,
evaluate whether the poll on lending regulations provides convincing evidence
that a majority of payday loan borrowers support
a new regulation that would
require lenders to pull credit reports
and evaluate debt payments.</p>
<hr />
<p>With hypotheses already set up and conditions checked,
we can move onto calculations.
The standard error in the context of a one-proportion
hypothesis test is computed using the null value, <span class="math inline">\(p_0\)</span>:
<span class="math display">\[\begin{align*}
  SE = \sqrt{\frac{p_0 (1 - p_0)}{n}}
      = \sqrt{\frac{0.5 (1 - 0.5)}{826}}
      = 0.017
  \end{align*}\]</span>
A picture of the normal model is shown below
with the p-value represented by the shaded region.</p>
<p>Based on the normal model, the test statistic can be
computed as the Z-score of the point estimate:
<span class="math display">\[\begin{align*}
  Z = \frac{\text{point estimate} - \text{null value}}{SE}
      = \frac{0.51 - 0.50}{0.017}
      = 0.59
  \end{align*}\]</span>
The single tail area which represents the p-value is 0.2776.
Because the p-value is larger than 0.05,
we do not reject <span class="math inline">\(H_0\)</span>.
The poll does not provide convincing evidence that
a majority of payday loan borrowers support regulations around credit checks and evaluation of
debt payments.</p>
<p>In Section <a href="inference-cat.html#two-prop-errors">5.3.1</a> we discuss two-sided hypothesis tests of which the payday example may have been better structured.<br />
That is, we might have wanted to ask whether the borrows <strong>support or oppose</strong> the regulations (to study opinion in either direction away from the 50% benchmark).
In that case, the p-value would have been doubled to 0.5552 (again, we would not reject <span class="math inline">\(H_0\)</span>).
In the two-sided hypothesis setting, the appropriate conclusion would be to claim that the poll does not provide convincing evidence that a majority of payday loan borrowers support or oppose regulations around credit checks and evaluation of debt payments.</p>
In both the one-sided or two-sided setting, the conclusion is somewhat unsatisfactory because there is no conclusion.
That is, there is no resolution one way or the other about public opinion.
We cannot claim that exactly 50% of people support the regulation, but we cannot claim a majority in either direction.
</div>
<p><img src="06-inference-cat_files/figure-html/paydayCC-norm-pvalue-1.png" width="70%" style="display: block; margin: auto;" /></p>
<!--
\oneprophtsummary{}
-->

<div class="onebox">
<p><strong>Mathematical model hypothesis test for a proportion.</strong></p>
Set up hypotheses and verify the conditions using the null value, <span class="math inline">\(p_0\)</span>, to ensure <span class="math inline">\(\hat{p}\)</span> is nearly normal under <span class="math inline">\(H_0\)</span>. If the conditions hold, construct the standard error, again using <span class="math inline">\(p_0\)</span>, and show the p-value in a drawing. Lastly, compute the p-value and evaluate the hypotheses.
</div>
<p>For additional one-proportion hypothesis test examples,
see Section <a href="inference-cat.html#HypothesisTesting">5.1.2</a>.</p>
<p>
<!--
\CalculatorVideos{confidence intervals and hypothesis tests for a single proportion}
--></p>
</div>
<div id="violating-conditions" class="section level4 unnumbered">
<h4>Violating conditions</h4>
<p>We’ve spent a lot of time discussing conditions for when
<span class="math inline">\(\hat{p}\)</span> can be reasonably modeled by a normal distribution.
What happens when the success-failure condition fails?
What about when the independence condition fails?
In either case, the general ideas of confidence intervals
and hypothesis tests remain the same, but the strategy
or technique used to generate the interval or p-value
change.</p>
<p>When the success-failure condition isn’t met
for a hypothesis test, we can simulate the null distribution
of <span class="math inline">\(\hat{p}\)</span> using the null value, <span class="math inline">\(p_0\)</span>, as seen in Section <a href="inference-cat.html#one-prop-null-boot">5.2.1</a>. Unfortunately, methods for dealing with observations which are not independent are outside the scope of this book.</p>
<!--
#### Choosing a sample size when estimating a proportion {-}

\BeginKnitrBlock{onebox}<div class="onebox">**Margin of error.**

In a confidence interval, $z^{\star}\times SE$ is called the **margin of error**\index{margin of error}.</div>\EndKnitrBlock{onebox}






\index{margin of error|(}

When collecting data, we choose a sample size suitable
for the purpose of the study. 
You might agree that the following interval estimate would not be particularly useful: a 95% confidence interval for the proportion of liver transplants with complications is between 0 and 1.
Often times "suitable for the study" means choosing a sample size large
enough that the **margin of error**\index{margin of error} --
which is the part we add and subtract from the point
estimate in a confidence interval --
is sufficiently small that the result is useful.
For example, our task might be to find a sample size
$n$ so that the sample proportion is within $\pm 0.04$
of the actual proportion in a 95% confidence interval.

<!--
% For example, the margin of error for a point estimate using 95% confidence can be written as $1.96\times SE$. We set up a general equation to represent the problem:
%\begin{align*}
%ME = z^{\star} \times SE \leq m
%\end{align*}
%where $ME$ represented the actual margin of error and $z^{\star}$ was chosen to correspond to the confidence level. The standard error formula is specified to correspond to the particular setting. For instance, in the case of means, the standard error was given as $\sigma / \sqrt{n}$. In the case of a single proportion, we use $\sqrt{p(1-p) / n\ }$ for the standard error.


\index{data!Student football stadium|(}

\BeginKnitrBlock{example}<div class="example">A university newspaper is conducting
    a survey to determine what fraction of students
    support a $200 per year increase in fees to pay
    for a new football stadium.
    How big of a sample is required to ensure the
    margin of error is smaller than 0.04 using a
    95% confidence level?

---
      
  The margin of error for a sample proportion is
  \begin{align*}
  z^{\star} \sqrt{\frac{p (1 - p)}{n}}
  \end{align*}
  Our goal is to find the smallest sample size $n$
  so that this margin of error is smaller than $0.04$.
  For a 95% confidence level, the value $z^{\star}$
  corresponds to 1.96:
  \begin{align*}
  1.96\times \sqrt{\frac{p(1-p)}{n}} \ < \ 0.04
  \end{align*}
  There are two unknowns in the equation: $p$ and $n$.
  If we have an estimate of $p$, perhaps from a prior
  survey, we could enter in that value and solve for $n$.
  If we have no such estimate, we must use some other
  value for $p$.
  It turns out that the margin of error is largest
  when $p$ is 0.5, so we typically use this
  \emph{worst case value} if no estimate of the
  proportion is available:
  \begin{align*}
	1.96\times \sqrt{\frac{0.5(1-0.5)}{n}} &\ < \ 0.04 \\
	1.96^2\times \frac{0.5(1-0.5)}{n} &\ < \ 0.04^2 \\
	1.96^2\times \frac{0.5(1-0.5)}{0.04^2} &\ < \ n \\
	600.25 &\ < \  n
  \end{align*}
  We would need over 600.25 participants, which means
  we need 601 participants or more, to ensure the
  sample proportion is within 0.04 of the true proportion
  with 95% confidence.</div>\EndKnitrBlock{example}

\index{data!Student football stadium|)}

When an estimate of the proportion is available, we use it in place of the worst case proportion value, 0.5.


\index{data!Tire failure rate|(}

\BeginKnitrBlock{todo}<div class="todo">not sure why the footnote didn't go into the footnote</div>\EndKnitrBlock{todo}


\BeginKnitrBlock{guidedpractice}<div class="guidedpractice">A manager is about to oversee the mass
production of a new tire model in her factory,
and she would like to estimate what proportion of
these tires will be rejected through quality control.
The quality control team has monitored the last three
tire models produced by the factory,
failing 1.7% of tires in the first model,
6.2% of the second model,
and 1.3% of the third model.
The manager would like to examine enough tires
to estimate the failure rate of the new tire model
to within about 1% with a 90% confidence level.
There are three different failure rates to choose from.
Perform the sample size computation for each separately,
and identify three sample sizes to consider.^[For a 90% confidence interval, $z^{\star} = 1.65$,
  and since an estimate of the proportion 0.017 is available,
  we'll use it in the margin of error formula:
  \begin{align*}
  1.65\times \sqrt{\frac{0.017(1-0.017)}{n}} &\ < \ 0.01
    \qquad\to\qquad
      \frac{0.017(1-0.017)}{n} \ < \ 
          \left(\frac{0.01}{1.65}\right)^2
    \qquad\to\qquad
      454.96 \ < \ n
  \end{align*}
  For sample size calculations, we always round up,
  so the first tire model suggests 455 tires would
  be sufficient.

  A similar computation can be accomplished using 0.062
  and 0.013 for $p$, and you should verify that using these
  proportions results in minimum sample sizes of 1584
  and 350 tires, respectively.]</div>\EndKnitrBlock{guidedpractice}

\BeginKnitrBlock{example}<div class="example">The sample sizes vary widely in the previous
    Guided Practice.
    Which of the three would you suggest using?
    What would influence your choice?

---
      
  We could examine which of the old models is most
  like the new model, then choose the corresponding sample
  size.
  Or if two of the previous estimates are based on small
  samples while the other is based on a larger sample,
  we might consider the value corresponding to the larger
  sample.
  There are also other reasonable approaches.

  Also observe that the success-failure
  condition would need to be checked in the final sample.
  For instance, if we sampled $n = 1584$ tires and found
  a failure rate of 0.5%, the normal approximation would
  not be reasonable, and we would require more advanced
  statistical methods for creating the confidence interval.</div>\EndKnitrBlock{example}

\index{data!Tire failure rate|)}
\index{data!Payday regulation poll|(}

\BeginKnitrBlock{guidedpractice}<div class="guidedpractice">Suppose we want to continually track the support
of payday borrowers for regulation on lenders,
where we would conduct a new poll every month.
Running such frequent polls is expensive, so we decide
a wider margin of error of 5% for each individual survey
would be acceptable.
Based on a previous sample of borrowers where
70% supported some form of regulation,
how big should our monthly sample be for a margin
of error of 0.04 with 95% confidence?^[We complete the same computations as before,
   except now we use $0.70$ instead of $0.5$
   for $p$:
   \begin{align*}
   1.96\times \sqrt{\frac{p(1-p)}{n}}
       \approx 1.96\times
           \sqrt{\frac{0.70(1-0.70)}
               {n}}
       &\leq 0.05
     \qquad\to\qquad
       n \geq 322.7
  \end{align*}
  A sample size of 323 or more would be reasonable.
  (Reminder: always round up for sample size calculations!)
  Given that we plan to track this poll over time,
  we also may want to periodically repeat these calculations
  to ensure that we're being thoughtful in our sample
  size recommendations in case the baseline rate fluctuates.]</div>\EndKnitrBlock{guidedpractice}

\index{data!Payday regulation poll|)}
\index{margin of error|)}

{\input{ch_inference_for_props/TeX/inference_for_a_single_proportion.tex}}
-->
</div>
</div>
</div>
<div id="diff-two-prop" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Difference of two proportions</h2>
<p>We now extend the methods from Section <a href="inference-cat.html#single-prop">5.2</a> to apply confidence intervals and hypothesis tests to differences in population proportions that come from two groups: <span class="math inline">\(p_1 - p_2\)</span>.</p>
<!--
%We consider three examples.
%In the first, we compare the utility of a blood thinner
%for heart attack patients.
%In the second application, we examine the efficacy of
%mammograms in reducing deaths from breast cancer.
%In the last example, a quadcopter company weighs whether
%to switch to a higher quality manufacturer of rotor blades.
-->
<p>In our investigations, we’ll identify a reasonable
point estimate of <span class="math inline">\(p_1 - p_2\)</span> based on the sample,
and you may have already guessed its form:
<span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span>.

Then we’ll look at the inferential analysis in three different ways: using a randomization test, applying bootstrapping for interval estimates, and, if
we verify that the point estimate
can be modeled using a normal distribution,
we compute the estimate’s standard error, and
we apply the mathematical framework.</p>
<div id="two-prop-errors" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Randomization test for <span class="math inline">\(H_0: p_1 - p_2 = 0\)</span></h3>
<div id="observed-data-1" class="section level4 unnumbered">
<h4>Observed data</h4>
<p>We consider a study on a new malaria vaccine
called PfSPZ.
In this study, volunteer patients were randomized
into one of two experiment groups:
14 patients received an experimental vaccine
or 6 patients received a placebo vaccine.
Nineteen weeks later, all 20 patients were exposed
to a drug-sensitive malaria virus strain;
the motivation of using a drug-sensitive strain
of virus here is for ethical considerations,
allowing any infections to be treated effectively.
The results are summarized in
Table <a href="inference-cat.html#tab:malaria-vaccine-20-exp-summary">5.1</a>,
where 9 of the 14 treatment patients remained free
of signs of infection while all of the 6 patients
in the control group patients showed some baseline
signs of infection.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:malaria-vaccine-20-exp-summary">Table 5.1: </span>Summary results for the malaria vaccine experiment.
</caption>
<thead>
<tr>
<th style="border-bottom:hidden" colspan="1">
</th>
<th style="border-bottom:hidden" colspan="1">
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
<code>outcome</code>
</div>
</th>
<th style="border-bottom:hidden" colspan="1">
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
infection
</th>
<th style="text-align:left;">
no infection
</th>
<th style="text-align:left;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
vaccine
</td>
<td style="text-align:left;">
5
</td>
<td style="text-align:left;">
9
</td>
<td style="text-align:left;">
14
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>treatment</code>
</td>
<td style="text-align:left;">
placebo
</td>
<td style="text-align:left;">
6
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
6
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
11
</td>
<td style="text-align:left;">
9
</td>
<td style="text-align:left;">
20
</td>
</tr>
</tbody>
</table>
<!--
\newcommand{\malariaAA}{5}
\newcommand{\malariaAB}{9}
\newcommand{\malariaAD}{14}
\newcommand{\malariaBA}{6}
\newcommand{\malariaBB}{0}
\newcommand{\malariaBD}{6}
\newcommand{\malariaDA}{11}
\newcommand{\malariaDB}{9}
\newcommand{\malariaDD}{20}
\newcommand{\malariaVIR}{0.357}
\newcommand{\malariaVIRPerc}{35.7\%}
\newcommand{\malariaPIR}{1.000}
\newcommand{\malariaPIRPerc}{100\%}
\newcommand{\malariaIRDiff}{0.643}
\newcommand{\malariaIRDiffPerc}{64.3\%}
-->

<div class="guidedpractice">
Is this an observational study or an experiment?
What implications does the study type have on what can
be inferred from the results?<a href="#fn54" class="footnote-ref" id="fnref54"><sup>54</sup></a>
</div>
<p>In this study, a smaller proportion of patients
who received the vaccine showed signs of an infection
(35.7% versus 100%).
However, the sample is very small,
and it is unclear whether the difference provides
<em>convincing evidence</em> that the vaccine is
effective.</p>
<!--
<div class="example">
<p>Data scientists are sometimes called upon to evaluate the strength of evidence. When looking at the rates of infection for patients in the two groups in this study, what comes to mind as we try to determine whether the data show convincing evidence of a real difference?</p>
<hr />
<p>The observed infection rates (35.7% for the treatment group versus 100% for the control group) suggest the vaccine may be effective. However, we cannot be sure if the observed difference represents the vaccine’s efficacy or is just from random chance. Generally there is a little bit of fluctuation in sample data, and we wouldn’t expect the sample proportions to be <em>exactly</em> equal, even if the truth was that the infection rates were independent of getting the vaccine. Additionally, with such small samples, perhaps it’s common to observe such large differences when we randomly split a group due to chance alone!</p>
</div>

The previous malaria example
is a reminder that the observed outcomes in the data
sample may not perfectly reflect the true relationships
between variables since there is \term{random noise}.
While the observed difference in rates of infection
is large, the sample size for the study is small,
making it unclear if this observed difference represents
efficacy of the vaccine or whether it is simply due to
chance.
We label these two competing claims, $H_0$ and $H_A$,
which are spoken as ``H-nought'' and ``H-A'':
\begin{itemize}
\setlength{\itemsep}{0mm}
\item[$H_0$:] \textbf{Independence model.}
    The variables \var{treatment} and \var{outcome}
    are independent.
    They have no relationship, and the observed difference
    between the proportion of patients who developed
    an infection in the two groups, 64.3%,
    was due to chance.
\item[$H_A$:] \textbf{Alternative model.}
    The variables are \emph{not} independent.
    The difference in infection rates of
    64.3%
    was not due to chance,
    and vaccine affected the rate of infection.
\end{itemize}

What would it mean if the independence model,
which says the vaccine had no influence on the
rate of infection, is true?
It would mean 11 patients were going to
develop an infection \emph{no matter which group
they were randomized into},
and 9 patients would not develop an infection
\emph{no matter which group they were randomized
into}.
That is, if the vaccine did not affect the rate
of infection, the difference in the infection rates
was due to chance alone in how the patients were
randomized.

Now consider the alternative model:
infection rates were influenced by whether a patient
received the vaccine or not.
If this was true, and especially if this influence
was substantial, we would expect to see some difference
in the infection rates of patients in the groups.

We choose between these two competing claims
by assessing if the data conflict so much with
$H_0$ that the independence model cannot be deemed
reasonable.
If this is the case, and the data support $H_A$,
then we will reject the notion of independence
and conclude there was discrimination.


\subsection{Simulating the study}
\label{simulatingTheStudy}

We're going to implement
\termsub{simulations}{simulation},
where we will pretend we know that the malaria
vaccine being tested does \emph{not} work.
Ultimately, we want to understand if the large
difference we observed is common in these
simulations.
If it is common, then maybe the difference
we observed was purely due to chance.
If it is very uncommon, then the possibility
that the vaccine was helpful seems more plausible.

Table \@ref(tab:malaria-vaccine-20-exp-summary)
shows that 11 patients developed infections and 9 did not.
For our simulation, we will suppose the infections
were independent of the vaccine and we were able to
\emph{rewind} back to when the researchers randomized
the patients in the study.
If we happened to randomize the patients differently,
we may get a different result in this hypothetical
world where the vaccine doesn't influence the infection.
Let's complete another \term{randomization} using
a simulation.


In this \term{simulation}, we take 20 notecards to
represent the 20 patients, where we write down ``infection''
on 11 cards and ``no infection'' on 9 cards.
In this hypothetical world, we believe each patient
that got an infection was going to get it regardless
of which group they were in, so let's see what happens
if we randomly assign the patients to the treatment
and control groups again.
We thoroughly shuffle the notecards and deal 14 into
a \resp{vaccine} pile and 6 into a \resp{placebo} pile.
Finally, we tabulate the results, which are shown in
Figure \ref{malaria-vaccine-20-exp-summary_rand_1}.

\begin{figure}[ht]
\centering
\begin{tabular}{l l cc rr}
  & & \multicolumn{2}{c}{\var{outcome}} \\
  \cline{3-4}
  &  &  {infection} & {no infection} & Total & \hspace{3mm}  \\ 
  \cline{2-5}
  treatment & {vaccine} & 7 & 7 & 14 \\ 
  (simulated) & {placebo} & 4 & 2 & 6 \\ 
  \cline{2-5}
  & Total & 11 & 9 & 20 \\
  \cline{2-5}
\end{tabular}
\caption{Simulation results, where any difference
    in infection rates is purely due to chance.}
\label{malaria-vaccine-20-exp-summary_rand_1}
\end{figure}

\begin{exercisewrap}
\begin{nexercise}
\label{malaria-vaccine-20-exp-summary_rand_1_diff}
What is the difference in infection rates between
the two simulated groups in
Figure \ref{malaria-vaccine-20-exp-summary_rand_1}?
How does this compare to the observed
64.3% difference
in the actual data?\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{$4 / 6 - 7 / 14 = 0.167$
  or about 16.7\% in favor of the vaccine.
  This difference due to chance is much smaller than the
  difference observed in the actual groups.}
-->
<p>As we saw in Section <a href="#inf-rand"><strong>??</strong></a>, we can randomize the responses (<code>infection</code> or <code>no infection</code>) to the treatment conditions under the null hypothesis of independence and compute possible differences in proportions.
The process by which we randomize observations to two groups is summarized and visualized in Figure <a href="#fig:fullrand"><strong>??</strong></a>.</p>
<!--
We computed one possible difference under the
independence model in Guided
Practice \ref{malaria-vaccine-20-exp-summary_rand_1_diff},
which represents one difference due to chance.
While in this first simulation, we physically dealt
out notecards to represent the patients,
it is more efficient to perform this simulation
using a computer.
Repeating the simulation on a computer, we get another
difference due to chance:
\begin{align*}
\frac{2}{\malariaBD{}} - \frac{9}{\malariaAD{}} = -0.310
\end{align*}
And another:
\begin{align*}
\frac{3}{\malariaBD{}} - \frac{8}{\malariaAD{}} = -0.071
\end{align*}
And so on until we repeat the simulation enough times
that we have a good idea of what represents the
\emph{distribution of differences from chance alone}.
-->
</div>
<div id="variability-of-the-statistic-2" class="section level4 unnumbered">
<h4>Variability of the statistic</h4>
<p>Figure <a href="inference-cat.html#fig:malaria-rand-dot-plot">5.5</a> shows a stacked plot
of the differences found from 100 randomization simulations (i.e., repeated iterations as described in Figure <a href="#fig:fullrand"><strong>??</strong></a>),
where each dot represents a simulated difference between
the infection rates (control rate minus treatment rate).</p>
<div class="figure" style="text-align: center"><span id="fig:malaria-rand-dot-plot"></span>
<img src="06-inference-cat_files/figure-html/malaria-rand-dot-plot-1.png" alt="A stacked dot plot of differences from 100 simulations produced under the independence model $H_0$, where in these simulations infections are unaffected by the vaccine. Two of the 100 simulations had a difference of at least 64.3%, the difference observed in the study." width="70%" />
<p class="caption">
Figure 5.5: A stacked dot plot of differences from 100 simulations produced under the independence model <span class="math inline">\(H_0\)</span>, where in these simulations infections are unaffected by the vaccine. Two of the 100 simulations had a difference of at least 64.3%, the difference observed in the study.
</p>
</div>
</div>
<div id="observed-statistic-vs-null-statistics" class="section level4 unnumbered">
<h4>Observed statistic vs null statistics</h4>
<p>Note that the distribution of these simulated differences
is centered around 0.
We simulated the differences assuming that the independence
model was true, and under this condition,
we expect the difference to be near zero with some random
fluctuation, where <em>near</em> is pretty generous in this
case since the sample sizes are so small in this study.</p>

<div class="example">
<p>How often would you observe a difference
of at least 64.3% (0.643)
according to Figure <a href="inference-cat.html#fig:malaria-rand-dot-plot">5.5</a>?
Often, sometimes, rarely, or never?</p>
<hr />
It appears that a difference of at least
64.3% due to chance alone would only
happen about 2% of the time according to
Figure <a href="inference-cat.html#fig:malaria-rand-dot-plot">5.5</a>.
Such a low probability indicates a rare event.
</div>
<p>The difference of 64.3% being
a rare event suggests two possible interpretations
of the results of the study:</p>
<ul>
<li><span class="math inline">\(H_0\)</span> Independence model. The vaccine has no effect on infection rate, and we just happened to observe a difference that would only occur on a rare occasion.</li>
<li><span class="math inline">\(H_A\)</span> Alternative model. The vaccine has an effect on infection rate, and the difference we observed was actually due to the vaccine being effective at combating malaria, which explains the large difference of 64.3%.</li>
</ul>
<p>Based on the simulations, we have two options:</p>
<ol style="list-style-type: decimal">
<li><p>We conclude that the study results do not provide
strong evidence against the independence model.
That is, we do not have sufficiently strong evidence
to conclude the vaccine had an effect in this
clinical setting.</p></li>
<li><p>We conclude the evidence is sufficiently strong
to reject <span class="math inline">\(H_0\)</span> and assert that the vaccine was useful.
When we conduct formal studies, usually we reject the
notion that we just happened to observe a rare
event.<a href="#fn55" class="footnote-ref" id="fnref55"><sup>55</sup></a>
In this case, we reject the independence model in favor
of the alternative.
That is, we are concluding the data provide strong evidence
that the vaccine provides some protection against malaria
in this clinical setting.</p></li>
</ol>
<p></p>
<p>Statistical inference, is built
on evaluating whether such differences are due to chance.
In statistical inference, data scientists evaluate which
model is most reasonable given the data.
Errors do occur, just like rare events, and we might choose
the wrong model.
While we do not always choose correctly, statistical
inference gives us tools to control and evaluate how
often these errors occur.</p>
</div>
<div id="decision-errors" class="section level4 unnumbered">
<h4>Decision errors</h4>
<p></p>
<p>Hypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free.
Similarly, data can point to the wrong conclusion.
However, what distinguishes statistical hypothesis tests from a court system is that our framework allows us to quantify and control how often the data lead us to the incorrect conclusion.</p>
<p>In a hypothesis test, there are two competing hypotheses: the null and the alternative.
We make a statement about which one might be true, but we might choose incorrectly. There are four possible scenarios in a hypothesis test, which are summarized in Table <a href="inference-cat.html#tab:fourHTScenarios">5.2</a>.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:fourHTScenarios">Table 5.2: </span>Four different scenarios for hypothesis tests.
</caption>
<thead>
<tr>
<th style="border-bottom:hidden" colspan="1">
</th>
<th style="border-bottom:hidden" colspan="1">
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
<strong>Test conclusion</strong>
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
<span class="math inline">\(H_0\)</span> true
</td>
<td style="text-align:left;">
good decision
</td>
<td style="text-align:left;">
Type 1 Error
</td>
</tr>
<tr>
<td style="text-align:left;">
<strong>Truth</strong>
</td>
<td style="text-align:left;">
<span class="math inline">\(H_A\)</span> true
</td>
<td style="text-align:left;">
Type 2 Error
</td>
<td style="text-align:left;">
good decision
</td>
</tr>
</tbody>
</table>
<p>A <strong>Type 1 Error</strong> is rejecting the null hypothesis when <span class="math inline">\(H_0\)</span> is actually true.
Since we rejected the null hypothesis in the gender discrimination and opportunity cost studies, it is possible that we made a Type 1 Error in one or both of those studies.
A <strong>Type 2 Error</strong> is failing to reject the null hypothesis when the alternative is actually true.</p>

<div class="example">
<p>In a US court, the defendant is either innocent (<span class="math inline">\(H_0\)</span>) or guilty (<span class="math inline">\(H_A\)</span>).
What does a Type 1 Error represent in this context?
What does a Type 2 Error represent?
Table <a href="inference-cat.html#tab:fourHTScenarios">5.2</a> may be useful.</p>
<hr />
If the court makes a Type 1 Error, this means the defendant is innocent (<span class="math inline">\(H_0\)</span> true) but wrongly convicted.
A Type 2 Error means the court failed to reject <span class="math inline">\(H_0\)</span> (i.e., failed to convict the person) when they were in fact guilty (<span class="math inline">\(H_A\)</span> true).
</div>

<div class="guidedpractice">
Consider the opportunity cost study where we concluded students were less likely to make a DVD purchase if they were reminded that money not spent now could be spent later. What would a Type 1 Error represent in this context?<a href="#fn56" class="footnote-ref" id="fnref56"><sup>56</sup></a>
</div>

<div class="example">
<p>How could we reduce the Type 1 Error rate in US courts?
What influence would this have on the Type 2 Error rate?</p>
<hr />
To lower the Type 1 Error rate, we might raise our standard for conviction from “beyond a reasonable doubt” to “beyond a conceivable doubt” so fewer people would be wrongly convicted. However, this would also make it more difficult to convict the people who are actually guilty, so we would make more Type 2 Errors.
</div>

<div class="guidedpractice">
How could we reduce the Type 2 Error rate in US courts?
What influence would this have on the Type 1 Error rate?<a href="#fn57" class="footnote-ref" id="fnref57"><sup>57</sup></a>
</div>
<p></p>
<p>The example and guided practice above provide an important lesson: if we reduce how often we make one type of error, we generally make more of the other type.</p>
<!--
%Hypothesis testing is built around rejecting or failing to reject the null hypothesis. That is, we do not reject $H_0$ unless the data provide strong evidence against it. But what precisely does *strong evidence* mean? As a general rule of thumb, for those cases where the null hypothesis is actually true, we do not want to incorrectly reject $H_0$ more than 5% of the time. This corresponds to our default significance level of $\alpha = 0.05$, which we use as a comparison with the p-value. In the next section, we discuss the appropriateness of different significance levels.
-->
</div>
<div id="choosing-a-significance-level" class="section level4 unnumbered">
<h4>Choosing a significance level</h4>
<p>
</p>
<p>Choosing a significance level for a test is important in many contexts, and the traditional level is 0.05.
However, it is sometimes helpful to adjust the significance level based on the application.
We may select a level that is smaller or larger than 0.05 depending on the consequences of any conclusions reached from the test.</p>
<p>If making a Type 1 Error is dangerous or especially costly, we should choose a small significance level (e.g., 0.01 or 0.001).
If we want to be very cautious about rejecting the null hypothesis, we demand very strong evidence favoring the alternative <span class="math inline">\(H_A\)</span> before we would reject <span class="math inline">\(H_0\)</span>.</p>
<p>If a Type 2 Error is relatively more dangerous or much more costly than a Type 1 Error, then we should choose a higher significance level (e.g., 0.10).
Here we want to be cautious about failing to reject <span class="math inline">\(H_0\)</span> when the null is actually false.</p>

<div class="tipbox">
<p><strong>Significance levels should reflect consequences of errors.</strong></p>
The significance level selected for a test should reflect the real-world consequences associated with making a Type 1 or Type 2 Error.
</div>
</div>
<div id="two-sided-hypotheses" class="section level4 unnumbered">
<h4>Two-sided hypotheses</h4>
<!--
%_________________
%\section[Case study: CPR and blood thinner (randomization)]{Case study: blood thinner and CPR\\(randomization)}
-->
<p></p>
<p>In Section <a href="#inf-rand"><strong>??</strong></a> we explored whether women were discriminated against and whether a simple trick could make students a little thriftier.
In these two case studies, we’ve actually ignored some possibilities:</p>
<ul>
<li>What if <em>men</em> are actually discriminated against?</li>
<li>What if the money trick actually makes students <em>spend more</em>?</li>
</ul>
<p>These possibilities weren’t considered in our original hypotheses or analyses.
The disregard of the extra alternatives may have seemed natural since the data pointed in the directions in which we framed the problems. However, there are two dangers if we ignore possibilities that disagree with our data or that conflict with our world view:</p>
<ol style="list-style-type: decimal">
<li><p>Framing an alternative hypothesis simply to match the direction that the data point will generally inflate the Type 1 Error rate. After all the work we’ve done (and will continue to do) to rigorously control the error rates in hypothesis tests, careless construction of the alternative hypotheses can disrupt that hard work.</p></li>
<li><p>If we only use alternative hypotheses that agree with our worldview, then we’re going to be subjecting ourselves to <strong>confirmation bias</strong>, which means we are looking for data that supports our ideas. That’s not very scientific, and we can do better!</p></li>
</ol>
<p>The original hypotheses we’ve seen are called <strong>one-sided hypothesis tests</strong> because they only explored one direction of possibilities.
Such hypotheses are appropriate when we are exclusively interested in the single direction, but usually we want to consider all possibilities.
To do so, let’s learn about <strong>two-sided hypothesis tests</strong> in the context of a new study that examines the impact of using blood thinners on patients who have undergone CPR.</p>
<p></p>
<p>Cardiopulmonary resuscitation (CPR) is a procedure used on individuals suffering a heart attack when other emergency resources are unavailable.
This procedure is helpful in providing some blood circulation to keep a person alive, but CPR chest compressions can also cause internal injuries. Internal bleeding and other injuries that can result from CPR complicate additional treatment efforts.
For instance, blood thinners may be used to help release a clot that is causing the heart attack once a patient arrives in the hospital. However, blood thinners negatively affect internal injuries.</p>
<p>Here we consider an experiment with patients who underwent CPR for a heart attack and were subsequently admitted to a hospital.<a href="#fn58" class="footnote-ref" id="fnref58"><sup>58</sup></a>
Each patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group).
The outcome variable of interest was whether the patient survived for at least 24 hours.</p>
<!--
% The way p_c and p_t are described make it sound like we are only considering the samples
-->

<div class="example">
<p>Form hypotheses for this study in plain and statistical language.
Let <span class="math inline">\(p_c\)</span> represent the true survival rate of people who do not receive a blood thinner (corresponding to the control group) and <span class="math inline">\(p_t\)</span> represent the survival rate for people receiving a blood thinner (corresponding to the treatment group).</p>
<hr />
<p>We want to understand whether blood thinners are helpful or harmful.
We’ll consider both of these possibilities using a two-sided hypothesis test.</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: Blood thinners do not have an overall survival effect, i.e., the survival proportions are the same in each group. <span class="math inline">\(p_t - p_c = 0\)</span>.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: Blood thinners have an impact on survival, either positive or negative, but not zero. <span class="math inline">\(p_t - p_c \neq 0\)</span>.</p></li>
</ul>
<p>Note that if we had done a one-sided hypothesis test, the resulting hypotheses would have been:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: Blood thinners do not have a positive overall survival effect, i.e., the survival proportions for the blood thinner group is the same or lower than the control group. <span class="math inline">\(p_t - p_c \leq 0\)</span>.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: Blood thinners have a positive impact on survival. <span class="math inline">\(p_t - p_c &gt; 0\)</span>.</p></li>
</ul>
</div>
<p>There were 50 patients in the experiment who did not receive a blood thinner and 40 patients who did.
The study results are shown in Table <a href="inference-cat.html#tab:resultsForCPRStudyInSmallSampleSection">5.3</a>.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:resultsForCPRStudyInSmallSampleSection">Table 5.3: </span>Results for the CPR study. Patients in the treatment group were given a blood thinner, and patients in the control group were not
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Survived
</th>
<th style="text-align:left;">
Died
</th>
<th style="text-align:left;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Control
</td>
<td style="text-align:left;">
11
</td>
<td style="text-align:left;">
39
</td>
<td style="text-align:left;">
50
</td>
</tr>
<tr>
<td style="text-align:left;">
Treatment
</td>
<td style="text-align:left;">
14
</td>
<td style="text-align:left;">
26
</td>
<td style="text-align:left;">
40
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
25
</td>
<td style="text-align:left;">
65
</td>
<td style="text-align:left;">
90
</td>
</tr>
</tbody>
</table>

<div class="guidedpractice">
What is the observed survival rate in the control group?
And in the treatment group?
Also, provide a point estimate of the difference in survival proportions of the two groups: <span class="math inline">\(\hat{p}_t - \hat{p}_c\)</span>.<a href="#fn59" class="footnote-ref" id="fnref59"><sup>59</sup></a>
</div>
<p>According to the point estimate, for patients who have undergone CPR outside of the hospital, an additional 13% of these patients survive when they are treated with blood thinners.
However, we wonder if this difference could be easily explainable by chance.</p>
<p>As we did in our past two studies this chapter, we will simulate what type of differences we might see from chance alone under the null hypothesis.
By randomly assigning “simulated treatment” and “simulated control” stickers to the patients’ files, we get a new grouping.
If we repeat this simulation 10,000 times, we can build a <strong>null distribution</strong> of the differences shown in Figure <a href="inference-cat.html#fig:CPR-study-right-tail">5.6</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:CPR-study-right-tail"></span>
<img src="06-inference-cat_files/figure-html/CPR-study-right-tail-1.png" alt="Null distribution of the point estimate for the difference in proportions, $\hat{p}_t - \hat{p}_c$. The shaded right tail shows observations that are at least as large as the observed difference, 0.13." width="70%" />
<p class="caption">
Figure 5.6: Null distribution of the point estimate for the difference in proportions, <span class="math inline">\(\hat{p}_t - \hat{p}_c\)</span>. The shaded right tail shows observations that are at least as large as the observed difference, 0.13.
</p>
</div>
<p>The right tail area is 0.131. (Note: it is only a coincidence that we also have <span class="math inline">\(\hat{p}_t - \hat{p}_c=0.13\)</span>.)
However, contrary to how we calculated the p-value in previous studies, the p-value of this test is not 0.131!</p>
<p>The p-value is defined as the chance we observe a result at least as favorable to the alternative hypothesis as the result (i.e., the difference) we observe.
In this case, any differences less than or equal to -0.13 would also provide equally strong evidence favoring the alternative hypothesis as a difference of +0.13 did.
A difference of -0.13 would correspond to 13% higher survival rate in the control group than the treatment group.
In Figure <a href="inference-cat.html#fig:CPR-study-p-value">5.7</a> we’ve also shaded these differences in the left tail of the distribution.
These two shaded tails provide a visual representation of the p-value for a two-sided test.</p>
<!--
%There is something different in this study than in the past studies: in this study, we are particularly interested in whether blood thinners increase *or* decrease the risk of death in patients who undergo CPR before arriving at the hospital.\footnote{Realistically, we probably are interested in either direction in the past studies as well, and so we should have used the approach we now discuss in this section. However, for simplicity and the sake of not introducing too many concepts at once, we skipped over these details in earlier sections.} For example, there are chance differences of $\hat{p}_t - \hat{p}_c = -0.14$, that would have been stronger evidence against the null hypothesis as our observed difference of +0.13. Likewise, anything less than or equal -0.13 would provide as much evidence against the null hypothesis as +0.13, and for this reason, we must count both tails towards the p-value, as shown in Figure \ref{CPR-study-p-value}.
-->
<div class="figure" style="text-align: center"><span id="fig:CPR-study-p-value"></span>
<img src="06-inference-cat_files/figure-html/CPR-study-p-value-1.png" alt="Null distribution of the point estimate for the difference in proportions, $\hat{p}_t - \hat{p}_c$. All values that are at least as extreme as +0.13 but in either direction away from 0 are shaded." width="70%" />
<p class="caption">
Figure 5.7: Null distribution of the point estimate for the difference in proportions, <span class="math inline">\(\hat{p}_t - \hat{p}_c\)</span>. All values that are at least as extreme as +0.13 but in either direction away from 0 are shaded.
</p>
</div>
<p>For a two-sided test, take the single tail (in this case, 0.131) and double it to get the p-value: 0.262.
Since this p-value is larger than 0.05, we do not reject the null hypothesis.
That is, we do not find statistically significant evidence that the blood thinner has any influence on survival of patients who undergo CPR prior to arriving at the hospital.</p>
<!--%Once again, we can discuss the causal conclusion since this is an experiment.
-->
<p></p>

<div class="onebox">
<p><strong>Default to a two-sided test.</strong></p>
We want to be rigorous and keep an open mind when we analyze data and evidence.
Use a one-sided hypothesis test only if you truly have interest in only one direction.
</div>

<div class="onebox">
<p><strong>Computing a p-value for a two-sided test.</strong></p>
First compute the p-value for one tail of the distribution, then double that value to get the two-sided p-value.
That’s it!
</div>

<div class="example">
<p>Consider the situation of the medical consultant.
Now that you know about one-sided and two-sided tests, which type of test do you think is more appropriate?</p>
<hr />
The setting has been framed in the context of the consultant being helpful (which is what led us to a one-sided test originally), but what if the consultant actually performed <em>worse</em> than the average?
Would we care?
More than ever!
Since it turns out that we care about a finding in either direction, we should run a two-sided test.
The p-value for the two-sided test is double that of the one-sided test, here the simulated p-value would be 0.2444.
</div>
<p>Generally, to find a two-sided p-value we double the single tail area, which remains a reasonable approach even when the sampling distribution is asymmetric.
However, the approach can result in p-values larger than 1 when the point estimate is very near the mean in the null distribution; in such cases, we write that the p-value is 1.
Also, very large p-values computed in this way (e.g., 0.85), may also be slightly inflated.
Typically, we do not worry too much about the precision of very large p-values because they lead to the same analysis conclusion, even if the value is slightly off.</p>
</div>
<div id="controlling-the-type-1-error-rate" class="section level4 unnumbered">
<h4>Controlling the Type 1 Error rate</h4>
<p>Now that we understand the difference between one-sided and two-sided tests, we must recognize when to use each type of test.
Because of the result of increased error rates, it is never okay to change two-sided tests to one-sided tests after observing the data.
We explore the consequences of ignoring this advice in the next example.</p>

<div class="example">
<p>Using <span class="math inline">\(\alpha=0.05\)</span>, we show that freely switching from two-sided tests to one-sided tests will lead us to make twice as many Type 1 Errors as intended.</p>
<hr />
<p>Suppose we are interested in finding any difference from 0.
We’ve created a smooth-looking <strong>null distribution</strong> representing differences due to chance in Figure <a href="inference-cat.html#fig:type1ErrorDoublingExampleFigure">5.8</a>.</p>
<p>Suppose the sample difference was larger than 0.
Then if we can flip to a one-sided test, we would use <span class="math inline">\(H_A\)</span>: difference <span class="math inline">\(&gt; 0\)</span>.
Now if we obtain any observation in the upper 5% of the distribution, we would reject <span class="math inline">\(H_0\)</span> since the p-value would just be a the single tail.
Thus, if the null hypothesis is true, we incorrectly reject the null hypothesis about 5% of the time when the sample mean is above the null value, as shown in Figure <a href="inference-cat.html#fig:type1ErrorDoublingExampleFigure">5.8</a>.</p>
<p>Suppose the sample difference was smaller than 0.
Then if we change to a one-sided test, we would use <span class="math inline">\(H_A\)</span>: difference <span class="math inline">\(&lt; 0\)</span>.
If the observed difference falls in the lower 5% of the figure, we would reject <span class="math inline">\(H_0\)</span>.
That is, if the null hypothesis is true, then we would observe this situation about 5% of the time.</p>
By examining these two scenarios, we can determine that we will make a Type 1 Error <span class="math inline">\(5\%+5\%=10\%\)</span> of the time if we are allowed to swap to the “best” one-sided test for the data.
This is twice the error rate we prescribed with our significance level: <span class="math inline">\(\alpha=0.05\)</span> (!).
</div>
<div class="figure" style="text-align: center"><span id="fig:type1ErrorDoublingExampleFigure"></span>
<img src="06-inference-cat_files/figure-html/type1ErrorDoublingExampleFigure-1.png" alt="The shaded regions represent areas where we would reject $H_0$ under the bad practices considered in when $\alpha = 0.05$." width="70%" />
<p class="caption">
Figure 5.8: The shaded regions represent areas where we would reject <span class="math inline">\(H_0\)</span> under the bad practices considered in when <span class="math inline">\(\alpha = 0.05\)</span>.
</p>
</div>

<div class="cautionbox">
<p><strong>Hypothesis tests should be set up <em>before</em> seeing the data.</strong></p>
After observing data, it is tempting to turn a two-sided test into a one-sided test.
Avoid this temptation.
Hypotheses should be set up <em>before</em> observing the data.
</div>
<!--
%\Comment{Should we scrap this subsection and example and just leave the caution box? Downside: weakens item 1 near the start of Section \@ref(IntroducingTwoSidedHypotheses).}
-->
<p></p>
</div>
</div>
<div id="two-prop-boot-ci" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Bootstrap confidence interval for <span class="math inline">\(p_1 - p_2\)</span></h3>
<p>The key will be to use two different bags to simulate from the original data.</p>
<p>Use the CPR data. After we find the CI (use percentile and SE methods), write the interval values down below in the math section that describes the generic confidence interval method.</p>
<p>In Section <a href="inference-cat.html#two-prop-errors">5.3.1</a>, we worked with the randomization distribution to understand the distribution of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> when the null hypothesis <span class="math inline">\(H_0: p_1 - p_2 = 0\)</span> is true.
Now, through bootstrapping, we study the variability of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> without the null assumption.</p>
<div id="observed-data-2" class="section level4 unnumbered">
<h4>Observed data</h4>
<p>Reconsider the CPR data from Section <a href="inference-cat.html#two-prop-errors">5.3.1</a> which is provided in Table <a href="inference-cat.html#tab:resultsForCPRStudyInSmallSampleSection">5.3</a>. The experiment consisted of two treatments on patients who underwent CPR for a heart attack and were subsequently admitted to a hospital. Each patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group).
The outcome variable of interest was whether the patient survived for at least 24 hours.</p>
<p>Again, we use the difference in sample proportions as the observed statistic of interest. Here, the value of the statistic is: <span class="math inline">\(\hat{p}_t - \hat{p}_c = 0.35 - 0.22 = 0.13\)</span>.</p>
</div>
<div id="variability-of-the-statistic-3" class="section level4 unnumbered">
<h4>Variability of the statistic</h4>
<p>The bootstrap method applied to two samples is an extension of the method described in Section <a href="#boot-ci"><strong>??</strong></a>. Now, we have two samples, so each sample estimates the population from which they came. In the CPR setting, the <code>treatment</code> sample estimates the population of all individuals who have gotten (or will get) the treatment; the <code>control</code> sample estimate the population of all individuals who do not get the treatment and are controls. Figure <a href="inference-cat.html#fig:boot2samp1">5.9</a> extends Figure <a href="#fig:boot1"><strong>??</strong></a> to show the bootstrapping process from two samples simultaneously.</p>
<div class="figure" style="text-align: center"><span id="fig:boot2samp1"></span>
<img src="06/figures/boot2samp1.png" alt="(probably populations only, no BS samples) two sample estimating two populations (two infinite popuations)" width="75%" />
<p class="caption">
Figure 5.9: (probably populations only, no BS samples) two sample estimating two populations (two infinite popuations)
</p>
</div>
<p>The variability of the statistic (the difference in sample proportions) can be calculated by taking one treatment bootstrap sample and one control bootstrap sample and calculating the difference of the bootstrap survival proportions. One sample from each of the estimated populations has been taken with the sample proportions calculated for the treatment bootstrap sample and the control bootstrap sample.</p>

<div class="todo">
once the image is in, we need to describe above (and below) the values (proportions, differences) in the image explicitly.
</div>
<div class="figure" style="text-align: center"><span id="fig:boot2samp2"></span>
<img src="06/figures/boot2samp2.png" alt="some way to connect the first BS sample on the left wiht the first BS sample on the right." width="75%" />
<p class="caption">
Figure 5.10: some way to connect the first BS sample on the left wiht the first BS sample on the right.
</p>
</div>
<p>As always, the variability of the difference in proportions can only be estimated by repeated simulations, in this case, repeated bootstrap samples. Figure <a href="inference-cat.html#fig:boot2samp2">5.10</a> shows multiple bootstrap differences calculated for each of the repeated bootstrap samples.</p>
<div class="figure" style="text-align: center"><span id="fig:boot2samp3"></span>
<img src="06/figures/boot2samp3.png" alt="in this graph, some kind of connection between each of the two sides" width="75%" />
<p class="caption">
Figure 5.11: in this graph, some kind of connection between each of the two sides
</p>
</div>

<div class="todo">
Do we also want to visualize “sampling with replacement” in the two sample case?
</div>
<p>Repeated bootstrap simulations lead to a bootstrap sampling distribution of the statistic of interest, here the difference in sample proportions.
Figure <a href="inference-cat.html#fig:bootCPR1000">5.12</a> shows 1000 bootstrap differences in proportions for the CPR data.</p>
<div class="figure" style="text-align: center"><span id="fig:bootCPR1000"></span>
<img src="06-inference-cat_files/figure-html/bootCPR1000-1.png" alt="A histogram of differences in proportions from 1000 bootstrap simulations." width="70%" />
<p class="caption">
Figure 5.12: A histogram of differences in proportions from 1000 bootstrap simulations.
</p>
</div>
</div>
<div id="percentile-vs.-se-bootstrap-confidence-intervals" class="section level4 unnumbered">
<h4>Percentile vs. SE bootstrap confidence intervals</h4>
<p>Figure <a href="inference-cat.html#fig:bootCPR1000">5.12</a> provides an estimate for the variability of the difference in survival proportions from sample to sample, The values in the histogram can be used in two different ways to create a confidence interval for the parameter of interest: <span class="math inline">\(p_1 - p_2\)</span>.</p>
<div id="percentile-bootstrap-interval" class="section level5 unnumbered">
<h5>Percentile bootstrap interval</h5>
<p>As in Section <a href="#boot-ci"><strong>??</strong></a>, the bootstrap confidence interval can be calculated directly from the bootstrapped differences in Figure <a href="inference-cat.html#fig:bootCPR1000">5.12</a>. The interval created from the percentiles of the distribution is called the <strong>percentile interval</strong>.
Note that here we calculate the 90% confidence interval by finding the <span class="math inline">\(5^{th}\)</span> and <span class="math inline">\(95^{th}\)</span> percentile values from the bootstrapped differences.
The bootstrap 5 percentile proportion is -0.155 and the 95 percentile is 0.167.
The result is: we are 90% confident that, in the population, the true difference in probability of survival is between -0.155 and 0.167.
The interval shows that we do not have much definitive evidence of the affect of blood thinners, one way or another.</p>
<div class="figure" style="text-align: center">
<img src="06-inference-cat_files/figure-html/CPR%20percentile%20interval-1.png" alt="The CPR data is bootstrapped 1000 times. Each simulation creates a sample from the original data where the probability of survival in the treatment group is $\hat{p}_{t}  = 14/40$ and the probability of survival in the control group is $\hat{p}_{c} = 11/50$. " width="70%" />
<p class="caption">
(#fig:CPR percentile interval)The CPR data is bootstrapped 1000 times. Each simulation creates a sample from the original data where the probability of survival in the treatment group is <span class="math inline">\(\hat{p}_{t} = 14/40\)</span> and the probability of survival in the control group is <span class="math inline">\(\hat{p}_{c} = 11/50\)</span>.
</p>
</div>
</div>
<div id="se-bootstrap-interval" class="section level5 unnumbered">
<h5>SE bootstrap interval</h5>
<p>Alternatively, we can use the variability in the bootstrapped differences to calculate a standard error of the difference.
The resulting interval is called the <strong>SE interval</strong>.
Section <a href="inference-cat.html#math-2prop">5.3.3</a> details the mathematical model for the standard error of the difference in sample proportions, but the bootstrap distribution typically does an excellent job of estimating the variability.</p>
<p><span class="math display">\[SE(\hat{p}_t - \hat{p}_c) \approx SD(\hat{p}_{bs,t} - \hat{p}_{bs,c}) = 0.0975\]</span></p>
<p>The calculation above was performed in R using the <code>sd()</code> function, but any statistical software will calculate the standard deviation of the differences, here, the exact quantity we hope to approximate.</p>
<p>Because we don’t know the true distribution of <span class="math inline">\(\hat{p}_t - \hat{p}_c\)</span>, we will use a rough approximation to find a confidence interval for <span class="math inline">\(p_t - p_c\)</span>. A 95% confidence interval for <span class="math inline">\(p_t - p_c\)</span> is given by:</p>
<p><span class="math display">\[\begin{align}
\hat{p}_t - \hat{p}_c &amp;\pm&amp; 2 SE(\hat{p}_t - \hat{p}_c)\\
14/40 - 11/50 &amp;\pm&amp; 0.0975\\
&amp;&amp;(-0.065, 0.325)
\end{align}\]</span></p>
<p>We are 95% confident that the true value of <span class="math inline">\(p_t - p_c\)</span> is between -0.065 and 0.325. Again, the wide confidence interval that overlaps zero indicates that the study provides very little evidence about the effectiveness of blood thinners.</p>
</div>
</div>
</div>
<div id="math-2prop" class="section level3" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> Mathematical model</h3>
<div id="variability-of-hatp_1---hatp_2" class="section level4 unnumbered">
<h4>Variability of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span></h4>
<p>Like with <span class="math inline">\(\hat{p}\)</span>, the difference of two sample
proportions <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> can be modeled
using a normal distribution when certain conditions
are met.
First, we require a broader independence condition,
and secondly,
the success-failure condition must be met by both groups.</p>

<div class="onebox">
<p><strong>Conditions for the sampling distribution of <span class="math inline">\(\hat{p}_1 -\hat{p}_2\)</span> to be normal.</strong></p>
<p>The difference <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> can be modeled
using a normal distribution when</p>
<ol style="list-style-type: decimal">
<li><em>Independence</em>, extended. The data are independent within and between
the two groups. Generally this is satisfied if the data come from two independent random samples or if the data come from a randomized experiment.</li>
<li><em>Success-failure condition.</em>
The success-failure condition holds for both
groups, where we check successes and failures
in each group separately.</li>
</ol>
<p>When these conditions are satisfied,
the standard error of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> is</p>
<span class="math display">\[\begin{eqnarray*}
  SE = \sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}
  \end{eqnarray*}\]</span>
where <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> represent the population proportions,
and <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> represent the sample sizes.
</div>
<p></p>
<!--
SE reference above?
    \label{seForDiffOfProp}
-->
</div>
<div id="confidence-interval-for-p_1---p_2" class="section level4 unnumbered">
<h4>Confidence interval for <span class="math inline">\(p_1 - p_2\)</span></h4>
<!--
{Confidence intervals for $\pmb{p_1 - p_2}$}
-->
<p></p>
<p>We can apply the generic confidence interval formula
for a difference of two proportions,
where we use <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> as the point
estimate and substitute the <span class="math inline">\(SE\)</span> formula:
<span class="math display">\[\begin{align*}
&amp;\text{point estimate} \ \pm\  z^{\star} \times SE
&amp;&amp;\to
&amp;&amp;\hat{p}_1 - \hat{p}_2 \ \pm\ 
    z^{\star} \times
   \sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}
\end{align*}\]</span></p>
<!--
We can also follow the same
Prepare, Check, Calculate, Conclude steps for
computing a confidence interval
or completing a hypothesis test.
The details change a little,
but the general approach remain the same.
Think about these steps when you apply statistical methods.
-->

<div class="onebox">
<p><strong>Standard Error of the difference in two proportions, <span class="math inline">\(\hat{p}_1 -\hat{p}_2\)</span>.</strong></p>
When the conditions are met so that the distribution fo <span class="math inline">\(\hat{p}\)</span> is nearly normal, the <strong>variability</strong> of the difference in proportions, <span class="math inline">\(\hat{p}_1 -\hat{p}_2\)</span>, is well described by:
<span class="math display">\[\begin{eqnarray*}
  SE(\hat{p}_1 -\hat{p}_2) = \sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}
  \end{eqnarray*}\]</span>
</div>
<!--
\BeginKnitrBlock{onebox}<div class="onebox">**Confidence interval for a difference of two proportions**
  Once you've determined a confidence interval for the
  difference of two proportions would be helpful for an
  application, there are four steps to constructing the interval:

* **Prepare.**
      Identify the sample proportions and sample sizes
      for each of the two groups,
      determine what confidence level you wish to use.
* **Check.**
      Verify the conditions to ensure each sample
      proportion is nearly normal.
      The success-failure condition should be checked
      for each group.
* **Calculate.**
      If the conditions hold, compute $SE$,
      find $z^{\star}$, and construct the interval.
* **Conclude.**
      Interpret the confidence interval in the context
      of the problem.</div>\EndKnitrBlock{onebox}
-->

<div class="example">
<p>We reconsider the experiment for patients
who underwent cardiopulmonary resuscitation (CPR)
for a heart attack and were
subsequently admitted to a
hospital.
These patients were randomly divided into a treatment
group where they received a blood thinner or the control
group where they did not receive a blood thinner.
The outcome variable of interest was whether the
patients survived for at least 24 hours.
The results are shown in
Table <a href="inference-cat.html#tab:resultsForCPRStudyInSmallSampleSection">5.3</a>.
Check whether we can model the difference in
sample proportions using the normal distribution.</p>
<hr />
<p>We first check for independence:
since this is a randomized experiment,
this condition is satisfied.</p>
<p>Next, we check the success-failure condition for
each group.
We have at least 10 successes and 10 failures in
each experiment arm (11, 14, 39, 26),
so this condition is also satisfied.</p>
With both conditions satisfied,
the difference in sample proportions can be
reasonably modeled using a normal distribution
for these data.
</div>
<!--
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>(\#tab:resultsForCPRStudyInSmallSampleSectionDup)Results for the CPR study.
    Patients in the treatment group were given
    a blood thinner, and patients in the control
    group were not.</caption>
 <thead>
  <tr>
   <th style="text-align:left;">  </th>
   <th style="text-align:left;"> Survived </th>
   <th style="text-align:left;"> Died </th>
   <th style="text-align:left;"> Total </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> Control </td>
   <td style="text-align:left;"> 11 </td>
   <td style="text-align:left;"> 39 </td>
   <td style="text-align:left;"> 50 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Treatment </td>
   <td style="text-align:left;"> 14 </td>
   <td style="text-align:left;"> 26 </td>
   <td style="text-align:left;"> 40 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Total </td>
   <td style="text-align:left;"> 25 </td>
   <td style="text-align:left;"> 65 </td>
   <td style="text-align:left;"> 90 </td>
  </tr>
</tbody>
</table>
-->

<div class="example">
<p>Create and interpret a 90% confidence interval of the
difference for the survival rates in the CPR study.</p>
<hr />
We’ll use <span class="math inline">\(p_t\)</span> for the survival
rate in the treatment group and <span class="math inline">\(p_c\)</span> for the control
group:
<span class="math display">\[\begin{align*}
  \hat{p}_{t} - \hat{p}_{c}
    = \frac{14}{40} - \frac{11}{50}
    = 0.35 - 0.22
    = 0.13
  \end{align*}\]</span>
We use the standard error formula previously provided.
As with the one-sample proportion case,
we use the sample estimates of each proportion
in the formula in the confidence interval context:
<span class="math display">\[\begin{align*}
  SE \approx \sqrt{\frac{0.35 (1 - 0.35)}{40} +
      \frac{0.22 (1 - 0.22)}{50}}
    = 0.095
  \end{align*}\]</span>
For a 90% confidence interval, we use <span class="math inline">\(z^{\star} = 1.65\)</span>:
<span class="math display">\[\begin{align*}
  \text{point estimate} \ \pm\ z^{\star} \times SE
    \quad \to \quad 0.13 \ \pm\ 1.65 \times  0.095
    \quad \to \quad (-0.027, 0.287)
  \end{align*}\]</span>
We are 90% confident that blood thinners have
a difference of -2.7% to +28.7% percentage point
impact on survival rate for patients who are like
those in the study.
Because 0% is contained in the interval,
we do not have enough information to say
whether blood thinners help or harm
heart attack patients who have been admitted after
they have undergone CPR.
</div>
<p></p>

<div class="todo">
<p>not sure why the footnote in the guidedpractice below doesn’t show up?</p>
should be right after: “Also interpret the interval in the context of the study.”
</div>

<div class="guidedpractice">
<p>A 5-year experiment
was conducted to evaluate the effectiveness
of fish oils on reducing cardiovascular events,
where each subject was randomized into one of two
treatment groups.
We’ll consider heart attack outcomes in the patients listed in Table <a href="inference-cat.html#tab:fish-oil-data">5.4</a>.</p>
<p>Create a 95% confidence interval for the effect of fish oils
on heart attacks for patients who are well-represented by
those in the study.
Also interpret the interval in the context of the
study.^[Because the patients were randomized, the subjects are independent, both within and between the two groups.
The success-failure condition is also met for both groups as all counts are at least 10.
This satisfies the conditions necessary to model the difference in proportions using a normal distribution.</p>
<p>Compute the sample proportions (<span class="math inline">\(\hat{p}_{\text{fish oil}} = 0.0112\)</span>, <span class="math inline">\(\hat{p}_{\text{placebo}} = 0.0155\)</span>), point estimate of the difference (<span class="math inline">\(0.0112 - 0.0155 = -0.0043\)</span>), and standard error <span class="math inline">\(SE = \sqrt{\frac{0.0112 \times 0.9888}{12933} + \frac{0.0155 \times 0.9845}{12938}} = 0.00145\)</span>.</p>
Next, plug the values into the general formula for a confidence interval, where we’ll use a 95% confidence level with <span class="math inline">\(z^{\star} = 1.96\)</span>:
<span class="math display">\[\begin{align*}
  -0.0043 \pm 1.96 \times 0.00145
      \quad \to \quad
      (-0.0071, -0.0015)
\end{align*}\]</span><br />
We are 95% confident that fish oils decreases
heart attacks by
0.15 to 0.71 percentage points
(off of a baseline of about 1.55%)
over a 5-year period for subjects who are similar
to those in the study.
Because the interval is entirely below 0, and the treatment was randomly assigned
the data provide strong evidence
that fish oil supplements reduce heart attacks
in patients like those in the study.]
</div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:fish-oil-data">Table 5.4: </span>Results for the study on n-3 fatty acid supplement and related health benefits.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
heart attack
</th>
<th style="text-align:right;">
no event
</th>
<th style="text-align:right;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
fish oil
</td>
<td style="text-align:right;">
145
</td>
<td style="text-align:right;">
12788
</td>
<td style="text-align:right;">
12933
</td>
</tr>
<tr>
<td style="text-align:left;">
placebo
</td>
<td style="text-align:right;">
200
</td>
<td style="text-align:right;">
12738
</td>
<td style="text-align:right;">
12938
</td>
</tr>
</tbody>
</table>
</div>
<div id="hypothesis-test-for-h_0-p_1---p_2-0" class="section level4 unnumbered">
<h4>Hypothesis test for <span class="math inline">\(H_0: p_1 - p_2 = 0\)</span></h4>
<p>
</p>
<p>A mammogram is an X-ray procedure used to check for
breast cancer.
Whether mammograms should be used is part of a
controversial discussion, and it’s the topic of our
next example where we learn about 2-proportion
hypothesis tests when <span class="math inline">\(H_0\)</span> is <span class="math inline">\(p_1 - p_2 = 0\)</span>
(or equivalently, <span class="math inline">\(p_1 = p_2\)</span>).</p>
<p>A 30-year study was conducted with nearly 90,000 female participants. During a 5-year screening period, each woman was randomized to one of two groups: in the first group, women received regular mammograms to screen for breast cancer, and in the second group, women received regular non-mammogram breast cancer exams. No intervention was made during the following 25 years of the study, and we’ll consider death resulting from breast cancer over the full 30-year period. Results from the study are summarized in Figure <a href="inference-cat.html#tab:mammogramStudySummaryTable">5.5</a>.</p>
<p>If mammograms are much more effective than non-mammogram breast cancer exams, then we would expect to see additional deaths from breast cancer in the control group. On the other hand, if mammograms are not as effective as regular breast cancer exams, we would expect to see an increase in breast cancer deaths in the mammogram group.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:mammogramStudySummaryTable">Table 5.5: </span>Summary results for breast cancer study.
</caption>
<thead>
<tr>
<th style="border-bottom:hidden" colspan="1">
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Death from breast cancer?
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Yes
</th>
<th style="text-align:left;">
No
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Mammogram
</td>
<td style="text-align:left;">
500
</td>
<td style="text-align:left;">
44,425
</td>
</tr>
<tr>
<td style="text-align:left;">
Control
</td>
<td style="text-align:left;">
505
</td>
<td style="text-align:left;">
44,405
</td>
</tr>
</tbody>
</table>

<div class="guidedpractice">
Is this study an experiment or an observational study?<a href="#fn60" class="footnote-ref" id="fnref60"><sup>60</sup></a>
</div>

<div class="guidedpractice">
Set up hypotheses to test whether there was a difference
in breast cancer deaths in the mammogram and control groups.<a href="#fn61" class="footnote-ref" id="fnref61"><sup>61</sup></a>
</div>
<p>Using the previous example,
we will check the conditions for using a normal distribution to
analyze the results of the study.
The details are very similar to that of confidence intervals.
However, when the null hypothesis is that <span class="math inline">\(p_1 - p_2 = 0\)</span>,
we use a special proportion called the
<strong>pooled proportion</strong> to check the success-failure condition:
<span class="math display">\[\begin{align*}
\hat{p}_{\textit{pool}}
    &amp;= \frac
        {\text{# of patients who died from breast cancer in the entire study}}
        {\text{# of patients in the entire study}} \\
	&amp;= \frac{500 + 505}{500 + \text{44,425} + 505 + \text{44,405}} \\
	&amp;= 0.0112
\end{align*}\]</span>
This proportion is an estimate of the breast cancer death rate
across the entire study, and it’s our best estimate of the
proportions <span class="math inline">\(p_{mgm}\)</span> and <span class="math inline">\(p_{ctrl}\)</span>
<em>if the null hypothesis is true that <span class="math inline">\(p_{mgm} = p_{ctrl}\)</span></em>.
We will also use this pooled proportion when computing
the standard error.</p>

<div class="example">
<p>Is it reasonable to model the difference
in proportions using a normal distribution in this
study?</p>
<hr />
Because the patients are randomized, they can be treated
as independent, both within and between groups.
We also must check the success-failure condition for each group.
Under the null hypothesis, the proportions <span class="math inline">\(p_{mgm}\)</span>
and <span class="math inline">\(p_{ctrl}\)</span> are equal, so we check the success-failure
condition with our best estimate of these values under <span class="math inline">\(H_0\)</span>,
the pooled proportion from the two samples,
<span class="math inline">\(\hat{p}_{\textit{pool}} = 0.0112\)</span>:
<span class="math display">\[\begin{align*}
  \hat{p}_{\textit{pool}} \times n_{mgm}
      &amp;= 0.0112 \times \text{44,925} = 503
    &amp; (1 - \hat{p}_{\textit{pool}}) \times n_{mgm}
      &amp;= 0.9888 \times \text{44,925} = \text{44,422} \\
  \hat{p}_{\textit{pool}} \times n_{ctrl}
      &amp;= 0.0112 \times \text{44,910} = 503
    &amp; (1 - \hat{p}_{\textit{pool}}) \times n_{ctrl}
      &amp;= 0.9888 \times \text{44,910} = \text{44,407}
  \end{align*}\]</span>
The success-failure condition is satisfied since
all values are at least 10.
With both conditions satisfied, we can safely model
the difference in proportions using a normal
distribution.
</div>

<div class="onebox">
<p><strong>Use the pooled proportion when <span class="math inline">\(H_0\)</span> is <span class="math inline">\(p_1 - p_2 = 0\)</span>.</strong></p>
When the null hypothesis is that the proportions are equal,
use the pooled proportion (<span class="math inline">\(\hat{p}_{\textit{pooled}}\)</span>)
to verify the
success-failure condition and estimate the standard error:
<span class="math display">\[\begin{eqnarray*}
  \hat{p}_{\textit{pooled}}
    = \frac{\text{number of ``successes&quot;}}
      {\text{number of cases}}
    = \frac{\hat{p}_1 n_1 + \hat{p}_2 n_2}{n_1 + n_2}
  \end{eqnarray*}\]</span>
Here <span class="math inline">\(\hat{p}_1 n_1\)</span> represents the number of successes in
sample 1 since
<span class="math display">\[\begin{eqnarray*}
  \hat{p}_1
    = \frac{\text{number of successes in sample 1}}{n_1}
  \end{eqnarray*}\]</span>
Similarly, <span class="math inline">\(\hat{p}_2 n_2\)</span> represents the number
of successes in sample 2.
</div>
<p>In the previous example,
the pooled proportion was used to check the success-failure
condition<a href="#fn62" class="footnote-ref" id="fnref62"><sup>62</sup></a>. In the next example, we see an additional place where the pooled
proportion comes into play: the standard error calculation.</p>

<div class="example">
<p>Compute the point estimate of the difference
in breast cancer death rates in the two groups,
and use the pooled proportion
<span class="math inline">\(\hat{p}_{\textit{pool}} = 0.0112\)</span> to calculate
the standard error.</p>
<hr />
The point estimate of the difference in breast cancer death
rates is
<span class="math display">\[\begin{align*}
  \hat{p}_{mgm} - \hat{p}_{ctrl}
    &amp;= \frac{500}{500 + 44,425} - \frac{505}{505 + 44,405} \\
    &amp;= 0.01113 - 0.01125 \\
    &amp;= -0.00012
  \end{align*}\]</span>
The breast cancer death rate in the mammogram group
was 0.012% less than in the control group.
Next, the standard error is calculated
, <span class="math inline">\(\hat{p}_{\textit{pool}}\)</span>:
<span class="math display">\[\begin{align*}
SE = \sqrt{
      \frac{\hat{p}_{\textit{pool}}(1-\hat{p}_{\textit{pool}})}
          {n_{mgm}}
      + \frac{\hat{p}_{\textit{pool}}(1-\hat{p}_{\textit{pool}})}
          {n_{ctrl}}
    }
	= 0.00070
\end{align*}\]</span>
</div>

<div class="example">
<p>Using the point estimate <span class="math inline">\(\hat{p}_{mgm} - \hat{p}_{ctrl} = -0.00012\)</span> and standard error <span class="math inline">\(SE = 0.00070\)</span>, calculate a p-value for the hypothesis test and write a conclusion.</p>
<hr />
<p>Just like in past tests, we first compute a test statistic and draw a picture:
<span class="math display">\[\begin{align*}
Z = \frac{\text{point estimate} - \text{null value}}{SE}
	= \frac{-0.00012 - 0}{0.00070}
	= -0.17
\end{align*}\]</span></p>
The lower tail area is 0.4325, which we double to get the p-value: 0.8650. Because this p-value is larger than 0.05, we do not reject the null hypothesis. That is, the difference in breast cancer death rates is reasonably explained by chance, and we do not observe benefits or harm from mammograms relative to a regular breast exam.
</div>
<p><img src="06-inference-cat_files/figure-html/unnamed-chunk-83-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Can we conclude that mammograms have no benefits or harm?
Here are a few considerations to keep in mind when reviewing
the mammogram study as well as any other medical study:</p>
<ul>
<li>We do not accept the null hypothesis, which means
we don’t have sufficient evidence to conclude that
mammograms reduce or increase breast cancer deaths.</li>
<li>If mammograms are helpful or harmful, the data
suggest the effect isn’t very large.</li>
<li>Are mammograms more or less expensive than
a non-mammogram breast exam?
If one option is much more expensive than the
other and doesn’t offer clear benefits,
then we should lean towards the less expensive
option.</li>
<li>The study’s authors also found that mammograms
led to over-diagnosis of breast cancer,
which means some breast cancers were found
(or thought to be found) but that these cancers
would not cause symptoms during patients’ lifetimes.
That is, something else would kill the patient
before breast cancer symptoms appeared.
This means some patients may have been treated
for breast cancer unnecessarily, and this
treatment is another cost to consider.
It is also important to recognize that
over-diagnosis can cause unnecessary physical
or emotional harm to patients.</li>
</ul>
<p>These considerations highlight the complexity around medical care and treatment recommendations. Experts and medical boards who study medical treatments use considerations like those above to provide their best recommendation based on the current evidence.</p>
<p>
</p>
<!--
\BeginKnitrBlock{onebox}<div class="onebox">**Hypothesis testing when ${H_0}$ is $p_1 - p_2 = 0$.**
  
  Once you've determined a hypothesis test for the difference
  of two proportions is the correct procedure, there are four
  steps to completing the test:

* **Prepare.**
      Identify the parameter of interest,
      list out hypotheses,
      identify the significance level,
      and compute summary statistics for each group.
* **Check.**
      Verify the conditions to ensure
      $\hat{p}_1 - \hat{p}_2$ is nearly normal under $H_0$.
      When the null hypothesis is that the difference is 0,
      use a pooled proportion to check the success-failure
      condition for each group.
* **Calculate.**
      If the conditions hold, compute the standard
      error, again using the pooled proportion,
      compute the Z-score, and identify the p-value.
* **Conclude.**
      Evaluate the hypothesis test by comparing the p-value
      to $\alpha$, and provide a conclusion in the context
      of the problem.</div>\EndKnitrBlock{onebox}
-->
<!--
\subsection{More on 2-proportion hypothesis tests (special topic)}

When we conduct a 2-proportion hypothesis test,
usually $H_0$ is $p_1 - p_2 = 0$. However, there are rare
situations where we want to check for some difference in
$p_1$ and $p_2$ that is some value other than 0.
For example, maybe we care about checking a null hypothesis
where $p_1 - p_2 = 0.1$. %\footnote{We can
%  also encounter a similar situation with a difference of
%  two means, though no such example is given in
%  Chapter \ref{inferenceForNumericalData} since the methods
%  remain exactly the same in the context of sample means.
%  On the other hand, the success-failure condition and the
%  calculation of the standard error vary slightly in different
%  proportion contexts.}
In contexts like these, we generally use $\hat{p}_1$ and
$\hat{p}_2$ to check the success-failure condition and
construct the standard error.

\begin{exercisewrap}
\begin{nexercise}
\label{carWheelBladeManufacturer}%
A quadcopter company is considering a new manufacturer
for rotor blades.
The new manufacturer would be more expensive,
but they claim
their higher-quality blades are more reliable,
with 3% more blades passing inspection than their
competitor.
Set up appropriate hypotheses for the test.\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{$H_0$: The higher-quality blades will pass
  inspection 3% more frequently than the standard-quality blades.
  $p_{highQ} - p_{standard} = 0.03$.
  $H_A$: The higher-quality blades will pass inspection
  some amount different than 3% more often than the
  standard-quality blades.
  $p_{highQ} - p_{standard} \neq 0.03$.}

\setlength{\captionwidth}{85mm}

\begin{figure}[h]
\centering
\Figures{0.6}{quadcopter}{quadcopter_david_j}
\caption{A Phantom quadcopter.\vspace{-1mm} \\
   -----------------------------\vspace{-2mm}\\
   {\footnotesize Photo by David J
   (\oiRedirect{textbook-quadcopter_david_j}
       {http://flic.kr/p/oiWLNu}).
   \oiRedirect{textbook-CC_BY_2}{CC-BY 2.0 license.}
   This photo has been cropped and a border has been added.}}
\label{quadcopter_david_j}
\end{figure}

\setlength{\captionwidth}{\mycaptionwidth}


%\Add{In Guided Practice \ref{qualityCtrlEngHypothesisEval}, the null difference is 0.03. However, in the vast majority of applications for differences in means or proportions, the null difference is 0. While the details for a difference of means does not change if the null difference is zero or non-zero, that is not the case for a difference in proportions. As we'll see in Section \ref{}, a hypothesis test for a difference in proportions where the null value is 0 requires additional care.}

\begin{examplewrap}
\begin{nexample}{The quality control engineer from
    Guided Practice \ref{carWheelBladeManufacturer}
    collects a sample of blades, examining 1000 blades
    from each company, and she finds that 899 blades pass
    inspection from the current supplier and 958 pass
    inspection from the prospective supplier.
    Using these data, evaluate the hypotheses from
    Guided Practice \ref{carWheelBladeManufacturer}
    with a significance level of 5%.}
  \label{qualityCtrlEngHypothesisEval}%
  First, we check the conditions.
  The sample is not necessarily random, so to proceed
  we must assume the blades are all independent;
  for this sample we will suppose this assumption
  is reasonable, but the engineer would be more knowledgeable
  as to whether this assumption is appropriate.
  The success-failure condition also holds for each sample.
  Thus, the difference in sample proportions,
  $0.958 - 0.899 = 0.059$, can be said to come from a nearly
  normal distribution.

  The standard error is computed using the two sample
  proportions since we do not use a pooled proportion
  for this context:
  \begin{align*}
  SE
    = \sqrt{\frac{0.958(1-0.958)}{1000} +
        \frac{0.899(1-0.899)}{1000}}
    = 0.0114
  \end{align*}
  In this hypothesis test, because the null is that
  $p_1 - p_2 = 0.03$, the sample proportions were used
  for the standard error calculation rather than a pooled
  proportion.

  Next, we compute the test statistic and use it to find the
  p-value, which is depicted in
  Figure \ref{bladesTwoSampleHTPValueQC}.
  \begin{align*}
  Z = \frac{\text{point estimate} - \text{null value}}{SE}
    = \frac{0.059 - 0.03}{0.0114} = 2.54
  \end{align*}
  Using a standard normal distribution for this test statistic,
  we identify the right tail area as 0.006,
  and we double it to get the p-value: 0.012.
  We reject the null hypothesis because 0.012 is less than 0.05.
  Since we observed a larger-than-3% increase in blades
  that pass inspection, we have statistically significant
  evidence that the higher-quality blades pass inspection
  \emph{more than} 3% as often as the currently used blades,
  exceeding the company's claims.
\end{nexample}
\end{examplewrap}

\begin{figure}[h]
  \centering
  \Figure{0.45}{bladesTwoSampleHTPValueQC}
  \caption{Distribution of the test statistic if the null
      hypothesis was true.
      The p-value is represented by the shaded areas.}
  \label{bladesTwoSampleHTPValueQC}
\end{figure}
-->
<!--
## Testing for goodness of fit using chi-square (special topic, include simulation version)
-->
</div>
</div>
</div>
<div id="independence-in-two-way-tables" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Independence in two-way tables</h2>
<p>Note that with two-way tables, there is not an obvious single parameter of interest.
Instead, research questions usually focus on how the proportions of the response variable changes (or not) across the different levels of the explanatory variable.
Because there is not a population parameter to estimate, bootstrapping to find the standard error of the estimate is not meaningful.
As such, for two-way tables, we will focus on the randomization test and corresponding mathematical approximation (and not bootstrapping).</p>
<div id="randomization-test-of-h_0-independence" class="section level3" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> Randomization test of <span class="math inline">\(H_0:\)</span> independence</h3>
<p></p>
<!--
\newcommand{\iPodAA}{2}
\newcommand{\iPodAB}{23}
\newcommand{\iPodAC}{36}
\newcommand{\iPodAD}{61}
\newcommand{\iPodAFraction}{0.2785}
\newcommand{\iPodAExpected}{20.33}
\newcommand{\iPodBA}{71}
\newcommand{\iPodBB}{50}
\newcommand{\iPodBC}{37}
\newcommand{\iPodBD}{158}
\newcommand{\iPodBFraction}{0.7215}
\newcommand{\iPodBExpected}{52.67}
\newcommand{\iPodDA}{73}
\newcommand{\iPodDB}{73}
\newcommand{\iPodDC}{73}
\newcommand{\iPodDD}{219}
\newcommand{\iPodN}{\iPodDD}
-->
<p>We all buy used products –
cars, computers, textbooks, and so on –
and we sometimes assume the sellers of those products
will be forthright about any underlying problems with
what they’re selling.
This is not something we should take for granted.
Researchers recruited 219 participants in a study where they
would sell a used iPod<a href="#fn63" class="footnote-ref" id="fnref63"><sup>63</sup></a>
that was known to have frozen twice in the past.
The participants were incentivized to get as much money
as they could for the iPod since they would receive a 5%
cut of the sale on top of $10 for participating.
The researchers wanted to understand what types of questions
would elicit the seller to disclose the freezing issue.</p>
<p>Unbeknownst to the participants who were the sellers
in the study,
the buyers were collaborating with the researchers
to evaluate the influence of different questions
on the likelihood of getting the sellers to disclose
the past issues with the iPod.
The scripted buyers started with
“Okay, I guess I’m supposed to go first.
So you’ve had the iPod for 2 years …”
and ended with one of three questions:</p>
<ul>
<li>General: What can you tell me about it?<br />
</li>
<li>Positive Assumption: It doesn’t have any problems, does it?<br />
</li>
<li>Negative Assumption: What problems does it have?</li>
</ul>
<p>The question is the treatment given to the sellers,
and the response is whether the question prompted them
to disclose the freezing issue with the iPod.
The results are shown in Table <a href="inference-cat.html#tab:ipod-ask-data-summary">5.6</a>,
and the data suggest that asking the,
<em>What problems does it have?</em>,
was the most effective at getting the seller to disclose
the past freezing issues.
However, you should also be asking yourself:
could we see these results due to chance alone,
or is this in fact evidence that some questions
are more effective for getting at the truth?</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:ipod-ask-data-summary">Table 5.6: </span>Summary of the iPod study, where a question was
posed to the study participant who acted.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
General
</th>
<th style="text-align:left;">
Positive Assumptions
</th>
<th style="text-align:left;">
Negative Assumptions
</th>
<th style="text-align:left;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Disclose Problem
</td>
<td style="text-align:left;">
2
</td>
<td style="text-align:left;">
23
</td>
<td style="text-align:left;">
36
</td>
<td style="text-align:left;">
61
</td>
</tr>
<tr>
<td style="text-align:left;">
Hide Problem
</td>
<td style="text-align:left;">
71
</td>
<td style="text-align:left;">
50
</td>
<td style="text-align:left;">
37
</td>
<td style="text-align:left;">
158
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
73
</td>
<td style="text-align:left;">
73
</td>
<td style="text-align:left;">
73
</td>
<td style="text-align:left;">
219
</td>
</tr>
</tbody>
</table>
<!--
\BeginKnitrBlock{onebox}<div class="onebox">**Differences of one-way tables vs two-way tables.**
  
  A one-way table describes counts for each outcome in a single
  variable.
  A two-way table describes counts for *combinations*
  of outcomes for two variables.
  When we consider a two-way table, we often would like to know,
  are these variables related in any way?
  That is, are they dependent (versus independent)?</div>\EndKnitrBlock{onebox}
-->
<p>The hypothesis test for the iPod experiment is really about
assessing whether there is statistically significant evidence
that there was a difference in the success rates that each question had on getting the participant
to disclose the problem with the iPod.
In other words, the goal is to check whether the buyer’s
question was independent of whether the seller disclosed
a problem.</p>
<div id="expected-counts-in-two-way-tables" class="section level4 unnumbered">
<h4>Expected counts in two-way tables</h4>
<p>While we would not expect the number of disclosures to be exactly the same across the three groups, the rate of disclosure seems substantially different across the three groups.
In order to investigate whether the differences in rates is due to natural variability or due to a treatment effect (i.e., the question causing the differences), we need to compute estimated counts for each cell in a two-way table.</p>

<div class="example">
<p>From the experiment,
we can compute the proportion of all sellers who disclosed
the freezing problem as <span class="math inline">\(61/219 = 0.2785\)</span>.
If there really is no difference among the questions
and 27.85% of sellers were going to disclose the freezing
problem no matter the question that was put to them,
how many of the 73 people in the <code>General</code>
group would we have expected to disclose the freezing
problem?</p>
<hr />
We would predict that <span class="math inline">\(0.2785 \times 73 = 20.33\)</span>
sellers would disclose the problem.
Obviously we observed fewer than this, though it is not
yet clear if that is due to chance variation or whether
that is because the questions vary in how effective they
are at getting to the truth.
</div>

<div class="guidedpractice">
If the questions were actually equally effective,
meaning about 27.85% of respondents would disclose the
freezing issue regardless of what question they were asked,
about how many sellers would we expect to  the
freezing problem from the Positive Assumption
group?<a href="#fn64" class="footnote-ref" id="fnref64"><sup>64</sup></a>
</div>
<p>We can compute the expected number of sellers who we would
expect to disclose or hide the freezing issue for all groups,
if the questions had no impact on what they disclosed,
using the same strategies employed in the previous Example and Guided Practice to computed expected counts.
These expected counts were used to construct Table <a href="inference-cat.html#tab:ipod-ask-data-summary-expected">5.7</a>,
which is the same as Table <a href="inference-cat.html#tab:ipod-ask-data-summary">5.6</a>,
except now the expected counts have been added in parentheses.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:ipod-ask-data-summary-expected">Table 5.7: </span>The observed counts and the (expected counts).
</caption>
<thead>
<tr>
<th style="border-bottom:hidden" colspan="1">
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
General
</div>
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Positive Assumptions
</div>
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Negative Assumptions
</div>
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="1">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Total
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Disclose Problem
</td>
<td style="text-align:left;">
2
</td>
<td style="text-align:left;font-style: italic;color: #569BBD !important;">
(20.33)
</td>
<td style="text-align:left;">
23
</td>
<td style="text-align:left;font-style: italic;color: #569BBD !important;">
(20.33)
</td>
<td style="text-align:left;">
36
</td>
<td style="text-align:left;font-style: italic;color: #569BBD !important;">
(20.33)
</td>
<td style="text-align:left;">
61
</td>
</tr>
<tr>
<td style="text-align:left;">
Hide Problem
</td>
<td style="text-align:left;">
71
</td>
<td style="text-align:left;font-style: italic;color: #569BBD !important;">
(52.67)
</td>
<td style="text-align:left;">
50
</td>
<td style="text-align:left;font-style: italic;color: #569BBD !important;">
(52.67)
</td>
<td style="text-align:left;">
37
</td>
<td style="text-align:left;font-style: italic;color: #569BBD !important;">
(52.67)
</td>
<td style="text-align:left;">
158
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
73
</td>
<td style="text-align:left;font-style: italic;color: #569BBD !important;">
</td>
<td style="text-align:left;">
73
</td>
<td style="text-align:left;font-style: italic;color: #569BBD !important;">
</td>
<td style="text-align:left;">
73
</td>
<td style="text-align:left;font-style: italic;color: #569BBD !important;">
</td>
<td style="text-align:left;">
219
</td>
</tr>
</tbody>
</table>
<p>The examples and exercises above provided some help
in computing expected counts.
In general, expected counts for a two-way table may
be computed using the row totals, column totals,
and the table total.
For instance, if there was no difference between the groups,
then about 27.85% of each column should be in the first row:</p>
<p><span class="math display">\[\begin{align*}
0.2785\times (\text{column 1 total}) &amp;= 20.33 \\
0.2785\times (\text{column 2 total}) &amp;= 20.33 \\
0.2785\times (\text{column 3 total}) &amp;= 20.33
\end{align*}\]</span>
Looking back to how 0.2785 was computed –
as the fraction of sellers who disclosed the freezing issue
(<span class="math inline">\(158/219\)</span>) –
these three expected counts could have been computed as
<span class="math display">\[\begin{align*}
\left(\frac{\text{row 1 total}}{\text{table total}}\right)
    \text{(column 1 total)} &amp;= 20.33 \\
\left(\frac{\text{row 1 total}}{\text{table total}}\right)
    \text{(column 2 total)} &amp;= 20.33 \\
\left(\frac{\text{row 1 total}}{\text{table total}}\right)
    \text{(column 3 total)} &amp;= 20.33
\end{align*}\]</span>
This leads us to a general formula for computing expected
counts in a two-way table when we would like to test whether
there is strong evidence of an association between the column
variable and row variable.</p>

<div class="onebox">
<p><strong>Computing expected counts in a two-way table.</strong></p>
To identify the expected count for the <span class="math inline">\(i^{th}\)</span> row
and <span class="math inline">\(j^{th}\)</span> column, compute
<span class="math display">\[\begin{align*}
  \text{Expected Count}_{\text{row }i,\text{ col }j}
    = \frac{(\text{row $i$ total}) \times
        (\text{column $j$ total})}{\text{table total}}
  \end{align*}\]</span>
</div>
</div>
<div id="the-chi-square-statistic" class="section level4 unnumbered">
<h4>The chi-square statistic</h4>
<div id="observed-data-3" class="section level5 unnumbered">
<h5>Observed data</h5>
<p>The chi-square test statistic for a two-way table is found
by comparing the observed and expected counts for each cell in the table.
For each table count, compute:</p>
<p><span class="math display">\[\begin{align*}
&amp;\text{General formula} &amp;&amp;
    \frac{(\text{observed count } - \text{expected count})^2}
        {\text{expected count}} \\
&amp;\text{Row 1, Col 1} &amp;&amp;
    \frac{(2 - 20.33)^2}{20.33} = 16.53 \\
&amp;\text{Row 1, Col 2} &amp;&amp;
    \frac{(23 - 20.33)^2}{20.33} = 0.35 \\
&amp; \hspace{9mm}\vdots &amp;&amp;
    \hspace{13mm}\vdots \\
&amp;\text{Row 2, Col 3} &amp;&amp;
    \frac{(37 - 52.67)^2}{52.67} = 4.66
\end{align*}\]</span>
Adding the computed value for each cell gives the chi-square test statistic <span class="math inline">\(X^2\)</span>:
<span class="math display">\[\begin{align*}
X^2 = 16.53 + 0.35 + \dots + 4.66 = 40.13
\end{align*}\]</span></p>
</div>
</div>
<div id="randomization-distribution-of-the-chi-square-statistic" class="section level4 unnumbered">
<h4>Randomization distribution of the chi-square statistic</h4>
<div id="variability-of-the-statistic-4" class="section level5 unnumbered">
<h5>Variability of the statistic</h5>
<p>Is 40.13 a big number? That is, does it indicate that the observed and expected values are really different? Or is 40.13 a value of the statistic that we’d expect to see just due to natural variability? Previously, we applied the randomization test to the setting where the research question investigated a difference in proportions. The same idea of shuffling the data under the null hypothesis can be used in the setting of the two-way table.</p>
<p>Assuming that the individuals would disclose or hide the problems <strong>regardless</strong> of the question they are given (i.e., that the null hypothesis is true), we can randomize the data by reassigning the 61 disclosed problems and 158 hidden problems to the three groups at random. Table <a href="inference-cat.html#tab:ipod-ask-data-summary-rand">5.8</a> shows a possible randomization of the observed data under the condition that the null hypothesis is true (in contrast to the original observed data in Table <a href="inference-cat.html#tab:ipod-ask-data-summary">5.6</a>).</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:ipod-ask-data-summary-rand">Table 5.8: </span>Randomized allocation of the data to the question posed in the iPod study.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
General
</th>
<th style="text-align:left;">
Positive Assumptions
</th>
<th style="text-align:left;">
Negative Assumptions
</th>
<th style="text-align:left;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Disclose Problem
</td>
<td style="text-align:left;">
15
</td>
<td style="text-align:left;">
26
</td>
<td style="text-align:left;">
20
</td>
<td style="text-align:left;">
61
</td>
</tr>
<tr>
<td style="text-align:left;">
Hide Problem
</td>
<td style="text-align:left;">
58
</td>
<td style="text-align:left;">
47
</td>
<td style="text-align:left;">
53
</td>
<td style="text-align:left;">
158
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
73
</td>
<td style="text-align:left;">
73
</td>
<td style="text-align:left;">
73
</td>
<td style="text-align:left;">
219
</td>
</tr>
</tbody>
</table>
<p>As before, the randomized data is used to find a single value for the test statistic (here a chi-squared statistic).
The chi-square statistic for the randomized two-way table is found
by comparing the observed and expected counts for each cell in the <em>randomized</em> table.
For each cell, compute:</p>
<p><span class="math display">\[\begin{align*}
&amp;\text{General formula} &amp;&amp;
    \frac{(\text{observed count } - \text{expected count})^2}
        {\text{expected count}} \\
&amp;\text{Row 1, Col 1} &amp;&amp;
    \frac{(15 - 20.33)^2}{20.33} = 1.399 \\
&amp;\text{Row 1, Col 2} &amp;&amp;
    \frac{(26 - 20.33)^2}{20.33} = 1.579 \\
&amp; \hspace{9mm}\vdots &amp;&amp;
    \hspace{13mm}\vdots \\
&amp;\text{Row 2, Col 3} &amp;&amp;
    \frac{(53 - 52.67)^2}{52.67} = 0.002
\end{align*}\]</span>
Adding the computed value for each cell gives the chi-square test statistic <span class="math inline">\(X^2\)</span>:
<span class="math display">\[\begin{align*}
X^2 = 1.399 + 1.579 + \dots + 0.002 = 4.136
\end{align*}\]</span></p>
</div>
<div id="observed-statistic-vs-null-statistics-1" class="section level5 unnumbered">
<h5>Observed statistic vs null statistics</h5>
<p>As before, one randomization will not be sufficient for understanding if the observed data are particularly different from the expected chi-square statistics when <span class="math inline">\(H_0\)</span> is true. To investigate whether 40.13 is large enough to indicate the the observed and expected counts are significantly different, we need to understand what values of the chi-squared statistic would happen just due to change. Figure <a href="inference-cat.html#fig:ipodRandDotPlot">5.13</a> plots 1000 chi-squared statistics generated under the null hypothesis. We can see that the observed value is so far from the null statistics that the simulated p-value is zero. That is, the probability of seeing the observed statistic when the null hypothesis is true is virtually zero. In this case we can conclude that the decision of whether or not to disclose the iPod’s problem is changed by the question asked. (We use the causal language of “changed” because the study was an experiment.) Note that with a chi-squared test, we only know that the two variables (here: <code>question</code> and <code>disclosure</code>) are related (i.e., not independent). We are not able to claim which type of question causes which type of disclosure.</p>
<div class="figure" style="text-align: center"><span id="fig:ipodRandDotPlot"></span>
<img src="06-inference-cat_files/figure-html/ipodRandDotPlot-1.png" alt="A stacked dot plot of chi-square statisics from 1000 simulations produced under the null hypothesis, $H_0$, where the question is independent of the disclosure. None of the 1000 simulations had a chi-square value of at least 40.13, the chi-square value observed in the study.  Indeed, none of the simulated chi-squared statistics came anywhere close to the observed statistic!" width="70%" />
<p class="caption">
Figure 5.13: A stacked dot plot of chi-square statisics from 1000 simulations produced under the null hypothesis, <span class="math inline">\(H_0\)</span>, where the question is independent of the disclosure. None of the 1000 simulations had a chi-square value of at least 40.13, the chi-square value observed in the study. Indeed, none of the simulated chi-squared statistics came anywhere close to the observed statistic!
</p>
</div>
</div>
</div>
</div>
<div id="mathematical-model" class="section level3" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> Mathematical model</h3>
<div id="the-chi-square-test-of-h_0-independence" class="section level4 unnumbered">
<h4>The chi-square test of <span class="math inline">\(H_0:\)</span> independence</h4>
<div id="variability-of-the-statistic-5" class="section level5 unnumbered">
<h5>Variability of the statistic</h5>
<p>As it turns out, the chi-squared test statistic follows a chi-square distribution when the null hypothesis is true.
For two way tables, the degrees of freedom is equal to:
<span class="math display">\[\begin{align*}
df = \text{(number of rows minus 1)}\times \text{(number of columns minus 1)}
\end{align*}\]</span>
In our example, the degrees of freedom parameter is
<span class="math display">\[\begin{align*}
df = (2-1)\times (3-1) = 2
\end{align*}\]</span></p>
</div>
<div id="observed-statistic-vs.-null-statistics-1" class="section level5 unnumbered">
<h5>Observed statistic vs. null statistics</h5>
<p>To bring it back to the example, if the null hypothesis is true
(i.e., the questions had no impact on the sellers in
the experiment),
then the test statistic <span class="math inline">\(X^2 = 40.13\)</span> closely follows
a chi-square distribution with 2 degrees of freedom.
Using this information, we can compute the p-value for
the test, which is depicted in
Figure <a href="inference-cat.html#fig:iPodChiSqTail">5.14</a>.</p>

<div class="onebox">
<strong>Computing degrees of freedom for a two-way table.</strong>
When applying the chi-square test to a two-way table,
we use
<span class="math display">\[\begin{align*}
  df = (R-1)\times (C-1)
  \end{align*}\]</span>
where <span class="math inline">\(R\)</span> is the number of rows in the table
and <span class="math inline">\(C\)</span> is the number of columns.
</div>
<div class="figure" style="text-align: center"><span id="fig:iPodChiSqTail"></span>
<img src="06-inference-cat_files/figure-html/iPodChiSqTail-1.png" alt="Visualization of the p-value for $X^2 = 40.13$ when $df = 2$." width="70%" />
<p class="caption">
Figure 5.14: Visualization of the p-value for <span class="math inline">\(X^2 = 40.13\)</span> when <span class="math inline">\(df = 2\)</span>.
</p>
</div>
<p>The software R can be used to find the p-value with the function <code>pchisq()</code>. Just like <code>pnorm()</code>, <code>pchisq()</code> always gives the area to the left of the cutoff value. Because, in this example, the p-value is represented by the area to the right of 40.13, we subtract the output of <code>pchisq()</code> from 1.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="inference-cat.html#cb12-1" aria-hidden="true"></a><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pchisq</span>(<span class="fl">40.13</span>, <span class="dt">df =</span> <span class="dv">2</span>)</span>
<span id="cb12-2"><a href="inference-cat.html#cb12-2" aria-hidden="true"></a><span class="co">#&gt; [1] 1.93e-09</span></span></code></pre></div>

<div class="example">
<p>Find the p-value and draw a conclusion
about whether the question affects the sellers likelihood
of reporting the freezing problem.</p>
<hr />
<p>Using a computer, we can compute a very precise value
for the tail area above <span class="math inline">\(X^2 = 40.13\)</span> for a chi-square
distribution with 2 degrees of freedom:
0.000000002.</p>
Using a significance level of <span class="math inline">\(\alpha=0.05\)</span>,
the null hypothesis is rejected since the p-value is smaller.
That is, the data provide convincing evidence that the
question asked did affect a seller’s likelihood to tell
the truth about problems with the iPod.
</div>
<p></p>
<p></p>

<div class="example">
<p>Table <a href="inference-cat.html#tab:diabetes2ExpMetRosiLifestyleSummary">5.9</a>
summarizes the results of an experiment evaluating
three treatments for Type 2 Diabetes in patients
aged 10-17 who were being treated with metformin.
The three treatments considered were
continued treatment with metformin (<code>met</code>),
treatment with metformin combined with rosiglitazone
(<code>rosi</code>),
or a lifestyle intervention program.
Each patient had a primary outcome, which was either lacked
glycemic control (failure)
or did not lack that control (success).
What are appropriate hypotheses for this test?</p>
<hr />
<ul>
<li><span class="math inline">\(H_0\)</span>: There is no difference in the effectiveness of the three treatments.</li>
<li><span class="math inline">\(H_A\)</span>: There is some difference in effectiveness between the three treatments, e.g., perhaps the <code>rosi</code> treatment performed better than <code>lifestyle</code>.
</div></li>
</ul>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:diabetes2ExpMetRosiLifestyleSummary">Table 5.9: </span>Results for the Type 2 Diabetes study.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Failure
</th>
<th style="text-align:left;">
Success
</th>
<th style="text-align:left;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<code>lifestyle</code>
</td>
<td style="text-align:left;">
109
</td>
<td style="text-align:left;">
125
</td>
<td style="text-align:left;">
234
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>met</code>
</td>
<td style="text-align:left;">
120
</td>
<td style="text-align:left;">
112
</td>
<td style="text-align:left;">
232
</td>
</tr>
<tr>
<td style="text-align:left;">
<code>rosi</code>
</td>
<td style="text-align:left;">
90
</td>
<td style="text-align:left;">
143
</td>
<td style="text-align:left;">
233
</td>
</tr>
<tr>
<td style="text-align:left;">
Total
</td>
<td style="text-align:left;">
319
</td>
<td style="text-align:left;">
380
</td>
<td style="text-align:left;">
699
</td>
</tr>
</tbody>
</table>

<div class="guidedpractice">
<p>A chi-square test for a two-way table may be used to test
the hypotheses in the diabetes Example above.</p>
As a first step, compute the expected values for each of the
six table cells.<a href="#fn65" class="footnote-ref" id="fnref65"><sup>65</sup></a>
</div>
<!--
\begin{exercisewrap}
\begin{nexercise}
Compute the chi-square test statistic for the data in
Figure \ref{diabetes2ExpMetRosiLifestyleSummary}.\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{For each cell,
    compute $\frac{(\text{obs} - \text{exp})^2}{exp}$.
    For instance, the first row and first column:
    $\frac{(109-106.8)^2}{106.8} = 0.05$.
    Adding the results of each cell gives the
    chi-square test statistic:
    {\scriptsize$X^2 = 0.05 + \cdots + 2.11 = 8.16$}.}

\begin{exercisewrap}
\begin{nexercise}
Because there are 3 rows and 2 columns,
the degrees of freedom for the test is
$df = (3 - 1) \times (2 - 1) = 2$.
Use $X^2 = 8.16$, $df = 2$, evaluate whether
to reject the null hypothesis using a significance level
of 0.05.\footnotemark
\end{nexercise}
\end{exercisewrap}
\footnotetext{
    If using a computer, we can identify the p-value
    as 0.017.
    That is, we reject the null hypothesis because
    the p-value is less than 0.05, and we conclude
    that at least one of the treatments is more or
    less effective than the others at treating
    Type 2 Diabetes for glycemic control.}

\index{data!diabetes|)}

-->
<p>Note, when analyzing 2-by-2 contingency tables (that is, when both variables only have two possible options), one guideline
is to use the two-proportion methods introduced in
Section <a href="inference-cat.html#diff-two-prop">5.3</a>.</p>
</div>
</div>
</div>
</div>
<div id="chp6-review" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Chapter 6 review</h2>

<div class="todo">
need to expand on the technical condition as the last row. also, is it helpful for the rest of the table to be repeated?
</div>
<div id="terms" class="section level3" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> Terms</h3>
<p>We introduced the following terms in the chapter.
If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions.
We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate.
However you should be able to easily spot them as <strong>bolded text</strong>.</p>
<table>
<tbody>
<tr>
<td style="text-align:left;">
alternative hypothesis
</td>
<td style="text-align:left;">
null hypothesis
</td>
<td style="text-align:left;">
pooled proportion
</td>
<td style="text-align:left;">
test statistic
</td>
</tr>
<tr>
<td style="text-align:left;">
confidence interval
</td>
<td style="text-align:left;">
one-sided hypothesis test
</td>
<td style="text-align:left;">
SE interval
</td>
<td style="text-align:left;">
two-sided hypothesis test
</td>
</tr>
<tr>
<td style="text-align:left;">
confirmation bias
</td>
<td style="text-align:left;">
p-value
</td>
<td style="text-align:left;">
standard error for difference in proportions
</td>
<td style="text-align:left;">
Type 1 Error
</td>
</tr>
<tr>
<td style="text-align:left;">
hypothesis test
</td>
<td style="text-align:left;">
parametric bootstrap
</td>
<td style="text-align:left;">
standard error of single proportion
</td>
<td style="text-align:left;">
Type 2 Error
</td>
</tr>
<tr>
<td style="text-align:left;">
margin of error
</td>
<td style="text-align:left;">
percentile interval
</td>
<td style="text-align:left;">
statistical inference
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
null distribution
</td>
<td style="text-align:left;">
point estimate
</td>
<td style="text-align:left;">
success-failure condition
</td>
<td style="text-align:left;">
</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="36">
<li id="fn36"><p>We would be assuming that these two variables are <strong>independent</strong>.<a href="inference-cat.html#fnref36" class="footnote-back">↩︎</a></p></li>
<li id="fn37"><p>If you are a STAT 216 student, you will recognize this from our first week’s in-class activity.<a href="inference-cat.html#fnref37" class="footnote-back">↩︎</a></p></li>
<li id="fn38"><p>Bumba is the Martian on the left!<a href="inference-cat.html#fnref38" class="footnote-back">↩︎</a></p></li>
<li id="fn39"><p>A fair coin has a 50% chance of landing on heads, which is the chance a student would guess Bumba correctly if they were just guessing. Thus, toss a coin 38 times with heads representing “guess correctly”; then calculate the proportion of tosses that landed on heads. Another option would be to 10 black cards and 10 red cards, letting red represent “guess correctly”. Shuffle the cards and draw one card, record if it is red or black, then replace the card and shuffle again. Do this 38 times and calculate the proportion of red cards observed.<a href="inference-cat.html#fnref39" class="footnote-back">↩︎</a></p></li>
<li id="fn40"><p>To explore this further, watch this <a href="https://www.ted.com/talks/vs_ramachandran_3_clues_to_understanding_your_brain">TED Talk</a> by neurologist Vilayanur Ramachandran (The synesthesia part begins at roughly 17:40 minutes).<a href="inference-cat.html#fnref40" class="footnote-back">↩︎</a></p></li>
<li id="fn41"><p>If you carry out the calculations, you’ll note that the upper bound is actually <span class="math inline">\(0.89 + 0.16 = 1.05\)</span>, but since a sample proportion cannot be greater than 1, we truncated the interval to 1.<a href="inference-cat.html#fnref41" class="footnote-back">↩︎</a></p></li>
<li id="fn42"><p>The first possibility (<em>We can’t read Martian, and these results just occurred by chance.</em>) was the null hypothesis; the second possibility (<em>We can read Martian, and these results reflect this ability.</em>) was the alternative hypothesis.<a href="inference-cat.html#fnref42" class="footnote-back">↩︎</a></p></li>
<li id="fn43"><p>Technically, the observed sample statistic or one more extreme in the direction of our alternative. But it is helpful to just remember this as “the data”.<a href="inference-cat.html#fnref43" class="footnote-back">↩︎</a></p></li>
<li id="fn44"><p>If we want to be more certain we will capture the fish, we might use a wider net. Likewise, we use a wider confidence interval if we want to be more certain that we capture the parameter.<a href="inference-cat.html#fnref44" class="footnote-back">↩︎</a></p></li>
<li id="fn45"><p><span class="math inline">\(H_0\)</span>: There is no association between the consultant’s contributions and the clients’ complication rate. In statistical language, <span class="math inline">\(p=0.10\)</span>.
<span class="math inline">\(H_A\)</span>: Patients who work with the consultant tend to have a complication rate lower than 10%, i.e., <span class="math inline">\(p&lt;0.10\)</span>.<a href="inference-cat.html#fnref45" class="footnote-back">↩︎</a></p></li>
<li id="fn46"><p>There isn’t sufficiently strong evidence to support an association between the consultant’s work and fewer surgery complications.<a href="inference-cat.html#fnref46" class="footnote-back">↩︎</a></p></li>
<li id="fn47"><p>No. It might be that the consultant’s work is associated with a reduction but that there isn’t enough data to convincingly show this connection.<a href="inference-cat.html#fnref47" class="footnote-back">↩︎</a></p></li>
<li id="fn48"><p>Because the <span class="math inline">\(p\)</span> is unknown but expected to be around 2/3, we will use 2/3 in place of <span class="math inline">\(p\)</span> in the formula for the standard error.<br />
<span class="math inline">\(SE = \sqrt{\frac{p(1-p)}{n}} \approx  \sqrt{\frac{2/3 (1 - 2/3)}  {300}} = 0.027\)</span>.<a href="inference-cat.html#fnref48" class="footnote-back">↩︎</a></p></li>
<li id="fn49"><p>This is equivalent to asking how often the <span class="math inline">\(Z\)</span> score will be larger than -2.58 but less than 2.58. (For a picture, see Figure <a href="inference-cat.html#fig:choosingZForCI">5.4</a>.) To determine this probability, look up -2.58 and 2.58 in the normal probability table (0.0049 and 0.9951). Thus, there is a <span class="math inline">\(0.9951-0.0049 \approx 0.99\)</span> probability that the unobserved random variable <span class="math inline">\(X\)</span> will be within 2.58 standard deviations of the mean.<a href="inference-cat.html#fnref49" class="footnote-back">↩︎</a></p></li>
<li id="fn50"><p>Since the necessary conditions for applying the normal model have already been checked for us, we can go straight to the construction of the confidence interval: <span class="math inline">\(\text{point estimate}\ \pm\ 2.58 \times SE \rightarrow (0.018, 0.162)\)</span>. We are 99% confident that implanting a stent in the brain of a patient who is at risk of stroke increases the risk of stroke within 30 days by a rate of 0.018 to 0.162 (assuming the patients are representative of the population).<a href="inference-cat.html#fnref50" class="footnote-back">↩︎</a></p></li>
<li id="fn51"><p>We must find <span class="math inline">\(z^{\star}\)</span> such that 90% of the distribution falls between -<span class="math inline">\(z^{\star}\)</span> and <span class="math inline">\(z^{\star}\)</span> in the standard normal model, <span class="math inline">\(N(\mu=0, \sigma=1)\)</span>. We can look up -<span class="math inline">\(z^{\star}\)</span> in the normal probability table by looking for a lower tail of 5% (the other 5% is in the upper tail), thus <span class="math inline">\(z^{\star}=1.65\)</span>. The 90% confidence interval can then be computed as <span class="math inline">\(\text{point estimate}\ \pm\ 1.65\times SE \to (4.4\%, 13.6\%)\)</span>. (Note: the conditions for normality had earlier been confirmed for us.) That is, we are 90% confident that implanting a stent in a stroke patient’s brain increased the risk of stroke within 30 days by 4.4% to 13.6%.<a href="inference-cat.html#fnref51" class="footnote-back">↩︎</a></p></li>
<li id="fn52"><p><span class="math inline">\(H_0\)</span>: there is not support for the regulation; <span class="math inline">\(H_0\)</span>: <span class="math inline">\(p \leq 0.50\)</span>. <span class="math inline">\(H_A\)</span>: the majority of borrowers support the regulation; <span class="math inline">\(H_A\)</span>: <span class="math inline">\(p &gt; 0.50\)</span>.<a href="inference-cat.html#fnref52" class="footnote-back">↩︎</a></p></li>
<li id="fn53"><p>Independence holds since the poll
is based on a random sample.
The success-failure condition also holds,
which is checked
using the null value (<span class="math inline">\(p_0 = 0.5\)</span>) from <span class="math inline">\(H_0\)</span>:
<span class="math inline">\(np_0 = 826 \times 0.5 = 413\)</span>,
<span class="math inline">\(n(1 - p_0) = 826 \times 0.5 = 413\)</span>. Recall that here, the best guess for <span class="math inline">\(p\)</span> is <span class="math inline">\(p_0\)</span> which comes from the null hypothesis (because we assume the null hypothesis is true when performing the testing procedure steps). <span class="math inline">\(H_0\)</span>: there is not support for the regulation; <span class="math inline">\(H_0\)</span>: <span class="math inline">\(p \leq 0.50\)</span>. <span class="math inline">\(H_A\)</span>: the majority of borrowers support the regulation; <span class="math inline">\(H_A\)</span>: <span class="math inline">\(p &gt; 0.50\)</span>.<a href="inference-cat.html#fnref53" class="footnote-back">↩︎</a></p></li>
<li id="fn54"><p>The
study is an experiment, as patients were randomly
assigned an experiment group.
Since this is an experiment, the results can be used
to evaluate a causal relationship between the malaria
vaccine and whether patients showed signs
of an infection.<a href="inference-cat.html#fnref54" class="footnote-back">↩︎</a></p></li>
<li id="fn55"><p>This reasoning does not generally extend
to anecdotal observations.
Each of us observes incredibly rare events every day,
events we could not possibly hope to predict.
However, in the non-rigorous setting of anecdotal
evidence, almost anything may appear to be a rare event,
so the idea of looking for rare events in day-to-day
activities is treacherous.
For example, we might look at the lottery:
there was only a 1 in 292 million chance that the
Powerball numbers for the largest jackpot in history
(January 13th, 2016) would be (04, 08, 19, 27, 34)
with a Powerball of (10),
but nonetheless those numbers came up!
However, no matter what numbers had turned up,
they would have had the same incredibly rare odds.
That is, <em>any set of numbers we could have
observed would ultimately be incredibly rare</em>.
This type of situation is typical of our daily lives:
each possible event in itself seems incredibly rare,
but if we consider every alternative, those outcomes
are also incredibly rare.
We should be cautious not to misinterpret such
anecdotal evidence.<a href="inference-cat.html#fnref55" class="footnote-back">↩︎</a></p></li>
<li id="fn56"><p>Making a Type 1 Error in this context would mean that reminding students that money not spent now can be spent later does not affect their buying habits, despite the strong evidence (the data suggesting otherwise) found in the experiment. Notice that this does <em>not</em> necessarily mean something was wrong with the data or that we made a computational mistake. Sometimes data simply point us to the wrong conclusion, which is why scientific studies are often repeated to check initial findings.<a href="inference-cat.html#fnref56" class="footnote-back">↩︎</a></p></li>
<li id="fn57"><p>To lower the Type 2 Error rate, we want to convict more guilty people. We could lower the standards for conviction from “beyond a reasonable doubt” to “beyond a little doubt”. Lowering the bar for guilt will also result in more wrongful convictions, raising the Type 1 Error rate.<a href="inference-cat.html#fnref57" class="footnote-back">↩︎</a></p></li>
<li id="fn58"><p>B<span class="math inline">\(\ddot{\text{o}}\)</span>ttiger et al. “Efficacy and safety of thrombolytic therapy after initially unsuccessful cardiopulmonary resuscitation: a prospective clinical trial.” The Lancet, 2001.<a href="inference-cat.html#fnref58" class="footnote-back">↩︎</a></p></li>
<li id="fn59"><p>Observed control survival rate: <span class="math inline">\(p_c = \frac{11}{50} = 0.22\)</span>.
Treatment survival rate: <span class="math inline">\(p_t = \frac{14}{40} = 0.35\)</span>.
Observed difference: <span class="math inline">\(\hat{p}_t - \hat{p}_c = 0.35 - 0.22 = 0.13\)</span>.<a href="inference-cat.html#fnref59" class="footnote-back">↩︎</a></p></li>
<li id="fn60"><p>This is an experiment. Patients were randomized
to receive mammograms or a standard breast cancer exam.
We will be able to make causal conclusions based on this study.<a href="inference-cat.html#fnref60" class="footnote-back">↩︎</a></p></li>
<li id="fn61"><p><span class="math inline">\(H_0\)</span>: the breast cancer death rate for patients
screened using mammograms is the same as the breast cancer
death rate for patients in the control,
<span class="math inline">\(p_{mgm} - p_{ctrl} = 0\)</span>.<br />
<span class="math inline">\(H_A\)</span>: the breast cancer death rate for patients screened
using mammograms is different than the breast cancer death
rate for patients in the control,
<span class="math inline">\(p_{mgm} - p_{ctrl} \neq 0\)</span>.<a href="inference-cat.html#fnref61" class="footnote-back">↩︎</a></p></li>
<li id="fn62"><p>For an example of a two-proportion
hypothesis test that does not require the
success-failure condition to be met, see
Section <a href="inference-cat.html#two-prop-errors">5.3.1</a>.<a href="inference-cat.html#fnref62" class="footnote-back">↩︎</a></p></li>
<li id="fn63"><p>For readers not as old as
the authors, an iPod is basically an iPhone without
any cellular service, assuming it was one of the later
generations. Earlier generations were more basic.<a href="inference-cat.html#fnref63" class="footnote-back">↩︎</a></p></li>
<li id="fn64"><p>We would expect
<span class="math inline">\((1 - 0.2785) \times 73 = 52.67\)</span>.
It is okay that this result,
like the result from Example ,
is a fraction.<a href="inference-cat.html#fnref64" class="footnote-back">↩︎</a></p></li>
<li id="fn65"><p>The expected count for
row one / column one is found by multiplying the
row one total (234) and column one total (319),
then dividing by the table total (699):
<span class="math inline">\(\frac{234\times 319}{699} = 106.8\)</span>.
Similarly for the second column and the first row:
<span class="math inline">\(\frac{234\times 380}{699} = 127.2\)</span>.
Row 2: 105.9 and 126.1.
Row 3: 106.3 and 126.7.<a href="inference-cat.html#fnref65" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mult-reg.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inference-num.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/MTstateIntroStats/IntroStatTextbook/edit/master/06-inference-cat.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["stat216-textbook.pdf", "stat216-textbook.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

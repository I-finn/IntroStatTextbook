[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"website Montana State Introductory Statistics R.\ntextbook accompanies curriculum STAT 216: Introduction Statistics Montana State University. syllabus course information can found\ncourse webpage.Excuse dust. book currently revision fall 2022 semester.Copyright © 2021.Version date: August 15, 2022.resource largely derivative OpenIntro project textbooks: Introduction Modern Statistics 1st Edition Çetinkaya-Rundel Hardin, OpenIntro Statistics 4th Edition Diez, Çetinkaya-Rundel, Barr, Introduction Statistics Randomization Simulation 1st Edition Diez, Barr, Çetinkaya-Rundel.\nMontana State Introductory Statistics R accompanying resources available Creative Commons Attribution-NonCommercial-ShareAlike 4.0 license unless otherwise noted.\nLicense details available Creative Commons website.Source files book may found GitHub atgithub.com/MTstateIntroStats/IntroStatTextbook.cite resource please use:Carnegie, N., Hancock, S., Meyer, E., Schmidt, J., Yager, M. (2021). Montana State Introductory Statistics R. Montana State University. https://mtstateintrostats.github.io/IntroStatTextbook/. Adapted Çetinkaya-Rundel, M. Hardin, J. (2021). Introduction Modern Statistics. OpenIntro. https://openintro-ims.netlify.app/.","code":""},{"path":"authors.html","id":"authors","chapter":"Authors","heading":"Authors","text":"","code":""},{"path":"authors.html","id":"montana-state-university-authors","chapter":"Authors","heading":"Montana State University Authors","text":"Nicole Carnegie \nFormer Associate Professor Statistics Stacey Hancock \nAssociate Professor Statistics stacey.hancock@montana.edu Elijah Meyer \nFormer PhD Statistics Graduate Student Jade Schmidt \nStudent Success Coordinator Statistics jade.schmidt2@montana.edu Melinda Yager \nAssistant Coordinator Statistics melinda.yager@montana.edu ","code":""},{"path":"authors.html","id":"openintro-authors","chapter":"Authors","heading":"OpenIntro Authors","text":"Mine Çetinkaya-Rundel mine@openintro.org \nDuke University, RStudio Johanna Hardin jo@openintro.org \nPomona College ","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"hope readers take away three ideas book addition forming foundation statistical thinking methods.Statistics applied field wide range practical applications.don’t math guru learn interesting, real data.Data messy, statistical tools imperfect. However, understand strengths weaknesses tools, can use learn interesting things world.","code":""},{"path":"preface.html","id":"textbook-overview","chapter":"Preface","heading":"Textbook overview","text":"Part 1: Introduction data. Data structures, variables, basic data collection techniques.Part 2: Exploratory data analysis. Data visualization summarization one variable, relationship two variables, exploring relationships among many variables.Part 3: Foundations inference. introduction ideas statistical inference randomization tests, bootstrap intervals, mathematical models.Part 4: Inference categorical data. Inference one two proportions using simulation randomization techniques well normal distribution.Part 5: Inference quantitative data. Inference one two means using simulation randomization techniques well \\(t\\)-distribution.Part 6: Inference regression. Inference regression slope correlation using simulation randomization techniques well \\(t\\)-distribution.Part 7: Probability. taste probability theory hypothetical two-way tables tree diagrams.part contains multiple chapters ends chapter demonstrating apply methods using RStudio software.chapter ends review section contains chapter summary well list key terms key ideas introduced chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully present alphabetical order, instead order appearance, little challenging locate.\nHowever, able easily spot bolded text.","code":""},{"path":"preface.html","id":"examples-and-exercises","chapter":"Preface","heading":"Examples and exercises","text":"Examples provided establish understanding apply methods.example.\nquestion asked , can answer found?answer can found , solution section example!think reader ready try determining solution , frame Guided Practice.reader may check learn answer Guided Practice problem reviewing full solution footnote.1","code":""},{"path":"preface.html","id":"data-sets-and-their-sources","chapter":"Preface","heading":"Data sets and their sources","text":"large majority datasets used book can found various R packages.\ntime new dataset introduced narrative, reference package like one provided.\nMany datasets openintro R package contains datasets used OpenIntro’s open-source textbooks.2The textbooks data can found openintro R package.datasets used throughout book come real sources like opinion polls scientific articles, except handful cases use toy data highlight particular feature explain particular concept.\nReferences sources real data provided end book.","code":""},{"path":"preface.html","id":"stat-216-coursepack","chapter":"Preface","heading":"STAT 216 Coursepack","text":"week, work -class activities team mates guidance instructor. activities, well reading guides guide taking notes required readings videos, included STAT 216 Coursepack. course requires purchase printed copy STAT 216 Coursepack bring class day.coursepack available purchase MSU Bookstore. may purchase coursepack person, may purchase online coursepack shipped . coursepack available MSU Bookstore first day classes. Chapter 1 coursepack provided coursepack first day class.STAT 216 Coursepack: Chapter 1","code":""},{"path":"preface.html","id":"acknowledgements","chapter":"Preface","heading":"Acknowledgements","text":"project possible without talented authors\nOpenIntro open resource textbooks volunteer OpenIntro.\nauthors \nalso like thank Montana State University Library,\ngenerously funded project.","code":""},{"path":"rstudio.html","id":"rstudio","chapter":"Preliminaries: Getting started in RStudio","heading":"Preliminaries: Getting started in RStudio","text":"STAT 216 textbook use R RStudio statistical computing.\nR RStudio free open source. R programming language runs computations, RStudio interface engage R (called “integrated development environment”, IDE). often use “R” “RStudio” interchangeably throughout textbook.preliminaries chapter introduce RStudio environment .\nbegin learn code RStudio Chapter @(#data-applications).","code":""},{"path":"rstudio.html","id":"accessing-rstudio","chapter":"Preliminaries: Getting started in RStudio","heading":"Accessing RStudio","text":"MSU hosts web based version RStudio, can found : rstudio.math.montana.edu.\nnavigate MSU RStudio server, see following sign-screen:username 7-character NetID (form x##x###, x letter # number), password password associated NetID.logging , see RStudio working environment, displayed Figure 0.1 .\nFigure 0.1: RStudio working environment.\nregistered STAT 216 students access server starting first day classes. enrolled course, receive error “Incorrect invalid username/password” attempting log , take following steps:Ensure using 7-character NetID username. form x##x###, x letter # number. email address work log RStudio server.Ensure using correct password associated NetID account. can logging another site requires NetID (e.g., MyInfo) credentials.Reset NetID password password.montana.edu. NetID password expires 180 days day set . password expired, able log RStudio server.tried steps , continue issues logging , please email STAT 216 Faculty Course Supervisor Dr. Stacey Hancock.\nmay also refer following section options accessing RStudio.Note work save server deleted access removed semester ends. Thus, like save files, export computer prior end semester.","code":""},{"path":"rstudio.html","id":"alternative-options-for-accessing-rstudio","chapter":"Preliminaries: Getting started in RStudio","heading":"Alternative options for accessing RStudio","text":"recommend using RStudio MSU RStudio server, options accessing free software:Use RStudio MSU virtual machine. highly recommend installing VMware Horizon Client using virtual machine regularly, using web browser runs risk losing work browser disconnects system (can happen number reasons).\nlog-7-character NetID password.\nSelect “MSU” domain (“GFCMSU” “MSUNORTHERN”).\nUpon logging , select “CLS-STAT-REMOTE” virtual machine. see RStudio icon virtual desktop.\nUse RStudio MSU virtual machine. highly recommend installing VMware Horizon Client using virtual machine regularly, using web browser runs risk losing work browser disconnects system (can happen number reasons).log-7-character NetID password.Select “MSU” domain (“GFCMSU” “MSUNORTHERN”).Upon logging , select “CLS-STAT-REMOTE” virtual machine. see RStudio icon virtual desktop.Use RStudio MSU -campus computer lab.Use RStudio MSU -campus computer lab.Use RStudio RStudio Cloud. resource allows use RStudio web browser. free use, limit certain number project hours per month.Use RStudio RStudio Cloud. resource allows use RStudio web browser. free use, limit certain number project hours per month.Download R RStudio laptop. (Note: R RStudio run iPad, notebooks, Chromebooks.)\nDownload install R.\nDownload install RStudio Desktop.\nInstall catstats package.\nView tutorial video installing R RStudio \nlike additional installation instructions.\nDownload R RStudio laptop. (Note: R RStudio run iPad, notebooks, Chromebooks.)Download install R.Download install RStudio Desktop.Install catstats package.\nView tutorial video installing R RStudio \nlike additional installation instructions.","code":""},{"path":"rstudio.html","id":"packages","chapter":"Preliminaries: Getting started in RStudio","heading":"Packages","text":"Since R open source, users can contribute “packages” (“libraries”) — collections R functions. 16,000 available packages! particular, use\ntidyverse collection packages designed data science.\nSTAT 216 also R package called catstats, contains functions\nrunning simulation-based inference course.","code":""},{"path":"rstudio.html","id":"using-packages","chapter":"Preliminaries: Getting started in RStudio","heading":"Using packages","text":"packages already installed R RStudio, others (tidyverse)\nfirst need installed can used. can install package\nclicking “Packages” tab RStudio clicking “Install” icon.\ncan also install package using install.packages() command.\nusing MSU RStudio server,\nnecessary packages already installed.package installed, need “load” package RStudio session\nusing library() command. example, want load tidyverse package, use following code:Packages need installed , need loaded time start new RStudio session. Think installing package installing light bulb, loading package flipping switch (see Figure 0.23).\nFigure 0.2: Installing (install.packages()) versus loading (library()) R packages.\n","code":"\nlibrary(tidyverse)"},{"path":"rstudio.html","id":"the-catstats-package","chapter":"Preliminaries: Getting started in RStudio","heading":"The catstats package","text":"STAT 216 uses R functions R package called catstats.\npackage already installed RStudio environment MSU RStudio server. However, need “load”\npackage (library) time start new session using following command:running RStudio computer laptop, need first install R packages used course. packages can installed directly within RStudio, catstats needs installed Github (since yet CRAN).use R functions catstats package, need first install remotes package. Functions package used install catstats Github. RStudio console, run following commands:, installation, gives option update \nrecent versions packages, type 1 (choose install ),\ntype Yes asks want install.Note catstats package install packages needed run code textbook,\nneed load packages (e.g., tidyverse) load catstats R session.","code":"\nlibrary(catstats)\ninstall.packages(\"remotes\")\nremotes::install_github(\"greenwood-stat/catstats\")"},{"path":"rstudio.html","id":"projects","chapter":"Preliminaries: Getting started in RStudio","heading":"Projects","text":"RStudio workflow operates best use “Projects”. Think “Project” R session folder. create separate project activity assignment course requires use R:top right corner, see dropdown menu next “Project” currently says “(None)”. Click menu choose “New Project”.Click “File” menu top left select “New Project”.“New Project Wizard” window pop . Click “New Directory”.\n         click “New Project”.Give project directory name (e.g., Assignment1). use spaces characters name.\nClick “Browse” choose location like save project. click “Home” button, leave location “~”, shown . Alternatively, can create new folder store project. Note location server account, computer.\nLeave boxes unchecked, click “Create Project”.\nClick “Browse” choose location like save project. click “Home” button, leave location “~”, shown . Alternatively, can create new folder store project. Note location server account, computer.Leave boxes unchecked, click “Create Project”.notice project name appear folder “Files” window bottom right. click folder, see project file (.Rproj extension). Save script files, data sets, files related project folder.","code":""},{"path":"rstudio.html","id":"r-script-files","chapter":"Preliminaries: Getting started in RStudio","heading":"R script files","text":"can type directly Console > symbol run R code:However, like save code future use, write R commands R script file. script file just text file extension .R.RStudio environment, click “New File” option “File” menu, select “R Script”.window top left RStudio environment appear. script file!","code":"> 3+5\n[1] 8"},{"path":"rstudio.html","id":"try-it","chapter":"Preliminaries: Getting started in RStudio","heading":"Try it!","text":"Open new R script file.Type following commands file:Highlight two lines just typed. Click “Run” button, looks like blank page green right arrow.code output code appear Console window.save script file, click Save icon, go File -> Save. Browse location ’d like save file (folder current Project file), name file, click Save. name R script file changed name chose (e.g., MyFirstScript.R), file appear list files bottom right.Documenting code always good practice. future self thank ! can include comments R code preceding comment #. comment can line, code .\nR knows ignore anything typed #.","code":"\n3+5\nsqrt(10)#> [1] 8\n#> [1] 3.16#> [1] 8\n#> [1] 3.16"},{"path":"rstudio.html","id":"loading-data","chapter":"Preliminaries: Getting started in RStudio","heading":"Loading data","text":"RStudio can load data variety sources, including .txt, .csv, .xlsx files, can even load data website. activities assignments course, loading data set Stat 216 website. code loading data sets included provided R script file. example, following code load “Current Population Survey” data set Activity 3, save object called “CPS”.running line code, see object CPS appear “Environment” list, information data set contains 534 observations 11 variables measured observations.Clicking name CPS typing command View(CPS) opens new window displays data set.course project, data set file need import RStudio. read data set file RStudio using MSU server, first need upload data set project. data set server account files, can use “Import Dataset” button import data set location.","code":"\nCPS <- read.csv(\"https://math.montana.edu/courses/s216/data/cps.csv\")"},{"path":"rstudio.html","id":"try-it-1","chapter":"Preliminaries: Getting started in RStudio","heading":"Try it!","text":"First, download data sets shown Stat 216 webpage, save computer. website note extension file (e.g., .txt, .csv, .xlsx).RStudio, click “Upload” button “Files” tab bottom right.Click “Browse” button navigate location server like save data set.Click “Choose File”, navigate saved data set computer. Click data set file name, click “Choose Upload”, click “OK”.“Environment” tab, click “Import Dataset”.drop-menu appear, can choose type file data stored. Common formats include text files (e.g., .csv, .txt) — select “Text (readr)” — Excel spreadsheets (.xlsx) — select “Excel”.Click “Browse”, navigate location server data uploaded. RStudio show data preview — observation single row, variable single column. click “Import”.see data object appear “Environment” (name whatever filename ), RStudio open window view data set.","code":""},{"path":"rstudio.html","id":"exporting-files","chapter":"Preliminaries: Getting started in RStudio","heading":"Exporting files","text":"Since working RStudio server, local computer, like use files generated RStudio, first need export .","code":""},{"path":"rstudio.html","id":"exporting-r-script-files","chapter":"Preliminaries: Getting started in RStudio","heading":"Exporting R script files","text":"can export R script files saved server files (type file) checking box next file, clicking “”, “Export”. ask specify name file. click “Download”.","code":""},{"path":"rstudio.html","id":"try-it-2","chapter":"Preliminaries: Getting started in RStudio","heading":"Try it!","text":"Try exporting R script file created .","code":""},{"path":"rstudio.html","id":"exporting-plots","chapter":"Preliminaries: Getting started in RStudio","heading":"Exporting plots","text":"export plot, first need create plot.","code":""},{"path":"rstudio.html","id":"try-it-3","chapter":"Preliminaries: Getting started in RStudio","heading":"Try it!","text":"Copy paste following code script file; highlight code click Run. (see code Activity 3!)bar plot appear “Plots” tab.Click “Export” button, can choose export plot either image file (e.g., .png, .jpeg), pdf file, copy clipboard (e.g., pasting Word document).saving plot computer (rather copying plot), window pop various options. Choose directory computer like save file, give plot name, change dimensions desired, click Save.","code":"\nlibrary(tidyverse) \n\nmyopia <- read.csv(\"https://math.montana.edu/courses/s216/data/ChildrenLightSight.csv\") \n\nmyopia %>% \n  ggplot(aes(y = Light)) +\n  geom_bar(stat = \"count\") +\n  labs(title = \"Frequency Bar Plot of Level of Myopia\",\n       x = \"Frequency\",\n       y = \"Level of Myopia\")  +\n  coord_flip()"},{"path":"rstudio.html","id":"home","chapter":"Preliminaries: Getting started in RStudio","heading":"Home","text":"RStudio environment, next NetID top right corner “home” icon. Click icon, take dashboard., can see many sessions running “Sessions” title, see list projects “Projects” title. can click sessions projects return session/project.","code":""},{"path":"rstudio.html","id":"troubleshooting","chapter":"Preliminaries: Getting started in RStudio","heading":"Troubleshooting","text":"One frustrating things learning using R error message pops . Sometimes error message descriptive, sometimes cryptic. times, error message include line code mistake made.tips happens:missing parentheses? Check opening parentheses associated ending parenthesis.forget comma?something quotes shouldn’t ? something quotes ?type variable object name correctly? R case sensitive, case letters needs match correctly.trying run entire script file, try running line--line see error happens.trying run R function, pull help file function make sure arguments specified correctly. E.g., Type ?lm see help file lm function.Copy--paste error message, input message quotes Google. Searching phrase quotes make sure specific error shows top results.Visit instructor’s office hours Math Learning Center, share screen show error, can help troubleshoot .common error messages reasons include:“find function”. error occurs R package loaded properly due misspelling function data set name. Remember, every R session, need “load” required packages using library command, e.g., code library(catstats) load catstats package.“object found” “error eval”. error occurs particular object question exist empty.“non-numeric argument binary operator”. error may occur ’re trying run function requires numerical vector (e.g., mean), input character vector.Remember, even experienced R users still get errors! ’s part learning process.","code":""},{"path":"rstudio.html","id":"extra-references","chapter":"Preliminaries: Getting started in RStudio","heading":"Extra references","text":"many websites designed provide help use R RStudio, sometimes hard find help right level. additional recommended websites getting started R RStudio STAT 216:RStudio IDE Cheatsheet: annotated picture RStudio environment, coding keyboard shortcuts, probably want know features RStudio, refer two-page “cheatsheet”. RStudio produces many cheatsheets. Data visualization ggplot2 cheatsheet also helpful course.RStudio IDE Cheatsheet: annotated picture RStudio environment, coding keyboard shortcuts, probably want know features RStudio, refer two-page “cheatsheet”. RStudio produces many cheatsheets. Data visualization ggplot2 cheatsheet also helpful course.Using RStudio: RStudio help pages introductory statistics course Gustavus Adolphus College. Like MSU, students use RStudio server; however, use R Markdown documents, slightly involved R script files use STAT 216. pages tips get started, brief instructions creating tables, statistics, plots, theory-based tests.Using RStudio: RStudio help pages introductory statistics course Gustavus Adolphus College. Like MSU, students use RStudio server; however, use R Markdown documents, slightly involved R script files use STAT 216. pages tips get started, brief instructions creating tables, statistics, plots, theory-based tests.R Data Science: book Hadley Wickham (creator tidyverse many R packages resources), much content need, sections particularly helpful:\nRStudio tidyverse?\nrun R code?\nR scripts?\nData visualization ggplot\nR Data Science: book Hadley Wickham (creator tidyverse many R packages resources), much content need, sections particularly helpful:RStudio tidyverse?run R code?R scripts?Data visualization ggplotModernDive: Statistical Inference via Data Science — Chapter 1: Getting Started Data R: Though textbook slightly higher level STAT 216, first chapter gives great explanation exactly R RStudio , explaining differences point--click interfaces “interpreted language” like R. Chapter 2 book gives detailed overview data visualization methods using ggplot2 R package.ModernDive: Statistical Inference via Data Science — Chapter 1: Getting Started Data R: Though textbook slightly higher level STAT 216, first chapter gives great explanation exactly R RStudio , explaining differences point--click interfaces “interpreted language” like R. Chapter 2 book gives detailed overview data visualization methods using ggplot2 R package.R Data: free “course” includes videos interactive elements. Topics include: variables data structures, visualizing data using ggplot2 R package, statistical tests, data wrangling. videos visualizing data particularly helpful first activities course.R Data: free “course” includes videos interactive elements. Topics include: variables data structures, visualizing data using ggplot2 R package, statistical tests, data wrangling. videos visualizing data particularly helpful first activities course.Chester Ismay great argument use R. ’re wondering STAT 216 uses R RStudio instead statistical software, read short chapter.Chester Ismay great argument use R. ’re wondering STAT 216 uses R RStudio instead statistical software, read short chapter.","code":""},{"path":"data-hello.html","id":"data-hello","chapter":"1 Hello data","heading":"1 Hello data","text":"Scientists seek answer questions using rigorous methods careful observations.\nobservations—collected likes field notes, surveys, experiments—form backbone statistical investigation called data.\nStatistics study best collect, analyze, draw conclusions data, first chapter, focus properties data collection data.Though calculating probabilities 16th century, first US Census directed Thomas Jefferson 17904, discipline statistics know came 1800s. 21st century, statistical investigation process looked something like (adapted Tintle et al. (2016)):Ask research question.Design study collect data.Summarize visualize data.Use statistical analysis methods draw inferences data.Communicate results answer research question.Revisit look forward.rise data science, however, might start research question,\ninstead start data set5.\ncase, statistical investigation process looks like data exploration cycle found Figure 1.1 taken Wickham Grolemund (2017).\nFigure 1.1: Wickham Grolemund’s data exploration cycle (2017).\neither case, ideas, concepts, methods presented book provide tools work statistical investigation process, whether starting research question starting data.","code":""},{"path":"data-hello.html","id":"basic-stents-strokes","chapter":"1 Hello data","heading":"1.1 Case study: using stents to prevent strokes","text":"section, introduce classic challenge statistics: evaluating efficacy medical treatment.\nTerms section, indeed much chapter, revisited later text.\nplan now simply get sense role statistics can play practice., consider experiment studies effectiveness stents treating patients risk stroke (Chimowitz et al. 2011).\nStents small mesh tubes placed inside narrow weak arteries assist patient recovery cardiac events reduce risk additional heart attack death.Many doctors hoped similar benefits patients risk stroke. start writing principal question researchers hope answer:use stents reduce risk stroke?researchers asked question conducted experiment 451 -risk patients. volunteer patient randomly assigned one two groups:Treatment group. Patients treatment group received stent medical management.\nmedical management included medications, management risk factors, help lifestyle modification.Control group. Patients control group received medical management treatment group, receive stents.Researchers randomly assigned 224 patients treatment group 227 control group.\nstudy, control group provides reference point can measure medical impact stents treatment group.Researchers studied effect stents two time points: 30 days enrollment 365 days enrollment.\ndata collected 5 patients summarized Table 1.1.\nPatient outcomes recorded stroke event, representing whether patient stroke time period.data study can found openintro package: stent30 stent365.\nTable 1.1: Results five patients stent study.\nConsidering data 451 patients individually long, cumbersome path towards answering original research question.\nInstead, performing statistical data analysis allows us consider data .\nTable 1.2 summarizes raw data helpful way.\ntable, can quickly see happened entire study.\ninstance, identify number patients treatment group stroke within 30 days treatment, look leftmost column (30 days), intersection treatment stroke: 33.\nidentify number control patients stroke 365 days receiving treatment, look rightmost column (365 days), intersection control event: 199.\nTable 1.2: Descriptive statistics stent study.\ndata summarized table can also visualized barplot, seen Figure 1.2:\nFigure 1.2: Segmented barplot outcomes stent study group time.\n224 patients treatment group, 45 stroke end first year.\nUsing two numbers, compute proportion patients treatment group stroke end first year.\n(Note: answers Guided Practice exercises provided footnotes!)6We can compute summary statistics table give us better idea impact stent treatment differed two groups.\nsummary statistic single number summarizing large amount data.\ninstance, primary results study 1 year described two summary statistics: proportion people stroke treatment control groups.Proportion stroke treatment (stent) group: \\(45/224 = 0.20 = 20\\)%.Proportion stroke control group: \\(28/227 = 0.12 = 12\\)%.two summary statistics useful looking differences groups, surprise: additional 8% patients treatment group stroke!\nimportant two reasons.\nFirst, contrary doctors expected, stents reduce rate strokes.\nSecond, leads statistical question: data show “real” difference groups?second question subtle, basis call statistical inference.\nSuppose flip coin 100 times. chance coin lands heads given coin flip 50%, probably won’t observe exactly 50 heads.\ntype fluctuation part almost type data generating process.\npossible 8% difference stent study due natural variation.\nHowever, larger difference observe (particular sample size), less believable difference due chance.\nreally asking following: difference large reject notion due chance?don’t yet statistical tools fully address question , can comprehend conclusions published analysis: compelling evidence harm stents study stroke patients.careful.\ngeneralize results study patients stents.\nstudy looked patients specific characteristics volunteered part study may representative stroke patients.\naddition, many types stents study considered self-expanding Wingspan stent (Boston Scientific).\nHowever, study leave us important lesson: keep eyes open surprises.","code":""},{"path":"data-hello.html","id":"data-basics","chapter":"1 Hello data","heading":"1.2 Data basics","text":"Effective presentation description data first step analyses. section introduces one structure organizing data well terminology used throughout book.","code":""},{"path":"data-hello.html","id":"observations-variables-and-data-frames","chapter":"1 Hello data","heading":"1.2.1 Observations, variables, and data frames","text":", consider loans offered Lending Club, peer--peer lending company. data used explore characteristics people receiving loans platform, job titles, annual income, home ownership. Table 1.3 displays six rows data set 50 randomly sampled loans. observations referred loan50 data set.data can found openintro package: loan50.row table represents single loan.\nformal name row case observational unit. Since 50 observational units data set, sample size, denoted \\(n\\), 50 (\\(n = 50\\)).\ncolumns represent characteristics loan, column referred variable. example, first row represents loan $7,500 interest rate 7.34%, borrower based Maryland (MD) income $70,000.variable something can measured individual observational unit.\ncareful confuse summary statistics—calculated group observational units—variables.grade first loan Table 1.3?\nhome ownership status borrower first loan?\nReminder: Guided Practice questions, can check answer footnote.7In practice, especially important ask clarifying questions ensure important aspects data understood.\ninstance, always important sure know variable means units measurement.\nDescriptions variables loan50 data set given Table 1.4.\nTable 1.3: Six rows loan50 data set.\n\nTable 1.4: Variables descriptions loan50 data set.\ndata Table 1.3 represent data frame (data matrix), convenient common way organize data, especially collecting data spreadsheet.\nrow data frame corresponds unique case (observational unit), column corresponds variable.recording data, use data frame unless good reason use different structure.\nstructure allows new cases added rows new variables new columns.grades assignments, quizzes, exams course often recorded gradebook takes form data frame.\nmight organize course’s grade data using data frame?8We consider data 3,142 counties United States,\ninclude name county, state resides, population 2017, population changed 2010 2017, poverty rate, nine additional characteristics.\nmight data organized data frame?9The data described Guided Practice represent county data set, shown data frame Table 1.5.\nvariables well variables data set fit Table 1.5 described Table 1.6\nTable 1.5: Six observations six variables county data set.\n\nTable 1.6: Variables descriptions county data set.\ndata can found usdata package: county.","code":""},{"path":"data-hello.html","id":"variable-types","chapter":"1 Hello data","heading":"1.2.2 Types of variables","text":"Examine unemployment_rate, pop2017, state, metro, median_edu variables county data set.\nvariables inherently different others, yet share certain characteristics.First consider unemployment_rate, said quantitative numerical variable since can take wide range numerical values, sensible add, subtract, take averages values.\nhand, classify variable reporting telephone area codes quantitative since average, sum, difference area codes doesn’t clear meaning.pop2017 variable also quantitative, although seems little different unemployment_rate.\nvariable population count can take whole non-negative numbers (0, 1, 2, …).\nreason, population variable said discrete since can take numerical values jumps.\nhand, unemployment rate variable said continuous.variable state can take 51 values accounting Washington, DC: AL, AK, …, WY.\nresponses categories, state called categorical variable, possible values called variable’s levels . variable metro also categorical, two levels (yes ). categorical variable two levels called binary variable. working generic binary variable, often call two possible levels “success” “failure.”Finally, consider median_edu variable, describes median education level county residents takes values below_hs, hs_diploma, some_college, bachelors county.\nvariable seems hybrid: categorical variable levels natural ordering.\nvariable properties called ordinal variable, regular categorical variable without type special ordering called nominal variable.\nsimplify analyses, ordinal variable book treated nominal (unordered) categorical variable.\nFigure 1.3: Breakdown variables respective types.\nData collected students statistics course.\nThree variables recorded student: number siblings, student height, whether student previously taken statistics course.\nClassify variables continuous quantitative, discrete quantitative, categorical.number siblings student height represent quantitative variables.\nnumber siblings count, discrete.\nHeight varies continuously, continuous quantitative variable.\nlast variable classifies students two categories—taken statistics course—makes variable categorical.experiment evaluating effectiveness new drug treating migraines.\ngroup variable used indicate experiment group patient: treatment control.\nnum_migraines variable represents number migraines patient experienced 3-month period. Classify variable either quantitative categorical.10","code":""},{"path":"data-hello.html","id":"variable-relations","chapter":"1 Hello data","heading":"1.2.3 Relationships between variables","text":"Many analyses motivated researcher looking relationship two variables.\nsocial scientist may like answer following questions:higher average increase county population tend correspond counties higher lower median household incomes?homeownership lower national average one county, percent multi-unit structures county tend national average?useful predictor median education level median household income US counties?answer questions, data must collected, county data set shown Table 1.5.\nExamining summary statistics provide insights three questions counties.\nAdditionally, graphs can used visually explore data.Scatterplots one type graph used study relationship two quantitative variables.\nFigure 1.4 displays relationship variables homeownership multi_unit, percent units multi-unit structures (e.g., apartments, condos).\npoint plot represents single county (single observational unit).\ninstance, highlighted dot corresponds County 413 county data set: Chattahoochee County, Georgia, 39.4% units multi-unit structures homeownership rate 31.3%.\nscatterplot suggests relationship two variables: counties higher rate multi-units tend lower homeownership rates.\nmight brainstorm relationship exists investigate idea determine reasonable explanations.\nFigure 1.4: scatterplot homeownership versus percent units multi-unit structures US counties. highlighted dot represents Chattahoochee County, Georgia, multi-unit rate 39.4% homeownership rate 31.3%.\nmulti-unit homeownership rates said associated plot shows discernible pattern.\ntwo variables show connection one another, called associated variables.\nAssociated variables can also called dependent variables vice-versa.Examine variables loan50 data set, described Table 1.4.\nCreate two questions possible relationships variables loan50 interest .11This example examines relationship change population 2010 2017 median household income counties, visualized scatterplot Figure 1.5.\nvariables associated?larger median household income county, higher population growth observed county.\ntrend isn’t true every county, trend plot evident. Since relationship variables, associated.\nFigure 1.5: scatterplot showing pop_change median_hh_income. Owsley County Kentucky, highlighted, lost 3.63% population 2010 2017 median household income $22,736.\ndownward trend Figure 1.4—counties units multi-unit structures associated lower homeownership—variables said negatively associated.\npositive association shown relationship median_hh_income pop_change variables Figure 1.5, counties higher median household income tend higher rates population growth.two variables associated, said independent.\n, two variables independent evident relationship two.Associated independent, .\npair variables either related way (associated) (independent).\npair variables associated independent.","code":""},{"path":"data-hello.html","id":"explanatory-and-response-variables","chapter":"1 Hello data","heading":"1.2.4 Explanatory and response variables","text":"ask questions relationship two variables, sometimes also want determine change one variable causes change .\nConsider following rephrasing earlier question county data set:increase median household income county, drive increase population?question, asking whether one variable affects another.\nunderlying belief, median household income explanatory variable variable population change response variable variable hypothesized relationship.12Explanatory response variables.suspect one variable might causally affect another,\nlabel first variable explanatory variable\nsecond response variable.\nmain reason observational studies control confounding variables.\nrevisit idea discuss experiments next chapter.\nmany pairs variables, hypothesized relationship, labels applied either variable cases.Bear mind act labeling variables way nothing guarantee causal relationship exists.\nformal evaluation check whether one variable causes change another requires experiment.","code":""},{"path":"data-hello.html","id":"introducing-observational-studies-and-experiments","chapter":"1 Hello data","heading":"1.2.5 Introducing observational studies and experiments","text":"two primary types data collection: observational studies experiments. already encountered experiment case study Section 1.1, observational study Lending Club data section.Researchers perform observational study collect data way directly interfere data arise.\ninstance, researchers may collect information via surveys, review medical company records, follow cohort many similar individuals form hypotheses certain diseases might develop.\nsituations, researchers merely observe data arise.\ngeneral, observational studies can provide evidence naturally occurring association variables, show causal connection.researchers want investigate possibility causal connection, conduct experiment.\nUsually explanatory response variable.\ninstance, may suspect administering drug reduce mortality heart attack patients following year.\ncheck really causal connection explanatory variable response, researchers collect sample individuals split groups.\nindividuals group assigned treatment.\nindividuals randomly assigned group, experiment called randomized experiment.\nexample, heart attack patient drug trial randomly assigned, perhaps flipping coin, one two groups: first group receives placebo (fake treatment) second group receives drug.\nNote case study Section 1.1 use placebo.Association \\(\\neq\\) Causation.\ngeneral, association imply causation, causation can inferred randomized experiment.","code":""},{"path":"data-hello.html","id":"chp1-review","chapter":"1 Hello data","heading":"1.3 Chapter review","text":"","code":""},{"path":"data-hello.html","id":"summary","chapter":"1 Hello data","heading":"Summary","text":"chapter introduced world data.\nData can organized many ways tidy data, row represents observation column represents variable, lends easily statistical analysis.\nMany ideas chapter seen move full data analyses.\nnext chapter ’re going learn can design studies collect data need make conclusions desired scope inference.","code":""},{"path":"data-hello.html","id":"terms","chapter":"1 Hello data","heading":"Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"data-hello.html","id":"key-ideas","chapter":"1 Hello data","heading":"Key ideas","text":"data set comprised measurements variables observational units.\ntype variable can quantitative categorical, variable’s role can explanatory variable response variable.data set comprised measurements variables observational units.\ntype variable can quantitative categorical, variable’s role can explanatory variable response variable.observational study, merely observe behavior individuals study; manipulate variables individuals way. randomized experiment, randomly assign values explanatory variable observational units, observe response variable. Random assignment allows us investigate causal relationships balances potential confounding variables, average. ideas discussed detail next chapter.observational study, merely observe behavior individuals study; manipulate variables individuals way. randomized experiment, randomly assign values explanatory variable observational units, observe response variable. Random assignment allows us investigate causal relationships balances potential confounding variables, average. ideas discussed detail next chapter.","code":""},{"path":"data-design.html","id":"data-design","chapter":"2 Study design","heading":"2 Study design","text":"digging details working data, stop think data come .\n, data used make broad complete conclusions, important understand data represent.\nOne important aspect data provenance sampling.\nKnowing observational units selected larger entity allow generalizations back population data randomly selected.\nAdditionally, understanding structure study, causal relationships can separated relationships associated.\ngood question ask oneself working data , “observations collected?”.\nlearn lot data understanding source.","code":""},{"path":"data-design.html","id":"sampling-principles-strategies","chapter":"2 Study design","heading":"2.1 Sampling principles and strategies","text":"\nfirst step conducting research identify topics questions investigated.\nclearly laid research question helpful identifying subjects cases studied variables important.\nalso important consider data collected reliable help achieve research goals.","code":""},{"path":"data-design.html","id":"populations-and-samples","chapter":"2 Study design","heading":"2.1.1 Populations and samples","text":"Consider following three research questions:average mercury content swordfish Atlantic Ocean?last 5 years, average time complete degree Duke undergrads?new drug reduce risk deaths patients severe heart disease?research question refers target population.\nfirst question, target population swordfish Atlantic ocean, fish represents case.\nOften times, expensive collect data every case population.\nInstead, sample taken.\nsample represents subset cases often small fraction population.\ninstance, 60 swordfish (number) population might selected, sample data may used provide estimate population average answer research question.second third questions , identify target population represents individual case.13","code":""},{"path":"data-design.html","id":"parameters-and-statistics","chapter":"2 Study design","heading":"2.1.2 Parameters and statistics","text":"statistical analysis procedures, research question hand boils understanding numerical summary.\nnumber (set numbers) may quantity already familiar (like average) may something learn text (like slope intercept least squares model, provided Section 6.2).numerical summary can calculated either sample observations entire population.\nHowever, measuring every unit population usually prohibitive (parameter rarely calculated).\n, “typical” numerical summary calculated sample.\nYet, can still conceptualize calculating average income adults Argentina.use specific terms order differentiate number calculated sample data (statistic) calculated considered calculation entire population (parameter).\nterms statistic parameter useful communicating claims models used extensively later chapters delve making inference populations.","code":""},{"path":"data-design.html","id":"anecdotal-evidence","chapter":"2 Study design","heading":"2.1.3 Anecdotal evidence","text":"Consider following possible responses three research questions:man news got mercury poisoning eating swordfish, average mercury concentration swordfish must dangerously high.met two students took 7 years graduate Duke, must take longer graduate Duke many colleges.friend’s dad heart attack died gave new heart disease drug, drug must work.conclusion based data.\nHowever, two problems.\nFirst, data represent one two cases.\nSecond, importantly, unclear whether cases actually representative population. Data collected haphazard fashion called anecdotal evidence.Anecdotal evidence.\ncareful data collected haphazard fashion.\nevidence may true verifiable, may represent extraordinary cases.\nFigure 2.1: February 2010, media pundits cited one large snow storm evidence global warming. comedian Jon Stewart pointed , “one storm, one region, one country.”\nAnecdotal evidence typically composed unusual cases recall based striking characteristics.\ninstance, likely remember two people met took 7 years graduate six others graduated four years.\nInstead looking unusual cases, examine sample many cases better represent population.","code":""},{"path":"data-design.html","id":"sampling-from-a-population","chapter":"2 Study design","heading":"2.1.4 Sampling from a population","text":"\nmight try estimate time graduation Duke undergraduates last 5 years collecting sample students.\ngraduates last 5 years represent population, graduates selected review collectively called sample.\ngeneral, always seek randomly select sample population.\nbasic type random selection equivalent raffles conducted–raffle ticket equal chance selected.\nexample, selecting graduates, write graduate’s name raffle ticket draw 100 tickets.\nselected names represent random sample 100 graduates.\npick samples randomly reduce chance introduce biases.\nFigure 2.2: graphic, five graduates randomly selected population (graduates last 5 years) included sample.\nSuppose ask student happens majoring nutrition select several graduates study.\nkind students think might collect?\nthink sample representative graduates?Perhaps pick disproportionate number graduates health-related fields. perhaps selection good representation population.\nselecting samples hand, run risk picking biased sample, even bias unintended.\nFigure 2.3: Asked pick sample graduates, nutrition major might inadvertently pick disproportionate number graduates health-related majors.\nsomeone permitted pick choose exactly graduates included sample, entirely possible sample skewed person’s interests, may entirely unintentional.\nintroduces bias sampling method.three common types sampling bias discuss:Selection bias: method sample selected tends produce samples either -represent -represent certain portions population.Non-response bias: individuals selected sample unwilling answer questions, respond, refuse participate.Response bias: individuals selected sample respond way accurately represent truth—due question wording, lack anonymity, issues.common downfall survey studies convenience sample, individuals easily accessible likely included sample.\ninstance, political survey done stopping people walking Bronx, represent New York City.\noften difficult discern sub-population convenience sample represents.convenience sample example selection bias, non-response bias,\nresponse bias?14Sampling randomly helps resolve selection bias.\nbasic random sample called simple random sample, equivalent using raffle select cases.\nmeans case population equal chance included implied connection cases sample.Even people picked random, however, caution must exercised non-response rate high, response bias present.\ninstance, 30% people randomly sampled survey actually respond, unclear whether results representative entire population.\nnon-response bias can produce results sample accurately reflect entire population.\nFigure 2.4: Due possibility non-response, survey studies may reach certain group within population. difficult, often times impossible, completely fix problem.\nAsking uninformed. Popular late night host Jimmy Kimmel segment show called “Lie Witness News,” Kimmel’s staff take streets ask pedestrians recent stories news. However, recent stories really stories —’re fake. Without fail, asked always express opinion, unflinchingly. ? People like appear don’t know ’re talking , make answers. entertaining display fascinating psychological example response bias, watch Coachella 2013 episode Lie Witness News.can easily access ratings products, sellers, companies websites. ratings based people go way provide rating. 50% online reviews product negative, think means 50% buyers dissatisfied product? ?15\n","code":""},{"path":"data-design.html","id":"samp-methods","chapter":"2 Study design","heading":"2.1.5 Four sampling methods (special topic)","text":"Almost statistical methods based notion implied randomness.\nobservational data collected random framework population, statistical methods—estimates errors associated estimates—reliable.\nconsider four random sampling techniques: simple, stratified, cluster, multistage sampling. Figures 2.5 2.6 provide graphical representations techniques.\n\nFigure 2.5: Examples simple random stratified sampling. top panel, simple random sampling used randomly select 18 cases (denoted red). bottom panel, stratified sampling used: cases grouped strata, simple random sampling employed randomly select 3 cases within stratum.\nSimple random sampling probably intuitive form random sampling.\nConsider salaries Major League Baseball (MLB) players, player member one league’s 30 teams.\ntake simple random sample 120 baseball players salaries, write names season’s several hundreds players onto slips paper, drop slips bucket, shake bucket around sure names mixed , draw slips sample 120 players.\ngeneral, sample referred “simple random” case population equal chance included final sample knowing case included sample provide useful information cases included.Stratified sampling divide--conquer sampling strategy.\npopulation divided groups called strata.\nstrata chosen similar cases grouped together, second sampling method, usually simple random sampling, employed within stratum.\nbaseball salary example, 30 teams represent strata, since teams lot money (4 times much!).\nmight randomly sample 4 players team sample 120 players.Stratified sampling especially useful cases stratum similar respect outcome interest.\ndownside analyzing data stratified sample complex task analyzing data simple random sample.\nanalysis methods introduced book need extended analyze data collected using stratified sampling.good cases within stratum similar?might get stable estimate subpopulation stratum cases similar, leading precise estimates within group.\ncombine estimates single estimate full population, population estimate tend precise since individual group estimate precise.cluster sample, break population many groups, called clusters.\nsample fixed number clusters include observations clusters sample.\nmultistage sample like cluster sample, rather keeping observations cluster, collect random sample within selected cluster.\nFigure 2.6: Examples cluster multistage sampling. top panel, cluster sampling used: data binned nine clusters, three clusters sampled, observations within three cluster included sample. bottom panel, multistage sampling used, differs cluster sampling randomly select subset cluster included sample rather measuring every case sampled cluster.\nSometimes cluster multistage sampling can economical alternative sampling techniques.\nAlso, unlike stratified sampling, approaches helpful lot case--case variability within cluster clusters don’t look different one another.\nexample, neighborhoods represented clusters, cluster multistage sampling work best neighborhoods diverse.\ndownside methods advanced techniques typically required analyze data, though methods book can extended handle data.Suppose interested estimating malaria rate densely tropical portion rural Indonesia.\nlearn 30 villages part Indonesian jungle, less similar next, distances villages substantial. goal test 150 individuals malaria.\nsampling method employed?simple random sample likely draw individuals 30 villages, make data collection extremely expensive.\nStratified sampling challenge since unclear build strata similar individuals.\nHowever, cluster sampling multistage sampling seem like good ideas.\ndecided use multistage sampling, might randomly select half villages, randomly select 10 people .\nprobably reduce data collection costs substantially comparison simple random sample, cluster sample still give us reliable information, even need analyze data slightly advanced methods discuss book.","code":""},{"path":"data-design.html","id":"observational-studies","chapter":"2 Study design","heading":"2.2 Observational studies","text":"Data treatment explicitly applied (explicitly withheld) called observational data.\ninstance, loan data county data described Section 1.2 examples observational data.Observational studies generally sufficient show associations form hypotheses can later checked experiments. Making causal conclusions based experiments often reasonable. However, making causal conclusions based observational data can treacherous recommended. Indeed, making causal conclusions based observational data arguably common mistake news headlines social media posts!Suppose observational study tracked sunscreen use skin cancer, found sunscreen someone used, likely person skin cancer. mean sunscreen causes skin cancer?16Some previous research tells us using sunscreen actually reduces skin cancer risk, maybe another variable can explain hypothetical association sunscreen usage skin cancer.\nOne important piece information absent sun exposure. someone sun day, likely use sunscreen likely get skin cancer. Exposure sun unaccounted simple investigation.Sun exposure called confounding variable17, variable associated explanatory response variables.\none method justify making causal conclusions observational studies exhaust search confounding variables, guarantee confounding variables can examined measured.confounding variable variable bothassociated explanatory variable, andassociated response variable.conditions met, observe association explanatory variable response variable data, sure association due explanatory variable confounding variable—explanatory confounding variables “confounded.”Figure 1.4 shows negative association homeownership rate percentage multi-unit structures county.\nHowever, unreasonable conclude causal relationship two variables.\nSuggest variable might explain negative relationship.18Houndstongue (noxious weed) found abundance private public lands grazed cattle. Houndstongue rarely found lands grazed mountain goats. One investigator concluded houndstongue infestations reduced importing mountain goats infested areas. wrong conclusion?19Observational studies come two forms: prospective retrospective studies.\nprospective study identifies individuals collects information events unfold.\ninstance, medical researchers may identify follow group patients many years assess possible influences behavior cancer risk.\nOne example study Nurses’ Health Study.\nStarted 1976 expanded 1989, Nurses’ Health Study collected data 275,000 nurses still enrolling participants.\nprospective study recruits registered nurses collects data using questionnaires.\nRetrospective studies collect data events taken place, e.g. researchers may review past events medical records.\ndata sets may contain prospectively- retrospectively-collected variables, medical studies gather information participants’ lives enter study subsequently collect data participants throughout study.","code":""},{"path":"data-design.html","id":"experiments","chapter":"2 Study design","heading":"2.3 Experiments","text":"Studies researchers assign treatments cases called experiments.\nassignment includes randomization, e.g., using coin flip decide treatment patient receives, called randomized experiment.\nRandomized experiments fundamentally important trying show causal connection two variables.","code":""},{"path":"data-design.html","id":"principles-of-experimental-design","chapter":"2 Study design","heading":"2.3.1 Principles of experimental design","text":"Randomized experiments generally built four principles:Controlling. Researchers assign treatments cases, best control differences groups20.\nexample, patients take drug pill form, patients take pill sip water others may entire glass water.\ncontrol effect water consumption, doctor may instruct every patient drink 12 ounce glass water pill.Controlling. Researchers assign treatments cases, best control differences groups20.\nexample, patients take drug pill form, patients take pill sip water others may entire glass water.\ncontrol effect water consumption, doctor may instruct every patient drink 12 ounce glass water pill.Randomization. Researchers randomize patients treatment groups account variables controlled.\nexample, patients may susceptible disease others due dietary habits.\nRandomizing patients treatment control group helps even differences, also prevents accidental bias entering study.Randomization. Researchers randomize patients treatment groups account variables controlled.\nexample, patients may susceptible disease others due dietary habits.\nRandomizing patients treatment control group helps even differences, also prevents accidental bias entering study.Replication. cases researchers observe, accurately can estimate effect explanatory variable response.\nsingle study, replicate collecting sufficiently large sample.\nAlternatively, group scientists may replicate entire study verify earlier finding.Replication. cases researchers observe, accurately can estimate effect explanatory variable response.\nsingle study, replicate collecting sufficiently large sample.\nAlternatively, group scientists may replicate entire study verify earlier finding.Blocking. Researchers sometimes know suspect variables, treatment, influence response.\ncircumstances, may first group individuals based variable blocks randomize cases within block treatment groups.\nstrategy often referred blocking.\ninstance, looking effect drug heart attacks, might first split patients study low-risk high-risk blocks, randomly assign half patients block control group half treatment group, shown Figure 2.7.\nstrategy ensures treatment group equal number low-risk high-risk patients.Blocking. Researchers sometimes know suspect variables, treatment, influence response.\ncircumstances, may first group individuals based variable blocks randomize cases within block treatment groups.\nstrategy often referred blocking.\ninstance, looking effect drug heart attacks, might first split patients study low-risk high-risk blocks, randomly assign half patients block control group half treatment group, shown Figure 2.7.\nstrategy ensures treatment group equal number low-risk high-risk patients.\nFigure 2.7: Blocking using variable depicting patient risk. Patients first divided low-risk high-risk blocks, block evenly separated treatment groups using randomization. strategy ensures equal representation patients treatment group low-risk high-risk categories.\nimportant incorporate first three experimental design principles study, book describes applicable methods analyzing data experiments.\nBlocking slightly advanced technique, statistical methods book may extended analyze data collected using blocking.","code":""},{"path":"data-design.html","id":"reducing-bias-human-experiments","chapter":"2 Study design","heading":"2.3.2 Reducing bias in human experiments","text":"Randomized experiments long considered gold standard data collection, ensure unbiased perspective cause effect relationship cases.\nHuman studies perfect examples bias can unintentionally arise.\nreconsider study new drug used treat heart attack patients.\nparticular, researchers wanted know drug reduced deaths patients.researchers designed randomized experiment wanted draw causal conclusions drug’s effect.\nStudy volunteers21 randomly placed two study groups.\nOne group, treatment group, received drug.\ngroup, called control group, receive drug treatment.Put place person study.\ntreatment group, given fancy new drug anticipate help .\nhand, person group doesn’t receive drug sits idly, hoping participation doesn’t increase risk death.\nperspectives suggest actually two effects study: one interest effectiveness drug, second emotional effect () taking drug, difficult quantify.Researchers aren’t usually interested emotional effect, might bias study.\ncircumvent problem, researchers want patients know group .\nresearchers keep patients uninformed treatment, study said blind.\none problem: patient doesn’t receive treatment, know ’re control group.\nsolution problem give fake treatments patients control group.\nfake treatment called placebo, effective placebo key making study truly blind.\nclassic example placebo sugar pill made look like actual treatment pill.\nOften times, placebo results slight real improvement patients.\neffect dubbed placebo effect.patients ones blinded: doctors researchers can accidentally bias study.\ndoctor knows patient given real treatment, might inadvertently give patient attention care patient know placebo.\nguard bias, found measurable effect instances, modern studies employ double-blind setup doctors researchers interact patients , just like patients, unaware receiving treatment.22Look back study Section 1.1 researchers testing whether stents effective reducing strokes -risk patients.\nexperiment? study blinded? double-blinded?23For study Section 1.1, researchers employed placebo?\n, placebo looked like?24You may many questions ethics sham surgeries create placebo.\nquestions may even arisen mind general experiment context, possibly helpful treatment withheld individuals control group; main difference sham surgery tends create additional risk, withholding treatment maintains person’s risk.always multiple viewpoints experiments placebos, rarely obvious ethically “correct”.\ninstance, ethical use sham surgery creates risk patient?\nHowever, don’t use sham surgeries, may promote use costly treatment real effect; happens, money resources diverted away treatments known helpful.\nUltimately, difficult situation perfectly protect patients volunteered study patients may benefit () treatment future.","code":""},{"path":"data-design.html","id":"scope-of-inference","chapter":"2 Study design","heading":"2.4 Scope of inference","text":"statisticians refer scope inference study,\nasking two questions:Generalizability: population can generalize results?Causation: results provide evidence causal relationship?answer first question determined sampling method—selected sample randomly, sources sampling bias, can reasonably generalize population sample taken. answer second question determined type study—study randomized experiment, can investigate whether changes explanatory variable caused changes response variable; observational study, one can investigate associations variables. summarize determine study’s scope inference Figure 2.8.\nFigure 2.8: Determining scope inference study.\n","code":""},{"path":"data-design.html","id":"chp2-review","chapter":"2 Study design","heading":"2.5 Chapter review","text":"","code":""},{"path":"data-design.html","id":"summary-1","chapter":"2 Study design","heading":"Summary","text":"strong analyst good sense types data working visualize data order gain complete understanding variables.\nEqually important however, understanding data source.\nchapter, discussed randomized experiments taking good, random, representative samples population.\ndiscuss inferential methods (starting Chapter 9), conclusions can drawn dependent data collected.\nFigure 2.9 summarizes differences random assignment treatments random samples.25\nRegularly revisiting Figure 2.9 important making conclusions given data analysis.\nFigure 2.9: see, analysis conclusions made carefully according data collected. Note datasets come top left box usually ethics require random assignment treatments can given volunteers. representative (ideally random) sampling experiments (random assignment treatments) important statistical conclusions can made populations.\n","code":""},{"path":"data-design.html","id":"terms-1","chapter":"2 Study design","heading":"Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"data-design.html","id":"key-ideas-1","chapter":"2 Study design","heading":"Key ideas","text":"Statistical inference uses data sample make inferences larger, target population.Statistical inference uses data sample make inferences larger, target population.TODO: parameters statisticsTODO: parameters statisticsWe can generalize results sample target population sampling methods unbiased.\nOne unbiased sampling method simple random sample, randomly select observational units sample complete list observational units target population.can generalize results sample target population sampling methods unbiased.\nOne unbiased sampling method simple random sample, randomly select observational units sample complete list observational units target population.Various types bias can arise studies, particularly surveys individuals.\nstudy biased way conducted systematically -represent -represent certain groups responses.\nSelection bias occurs method selecting sample biased (e.g., convenience sample);\nnon-response bias occurs individuals selected study respond reached;\nresponse bias occurs responses individuals reflect truth (e.g., due confidentiality concerns, question wording, sensitive topics).Various types bias can arise studies, particularly surveys individuals.\nstudy biased way conducted systematically -represent -represent certain groups responses.\nSelection bias occurs method selecting sample biased (e.g., convenience sample);\nnon-response bias occurs individuals selected study respond reached;\nresponse bias occurs responses individuals reflect truth (e.g., due confidentiality concerns, question wording, sensitive topics).observational study, merely observe behavior individuals study; manipulate variables individuals way. randomized experiment, randomly assign values explanatory variable observational units, observe response variable. Random assignment allows us investigate causal relationships balances potential confounding variables, average.observational study, merely observe behavior individuals study; manipulate variables individuals way. randomized experiment, randomly assign values explanatory variable observational units, observe response variable. Random assignment allows us investigate causal relationships balances potential confounding variables, average.scope inference study answers two questions: (1) population can results can generalized? (2) Can assume associations data due cause--effect relationship?\ncan generalize target population sampling method unbiased (e.g., simple random sample).\ncan conclude cause--effect study design randomized experiment.scope inference study answers two questions: (1) population can results can generalized? (2) Can assume associations data due cause--effect relationship?\ncan generalize target population sampling method unbiased (e.g., simple random sample).\ncan conclude cause--effect study design randomized experiment.","code":""},{"path":"data-applications.html","id":"data-applications","chapter":"3 Applications: Data","heading":"3 Applications: Data","text":"Preliminaries Chapter introduced RStudio statistical computing environment.\nchapter, explore tidy data R—import export data sets,\nfilter data set groups, select create new variables.\nencourage work code chapter \nRStudio session.R powerful open source software tool working data.\nuse R programming environment within RStudio integrated\ndevelopment environment (IDE), offers suite additional features\naddition R programming console.\nThroughout text, provide guidance use R within \ncontext statistical content covered \n“Applications” chapters end part.educators, see value teaching modern software \nempower students take optimal advantage concepts learning.\nGenerally, present R techniques “Applications” chapter \nend part. times text concepts \ndistinguishable software, cases, provided \nR code within main body chapter.start introduction R, focused data sets structured \nR user can work data object R.","code":""},{"path":"data-applications.html","id":"dataframes-in-r","chapter":"3 Applications: Data","heading":"3.1 Dataframes in R","text":"Throughout text, work many different data sets. data sets\npre-loaded R, get loaded R packages, data sets\ncreated student. Data sets can viewed RStudio\nenvironment, can also investigated various R functions.Similar notation mathematical function, R function takes form:function_name(arguments function)function_name name function, mean, read.csv, lm.\ncan access help file named function preceding question mark: ?read.csv.arguments function inputs function. can data sets, parameter values, options.R, functions take arguments round parentheses (opposed \nsubsetting observations variables data objects happen\nsquare parentheses).R console, type ?read.csv. bring help file read.csv() function. function ? first two arguments function?According “Description” section help file, read.csv function\nreads data file creates data frame object R, cases corresponding\nlines variables fields file. read.csv function particular\ndesigned read .csv (comma separated value) data files.first argument file — name file website data read . second argument header — logical argument (TRUE FALSE) whether file contains header row variables .\ndefault function header = TRUE, argument isn’t used,\nR assume .csv file header row.Let’s start exploring data set containing information 50 emails\nsent David Diez’s Gmail account (one authors OpenIntro\ntextbooks). data set, , built openintro\npackage R comprised subset emails larger email\ndata set ’ll see Chapter 4.\nData sets built R packages can loaded using data() function.email50 data can found openintro package.can use glimpse() function see variables included data set\ndata type. , use head() function see first\nrows data set.Sometimes necessary extract column row data set.\nR, $ operator can used extract column data set.\nexample, data$variable extract variable column data dataframe.\nextracted, columns can thought vectors. vectors, desired pull specific entry, use square brackets ([ ]), index (number) entry wish extract brackets.\nexample, data$variable[2] extract second entry (row) variable column.dataframe can (roughly) thought set many different vectors, can extract rows columns dataframe using matrix notation (e.g. [row, column]).\nexample data[,j] extract entry row \\(\\) column \\(j\\);\ndata[, ] extract \\(^{th}\\) row, data[ , j] extract \\(j^{th}\\) column.\nNotice, extracting entire row (column), need specify columns (rows) like, second entry contain number.","code":"\ndata(email50)\nglimpse(email50)\n#> Rows: 50\n#> Columns: 21\n#> $ spam         <fct> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, …\n#> $ to_multiple  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n#> $ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n#> $ cc           <int> 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n#> $ sent_email   <fct> 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, …\n#> $ time         <dttm> 2012-01-04 06:19:16, 2012-02-16 13:10:06, 2012-01-04 08:…\n#> $ image        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ attach       <dbl> 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, …\n#> $ dollar       <dbl> 0, 0, 0, 0, 9, 0, 0, 0, 0, 23, 4, 0, 3, 2, 0, 0, 0, 0, 0,…\n#> $ winner       <fct> no, no, no, no, no, no, no, no, no, no, no, no, yes, no, …\n#> $ inherit      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ password     <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 8, …\n#> $ num_char     <dbl> 21.705, 7.011, 0.631, 2.454, 41.623, 0.057, 0.809, 5.229,…\n#> $ line_breaks  <int> 551, 183, 28, 61, 1088, 5, 17, 88, 242, 578, 1167, 198, 7…\n#> $ format       <fct> 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, …\n#> $ re_subj      <fct> 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, …\n#> $ exclaim_subj <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n#> $ exclaim_mess <dbl> 8, 1, 2, 1, 43, 0, 0, 2, 22, 3, 13, 1, 2, 2, 21, 10, 0, 0…\n#> $ number       <fct> small, big, none, small, small, small, small, small, smal…\nhead(email50) \n#> # A tibble: 6 × 21\n#>   spam  to_multiple from     cc sent_e…¹ time                image attach dollar\n#>   <fct> <fct>       <fct> <int> <fct>    <dttm>              <dbl>  <dbl>  <dbl>\n#> 1 0     0           1         0 1        2012-01-04 06:19:16     0      0      0\n#> 2 0     0           1         0 0        2012-02-16 13:10:06     0      0      0\n#> 3 1     0           1         4 0        2012-01-04 08:36:23     0      2      0\n#> 4 0     0           1         0 0        2012-01-04 10:49:52     0      0      0\n#> 5 0     0           1         0 0        2012-01-27 02:34:45     0      0      9\n#> 6 0     0           1         0 0        2012-01-17 10:31:57     0      0      0\n#> # … with 12 more variables: winner <fct>, inherit <dbl>, viagra <dbl>,\n#> #   password <dbl>, num_char <dbl>, line_breaks <int>, format <fct>,\n#> #   re_subj <fct>, exclaim_subj <dbl>, urgent_subj <fct>, exclaim_mess <dbl>,\n#> #   number <fct>, and abbreviated variable name ¹​sent_email\n#> # ℹ Use `colnames()` to see all variable names\nemail50$num_char # The num_char variable column\n#>  [1] 21.705  7.011  0.631  2.454 41.623  0.057  0.809  5.229  9.277 17.170\n#> [11] 64.401 10.368 42.793  0.451 29.233  9.794  2.139  0.130  4.945 11.533\n#> [21]  5.682  6.768  0.086  3.070 26.520 26.255  5.259  2.780  5.864  9.928\n#> [31] 25.209  6.563 24.599 25.757  0.409 11.223  3.778  1.493 10.613  0.493\n#> [41]  4.415 14.156  9.491 24.837  0.684 13.502  2.789  1.169  8.937 15.829\nemail50[47,3] # The entry in the 47th row and 3rd column\n#> # A tibble: 1 × 1\n#>   from \n#>   <fct>\n#> 1 1\nemail50[47,] # The 47th row\n#> # A tibble: 1 × 21\n#>   spam  to_multiple from     cc sent_e…¹ time                image attach dollar\n#>   <fct> <fct>       <fct> <int> <fct>    <dttm>              <dbl>  <dbl>  <dbl>\n#> 1 0     1           1         0 0        2012-03-06 07:10:00     0      0      0\n#> # … with 12 more variables: winner <fct>, inherit <dbl>, viagra <dbl>,\n#> #   password <dbl>, num_char <dbl>, line_breaks <int>, format <fct>,\n#> #   re_subj <fct>, exclaim_subj <dbl>, urgent_subj <fct>, exclaim_mess <dbl>,\n#> #   number <fct>, and abbreviated variable name ¹​sent_email\n#> # ℹ Use `colnames()` to see all variable names"},{"path":"data-applications.html","id":"datastruc","chapter":"3 Applications: Data","heading":"3.2 Tidy structure of data","text":"plotting, analyses, model building, etc., data structured according certain principles.\nHadley Wickham provides thorough discussion advice cleaning data Wickham (2014).Tidy data.data set rows observational units columns variables.\nkey every row case every column variable.\nexceptions.Creating tidy data often trivial! However, data sets provided course always provided tidy data form.","code":""},{"path":"data-applications.html","id":"using-the-pipe-to-chain","chapter":"3 Applications: Data","heading":"3.3 Using the pipe to chain","text":"Within R (really within type computing language, Python, SQL, Java, etc.), important understand build data using patterns language.R, syntax <- called assignment character; used store output function set operations call object. things consider:object_name <- anything way assigning anything new object_name.object_name <- function_name(data_table, arguments) way using function create new object.object_name <- data_table %>% function_name(arguments) uses chaining syntax called pipe operator (%>%) extension ideas functions.pipe syntax (%>%) takes data frame (data table) sends argument function. mapping goes first available argument function.\nexample:x %>% f(y) f(x, y)y %>% f(x, ., z) f(x,y,z)chaining, value left side %>% becomes first argument function right side. can extended multiple lines code:called extended chaining.properties pipe syntax (%>%) keep mind:%>% never front line, always connecting one idea continuation idea next line.%>% never front line, always connecting one idea continuation idea next line.spot left first %>% always data table.spot left first %>% always data table.pipe syntax read , %>%.pipe syntax read , %>%.Pipes used commonly functions dplyr package\n(included tidyverse package) allow us \nsequentially build data wrangling operations.\nPipes also helpful creating data visualizations ggplot2 package.","code":"object_name <- data_table %>%\n                    function_name(arguments) %>% \n                    another_function_name(other_arguments)"},{"path":"data-applications.html","id":"a-data-wrangling-example","chapter":"3 Applications: Data","heading":"3.4 A data wrangling example","text":"Consider built-data set hsb2 — “High School Beyond” survey.\nTwo hundred observations randomly sampled \n“High School Beyond” survey, survey conducted high\nschool seniors National Center Education Statistics.\ninterest proportion students \ntwo types school, public private.First, load data R session:use table command tabulate many type\nschool data set. Notice result \nproduced $ command table chaining\nsyntax done %>%.equivalent toIn code , select() function selected schtyp\nvariable (column) hsb2 data set.\nSince data set first argument select() function\n(examine help file typing ?select), need \ninclude second argument select()—variable \nlike select.","code":"\ndata(hsb2) # Load the data\ntable(hsb2$schtyp) \n#> \n#>  public private \n#>     168      32\nhsb2 %>%\n  select(schtyp) %>% # Select the schtyp variable\n  table()\n#> schtyp\n#>  public private \n#>     168      32"},{"path":"data-applications.html","id":"filtering","chapter":"3 Applications: Data","heading":"Filtering","text":"interested public schools?\nFirst, take note another piece R syntax:\ndouble equal sign: ==.\nlogical test “equal ”.\nwords, first determine school type equal\npublic observations data set \nfilter true.can read : “take hsb2 data frame pipe \nfilter function. Filter data \ncases school type equal public. ,\nassign resulting data frame new object\ncalled hsb2 underscore public.”","code":"\n# Filter for public schools\nhsb2_public <- hsb2 %>%  # Save final result as hsb2_public\n  filter(schtyp == \"public\")  # Only include observations\n                              # where the variable schtyp\n                              # is equal to \"public\""},{"path":"data-applications.html","id":"mutating","chapter":"3 Applications: Data","heading":"Mutating","text":"Suppose interested actual reading score\nstudents, instead whether reading score \naverage average.\nFirst, need calculate average reading score \nmean() function.\ngive us mean value, 52.23.\nHowever, order able refer back value later ,\nmight want store object can refer name.instead just printing result, let’s save \nnew object called avg_read.Next need determine whether student \naverage. example, reading score 57 \naverage, 68, 44 .\nObviously, going record like tedious\nerror prone.Instead can create new variable mutate()\nfunction dplyr package.start data frame, hsb2, pipe \nmutate(), create new variable called read_cat (cat categorical).\nNote using new variable name order\noverwrite existing reading score variable.\nnew variable read_cat column \nexisting data frame hsb2.decision criteria new variable based \nTRUE/FALSE question: reading score student\naverage reading score, label “average”,\notherwise, label “average”.can accomplished using ifelse() function R.\nLook helpfile typing ?ifelse R console window.first argument function logical test.first argument function logical test.second argument result logical test TRUE, words, student’s score average score.second argument result logical test TRUE, words, student’s score average score.last argument result FALSE.last argument result FALSE.","code":"\n# Calculate average reading score and show the value\nmean(hsb2$read)\n#> [1] 52.2\n# Calculate average reading score and store as avg_read\navg_read <- mean(hsb2$read)\nhsb2 <- hsb2 %>% mutate(read_cat = \n                          ifelse(read < avg_read, \n                                 \"below average\", \n                                 \"at or above average\"\n                                 )\n                        )"},{"path":"data-applications.html","id":"terms-2","chapter":"3 Applications: Data","heading":"Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"data-applications.html","id":"r-functions","chapter":"3 Applications: Data","heading":"R functions","text":"following R functions introduced chapter.","code":""},{"path":"explore-categorical.html","id":"explore-categorical","chapter":"4 Exploring categorical data","heading":"4 Exploring categorical data","text":"chapter focuses exploring categorical data using summary statistics visualizations.\nsummaries graphs presented chapter created using statistical software; however, since might first exposure concepts, take time chapter detail create .\npossible, present multivariate plots; plots visualize relationship multiple variables.\nMastery content presented chapter crucial understanding methods techniques introduced rest book.chapter, introduce tables basic tools organizing analyzing categorical data used throughout book. Table 4.1 displays first six rows email data set containing information 3,921 emails sent David Diez’s Gmail account (one authors OpenIntro textbooks). section examine whether presence numbers, small large, email provides useful value classifying email spam spam.\nDescriptions five email variables given Table 4.2.email data can found openintro package.26\nTable 4.1: Six rows email data set.\n\nTable 4.2: Variables descriptions email data set.\n","code":""},{"path":"explore-categorical.html","id":"contingency-tables-and-conditional-proportions","chapter":"4 Exploring categorical data","heading":"4.1 Contingency tables and conditional proportions","text":"summary table single categorical variable reports number observations (frequency) category called frequency table. Table\n4.3 frequency table number variable.\nreplaced counts percentages proportions (relative frequencies),\ntable called relative frequency table.\nTable 4.3: Frequency table Number variable.\nTable 4.4 summarizes two variables:\ntype (spam spam) number. table summarizes data two categorical variables\nway called contingency table two-way table.\nvalue table represents number times, frequency\nparticular combination variable outcomes occurred.\nexample, value 149 corresponds number emails\ndata set spam number listed email.\nRow column totals also included.\nrow totals provide total counts across row\n(e.g., \\(149 + 168 + 50 = 367\\) emails classified spam), column totals total\ncounts column.textbook, generally take convention putting categories explanatory variable columns categories response variable rows (exists explanatory-response relationship two variables).\nTable 4.4: Contingency table number (cols) type (rows) variables.\nlike examine whether presence numbers—none, small large—email provides useful value classifying email spam spam—, association variables number type?determine relationship exists whether email spam , whether email numbers, small number, big number, isn’t helpful compare number spam emails across number categories?27The proportion emails classified spam data set \\(3554/3921 = 0.906\\), 91%. Let’s compare unconditional proportion conditional proportions spam within number category: \\(400/549 \\approx 73\\%\\) emails numbers spam; \\(2659/2827 \\approx 94\\%\\) emails small numbers spam; \\(495/545 \\approx 91\\%\\) emails big numbers spam. Since three conditional proportions differ, say variables number type associated data set. Note differ overall, unconditional, proportion spam emails data set—91%.Association two categorical variables.unconditional proportion proportion measured total sample size. conditional proportion proportion measured subgroup sample.conditional proportions particular outcome (e.g., spam email) within levels categorical variable (e.g., whether number, small number, big number appears email) differ across levels, say two variables associated. can also determine two categorical variables associated checking conditional proportions outcome within categories differ overall, unconditional proportion.","code":""},{"path":"explore-categorical.html","id":"row-and-column-proportions","chapter":"4 Exploring categorical data","heading":"4.1.1 Row and column proportions","text":"Conditional proportions condition row category called row proportions; conditional proportions condition column category\ncalled column proportions.Table 4.5 shows row proportions Table 4.4. row proportions computed counts divided row totals. frequnecy 149 intersection spam none replaced \\(149/367=0.406\\), .e., 149 divided row total, 367. 0.406 represent? corresponds conditional proportion non-spam emails sample numbers.\nTable 4.5: contingency table row proportions type number variables.\ncontingency table column proportions computed similar way, column proportion computed count divided corresponding column total. Table 4.6 shows table, value 0.729 indicates 72.9% emails numbers spam. rate spam much lower emails small numbers (94.1%) big numbers (90.8%). spam rates vary three levels number (none, small, big), provides evidence spam number variables associated data set.\nTable 4.6: contingency table column proportions type number variables.\n0.458 represent Table 4.5? 0.059 represent Table 4.6?28What 0.139 intersection ~spam big represent Table 4.5? 0.908 represent Table 4.6?29Data scientists use statistics filter spam incoming email messages. noting specific characteristics email, data scientist may able classify emails spam spam high accuracy.\nOne characteristics whether email contains numbers, small numbers, big numbers. Another characteristic whether email HTML content. contingency table type format variables email data set shown Table 4.7. Recall HTML email email capacity special formatting, e.g., bold text. Table~ 4.7, helpful someone hoping classify email spam regular email: row column proportions?person interested proportion spam changes within email format. corresponds column proportions: proportion spam plain text emails proportion spam HTML emails.generate column proportions, can see higher fraction plain text emails spam (\\(209/1195 = 17.5\\%\\)) compared HTML emails (\\(158/2726 = 5.8\\%\\)). information insufficient classify email spam spam, 80% plain text emails spam. Yet, carefully combine information many characteristics, variables, stand reasonable chance able classify email spam spam. \nTable 4.7: contingency table type format.\nprevious Example points row column proportions equivalent. settling one form table, important consider ensure useful table constructed.Look back Tables 4.5 4.6. useful someone hoping identify spam emails using number variable?30","code":""},{"path":"explore-categorical.html","id":"sample-proportions-and-population-proportions","chapter":"4 Exploring categorical data","heading":"4.1.2 Sample proportions and population proportions","text":"field statistics, summary measures summarize sample data called statistics. Numbers summarize entire population called parameters. can remember\ndistinction looking first letter term:Statistics summarize Samples.Parameters summarize Populations.Proportions calculated sample data denoted \\(\\hat{p}\\).\nexample, interested proportion spam emails data set, denote \\(\\hat{p} = 0.91\\). different groups want summarize proportion, can add subscripts: \\(\\hat{p}_{none} = 0.73\\), \\(\\hat{p}_{small} = 0.94\\), \\(\\hat{p}_{big} = 0.91\\). values statistic since computed sample data.3921 emails sample larger group emails—emails sent David Diez, either past future. larger group emails population. unknown value proportion emails population classified spam, denote \\(\\pi\\). Similarly, unknown values proportion emails numbers population classified spam, denoted \\(\\pi_{none}\\). unknown values called parameters.typically use Roman letters symbolize statistics (e.g., \\(\\bar{x}\\), \\(\\hat{p}\\)), Greek letters symbolize parameters (e.g., \\(\\mu\\), \\(\\pi\\)).\nSince rarely can measure entire population, thus rarely know\nactual parameter values, like say, “don’t know Greek,\ndon’t know parameters!”","code":""},{"path":"explore-categorical.html","id":"bar-plots-and-mosaic-plots","chapter":"4 Exploring categorical data","heading":"4.2 Bar plots and mosaic plots","text":"bar plot common way display single categorical variable. left panel Figure 4.1 shows bar plot number variable.\nright panel, counts converted proportions (e.g., \\(549/3921=0.140\\) none).\nFigure 4.1: Two bar plots number. left panel shows counts \\(y\\)-axis, right panel shows proportions group \\(y\\)-axis.\nBar plots also used display relationship two categorical variables.\nbars stacked bar totals 100% segmented \nanother categorical variable, called segmented bar plot.segmented bar plot graphical display contingency table information. example, segmented bar plots representing Table 4.6 shown Figure 4.2, first created non-standardized segmented bar plot using number variable separated group levels type. standardized segmented bar plot using column proportions Table 4.6 helpful visualization fraction spam emails level number.segmented bar plot, explanatory variable plotted \\(x\\)-axis, response variable displayed different colors within bar, defined legend.\nFigure 4.2: () Segmented bar plot numbers found emails, counts broken type. (b) Segmented bar plot using column proportions type within number category.\nsegmented bar plots Figure 4.2, variable explanatory variable? response variable?31Examine segmented bar plots Figure 4.2. useful?Plot () contains information, plot (b) presents information clearly. Plot (b) makes clear emails number relatively high rate spam email—27%! hand, less 10% email small big numbers spam.Since proportion spam changes across groups Figure 4.2 (seen plot (b)), can conclude variables dependent, something also able discern using column proportions Table 4.6. none big groups relatively observations compared small group, association difficult see plot () Figure 4.2.cases, segmented bar plot standardized useful communicating important information. settling particular segmented bar plot, create standardized non-standardized forms decide effective communicating features data.","code":""},{"path":"explore-categorical.html","id":"mosaic-plots","chapter":"4 Exploring categorical data","heading":"4.2.1 Mosaic plots","text":"mosaic plot graphical display contingency table information similar bar plot one variable segmented bar plot using two variables. Figure 4.3 plot () shows mosaic plot number variable. column represents level number, column widths correspond proportion emails number type. instance, fewer emails numbers emails small numbers, number email column slimmer. general, mosaic plots use box areas represent number observations.\nFigure 4.3: () Mosaic plot numbers found emails. (b) Mosaic plot number counts broken type.\none-variable mosaic plot divided pieces Figure 4.3 plot (b) using type variable. column split proportionally according fraction emails spam number category. example, second column, representing emails small numbers, divided emails spam (lower) spam (upper).\nanother example, bottom third column represents spam emails big numbers, upper part third column represents regular emails big numbers. can use plot see type number variables associated since columns divided different vertical locations others, technique used checking association standardized version segmented bar plot.segmented bar plot, explanatory variable plotted \\(x\\)-axis mosaic plot, .e., explanatory variable represented columns, response variable displayed different colors within column, defined legend.","code":""},{"path":"explore-categorical.html","id":"why-not-pie-charts","chapter":"4 Exploring categorical data","heading":"4.3 Why not pie charts?","text":"pie charts well known, typically useful charts data analysis. pie chart shown Figure 4.4 alongside bar plot. generally difficult compare group sizes pie chart (comparing angles) bar plot (comparing heights), especially categories nearly identical counts proportions. case none big categories, difference slight may unable distinguish difference group sizes either plot!\nFigure 4.4: pie chart bar plot number email data set. pie chart see book!\nPie charts nearly useless trying compare two categorical variables, shown Figure 4.5.\nFigure 4.5: Try comparing distributions colors across pie charts , B, C—’s impossible!32\n’re still convinced shouldn’t use pie charts, read “Issue Pie Chart” “Data Viz” blog, “Worst Chart World” article Business Insider.","code":""},{"path":"explore-categorical.html","id":"simpson","chapter":"4 Exploring categorical data","heading":"4.4 Simpson’s paradox","text":"1991 study Radelet Pierce examined whether race associated whether death penalty invoked homicide cases33. Table 4.8 Figure 4.6 summarize data 674 defendants indictments involving cases multiple murders Florida 1976 1987.\nTable 4.8: Contingency table homicide cases Florida 1976 1987.\n\nFigure 4.6: Segmented bar plot comparing proportion defendants received death penalty Caucasians African Americans.\nrace defendant associated sentence trial?34Overall, lower percentage African American defendants received death penalty Caucasian defendants (8% compared 11%). Given studies shown racial bias sentencing, may surprising. Let’s look data closely.Since observational data, confounding variables likely present. Recall, confounding variable one associated response variable (sentence) explanatory variable (race defendant). confounding variables present?35If subset data race victim, see different picture. Table 4.9 Figure 4.7 summarize data, separately Caucasian African American homicide victims.\nTable 4.9: Contingency table homicide cases Florida 1976 1987; sentences classified defendant’s race victim’s race.\n\nFigure 4.7: Segmented bar plots comparing proportion Caucasian African American defendants received death penalty; separate plots Caucasian victims African American victims.\ncompare Figures 4.6 4.7, see direction association race defendant sentence reversed subgroup race victim. Overall, larger proportion Caucasians sentenced death penalty African Americans. However, compare cases victim’s race, larger proportion African Americans sentenced death penalty Caucasians!happen? answer race victim confounding variable. Figure 4.8 shows two segmented barplots examining relationship race victim sentence (response variable), relationship race victim race defendant (explanatory variable). see race victim associated response explanatory variables: defendants likely involve victim race, cases African American victims less likely result death penalty.\nFigure 4.8: race victim associated sentence (death penalty death penalty) race defendant. Defendants likely involve victim race, cases African American victims less likely result death penalty.\nThus, extremely low chance homicide case resulting death penalty African Americans combined fact cases African American defendants also African American victim results overall lower rate death penalty sentences African American defendants Caucasian defendants. overall results Figure 4.6 results subgroup Figure 4.7 valid—result “bad statistics”—suggest opposite conclusions. Data , observed effect reverses examine variables within subgroups, exhibit Simpson’s Paradox.Simpson’s Paradox.association explanatory variable response variable reverses examine association within different levels confounding variable, say data exhibit Simpson’s Paradox.","code":""},{"path":"explore-categorical.html","id":"chp4-review","chapter":"4 Exploring categorical data","heading":"4.5 Chapter review","text":"","code":""},{"path":"explore-categorical.html","id":"summary-2","chapter":"4 Exploring categorical data","heading":"Summary","text":"Fluently working categorical variables important skill data analysts.\nchapter introduced different visualizations numerical summaries applied categorical variables.\ngraphical visualizations even descriptive two variables presented simultaneously.\npresented bar plots, mosaic plots, pie charts (downfalls), estimations conditional proportions.","code":""},{"path":"explore-categorical.html","id":"terms-3","chapter":"4 Exploring categorical data","heading":"Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"explore-categorical.html","id":"key-ideas-2","chapter":"4 Exploring categorical data","heading":"Key ideas","text":"Proportions can either unconditional conditional. unconditional proportion proportion entire sample shares characteristic; whereas conditional proportion proportion subgroup sample shares characteristic. computing proportions using contingency table, unconditional proportions computed dividing cell total overall total sample size; conditional proportions computed dividing cell total row column total.Proportions can either unconditional conditional. unconditional proportion proportion entire sample shares characteristic; whereas conditional proportion proportion subgroup sample shares characteristic. computing proportions using contingency table, unconditional proportions computed dividing cell total overall total sample size; conditional proportions computed dividing cell total row column total.distribution single categorical variable can described examining proportions observations category.distribution single categorical variable can described examining proportions observations category.Two variables associated behavior one variable changes value variable. two categorical variables, occurs proportions category one variable change across categories variable. Recall Chapter 1, association imply causation!Two variables associated behavior one variable changes value variable. two categorical variables, occurs proportions category one variable change across categories variable. Recall Chapter 1, association imply causation!","code":""},{"path":"explore-numerical.html","id":"explore-numerical","chapter":"5 Exploring quantitative data","heading":"5 Exploring quantitative data","text":"chapter focuses exploring quantitative data using summary statistics visualizations.\nsummaries graphs presented chapter created using statistical software; however, since might first exposure concepts, take time chapter detail create .\nMastery content presented chapter crucial understanding methods techniques introduced rest book.Consider loan_amount variable loan50 data set, represents loan size 50 loans data set.\nvariable quantitative (numerical) since can sensibly discuss numerical difference size two loans.\nhand, area codes zip codes quantitative, rather categorical variables.Throughout chapter, apply methods using loan50, county, email50 data sets, introduced Section 1.2.\n’d like review variables either data set, see Tables 1.4 1.6.loan50 email50 data sets can found openintro package.\ncounty data can found usdata package.","code":""},{"path":"explore-numerical.html","id":"scatterplots","chapter":"5 Exploring quantitative data","heading":"5.1 Scatterplots for paired data","text":"scatterplot provides case--case view data two quantitative variables.\nFigure 1.4, scatterplot used examine homeownership rate fraction housing units part multi-unit properties (e.g. apartments) county data set.\nAnother scatterplot shown Figure 5.1, comparing total income borrower total_income amount borrowed loan_amount loan50 data set.\nscatterplot, point represents single case.\nSince 50 cases loan50, 50 points Figure 5.1.examining scatterplots, describe four features:Form - trace trend points,\ntrend linear nonlinear?Direction - values x-axis increase, y-values\ntend increase (positive direction) decrease (negative direction)?Strength - closely points follow trend?Unusual observations outliers- unusual observations\nseem match overall pattern scatterplot?\nFigure 5.1: scatterplot loan_amount versus total_income loan50 data set.\nLooking Figure 5.1, see many\nborrowers income $100,000 left side graph, \nhandful borrowers income $250,000. loan amounts vary \n$10,000 around $40,000. data seem linear form, though\nrelationship two variables quite weak. direction \npositive—total income increases, loan amount also tends increase—may unusual observations higher income range,\nthough since relationship weak, hard tell.\nFigure 5.2: scatterplot median household income poverty rate county data set. Data 2017. statistical model also fit data shown dashed line.\nFigure 5.2 shows plot median household income poverty rate 3,142 counties.\ncan said relationship variables?relationship evidently nonlinear, highlighted dashed line. different previous scatterplots seen, show relationships show much, , curvature trend.\nrelationship moderate strong, direction negative,\nappear unusual observations.scatterplots reveal data, useful?36Describe two variables horseshoe-shaped association scatterplot (\\(\\cap\\) \\(\\frown\\))37","code":""},{"path":"explore-numerical.html","id":"dotplots","chapter":"5 Exploring quantitative data","heading":"5.2 Dot plots and the mean","text":"Sometimes interested distribution single variable.\ncases, dot plot provides basic displays.\ndot plot one-variable scatterplot; example using interest rate 50 loans shown Figure 5.3.\nFigure 5.3: dot plot interest_rate loan50 data set. rates rounded nearest percent plot, distribution’s mean shown red triangle.\ndistribution variable description possible values\ntakes frequently value occurs. mean, often called average, common way measure center distribution data.\ncompute mean interest rate 50 loans , add interest rates divide number observations.sample mean often labeled \\(\\bar{x}\\).\nletter \\(x\\) used generic placeholder variable bar \\(x\\) communicates ’re looking average variable. example \\(x\\) represent interest rate, \\(\\bar{x}\\) = 11.57%.\nuseful think mean balancing point distribution38, ’s shown triangle Figure 5.3.Mean.sample mean can calculated sum observed values divided number observations:\\[ \\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n} \\]Examine equation mean. \\(x_1\\) correspond ? \\(x_2\\) Can infer general meaning \\(x_i\\) might represent?39What \\(n\\) sample loans?40The loan50 data set represents sample larger population loans made Lending Club.\ncompute mean population way sample mean.\nHowever, population mean special label: \\(\\mu\\).\nsymbol \\(\\mu\\) Greek letter mu represents average observations population.\nSometimes subscript, \\(_x\\), used represent variable population mean refers , e.g., \\(\\mu_x\\).\nOften times expensive time consuming measure population mean precisely, often estimate \\(\\mu\\) using sample mean, \\(\\bar{x}\\).Greek letter \\(\\mu\\) pronounced mu, listen pronunciation .average interest rate across loans population can estimated using sample data. Based sample 50 loans, reasonable estimate \\(\\mu_x\\), mean interest rate loans full data set?sample mean, 11.57%, provides rough estimate \\(\\mu_x\\). perfect, statistic single best guess point estimate average interest rate loans population study, parameter. Chapter 9 beyond, develop tools characterize accuracy point estimates, like sample mean. might guessed, point estimates based larger samples tend accurate based smaller samples.mean useful making comparisons across different samples may different sample sizes allows us rescale standardize metric something easily interpretable comparable.Suppose like understand new drug effective treating asthma attacks standard drug.\ntrial 1500 adults set , 500 receive new drug, 1000 receive standard drug control group:Comparing raw counts 200 300 asthma attacks make appear new drug better, artifact imbalanced group sizes.\nInstead, look average number asthma attacks per patient group:New drug: \\(200 / 500 = 0.4\\) asthma attacks per patientStandard drug: \\(300 / 1000 = 0.3\\) asthma attacks per patientThe standard drug lower average number asthma attacks per patient average treatment group.Emilio opened food truck last year sells burritos, business stabilized last 4 months.\n4 month period, made $11,000 working 625 hours.\nEmilio’s competition, Francis, made $13,000 last 4 months working 800 hours. Francis brags Emilio business profitable. Francis’ claim warranted?Emilio’s average hourly earnings provide useful statistic evaluating much venture , least financial perspective, worth:\\[ \\frac{\\$11000}{625\\text{ hours}} = \\$17.60\\text{ per hour} \\]knowing average hourly wage,\nEmilio now put earnings standard unit easier compare many jobs might consider.comparison, Francis’ average hourly wage \\[ \\frac{\\$13000}{800\\text{ hours}} = \\$16.25\\text{ per hour} \\]Thus, Francis’ total earnings larger Emilio’s, standardizing hour, Francis shouldn’t brag.Suppose want compute average income per person US. , might first think take mean per capita incomes across 3,142 counties data set. better approach?county data set special county actually represents many individual people.\nsimply average across income variable, treating counties 5,000 5,000,000 residents equally calculations.\nInstead, compute total income county, add counties’ totals, divide number people counties.\ncompleted steps data, find per capita income US $30,861.\ncomputed simple mean per capita income across counties, result just $26,093!example used called weighted mean.\ninformation topic, check following online supplement regarding weighted means.","code":""},{"path":"explore-numerical.html","id":"histograms","chapter":"5 Exploring quantitative data","heading":"5.3 Histograms and shape","text":"Dot plots show exact value observation. useful small data sets, can become hard read larger samples. Rather showing value observation, prefer think value belonging bin. example, loan50 data set, created table counts number loans interest rates 5.0% 7.5%, number loans rates 7.5% 10.0%, . Observations fall boundary bin (e.g., 10.00%) allocated lower bin. tabulation shown Table 5.1. binned counts plotted bars Figure 5.4 called histogram, resembles heavily binned version stacked dot plot shown Figure 5.3.\nTable 5.1: Counts binned interest_rate data.\n\nFigure 5.4: histogram interest_rate. distribution strongly skewed right.\nHistograms provide view data density. Higher bars represent data relatively common, \n“dense.” instance, many loans rates 5% 10% loans rates 20% 25% data set. bars make easy see density data changes relative interest rate.Histograms especially convenient understanding shape data distribution. Figure 5.4 suggests loans rates 15%, handful loans rates 20%. data trail right way longer right tail, shape said right skewed41Data sets reverse characteristic—long, thinner tail left—said left skewed. also say distribution long left tail. Data sets show roughly equal trailing directions called symmetric.data trail one direction, distribution long tail.\ndistribution long left tail, left skewed negatively skewed.\ndistribution long right tail, right skewed positively skewed.Besides mean (since labeled), can see dot plot Figure 5.3 see histogram Figure 5.4?42In addition looking whether distribution skewed symmetric, histograms can used identify modes. mode represented prominent peak distribution. one prominent peak histogram interest_rate.definition mode sometimes taught math classes value occurrences data set. However, many real-world data sets, common observations value data set, making definition impractical data analysis.Figure 5.5 shows histograms one, two, three prominent peaks. distributions called unimodal, bimodal, multimodal, respectively. distribution two prominent peaks called multimodal. Notice one prominent peak unimodal distribution second less prominent peak counted since differs neighboring bins observations.\nFigure 5.5: Counting prominent peaks, distributions (left right) unimodal, bimodal, multimodal. Note left plot unimodal counting prominent peaks, just peak.\nFigure 5.4 reveals one prominent mode interest rate. distribution unimodal, bimodal, multimodal?43Height measurements young students adult teachers K-3 elementary school taken.\nmany modes expect height data set?44.Looking modes isn’t finding clear correct answer number modes distribution, prominent rigorously defined book. important part examination better understand data.Another type plot helpful exploring shape distribution smoothed histogram, called density plot. density plot scale \\(y\\)-axis total area density curve equal one. allows us get sense proportion data lie certain interval, rather frequency data interval. can change scale histogram plot proportions rather frequencies, overlay density curve rescaled histogram, seen Figure 5.6.\nFigure 5.6: density plot interest_rate overlayed histogram using density scale.\n","code":""},{"path":"explore-numerical.html","id":"variance-sd","chapter":"5 Exploring quantitative data","heading":"5.4 Variance and standard deviation","text":"mean introduced method describe center data set, variability data also important. , introduce two measures variability: variance standard deviation. useful data analysis, even though formulas bit tedious calculate hand. standard deviation easier two comprehend, roughly describes far away typical observation mean.call distance observation mean deviation. deviations \\(1^{st}\\), \\(2^{nd}\\), \\(3^{rd}\\), \\(50^{th}\\) observations interest_rate variable:\\[ x_1 - \\bar{x} = 10.9 - 11.57 = -0.67 \\] \\[ x_2 - \\bar{x} = 9.92 - 11.57 = -1.65 \\] \\[ x_3 - \\bar{x} = 26.3 - 11.57 = 14.73 \\] \\[ \\vdots \\] \\[ x_{50} - \\bar{x} = 6.08 - 11.57 = -5.49 \\]square deviations take average, result equal sample variance, denoted \\(s^2\\):\\[\\begin{align*}\ns^2 &= \\frac{(-0.67)^2 + (-1.65)^2 + (14.73)^2 + \\cdots + (-5.49)^2}{50 - 1} \\\\\n&= \\frac{0.45 + 2.72 + \\cdots + 30.14}{49} \\\\\n&= 25.52\n\\end{align*}\\]divide \\(n - 1\\), rather dividing \\(n\\), computing sample’s variance; ’s mathematical nuance , end result makes statistic slightly reliable useful.Notice squaring deviations two things. First, makes large values relatively much larger. Second, gets rid negative signs.standard deviation defined square root variance:\\[ s = \\sqrt{25.52} = 5.05 \\]often omitted, subscript \\(_x\\) may added variance standard deviation, .e., \\(s_x^2\\) \\(s_x\\), useful reminder variance standard deviation observations represented \\(x_1\\), \\(x_2\\), …, \\(x_n\\).Variance standard deviation.sample variance (near) average squared distance mean:\n\\[\n  s^2 = \\frac{((x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\cdots + (x_n - \\bar{x})^2)}{n-1}\n\\]\nsample standard deviation square root variance: \\(s = \\sqrt{s^2}\\).standard deviation useful considering far data distributed mean.\nstandard deviation represents typical deviation observations mean.\ndistribution bell-shaped, 70% data within one standard deviation mean 95% within two standard deviations.\nHowever, percentages necessarily hold shaped distributions!Like mean, population values variance standard deviation special symbols: \\(\\sigma^2\\) variance \\(\\sigma\\) standard deviation.Greek letter \\(\\sigma\\) pronounced sigma, listen pronunciation .\nFigure 5.7: interest_rate variable, 34 50 loans (68%) interest rates within 1 standard deviation mean, 48 50 loans (96%) rates within 2 standard deviations. Usually 70% data within 1 standard deviation mean 95% within 2 standard deviations, though far hard rule.\n\nFigure 5.8: Three different population distributions mean (0) standard deviation (1).\ngood description shape distribution include modality whether distribution symmetric skewed one side.\nUsing Figure 5.8 example, explain description important.45Describe distribution interest_rate variable using histogram Figure 5.4.\ndescription incorporate center, variability, shape distribution, also placed context.\nAlso note especially unusual cases.distribution interest rates unimodal skewed high end. Many rates fall near mean 11.57%, fall within one standard deviation (5.05%) mean.\nexceptionally large interest rates sample 20%.practice, variance standard deviation sometimes used means end, “end” able accurately estimate uncertainty associated sample statistic. example, Chapter 17 standard deviation used calculations help us understand much sample mean varies one sample next.","code":""},{"path":"explore-numerical.html","id":"box-plots-quartiles-and-the-median","chapter":"5 Exploring quantitative data","heading":"5.5 Box plots, quartiles, and the median","text":"box plot (box--whisker plot) summarizes data set using five statistics \nalso identifying unusual observations. five statistics—minimum, first quartile,\nmedian, third quartile, maximum—together called five number summary.\nFigure 5.9 provides dot plot alongside box plot interest_rate variable loan50 data set.\nFigure 5.9: Plot shows dot plot Plot B shows box plot distribution interest rates loan50 dataset.\ndark line inside box represents median, splits data half:\n50% data fall value 50% fall .\nSince loan50 dataset 50 observations (even number),\nmedian defined average two observations closest \n\\(50^{th}\\) percentile. Table 5.2 shows \ninterest rates, arranged ascending order.\ncan see \\(25^{th}\\) \\(26^{th}\\) values \n9.93, corresponds dark line \nbox plot Figure 5.9.\nTable 5.2: Interest rates loan50 dataset, arranged ascending order.\nodd number observations, exactly one\nobservation splits data two halves, case \nobservation median (average needed).Median: number middle.data ordered smallest largest, median observation\nright middle.\neven number observations, two values \nmiddle, median taken average.Mathematically, denote sample size \\(n\\), thenif \\(n\\) odd, median \\([(n+1)/2]^{th}\\) smallest value data set, andif \\(n\\) even, median average \\((n/2)^{th}\\) \\((n/2+1)^{th}\\) smallest values data set.median example percentile. Since 50% data fall \nmedian, median \\(50^{th}\\) percentile.Percentiles.\\(p^{th}\\) percentile value \\(p\\)% data fall \nvalue. example, 7.96\n\\(25^{th}\\) percentile interest rates shown Table 5.2 since 25% data fall \n7.96 (75% fall ).second step building box plot drawing rectangle represent \nmiddle 50% data.\nlength box called interquartile range, IQR short.\n, like standard deviation, measure variability data.\nvariable data, larger standard deviation IQR tend .\ntwo boundaries box called first quartile (\\(25^{th}\\) percentile) third quartile (\\(75^{th}\\) percentile) , often labeled \\(Q_1\\) \\(Q_3\\), respectively46Interquartile range (IQR).IQR interquartile range length box box plot.\ncomputed \n\\[\n  IQR = Q_3 - Q_1,\n\\]\n\\(Q_1\\) \\(Q_3\\) \\(25^{th}\\) \\(75^{th}\\) percentiles, respectively.percent data fall \\(Q_1\\) median?\npercent median \\(Q_3\\)?47Extending box, whiskers attempt capture data outside box.\nwhiskers box plot reach minimum maximum values data, unless points considered unusually high unusually low, identified potential outliers box plot.\nlabeled dot box plot.\npurpose labeling points—instead extending whiskers minimum maximum observed values—help identify observations appear unusually distant rest data.\nvariety formulas determining whether particular data point considered outlier, different statistical software use different formulas.\ncommonly used formula value beyond \\(1.5\\times IQR\\)[choice exactly 1.5 arbitrary, commonly used value box plots.] away box considered outlier.\nsense, box like body box plot whiskers like arms trying reach rest data, outliers.Figure 5.9,\nupper whisker extend last two points, 24.85% 26.3%, \n\\(Q_3 + 1.5\\times IQR\\), extends last point limit.\nlower whisker stops minimum value data set, 5.31%, since outliers lower end distribution.whiskers extend actual data points—limits outliers. ,\nvalues \\(Q_1 - 1.5\\times IQR\\) \\(Q_3 + 1.5\\times IQR\\) shown\nplot.Outliers extreme.outlier observation appears extreme relative rest data.\nExamining data outliers serves many useful purposes, includingidentifying strong skew distribution,identifying possible data collection data entry errors, andproviding insight interesting properties data.Using box plot Figure 5.9, estimate values \\(Q_1\\), \\(Q_3\\), IQR interest_rate loan50 data set.48","code":""},{"path":"explore-numerical.html","id":"describing-and-comparing-quantitative-distributions","chapter":"5 Exploring quantitative data","heading":"5.6 Describing and comparing quantitative distributions","text":"review, describing scatterplot—association \ntwo quantitative variables, look four features:FormDirectionStrengthOutliersWhen asked describe compare univariate (single variable) quantitative distributions, look four features:CenterVariabilityShapeOutliersWe can compare quantitative distributions using side--side box plots,\nstacked histograms dot plots. Recall loan50 data set represents sample larger loan data set called loans.\nlarger data set contains information 10,000 loans made Lending Club. Figure 5.10 examines relationship homeownership, loans data can take value rent, mortgage (owns mortgage), , interest_rate. Note homeownership\ncategorical variable interest_rate quantitative variable.\nFigure 5.10: Side--side box plots loan interest rates homeownership category corresponding histograms.\nsee immediately features easier discern box plots, others histograms. Shape shown clearly histograms, center (measured median) easy compare across groups side--side box plots.Using Figure 5.10 write sentences comparing distributions loan amount\nacross different homeownership categories.median loan amount higher mortgage (around $16,000) rent (around $12,000-$13,000). However, variability loan amounts similar across homeownership categories, IQR around $15,000 loans ranging hundred dollars $40,000. see histograms distribution loan amounts skewed right three homeownership categories, means mean loan amount higher median loan amount. apparent outliers mortgage category, rent categories outliers $40,000.Besides center, variability, shape, outliers, another interesting feature distributions result rounding. Loan amounts data set often rounded nearest 100, see spikes values histogram—something evident box plots.","code":""},{"path":"explore-numerical.html","id":"robust-statistics","chapter":"5 Exploring quantitative data","heading":"5.7 Robust statistics","text":"sample statistics interest_rate data set affected observation, 26.3%?\nhappened loan instead 15%?\nhappen summary statistics observation 26.3% even larger, say 35%?\nscenarios plotted alongside original data Figure 5.11, sample statistics computed scenario Table 5.3.\nFigure 5.11: Dot plots original interest rate data two modified data sets.\n\nTable 5.3: comparison median, IQR, mean, standard deviation change value extereme observation original interest data changes.\naffected extreme observations, mean median?standard deviation IQR affected extreme observations?49The median IQR called robust statistics extreme observations little effect values—moving extreme value generally little influence statistics.\nhand, mean standard deviation heavily influenced changes extreme observations, can important situations. Additionally, mean tends get pulled direction distribution’s skewness, skewness little affect median.median IQR change three scenarios Table 5.3.\nmight case?median IQR sensitive numbers near \\(Q_1\\), median, \\(Q_3\\).\nSince values regions stable three data sets, median IQR estimates also stable.distribution loan amounts loan50 data set right skewed, large loans lingering right tail.\nwanting understand typical loan size, interested mean median?50","code":""},{"path":"explore-numerical.html","id":"transforming-data-special-topic","chapter":"5 Exploring quantitative data","heading":"5.8 Transforming data (special topic)","text":"data strongly skewed, sometimes transform easier model. transformation rescaling data using function.\nFigure 5.12: Plot : histogram populations US counties. Plot B: histogram log\\(_{10}\\)-transformed county populations. plot, x-value corresponds power 10, e.g. 4 x-axis corresponds \\(10^4 =\\) 10,000. Data 2017.\nConsider histogram county populations shown left Figure 5.12, shows extreme skew. useful plot?Nearly data fall left-bin, extreme skew obscures many potentially interesting details data.standard transformations may useful strongly right skewed data much data positive clustered near zero.\ninstance, plot logarithm (base 10) county populations results new histogram Figure 5.12.\ndata symmetric, potential outliers appear much less extreme original data set.\nreigning outliers extreme skew, transformations like often make easier build statistical models data.Transformations can also applied one variables scatterplot.\nscatterplot population change 2010 2017 population 2010 shown Figure 5.13.\nfirst scatterplot, ’s hard decipher interesting patterns population variable strongly skewed.\nHowever, apply log\\(_{10}\\) transformation population variable, shown \nFigure 5.13, positive association variables revealed.\nfact, may interested fitting trend line data explore methods around fitting regression lines Chapter 6.\nFigure 5.13: Plot : Scatterplot population change population change. Plot B: ~scatterplot data population size log-transformed.\nTransformations logarithm can useful, .\ninstance, square root (\\(\\sqrt{\\text{original observation}}\\)) inverse (\\(\\frac{1}{\\text{original observation}}\\)) commonly used data scientists.\nCommon goals transforming data see data structure differently, reduce skew, assist modeling, straighten nonlinear relationship scatterplot.","code":""},{"path":"explore-numerical.html","id":"mapping-data-special-topic","chapter":"5 Exploring quantitative data","heading":"5.9 Mapping data (special topic)","text":"county data set offers many numerical variables plot using dot plots, scatterplots, box plots, miss true nature data.\nRather, encounter geographic data, create intensity map, colors used show higher lower values variable.\nFigures 5.14 5.15 show intensity maps poverty rate percent (poverty), unemployment rate (unemployment_rate), homeownership rate percent (homeownership), median household income (median_hh_income).\ncolor key indicates colors correspond values.\nintensity maps generally helpful getting precise values given county, helpful seeing geographic trends generating interesting research questions hypotheses.interesting features evident poverty unemployment rate intensity maps?Poverty rates evidently higher locations.\nNotably, deep south shows higher poverty rates, much Arizona New Mexico.\nHigh poverty rates evident Mississippi flood plains little north New Orleans also large section Kentucky.\nunemployment rate follows similar trends, can see correspondence two\nvariables.\nfact, makes sense higher rates unemployment closely related poverty rates.\nOne observation stands comparing two maps: poverty rate much higher unemployment rate, meaning many people may working, making enough break poverty.interesting features evident median household income intensity map Figure 5.15?51\nFigure 5.14: Plot : Intensity map poverty rate (percent). Plot B: Intensity map unemployment rate (percent).\n\nFigure 5.15: Plot : Intensity map homeownership rate (percent). Plot B: Intensity map median household income ($1000s).\n","code":""},{"path":"explore-numerical.html","id":"chp5-review","chapter":"5 Exploring quantitative data","heading":"5.10 Chapter review","text":"","code":""},{"path":"explore-numerical.html","id":"summary-3","chapter":"5 Exploring quantitative data","heading":"Summary","text":"Fluently working quantitaitve variables important skill data analysts.\nchapter introduced different visualizations numerical summaries applied quantitative variables.\ngraphical visualizations even descriptive two variables presented simultaneously.\npresented scatterplots, dot plots, histograms, box plots.\nQuantitative variables can summarized using mean, median, quartiles, standard deviation, variance.","code":""},{"path":"explore-numerical.html","id":"terms-4","chapter":"5 Exploring quantitative data","heading":"Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"explore-numerical.html","id":"key-ideas-3","chapter":"5 Exploring quantitative data","heading":"Key ideas","text":"Two variables associated behavior one variable depends value variable. Two quantitative variables associated trend apparent scatterplot. Recall Chapter 1, association imply causation!Two variables associated behavior one variable depends value variable. Two quantitative variables associated trend apparent scatterplot. Recall Chapter 1, association imply causation!describing distribution single quantitative variable histogram, dot plot, box plot, look (1) center, (2) variability, (3) shape, (4) outliers.describing distribution single quantitative variable histogram, dot plot, box plot, look (1) center, (2) variability, (3) shape, (4) outliers.describing relationship shown two quantitative variables scatterplot, look (1) form, (2) direction, (3) strength, (4) outliers.describing relationship shown two quantitative variables scatterplot, look (1) form, (2) direction, (3) strength, (4) outliers.","code":""},{"path":"explore-regression.html","id":"explore-regression","chapter":"6 Correlation and regression","heading":"6 Correlation and regression","text":"Linear regression powerful statistical technique.\nMany people familiarity regression just reading news, straight lines overlaid scatterplots.\nLinear models can used prediction evaluate whether linear relationship two numerical variables.","code":""},{"path":"explore-regression.html","id":"fit-line-res-cor","chapter":"6 Correlation and regression","heading":"6.1 Fitting a line, residuals, and correlation","text":"’s helpful think deeply line fitting process. section, define form linear model, explore criteria makes good fit, introduce new statistic called correlation.","code":""},{"path":"explore-regression.html","id":"fitting-a-line-to-data","chapter":"6 Correlation and regression","heading":"6.1.1 Fitting a line to data","text":"Figure 6.1 shows two variables whose relationship can modeled perfectly straight line.\nequation line \\(y = 5 + 64.96 x\\).\nConsider perfect linear relationship means: know exact value \\(y\\) just knowing value \\(x\\).\nunrealistic almost natural process.\nexample, took family income (\\(x\\)), value provide useful information much financial support college may offer prospective student (\\(y\\)).\nHowever, prediction far perfect, since factors play role financial support beyond family’s finances.\nFigure 6.1: Requests twelve separate buyers simultaneously placed trading company purchase Target Corporation stock (ticker TGT, December 28th, 2018), total cost shares reported. cost computed using linear formula, linear fit perfect.\nLinear regression statistical method fitting line data relationship two variables, \\(x\\) \\(y\\), can modeled straight line error:\\[ y = \\beta_0 + \\beta_1x + \\varepsilon\\]values \\(\\beta_0\\) \\(\\beta_1\\) represent model’s parameters (\\(\\beta\\) Greek letter beta), error represented \\(\\varepsilon\\) (Greek letter epsilon).\nparameters estimated using data, write point estimates \\(b_0\\) \\(b_1\\).\nuse \\(x\\) predict \\(y\\), usually call \\(x\\) explanatory predictor variable, call \\(y\\) response. also often drop \\(\\epsilon\\) term writing model since main focus often prediction average outcome. \\(\\epsilon\\) term dropped, put “hat” \\(y\\) (\\(\\hat{y}\\)) signal model yields prediction \\(y\\), actual value.rare data fall perfectly straight line.\nInstead, ’s common data appear cloud points,\nexamples shown Figure 6.2.\ncase, data fall around straight line, even none observations fall exactly line.\nfirst plot shows relatively strong downward linear trend, remaining variability data around line minor relative strength relationship \\(x\\) \\(y\\).\nsecond plot shows upward trend , evident, strong first. last plot shows weak downward trend data, slight can hardly notice .\nexamples, uncertainty regarding estimates model parameters, \\(\\beta_0\\) \\(\\beta_1\\).\ninstance, might wonder, move line little, tilt less?\nmove forward chapter, learn criteria line-fitting, also learn uncertainty associated estimates model parameters.\nFigure 6.2: Three data sets linear model may useful even though data fall exactly line.\nalso cases fitting straight line data, even clear relationship variables, helpful.\nOne case shown Figure 6.3 clear relationship variables even though trend linear.\ndiscuss nonlinear trends chapter next, details fitting nonlinear models saved later course.\nFigure 6.3: best fitting line data flat, useful nonlinear case. data physics experiment.\n","code":""},{"path":"explore-regression.html","id":"using-linear-regression-to-predict-possum-head-lengths","chapter":"6 Correlation and regression","heading":"6.1.2 Using linear regression to predict possum head lengths","text":"Brushtail possums marsupial lives Australia, photo\none shown Figure 6.4.\nResearchers captured 104 animals took body measurements releasing animals back wild.\nconsider two measurements: total length possum, head tail, length possum’s head.\nFigure 6.4: common brushtail possum Australia. Photo Greg Schecter, flic.kr/p/9BAFbR, CC 2.0 license.\npossum data can found openintro package.Figure 6.5 shows scatterplot head length (mm) total length (cm) possums.\npoint represents single possum data.\nhead total length variables associated: possums average total length also tend average head lengths.\nrelationship perfectly linear, helpful partially explain connection variables straight line.\nFigure 6.5: scatterplot showing head length total length 104 brushtail possums. point representing possum head length 86.7 mm total length 84 cm highlighted.\nwant describe relationship head length total length variables possum data set using line.\nexample, use total length predictor variable, \\(x\\), predict possum’s head length, \\(y\\).\nfit linear relationship using technology (criteria discussed Section 6.2), Figure 6.6.\nFigure 6.6: reasonable linear model fit represent relationship head length total length.\nequation line \\[\\hat{y} = 43+0.57x.\\]“hat” \\(y\\) used signify estimate.\ncan use line discuss properties possums.\ninstance, equation predicts possum total length 80 cm head length \\[\\hat{y} = 43 + 0.57 \\times 80 = 88.6 \\text{ mm}.\\]estimate may viewed average: equation predicts possums total length 80 cm average head length 88.6 mm.\nAbsent information 80 cm possum, prediction head length uses average reasonable estimate.may variables help us predict head length possum besides length.\nPerhaps relationship little different male possums female possums, perhaps differ possums one region Australia versus another region.\nPlot Figure 6.7 shows relationship total length head length brushtail possums, taking consideration sex.\nMale possums (represented blue triangles) seem larger terms total length head length female possums (represented red circles).\nPlot B Figure 6.7 shows relationship, taking consideration age.\n’s harder tell age changes relationship total length head length possums.\nFigure 6.7: Relationship total length head lentgh brushtail possums, taking consideration sex (Plot ) age (Plot B).\nChapter 7, ’ll learn can include one predictor model.\nget , first need better understand best build simple linear model one predictor.","code":""},{"path":"explore-regression.html","id":"residuals","chapter":"6 Correlation and regression","heading":"6.1.3 Residuals","text":"Residuals leftover variation data accounting model fit:\\[\\text{Data} = \\text{Fit} + \\text{Residual}\\]observation residual, three residuals linear model fit data shown Figure 6.8.\nobservation regression line, residual, vertical distance observation line, positive.\nObservations line negative residuals.\nOne goal picking right linear model residuals small possible.Figure 6.8 almost replica Figure 6.6, three points data highlighted.\nobservation marked red circle small, negative residual -1; observation marked green diamond large residual +7; observation marked yellow triangle moderate residual -4.\nsize residual usually discussed terms absolute value.\nexample, residual observation marked yellow triangle larger observation marked red circle \\(|-4|\\) larger \\(|-1|\\).\nFigure 6.8: reasonable linear model fit represent relationship head length total length, three points residuals highlighted.\nResidual: Difference observed expected.residual \\(^{th}\\) observation \\((x_i, y_i)\\) difference observed response (\\(y_i\\)) response predict based model fit (\\(\\hat{y}_i\\)):\\[e_i = y_i - \\hat{y}_i\\]typically identify \\(\\hat{y}_i\\) plugging \\(x_i\\) model.linear fit shown Figure 6.8 given \\(\\hat{y} = 43+0.57x\\).\nBased line, formally compute residual observation\n\\((76.0, 85.1)\\).\nobservation marked red circle Figure 6.8.\nCheck earlier visual estimate, \\(-1\\).first compute predicted value observation marked red circle based model:\\[\\hat{y} = 43+0.57x = 43+0.57\\times 76.0 = 86.3mm\\]Next, compute difference actual head length predicted head length:\\[e = y - \\hat{y} = 85.1 -  86.3 = -1.2 mm\\]model’s error \\(e = -1.2\\) mm, close \nvisual estimate \\(-1\\) mm. negative residual indicates linear model overpredicted head length particular possum.model underestimates observation, residual positive negative? overestimates observation?52Compute residuals observation marked green diamond, \\((85.0, 98.6)\\), observation marked yellow triangle, \\((95.5, 94.0)\\), figure using linear relationship \\(\\hat{y} = 43 + 0.57x\\).53Residuals helpful evaluating well linear model fits data set.\noften display residual plot one shown Figure 6.9 regression line Figure 6.8.\nresiduals plotted fitted values \\(x\\)-axis vertical coordinate residual.\ninstance, point \\((85.0, 98.6)\\) (marked green diamond) predicted value 91.45 mm residual 7.15 mm, residual plot placed \\((91.45, 7.15)\\).\nCreating residual plot sort like tipping scatterplot regression line horizontal.\nFigure 6.9: Residual plot model predicting head length total length brushtail possums.\nOne purpose residual plots identify characteristics patterns still apparent data fitting model.\nFigure 6.10 shows three scatterplots linear models first row residual plots second row. Can identify patterns remaining residuals?first data set (first column), residuals show obvious patterns.\nresiduals appear scattered randomly around dashed line represents 0.second data set shows pattern residuals.\ncurvature scatterplot, obvious residual plot.\nuse straight line model data. Instead, advanced technique used.last plot shows little upwards trend, residuals also show obvious patterns.\nreasonable try fit linear model data.\nHowever, unclear whether slope parameter statistically discernible zero.\npoint estimate slope parameter, labeled \\(b_1\\), zero, might wonder just due chance.\naddress sort scenario Chapter 21.\nFigure 6.10: Sample data best fitting lines (top row) corresponding residual plots (bottom row).\n","code":""},{"path":"explore-regression.html","id":"describing-linear-relationships-with-correlation","chapter":"6 Correlation and regression","heading":"6.1.4 Describing linear relationships with correlation","text":"’ve seen plots strong linear relationships others weak linear relationships.\nuseful quantify strength linear relationships statistic.Correlation: strength direction linear relationship.Correlation always takes values -1 1, summary statistic describes strength (magnitude) direction (sign) linear relationship two variables. denote correlation \\(R\\) \\(r\\).can compute correlation using formula, just sample mean standard deviation.\nformula rather complex54,\nlike statistics, generally perform calculations computer calculator.\nFigure 6.11 shows eight plots corresponding correlations. relationship perfectly linear correlation either -1  1.  relationship strong positive, correlation near +1.  strong negative, near -1.  apparent linear relationship variables, correlation near zero.\nFigure 6.11: Sample scatterplots correlations. first row shows variables positive relationshiop, represented trend right. second row shows variables negative trend, large value one variable associated low value .\ncorrelation intended quantify strength direction linear trend.\nNonlinear trends, even strong, sometimes produce correlations reflect strength relationship; see three examples \nFigure 6.12.\nFigure 6.12: Sample scatterplots correlations. case, strong relationship variables, However, relationship nonlinear, correlation relatively weak.\nstraight line good fit data sets represented Figure 6.12.\nTry drawing nonlinear curves plot.\ncreate curve , describe important  fit.55","code":""},{"path":"explore-regression.html","id":"least-squares-regression","chapter":"6 Correlation and regression","heading":"6.2 Least squares regression","text":"Fitting linear models eye open criticism since based individual’s preference. section, use least squares regression rigorous approach.","code":""},{"path":"explore-regression.html","id":"gift-aid-for-freshman-at-elmhurst-college","chapter":"6 Correlation and regression","heading":"6.2.1 Gift aid for freshman at Elmhurst College","text":"section considers family income gift aid data random sample fifty students freshman class Elmhurst College Illinois.\nGift aid financial aid need paid back, opposed loan.\nscatterplot data shown Figure 6.13 along two linear fits.\nlines follow negative trend data; students higher family incomes tended lower gift aid university.\nFigure 6.13: Gift aid family income random sample 50 freshman students Elmhurst College, shown least squares line (solid line) line fit minimizing sum residual magnitudes (dashed line).\ncorrelation positive negative Figure 6.13?56","code":""},{"path":"explore-regression.html","id":"an-objective-measure-for-finding-the-best-line","chapter":"6 Correlation and regression","heading":"6.2.2 An objective measure for finding the best line","text":"begin thinking mean “best”. Mathematically, want line small residuals.\nfirst option may come mind minimize sum residual magnitudes:\\[|e_1| + |e_2| + \\dots + |e_n|\\]accomplish computer program.\nresulting dashed line shown Figure 6.13 demonstrates fit can quite reasonable.\nHowever, common practice choose line minimizes sum squared residuals:\\[e_{1}^2 + e_{2}^2 + \\dots + e_{n}^2\\]line minimizes least squares criterion represented solid line Figure 6.13.\ncommonly called least squares line.\nfollowing three possible reasons choose option instead trying minimize sum residual magnitudes without squaring:commonly used method.Computing least squares line widely supported statistical software.many applications, residual twice large another residual twice bad. example, 4 usually twice bad 2. Squaring residuals accounts discrepancy.first two reasons largely tradition convenience; last reason explains least squares criterion typically helpful.57","code":""},{"path":"explore-regression.html","id":"finding-and-interpreting-the-least-squares-line","chapter":"6 Correlation and regression","heading":"6.2.3 Finding and interpreting the least squares line","text":"Elmhurst data, write equation linear regression model \n\\[aid = \\beta_0 + \\beta_{1}\\times \\textit{family_income} + \\epsilon.\\]\nmodel equation set predict gift aid based student’s family income, useful students considering Elmhurst.\ntwo unknown values \\(\\beta_0\\) \\(\\beta_1\\) parameters linear regression model.least squares regression line, computed based observed data, provides estimates parameters \\(\\beta_0\\) \\(\\beta_1\\):\n\\[\\widehat{aid} = b_0 + b_{1}\\times \\textit{family_income}.\\]\npractice, estimation done using computer way estimates, like sample mean, can estimated using computer calculator.dataset data stored called elmhurst.\nfirst 5 rows dataset given Table 6.1.\nTable 6.1: First five rows elmhurst dataset.\ncan see family income recorded variable called family_income gift aid university recorded variable called gift_aid.\nnow, won’t worry price_paid variable.\nalso note data 2011-2012 academic year, monetary amounts given $1,000s, .e., family income first student data shown Table 6.1 $92,900 received gift aid $21,700. (data source states numbers rounded nearest whole dollar.)Using data, can estimate linear regression line fitting linear model data lm() function R.model output tells us intercept approximately 24.319 slope approximately -0.043.values mean?\nInterpreting parameters regression model often one important steps analysis.intercept slope estimates Elmhurst data \\(b_0\\) = 24.319 \\(b_1\\) = -0.043.\nnumbers really mean?Interpreting slope parameter helpful almost application.\nadditional $1,000 family income, expect student receive net difference 1,000 \\(\\times\\) (-0.0431) = -$43.10 aid average, .e., $43.10 less.\nNote higher family income corresponds less aid coefficient family income negative model.\nmust cautious interpretation: real association, interpret causal connection variables data observational.\n, increasing student’s family income may cause student’s aid drop. (reasonable contact college ask relationship causal, .e., Elmhurst College’s aid decisions partially based students’ family income.) appropriate interpretation : additional $1,000 family income associated estimated decrease $43.10 aid average.estimated intercept \\(b_0\\) = 24.319 describes average aid student’s family income.\nmeaning intercept relevant application since family income students Elmhurst  $0.\napplications, intercept may little practical value observations \\(x\\) near zero.Interpreting parameters estimated least squares.slope describes estimated difference \\(y\\) variable explanatory variable \\(x\\) case happened one unit larger.intercept describes average outcome \\(y\\) \\(x=0\\) linear model valid way \\(x=0\\), many applications case.Suppose high school senior considering Elmhurst College.\nCan simply use linear equation estimated calculate financial aid university?may use estimate, though qualifiers approach important.\nFirst, data come one freshman class, way aid determined university may change year year.\nSecond, equation provide imperfect estimate.\nlinear equation good capturing trend data, individual student’s aid perfectly predicted.Statistical software usually used compute least squares line typical output generated result fitting regression models looks like one shown Table 6.2.\nnow focus first column output, lists \\({b}_0\\) \\({b}_1\\).\nChapter 21 dive deeper remaining columns give us information accurate precise values intercept slope calculated sample 50 students estimating population parameters intercept slope students.\nTable 6.2: Summary least squares fit Elmhurst data.\n","code":"\nlm(gift_aid ~ family_income, data = elmhurst)\n#> \n#> Call:\n#> lm(formula = gift_aid ~ family_income, data = elmhurst)\n#> \n#> Coefficients:\n#>   (Intercept)  family_income  \n#>       24.3193        -0.0431"},{"path":"explore-regression.html","id":"calculating-the-least-squares-regression-line-using-summary-statistics-special-topic","chapter":"6 Correlation and regression","heading":"6.2.4 Calculating the least squares regression line using summary statistics (special topic)","text":"alternative way calculating values intercept slope least squares line manual calculations using formulas.\nmethod commonly used practicing statisticians data scientists, useful work first time ’re learning least squares line modeling general.\nCalculating values hand leverages two properties least squares line:slope least squares line can estimated \\[b_1 = \\frac{s_y}{s_x} R \\]\\(R\\) correlation two variables, \\(s_x\\) \\(s_y\\) sample standard deviations explanatory variable response, respectively.\\(\\bar{x}\\) sample mean explanatory variable \\(\\bar{y}\\) sample mean vertical variable, point \\((\\bar{x}, \\bar{y})\\) least squares line.Table 6.3 shows sample means family income gift aid $101,780 $19,940, respectively.\nplot point \\((102, 19.9)\\) Figure 6.13 verify falls least squares line (solid line).\nTable 6.3: Summary statistics family income gift aid.\nNext, formally find point estimates \\(b_0\\) \\(b_1\\) parameters \\(\\beta_0\\) \\(\\beta_1\\).Using summary statistics Table 6.3, compute slope regression line gift aid family income.Compute slope using summary statistics Table 6.3:\\[b_1 = \\frac{s_y}{s_x} r = \\frac{5.46}{63.2}(-0.499) = -0.0431\\]might recall form line math class, can use find model fit, including estimate \\(b_0\\). Given slope line point line, \\((x_0, y_0)\\), equation line can written \\[y - y_0 = slope\\times (x - x_0)\\]Identifying least squares line summary statistics.identify least squares line summary statistics:Estimate slope parameter, \\(b_1 = (s_y / s_x) R\\).Noting point \\((\\bar{x}, \\bar{y})\\) least squares line, use \\(x_0 = \\bar{x}\\) \\(y_0 = \\bar{y}\\) point-slope equation: \\(y - \\bar{y} = b_1 (x - \\bar{x})\\).Simplify equation, reveal \\(b_0 = \\bar{y} - b_1 \\bar{x}\\).Using point (102, 19.9) sample means slope estimate \\(b_1 = -0.0431\\), find least-squares line predicting aid based family income.Apply point-slope equation using \\((102, 19.9)\\) slope \\(b_1 = -0.0431\\):\\[\\begin{aligned}\ny - y_0  &= b_1 (x - x_0) \\\\\ny - 19.9 &= -0.0431(x - 102)\n\\end{aligned}\\]Expanding right side adding 19.9 side, equation simplifies:\\[\\begin{aligned}\n\\widehat{aid} = 24.3 - 0.0431 \\times \\textit{family_income}\n\\end{aligned}\\]replaced \\(y\\) \\(\\widehat{aid}\\) \\(x\\) family_income put equation context.\nfinal equation always include “hat” variable predicted, whether generic “\\(y\\)” named variable like “\\(aid\\)”.","code":""},{"path":"explore-regression.html","id":"extrapolation-is-treacherous","chapter":"6 Correlation and regression","heading":"6.2.5 Extrapolation is treacherous","text":"blizzards hit East Coast winter, proved satisfaction global warming fraud. snow freezing cold. alarming trend, temperatures spring risen. Consider : February \\(6^{th}\\) 10 degrees. Today hit almost 80. rate, August 220 degrees. clearly folks climate debate rages .58Stephen Colbert\nApril 6th, 2010Linear models can used approximate relationship two variables. However, models real limitations.\nLinear regression simply modeling framework.\ntruth almost always much complex simple line.\nexample, know data outside limited window behave.Use model \\(\\widehat{aid} = 24.3 - 0.0431 \\times \\textit{family_income}\\) estimate aid another freshman student whose family income $1 million.want calculate aid family $1 million income.\nNote model, represented 1,000 since data $1,000s.\\[24.3 - 0.0431 \\times 1000 = -18.8 \\]model predicts student -$18,800 aid (!).\nHowever, Elmhurst College offer negative aid select students pay extra top tuition attend.Applying model estimate values outside realm original data called extrapolation.\nGenerally, linear model approximation real relationship two variables.\nextrapolate, making unreliable bet approximate linear relationship valid places analyzed.","code":""},{"path":"explore-regression.html","id":"describing-the-strength-of-a-fit","chapter":"6 Correlation and regression","heading":"6.2.6 Describing the strength of a fit","text":"evaluated strength linear relationship two variables earlier using correlation, \\(R\\). However, common explain strength linear fit using \\(R^2\\), called R-squared.\nprovided linear model, might like describe closely data cluster around linear fit.\\(R^2\\) linear model describes amount variation response explained least squares line.\nexample, consider Elmhurst data, shown Figure 6.13.\nvariance response variable, aid received, \\(s_{aid}^2 \\approx 29.8\\) million.\nHowever, apply least squares line, model reduces uncertainty predicting aid using student’s family income.\nvariability residuals describes much variation remains using model: \\(s_{_{RES}}^2 \\approx 22.4\\) million.\nshort, reduction \n\\[\\frac{s_{aid}^2 - s_{_{RES}}^2}{s_{aid}^2}\n  = \\frac{29800 - 22400}{29800}\n  = \\frac{7500}{29800}\n  \\approx 0.25\\]\n25% data’s variation using information family income predicting aid using linear model.\ncorresponds exactly R-squared value:\\[R = -0.499 \\rightarrow R^2 = 0.25\\]\nsquared correlation coefficient, \\(R^2\\), also called coefficient determination.Coefficient determination: proportion variability response explained model.Since \\(R\\) always \\(-1\\) \\(1\\), \\(R^2\\) always \\(0\\) \\(1\\). statistic called coefficient determination measures proportion variation response variable, \\(y\\), can explained linear model predictor \\(x\\).Examine scatterplot head length (mm) versus total length (cm) possums Figure 6.6.\ncorrelation two variables \\(R = 0.69\\).\nFind interpret coefficient determination.find \\(R^2\\), square correlation: \\(R^2 = (0.69)^2 = 0.48\\).\ntells us 48% variation possum head length can explained total length.\nvisualized Figure 6.14.\nFigure 6.14: 104 possums, range head lengths 103 \\(-\\) 83 = 20 mm. However, among possums total length (e.g., 85 cm), range head lengths reduced 10 mm, 50% reduction, matches \\(R^2 = 0.48\\), 48%.\nlinear model strong negative relationship correlation -0.97, much variation response explained explanatory variable?59More generally, \\(R^2\\) can calculated ratio measure variability around line divided measure total variability.Sums squares measure variability \\(y\\).can measure variability \\(y\\) values far tend fall mean, \\(\\bar{y}\\). define value total sum squares,\\[\nSST = (y_1 - \\bar{y})^2 + (y_2 - \\bar{y})^2 + \\cdots + (y_n - \\bar{y})^2.\n\\]Left-variability \\(y\\) values know \\(x\\) can measured sum squared errors, sum squared residuals60,\\[\nSSE = (y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\cdots + (y_n - \\hat{y}_n)^2 = e_{1}^2 + e_{2}^2 + \\dots + e_{n}^2\n\\]coefficient determination can calculated \\[\nR^2 = \\frac{SST - SSE}{SST} = 1 - \\frac{SSE}{SST}\n\\]Among 104 possums, total variability head length (mm) \\(SST = 1315.2\\)61. sum squared residuals \\(SSE = 687.0\\). Find \\(R^2\\).Since know \\(SSE\\) \\(SST\\), can calculate \\(R^2\\) \\[\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{687.0}{1315.2} = 0.48,\n\\]\nvalue found squared correlation: \\(R^2 = (0.69)^2 = 0.48\\).","code":""},{"path":"explore-regression.html","id":"categorical-predictor-two-levels","chapter":"6 Correlation and regression","heading":"6.2.7 Categorical predictors with two levels (special topic)","text":"Categorical variables also useful predicting outcomes.\nconsider categorical predictor two levels (recall level category).\n’ll consider Ebay auctions video game, Mario Kart Nintendo Wii, total price auction condition game recorded. want predict total price based game condition, takes values used new.mariokart data can found openintro package.plot auction data shown Figure 6.15.\nNote original dataset contains Mario Kart games sold prices $100 analysis limited focus 141 Mario Kart games sold $100.\nFigure 6.15: Total auction prices video game Mario Kart, divided used (\\(x = 0\\)) new (\\(x = 1\\)) condition games. least squares regression line also shown.\nincorporate game condition variable regression equation, must convert categories numerical form.\nusing indicator variable called condnew, takes value 1 game new 0 game used.\nUsing indicator variable, linear model may written \\[\\widehat{price} = \\beta_0 + \\beta_1 \\times condnew\\]Clean population model equations - add error remove hat.parameter estimates given Table 6.4.\nTable 6.4: Least squares ression summary final auction price condition game.\nUsing values Table 6.4, model equation can summarized \\[\\widehat{price} = 42.871 + 10.90 \\times condnew\\]Interpret two parameters estimated model price Mario Kart eBay auctions.\nintercept estimated price condnew takes value 0, .e. game used condition.\n, average selling price used version game $42.87.slope indicates , average, new games sell $10.90 used games.Interpreting model estimates categorical predictors.estimated intercept value response variable first category (.e. category corresponding indicator value  0).\nestimated slope average change response variable two categories.’ll elaborate topic Chapter 7, examine influence many predictor variables simultaneously using multiple regression.","code":""},{"path":"explore-regression.html","id":"outliers-in-regression","chapter":"6 Correlation and regression","heading":"6.3 Outliers in linear regression","text":"section, identify criteria determining outliers important influential.\nOutliers regression observations fall far cloud points.\npoints especially important can strong influence least squares line.","code":""},{"path":"explore-regression.html","id":"types-of-outliers","chapter":"6 Correlation and regression","heading":"6.3.1 Types of outliers","text":"three plots shown Figure 6.16 along least squares line residual plots.\n scatterplot residual plot pair, identify outliers note influence least squares line.\nRecall outlier point doesn’t appear belong vast majority points.: one outlier far points, though appears slightly influence  line.: one outlier far points, though appears slightly influence  line.B: one outlier right, though quite close least squares line, suggests wasn’t influential.B: one outlier right, though quite close least squares line, suggests wasn’t influential.C: one point far away cloud, outlier appears pull least squares line right; examine line around primary cloud doesn’t appear fit  well.C: one point far away cloud, outlier appears pull least squares line right; examine line around primary cloud doesn’t appear fit  well.\nFigure 6.16: Three plots, least squares line residual plot. data sets least one outlier.\nthree plots shown Figure 6.17 along least squares line residual plots.\nprevious exercise,  scatterplot residual plot pair, identify outliers note influence least squares line.\nRecall outlier point doesn’t appear belong vast majority points.D: primary cloud small secondary cloud four outliers. secondary cloud appears influencing line somewhat strongly, making least square line fit poorly almost everywhere. might interesting explanation dual clouds, something investigated.D: primary cloud small secondary cloud four outliers. secondary cloud appears influencing line somewhat strongly, making least square line fit poorly almost everywhere. might interesting explanation dual clouds, something investigated.E: obvious trend main cloud points outlier right appears largely control slope least squares line.E: obvious trend main cloud points outlier right appears largely control slope least squares line.F: one outlier far cloud. However, falls quite close least squares line appear influential.F: one outlier far cloud. However, falls quite close least squares line appear influential.\nFigure 6.17: Three plots, least squares line residual plot. data sets least one outlier.\nExamine residual plots Figures 6.16 6.17.\nprobably find trend main clouds  Plots C, D, E.\ncases, outliers influenced slope least squares lines.\n Plot E, data clear trend assigned line large trend simply due one outlier (!).Leverage.Points fall horizontally away center cloud tend pull harder line, call points high leverage.Points fall horizontally far line points high leverage; points can strongly influence slope least squares line.\none high leverage points appear actually invoke influence slope line – Plots C, D, E Figures 6.16 6.17 – call influential point.Influential point.point influential , fitted line without , influential point unusually far least squares line.\nInfluential points tend pull slope line seen fit regression line without .tempting remove outliers. Don’t without good reason.\nModels ignore exceptional (interesting) cases often perform poorly.\ninstance, financial firm ignored largest market swings – “outliers” – soon go bankrupt making poorly thought-investments.","code":""},{"path":"explore-regression.html","id":"chp6-review","chapter":"6 Correlation and regression","heading":"6.4 Chapter review","text":"","code":""},{"path":"explore-regression.html","id":"summary-4","chapter":"6 Correlation and regression","heading":"Summary","text":"Throughout chapter, nuances simple linear regression model described.\nlearned create regression model two quantitative variables.\nresiduals linear model important metric used understand well model fits; high leverage points, influential points, types outliers can impact fit model.\nCorrelation measure strength direction linear relationship two variables, without specifying variable explanatory outcome.\nFuture chapters focus generalizing linear model sample data claims population interest.","code":""},{"path":"explore-regression.html","id":"data-visualization-summary","chapter":"6 Correlation and regression","heading":"Data visualization summary","text":"Now looked summarize visualize one two variables either type (categorical quantitative), can organize visualization methods single decision tree.\nFigure 6.18 presents decision tree deciding type plot appropriate given number types variables. next chapter, ’ll look exploratory data analysis methods two variables.\nFigure 6.18: Decision tree determining appropriate plot given number variables types.\n","code":""},{"path":"explore-regression.html","id":"summary-measures","chapter":"6 Correlation and regression","heading":"Summary measures","text":"Though summary measures covered later chapters, Table 6.5 provides comprehensive summary measures according type(s) variable(s) summarize.\nTable 6.5: Summary measures different types variables covered textbook Sections appear. binary variable categorical variable two categories.\n","code":""},{"path":"explore-regression.html","id":"notation-summary","chapter":"6 Correlation and regression","heading":"Notation summary","text":"field statistics, summary measures summarize sample data called statistics. Numbers summarize entire population called parameters. can remember\ndistinction looking first letter term:Statistics summarize Samples.Parameters summarize Populations.typically use Roman letters symbolize statistics (e.g., \\(\\bar{x}\\), \\(\\hat{p}\\)), Greek letters symbolize parameters (e.g., \\(\\mu\\), \\(\\pi\\)).","code":""},{"path":"explore-regression.html","id":"terms-5","chapter":"6 Correlation and regression","heading":"Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"explore-regression.html","id":"key-ideas-4","chapter":"6 Correlation and regression","heading":"Key ideas","text":"Two variables associated behavior one variable depends value variable. two quantitative variables, occurs trend apparent scatterplot. trend linear non-zero slope, say two quantitative variables correlated. Recall Chapter 1, association imply causation!Two variables associated behavior one variable depends value variable. two quantitative variables, occurs trend apparent scatterplot. trend linear non-zero slope, say two quantitative variables correlated. Recall Chapter 1, association imply causation!least squares regression line represents predicted value response variable, \\(y\\), given \\(x\\)-value. Since actual observed values response variable denoted \\(y\\), denote predicted values \\(\\hat{y}\\).least squares regression line represents predicted value response variable, \\(y\\), given \\(x\\)-value. Since actual observed values response variable denoted \\(y\\), denote predicted values \\(\\hat{y}\\).slope regression line predicted change response variable associated one-unit increase \\(x\\).slope regression line predicted change response variable associated one-unit increase \\(x\\).\\(y\\)-intercept regression line predicted value response variable \\(x = 0\\). collected data include \\(x\\)-values near zero, prediction example extrapolation — using regression line make predictions outside range observed data.\\(y\\)-intercept regression line predicted value response variable \\(x = 0\\). collected data include \\(x\\)-values near zero, prediction example extrapolation — using regression line make predictions outside range observed data.regression line provides predicted response value, may may close value actually observe. numerical measure “prediction error” residual = (observed \\(y\\)-value) \\(-\\) (predicted \\(\\hat{y}\\)-value); , distance observed \\(y\\)-value regression line. Positive residuals indicate observed \\(y\\)-value regression line (regression model underestimated response); negative residuals indicate observed \\(y\\)-value regression line (regression model overestimated response).regression line provides predicted response value, may may close value actually observe. numerical measure “prediction error” residual = (observed \\(y\\)-value) \\(-\\) (predicted \\(\\hat{y}\\)-value); , distance observed \\(y\\)-value regression line. Positive residuals indicate observed \\(y\\)-value regression line (regression model underestimated response); negative residuals indicate observed \\(y\\)-value regression line (regression model overestimated response).correlation coefficient (just “correlation”) two quantitative variables, denoted \\(r\\) \\(R\\), number \\(-1\\) \\(1\\) measures strength (magnitude) direction (sign) linear relationship two variables. Correlation useful two quantitative variables linearly associated.correlation coefficient (just “correlation”) two quantitative variables, denoted \\(r\\) \\(R\\), number \\(-1\\) \\(1\\) measures strength (magnitude) direction (sign) linear relationship two variables. Correlation useful two quantitative variables linearly associated.coefficient determination, R-squared number \\(0\\) \\(1\\) measures proportion variation response variable can explained knowing \\(x\\)-value. can computed squaring correlation coefficient (\\(r^2\\)), using sample variances:\n\\[\nR^2 = \\frac{s^2_{y}-s^2_{RES}}{s^2_{y}},\n\\]\n\\(s^2_{y}\\) sample variance observed \\(y\\)-values, \\(s^2_{RES}\\) sample variance residuals.coefficient determination, R-squared number \\(0\\) \\(1\\) measures proportion variation response variable can explained knowing \\(x\\)-value. can computed squaring correlation coefficient (\\(r^2\\)), using sample variances:\n\\[\nR^2 = \\frac{s^2_{y}-s^2_{RES}}{s^2_{y}},\n\\]\n\\(s^2_{y}\\) sample variance observed \\(y\\)-values, \\(s^2_{RES}\\) sample variance residuals.outlier point follow general pattern data. influential point outlier tends pull slope line (correlation) seen fit regression line without . observation \\(x\\)-value far away center observed \\(x\\)-values said high leverage, potential influential point.outlier point follow general pattern data. influential point outlier tends pull slope line (correlation) seen fit regression line without . observation \\(x\\)-value far away center observed \\(x\\)-values said high leverage, potential influential point.","code":""},{"path":"explore-mult-reg.html","id":"explore-mult-reg","chapter":"7 Multivariable models","heading":"7 Multivariable models","text":"principles simple linear regression lay foundation sophisticated regression models used wide range challenging settings.\nchapter, explore idea “multivariable thinking” – investigating multiple variables interact response variable – examples.\nMultiple regression, introduces possibility one predictor linear model, logistic regression, technique predicting categorical outcomes two levels, presented special topics covered course.","code":""},{"path":"explore-mult-reg.html","id":"gapminder-world","chapter":"7 Multivariable models","heading":"7.1 Gapminder world","text":"Gapminder “fact tank” uses publicly available world data produce data visualizations teaching resources global development. use excerpt data explore relationships among world health metrics across countries regions years 1952 2007.gapminder data can found gapminder package.First, let’s look relationship Gross Domestic Product (GDP) per capita (measure wealth country) Life Expectancy (years) year 2007 Figure 7.1.\nFigure 7.1: Scatterplot displaying relationship Life Expectancy GDP per capita year 2007. Note GDP per capita plotted log scale. dot represent?62\n\nFigure 7.2: Scatterplot displaying relationship Life Expectancy GDP per capita region year 2007. Note GDP per capita plotted log scale. Regression lines continent added.\nrelationship GDP per capita life expectancy differ across regions world?Yes. Looking Figure 7.2, five regression lines differing slopes, telling us estimated change life expectancy given increase GDP per capita differs across countries. Americas Oceania, life expectancy seems rise faster GDP per capita three regions. case, say GDP per capita interacts continent relationship life expectancy.Interaction two explanatory variables.relationship explanatory variable \\(x\\) response variable \\(y\\) changes different levels another variable \\(z\\), say \\(x\\) \\(z\\) interact relationship \\(y\\).\\(x\\) \\(y\\) quantitative, \\(z\\) categorical, Figure 7.2 – \\(x\\) = GDP per capita, \\(y\\) = life expectancy, \\(z\\) = continent – different regression lines level \\(z\\) parallel slopes, say \\(x\\) \\(z\\) interact. slopes parallel, interaction exists \\(x\\) \\(z\\).far, ’ve explored relationships three variables, visualize relationships five variables?63Let’s add another variable plot – population. aesthetic visual property objects plot. variable mapped aesthetic. possible aesthetics whether used quantitative categorical variables listed Table 7.1.\nTable 7.1: Examples aesthetics types variables mapped aesthetics.\nFigure 7.3, quantitative variables GDP per capita, life expectancy, population mapped aesthetics: position \\(x\\)-axis, position \\(y\\)-axis, population, respectively. categorical variable Region mapped color. Explore individual countries hovering points.\nFigure 7.3: Scatterplot displaying relationship four variables year 2007: GDP per capita (x-axis), Life Expectancy (y-axis), Population (size), Region (color).]\npattern compare happening 1952 (see Figure 7.4)?\nFigure 7.4: Scatterplot displaying relationship four variables year 1952: GDP per capita (x-axis), Life Expectancy (y-axis), Population (size), Region (color).\ncan visualize relationships among four variables plots (three quantitative variables x- y-axes size, categorical variable color). even add fifth variable using another aesthetic, like using shape represent popular religion country. visualize happens across time? Hans Rosling answer dynamic visualization. Click image watch.","code":""},{"path":"explore-mult-reg.html","id":"simpsons-paradox-revisited","chapter":"7 Multivariable models","heading":"7.2 Simpson’s paradox revisited","text":"Simpson’s Paradox introduced Section 4.4 example race capital punishment. example, three variables interest categorical. section, present another example paradox using three quantitative variables.1993, respected political essayist George , wrote following criticism spending public education United States.“10 states lowest per pupil spending included four – North Dakota, South Dakota, Tennessee, Utah – among 10 states top SAT scores. one 10 states highest per pupil expenditures – Wisconsin – among 10 states highest SAT scores. New Jersey highest per pupil expenditures, astonishing $10,561, teachers’ unions elsewhere try use negotiating benchmark. New Jersey’s rank regarding SAT scores? Thirty-ninth… fact quality schools… [fails correlate] education appropriations effect teacher unions’ insistence money crucial variable.”— George F. , September 12, 1993, “Meaningless Money Factor,” Washington Post, C7.George based claim state expenditures, average SAT scores, education-based variables. data data set SAT64, first six rows displayed Table 7.2. Variables data set described Table 7.3\nTable 7.2: Six rows SAT data set.\n\nTable 7.3: Variables descriptions SAT data set.\nMr. claims expenditure per pupil negative correlation average SAT scores across states. true? Indeed, correlation expend sat equal \\(r\\) = -0.381, relationship two variables shown Figure 7.5. Hover point view data particular State.\nFigure 7.5: Expenditure per pupil average daily attendance public elementary secondary schools ($1000) verses average SAT score 50 states plus District Columbia school year 1994-1995.\nmay seem surprising, remember – observational data. conclude, George , decreasing expenditures increase SAT scores. fact, one clear confounding variable data: percentage eligible students taking SAT.confounding variables may present study? determine whether variable confounding relationship school expenditures SAT scores?states time data collected, common take ACT SAT. students states, wanted go state school, need take ACT. However, wanted attend college another state, might take SAT. Thus, percent students taking SAT state, frac, confounding variable.order frac confounding, needs associated explanatory variable, expend, well response variable, sat. One look scatterplots correlation frac expend, frac sat, determine frac confounding relationship expend sat.Scatterplots expend versus frac sat versus frac displayed Figure 7.6. correlation expend frac 0.593, correlation sat frac -0.887.\nFigure 7.6: Expenditure per pupil average daily attendance public elementary secondary schools ($1000) average SAT score plotted percent students taking SAT 50 states plus District Columbia school year 1994-1995.\nNow ’ve determined frac confounding variable, let’s examine modifies relationship expend sat. Since hard visualize three quantitative variables – 3-D scatterplots difficult visualize – let’s bin variable frac three groups. States fewer 15% eligible students taking SAT classified low percentage. States 15% - 55% eligible students taking SAT classified medium states 55% eligible students taking SAT called high. Next, fit separate regression lines group. model shown Figure 7.7.\nFigure 7.7: Average SAT score plotted school expenditures per pupil, categorized Low (\\(<\\) 15%), Medium (15-55%), High (\\(>\\) 55%) percent students taking SAT.\nFigure 7.7 demonstrates overall negative correlation SAT scores expenditures disappears, even turns slightly positive, examine relationship within states similar fractions students taking SAT.data exhibit Simpson’s Paradox?65","code":""},{"path":"explore-mult-reg.html","id":"regression-multiple-predictors","chapter":"7 Multivariable models","heading":"7.3 Multiple regression (special topic)","text":"principles simple linear regression lay foundation sophisticated regression models used wide range challenging settings.\nsection, explore multiple regression, introduces possibility one predictor linear model.Multiple regression extends simple two-variable regression case still one response many predictors (denoted \\(x_1\\), \\(x_2\\), \\(x_3\\), ...). method motivated scenarios many variables may simultaneously connected output.consider data loans peer--peer lender, Lending Club, data set first encountered Chapter 1.\nloan data includes terms loan well information borrower.\noutcome variable like better understand interest rate assigned loan.\ninstance, characteristics held constant, matter much debt someone already ? matter income verified?\nMultiple regression help us answer questions.data set includes results 10,000 loans, ’ll looking subset available variables, new saw earlier chapters.\nfirst six observations data set shown Table 7.4, descriptions variable shown Table 7.5.\nNotice past bankruptcy variable (bankruptcy) indicator variable, takes value 1 borrower past bankruptcy record 0 .\nUsing indicator variable place category name allows variables \ndirectly used regression.\nTwo variables categorical (verified_income issue_month), can take one different non-numerical values; ’ll discuss handled model Section 7.3.1.data can found openintro package: loans_full_schema. Based data dataset created new variables: credit_util calculated total credit utilized divided total credit limit bankruptcy turns number bankruptcies indicator variable (0 bankrupties 1 least 1 bankruptcies). refer modified dataset loans.\nTable 7.4: First six rows loans_full_schema data set.\n\nTable 7.5: Variables descriptions loans data set.\n","code":""},{"path":"explore-mult-reg.html","id":"ind-and-cat-predictors","chapter":"7 Multivariable models","heading":"7.3.1 Indicator and categorical predictors","text":"Let’s start fitting linear regression model interest rate single predictor indicating whether person bankruptcy record:\\[\\widehat{\\texttt{interest_rate}} = 12.33 + 0.74 \\times bankruptcy\\]Results model shown Table 7.6.\nTable 7.6: Summary linear model predicting interest rate based whether borrower bankruptcy record. Degrees freedom model 9998.\nInterpret coefficient past bankruptcy variable model. coefficient significantly different 0?variable takes one two values: 1 borrower bankruptcy history 0 otherwise. slope 0.74 means model predicts 0.74%\nhigher interest rate borrowers bankruptcy \nrecord.\n(See Section 6.2.7 review interpretation two-level categorical predictor variables.)\nExamining regression output Table 7.6, can see p-value close zero, indicating strong evidence coefficient different zero using simple one-predictor model.Suppose fit model using 3-level categorical variable, verified_income.\noutput software shown Table 7.7.\nregression output provides multiple rows variable.\nrow represents relative difference level verified_income.\nHowever, missing one levels: Verified.\nmissing level called reference level represents default level levels measured .\nTable 7.7: Summary linear model predicting interest rate based whether borrower’s income source amount verified. predictor three levels, results 2 rows regression output.\nwrite equation regression model?equation regression model may written model two predictors:\\[\n\\begin{align*}\n\\widehat{\\texttt{interest_rate}} &= 11.10 + 1.42 \\times \\text{verified_income}_{\\text{Source Verified}}\\\\\n&\\qquad\\ + 3.25 \\times \\text{verified_income}_{\\text{Verified}}\n\\end{align*}\n\\]use notation \\(\\text{variable}_{\\text{level}}\\) represent indicator variables categorical variable takes particular value.\nexample, \\(\\text{verified_income}_{\\text{Source Verified}}\\) take value 1 \nloan, take value 0 otherwise.\nLikewise, \\(\\text{verified_income}_{\\text{Verified}}\\) take value 1 took \nvalue verified 0 took value.notation \\(\\text{variable}_{\\text{level}}\\) may feel bit confusing.\nLet’s figure use equation level verified_income variable.Using model predicting interest rate income verification type, compute average interest rate borrowers whose income source amount unverified.verified_income takes value Verified, indicator functions equation linear model set 0:\\[\\begin{align*}\n\\widehat{\\texttt{interest_rate}} &= 1.10 + 1.42 \\times 0 \\\\\n&\\qquad\\ + 3.25 \\times 0 \\\\\n&= 11.10\n\\end{align*}\\]average interest rate borrowers 11.1%.\nlevel coefficient reference value, indicators levels variable drop .Using model predicting interest rate income verification type, compute average interest rate borrowers whose income source amount unverified.verified_income takes value Source Verified, corresponding variable takes value 1 (\\(\\text{verified_income}_{\\text{Verified}}\\)) 0:\\[\\widehat{\\texttt{interest_rate}} = 11.10 + 1.42 \\times 1 + 3.25 \\times 0 = 12.52\\]average interest rate borrowers 12.52%.Compute average interest rate borrowers whose income source amount verified.66Predictors several categories.fitting regression model categorical variable \\(k\\) levels \\(k > 2\\), software provide coefficient \\(k - 1\\) levels.\nlast level receive coefficient, , coefficients listed levels considered relative reference level.Interpret coefficients model.67The higher interest rate borrowers verified income source amount surprising.\nIntuitively, ’d think loan look less risky borrower’s income verified.\nHowever, note situation may complex, may confounding variables didn’t account .\nexample, perhaps lender require borrowers poor credit verify income.\n, verifying income data set might signal concerns borrower rather reassurance borrower pay back loan.\nreason, borrower deemed higher risk, resulting higher interest rate.\n(confounding variables might explain counter-intuitive relationship suggested model?)much larger interest rate expect borrower verified income source amount vs borrower whose income source verified?68","code":""},{"path":"explore-mult-reg.html","id":"many-predictors-in-a-model","chapter":"7 Multivariable models","heading":"7.3.2 Many predictors in a model","text":"world complex, can helpful consider many factors statistical modeling.\nexample, might like use full context borrower predict interest rate receive rather using single variable.\nstrategy used multiple regression.\nremain cautious making causal interpretations using multiple regression observational data, models common first step gaining insights providing evidence causal connection.want fit model accounts past bankruptcy whether borrower income source amount verified, simultaneously accounts variables loans data set: verified_income, debt_to_income, credit_util, bankruptcy, term, issue_month, credit_checks.\\[\\begin{align*}\n\\widehat{\\texttt{interest_rate}}\n    &= b_0 +\n        b_1\\times \\texttt{verified_income}_{\\texttt{Source Verified}} \\\\\n    &\\qquad\\  +\n        b_2\\times \\texttt{verified_income}_{\\texttt{Verified}} \\\\\n    &\\qquad\\  +\n        b_3\\times \\texttt{debt_to_income} \\\\\n    &\\qquad\\  +\n        b_4 \\times \\texttt{credit_util} \\\\\n    &\\qquad\\  +\n        b_5 \\times \\texttt{bankruptcy} \\\\\n    &\\qquad\\  +\n        b_6 \\times \\texttt{term} \\\\\n    &\\qquad\\  +\n        b_7 \\times \\texttt{issue_month}_{\\texttt{Jan-2018}} \\\\\n    &\\qquad\\ +\n        b_8 \\times \\texttt{issue_month}_{\\texttt{Mar-2018}} \\\\\n    &\\qquad\\  +\n        b_9 \\times \\texttt{credit_checks}\n\\end{align*}\\]equation represents holistic approach modeling variables simultaneously.\nNotice two coefficients verified_income also two coefficients issue_month, since 3-level categorical variables.estimate coefficients way case single predictor—select \\(b_0\\), \\(b_1\\), \\(b_2\\), \\(\\cdots\\), \\(b_9\\) minimize sum squared residuals:\\[SSE = e_1^2 + e_2^2 + \\dots + e_{10000}^2 = \\sum_{=1}^{10000} e_i^2 = \\sum_{=1}^{10000} \\left(y_i - \\hat{y}_i\\right)^2\\]\\(y_i\\) \\(\\hat{y}_i\\) represent observed interest rates estimated values according model, respectively.\n10,000 residuals calculated, one observation.\ntypically use computer minimize sum squares compute point estimates, shown sample output \nTable 7.8.\nUsing output, identify point estimates \\(b_i\\) just one-predictor case.\nTable 7.8: Output regression model, interest rate outcome variables listed predictors. Degrees freedom model 9990.\nMultiple regression model.multiple regression model linear model many predictors. general,\nwrite fitted model \\[\\hat{y} = b_0 + b_1 x_1 + b_2 x_2 + \\cdots + b_k x_k\\]\\(k\\) predictor variables. coefficient estimates, \\(b_0,\\ldots, b_k\\),\neasily found using statistical software.Write regression model using point estimates Table 7.8.\nmany predictors model?fitted model interest rate given :\\[\\begin{align*}\n\\widehat{\\texttt{interest_rate}}\n    &= 1.925 +\n        0.975 \\times \\texttt{verified_income}_{\\texttt{Source Verified}} \\\\\n    &\\qquad\\  +\n        2.537 \\times \\texttt{verified_income}_{\\texttt{Verified}} \\\\\n    &\\qquad\\  +\n        0.021 \\times \\texttt{debt_to_income} \\\\\n    &\\qquad\\  +\n        4.896 \\times \\texttt{credit_util} \\\\\n    &\\qquad\\  +\n        0.386 \\times \\texttt{bankruptcy} \\\\\n    &\\qquad\\  +\n        0.154 \\times \\texttt{term} \\\\\n    &\\qquad\\  +\n        0.028 \\times \\texttt{issue_month}_{\\texttt{Jan-2018}} \\\\\n    &\\qquad\\  -\n        0.040 \\times \\texttt{issue_month}_{\\texttt{Mar-2018}} \\\\\n    &\\qquad\\  +\n        0.228 \\times \\texttt{credit_checks}\n\\end{align*}\\]count number predictor coefficients, get effective number predictors model: \\(k = 9\\).\nNotice categorical predictor counts two, two levels shown model.\ngeneral, categorical predictor \\(p\\) different levels represented \\(p - 1\\) terms multiple regression model.\\(b_4\\), estimated coefficient variable credit_util, represent?\nvalue?69Compute residual first observation Table 7.4 page using full model.70We estimated coefficient Section 7.3.1 \\(b_4 = 0.74\\) standard error \\(SE_{b_1} = 0.15\\) using simple linear regression.\ndifference estimate estimated coefficient 0.39 multiple regression setting?examined data carefully, see predictors correlated.\ninstance, estimated connection outcome interest_rate predictor bankruptcy using simple linear regression, unable control variables like whether borrower income verified, borrower’s debt--income ratio, variables.\noriginal model constructed vacuum consider full context.\ninclude variables, underlying unintentional bias missed variables reduced eliminated.\ncourse, bias can still exist confounding variables.previous example describes common issue multiple regression: correlation among predictor variables.\nsay two predictor variables (pronounced co-linear) correlated, collinearity complicates model estimation.\nimpossible prevent collinearity arising observational data, experiments usually designed prevent predictors collinear.estimated value intercept 1.925, one might tempted make interpretation coefficient, , model’s predicted price variables take value zero: income source verified, borrower debt (debt--income credit utilization zero), .\nreasonable?\nvalue gained making interpretation?71","code":""},{"path":"explore-mult-reg.html","id":"chp7-review","chapter":"7 Multivariable models","heading":"7.4 Chapter review","text":"","code":""},{"path":"explore-mult-reg.html","id":"summary-5","chapter":"7 Multivariable models","heading":"Summary","text":"real data, often need describe visualize multiple variables can modeled together.\nchapter, presented one approach using multiple linear regression.\ncoefficient represents one unit increase predictor variable response variable given rest predictor variables model.\nWorking interpreting multivariable models can tricky, since relationship two variables may appear different overall look relationship within particular group.","code":""},{"path":"explore-mult-reg.html","id":"terms-6","chapter":"7 Multivariable models","heading":"Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"explore-mult-reg.html","id":"key-ideas-5","chapter":"7 Multivariable models","heading":"Key ideas","text":"building data visualizations, map different variables different aesthetics. example, quantitative explanatory variable mapped position \\(x\\)-axis, quantitative response variable mapped position \\(y\\)-axis, categorical variable may mapped color shape plotting character.building data visualizations, map different variables different aesthetics. example, quantitative explanatory variable mapped position \\(x\\)-axis, quantitative response variable mapped position \\(y\\)-axis, categorical variable may mapped color shape plotting character.common three-variable situation one add categorical variable scatterplot, using colors /different point symbols. slope regression line differs across categories, say relationship association two quantitative variables differs across levels categorical variable. called interaction — two explanatory variables interact relationship response variable.common three-variable situation one add categorical variable scatterplot, using colors /different point symbols. slope regression line differs across categories, say relationship association two quantitative variables differs across levels categorical variable. called interaction — two explanatory variables interact relationship response variable.Simpson’s Paradox can occur three variables type (categorical quantitative). Simpson’s Paradox occurs overall association two variables interest reverses account third variable. example, overall, slope two quantitative variables \\(x\\) \\(y\\) may positive, look slope fitted subset data certain category third variable, slope negative.Simpson’s Paradox can occur three variables type (categorical quantitative). Simpson’s Paradox occurs overall association two variables interest reverses account third variable. example, overall, slope two quantitative variables \\(x\\) \\(y\\) may positive, look slope fitted subset data certain category third variable, slope negative.","code":""},{"path":"explore-applications.html","id":"explore-applications","chapter":"8 Applications: Explore","heading":"8 Applications: Explore","text":"TODOTODO","code":""},{"path":"foundations-randomization.html","id":"foundations-randomization","chapter":"9 Hypothesis testing with randomization","heading":"9 Hypothesis testing with randomization","text":"Statistical inference primarily concerned understanding quantifying uncertainty parameter estimates—, variable sample statistic sample sample? equations details change depending setting, foundations inference throughout statistics.start two case studies designed motivate process making decisions research claims.\nformalize process introduction hypothesis testing framework, allows us formally evaluate claims population.Throughout book far, worked data variety contexts. learned summarize visualize data well visualize multiple variables time. Sometimes data set hand represents entire research question. often , data collected answer research question larger group data (hopefully) representative subset.may agree almost always variability data (one data set identical second data set even collected population using methods).\nHowever, quantifying variability data neither obvious easy , .e., answering question “different one data set another?” trivial.First, reminder notation.\ngenerally use \\(\\pi\\) denote population proportion \\(\\hat{p}\\) sample proportion.\nSimilarly, generally use \\(\\mu\\) denote population mean \\(\\bar{x}\\) denote sample mean.Suppose professor splits students class two groups: students sit left side classroom students sit right side classroom.\n\\(\\hat{p}_{L}\\) represents proportion students prefer read books screen sit left side classroom \\(\\hat{p}_{R}\\) represents proportion students prefer read books screen sit right side classroom, surprised \\(\\hat{p}_{L}\\) exactly equal \\(\\hat{p}_{R}\\)?proportions \\(\\hat{p}_{L}\\) \\(\\hat{p}_{R}\\) probably close , unusual exactly .\nprobably observe small difference due chance.think side room person sits class related whether prefer read books screen, assumption making relationship two variables?72Studying randomness form key focus statistics. Throughout chapter, follow, provide two different approaches quantifying variability inherent data: simulation-based methods theory-based methods (mathematical models). Using methods provided future chapters, able draw conclusions beyond data set hand research questions larger populations samples come .Given results seen sample, process determining can infer population based sample results called statistical inference. Statistical inferential methods enable us understand quantify uncertainty sample results. Statistical inference helps us answer two questions population:strong evidence effect?large effect?first question answered hypothesis test, second addressed confidence interval. chapter introduce foundations hypothesis testing, ideas behind confidence intervals presented next chapter.Statistical inference practice making decisions conclusions data context uncertainty. Errors occur, just like rare events, data set hand might lead us wrong conclusion. given data set may always lead us correct conclusion, statistical inference gives us tools control evaluate often errors occur.","code":""},{"path":"foundations-randomization.html","id":"Martian","chapter":"9 Hypothesis testing with randomization","heading":"9.1 Motivating example: Martian alphabet","text":"well can humans distinguish one “Martian” letter another? Figure 9.1 displays two Martian letters—one Kiki another Bumba. think Kiki think Bumba? Take moment write guess.\nFigure 9.1: Two Martian letters: Bumba Kiki. think letter Bumba left right?73\n","code":""},{"path":"foundations-randomization.html","id":"observed-data","chapter":"9 Hypothesis testing with randomization","heading":"9.1.1 Observed data","text":"image question Figure 9.1 presented introductory statistics class 38 students. class, 34 students correctly identified Bumba Martian letter left. , sample proportion \\(\\hat{p} = 34/38 = 0.90\\). Assuming can’t read Martian, result surprising?One two possibilities occurred:can’t read Martian, results just occurred chance.can read Martian, results reflect ability.decide two possibilities, calculate probability observing results randomly selected sample 38 students, assumption students just guessing. probability low, ’d reason reject first possibility favor second. can calculate probability using one two methods:Simulation-based method: simulate lots samples (classes) 38 students assumption students just guessing, calculate proportion simulated samples saw 34 students guessing correctly, orTheory-based method: develop mathematical model sample proportion scenario use model calculate probability.","code":""},{"path":"foundations-randomization.html","id":"variability-in-a-statistic","chapter":"9 Hypothesis testing with randomization","heading":"9.1.2 Variability in a statistic","text":"use coin cards simulate guesses one sample 38 students read Martian?74The observed data showed 34 students correctly identifying Bumba class 38 students, 90%. Now, suppose students truly “just guessing”, meaning student 50% chance guessing correctly. , conducted study different sample 38 students, expect half (19 students) guess correctly, half guess incorrectly. variation 19 based random fluctuation sample selection process. actually perform simulation happen randomly chose another 38 students “just guessing” flipping coin 38 times counting number times lands heads. Try —many correctly guessed Bumba simulated class 38 students? proportion guessed correctly?","code":""},{"path":"foundations-randomization.html","id":"observed-statistic-vs.-null-statistics","chapter":"9 Hypothesis testing with randomization","heading":"9.1.3 Observed statistic vs. null statistics","text":"flipping coin 38 times, computed one possible sample proportion students guessing correctly assumption “just guessing”.\nfirst simulation, physically flipped coin, much efficient perform simulation using computer.\nUsing computer repeat process 1,000 times, create dot plot simulated sample proportions shown Figure 9.2.\nFigure 9.2: dot plot 1,000 sample proportions; calculated flipping coin 38 times calculating proportion times coin landed heads. None 1,000 simulations sample proportion least 89%, proportion observed study.\nobserved statistic, \\(\\hat{p} = 0.90\\), represented Figure 9.2 \nred triangle.\nsimulated sample proportions plotted blue represent null statistics, since simulated assumption “just guessing” “nothing” “null”.\nNone simulated null statistics got even close observed statistic!\n, students just guessing, nearly impossible observe 34 students guessing correctly sample 38 students.\nGiven low probability, plausible possibility 2. can read Martian, results reflect ability. ’ve just completed first hypothesis test!Now, obviously one can read Martian, realistic possibility humans tend choose Bumba left often right—greater 50% chance choosing Bumba letter left. Even though may think ’re guessing just chance, preference Bumba left. turns explanation preference called synesthesia, tendency humans correlate sharp sounding noises (e.g., Kiki) sharp looking images.75","code":""},{"path":"foundations-randomization.html","id":"caseStudySexDiscrimination","chapter":"9 Hypothesis testing with randomization","heading":"9.2 Case study: Sex discrimination","text":"getting nuances hypothesis testing, let’s work another case study.\nconsider study investigating sex discrimination 1970s, set context personnel decisions within bank.\nresearch question hope answer , “individuals identify female discriminated promotion decisions made managers identify male?” (Rosen Jerdee 1974)sex_discrimination data can found openintro R package.study considered sex roles, allowed options “male” “female”.\nnote identities considered gender identities study allowed binary classification sex.","code":""},{"path":"foundations-randomization.html","id":"observed-data-1","chapter":"9 Hypothesis testing with randomization","heading":"9.2.1 Observed data","text":"participants study 48 bank supervisors identified male, attending management institute University North Carolina 1972.\nasked assume role personnel director bank given personnel file judge whether person promoted branch manager position.\nfiles given participants identical, except half indicated candidate identified male half indicated candidate identified female.\nfiles randomly assigned subjects.observational study experiment?\ntype study impact can inferred results?76For supervisor sex associated assigned file promotion decision recorded.\nUsing results study summarized Table 9.1, like evaluate individuals identify female unfairly discriminated promotion decisions.\nstudy, smaller proportion female identifying applications promoted males (0.583 versus 0.875), unclear whether difference provides convincing evidence individuals identify female unfairly discriminated .\nTable 9.1: Summary results sex discrimination study.\ndata visualized Figure 9.3 set cards.\nNote card denotes personnel file (observation data set) colors indicate decision: red promoted white promoted.\nAdditionally, observations broken groups male female identifying groups.\nFigure 9.3: sex discrimination study can thought 48 red white cards.\nStatisticians sometimes called upon evaluate strength evidence.\nlooking rates promotion study, might tempted immediately conclude individuals identifying female discriminated ?large difference promotion rates (58.3% female personnel versus 87.5% male personnel) suggest might discrimination women promotion decisions.\nHowever, yet sure observed difference represents discrimination just due random chance discrimination occurring.\nSince wouldn’t expect sample proportions exactly equal, even truth promotion decisions independent sex, can’t rule random chance possible explanation simply comparing sample proportions.previous example reminder observed outcomes sample may perfectly reflect true relationships variables underlying population.\nTable 9.1 shows 7 fewer promotions female identifying personnel male personnel, difference promotion rates 29.2% \\(\\left( \\frac{21}{24} - \\frac{14}{24} = 0.292 \\right).\\) observed difference call point estimate true difference.\npoint estimate difference promotion rate large, sample size study small, making unclear observed difference represents discrimination whether simply due chance discrimination occurring.\nChance can thought claim due natural variability; discrimination can thought claim researchers set demonstrate.\nlabel two competing claims, \\(H_0\\) \\(H_A:\\)\\(H_0:\\) Null hypothesis.\nvariables sex decision independent.\nrelationship, observed difference proportion males females promoted, 29.2%, due natural variability inherent population.\\(H_A:\\) Alternative hypothesis.\nvariables sex decision independent.\ndifference promotion rates 29.2% due natural variability, equally qualified female personnel less likely promoted male personnel.Hypothesis testing.hypotheses part called hypothesis test.\nhypothesis test statistical technique used evaluate competing claims using data.\nOften times, null hypothesis takes stance difference effect.\nhypothesis assumes differences seen due variability inherent population occurred random chance.null hypothesis data notably disagree, reject null hypothesis favor alternative hypothesis.many nuances hypothesis testing, worry aren’t master hypothesis testing end chapter.\n’ll discuss ideas details many times chapter well chapters follow.mean null hypothesis, says variables sex decision unrelated, true?\nmean banker decide whether promote candidate without regard sex indicated personnel file.\n, difference promotion percentages due natural variability files randomly allocated different bankers, randomization just happened give rise relatively large difference 29.2%.Consider alternative hypothesis: bankers influenced sex listed personnel file.\ntrue, especially influence substantial, expect see difference promotion rates male female candidates.\nsex bias female candidates, expect smaller fraction promotion recommendations female personnel relative male personnel.choose two competing claims assessing data conflict much \\(H_0\\) null hypothesis deemed reasonable.\ndata null claim seem odds one another, data seem support \\(H_A,\\) reject notion independence conclude data provide evidence discrimination.","code":""},{"path":"foundations-randomization.html","id":"variability-of-the-statistic","chapter":"9 Hypothesis testing with randomization","heading":"9.2.2 Variability of the statistic","text":"Table 9.1 shows 35 bank supervisors recommended promotion 13 .\nNow, suppose bankers’ decisions independent sex candidate.\n, conducted experiment different random assignment sex files, differences promotion rates based random fluctuation promotion decisions.\ncan actually perform randomization, simulates happened bankers’ decisions independent sex distributed file sexes differently.77In simulation, thoroughly shuffle 48 personnel files, 35 labelled promoted 13 labelled promoted, together deal files two new stacks.\nNote keeping 35 promoted 13 promoted, assuming 35 bank managers promoted individual whose content contained file independent sex indicated file.\ndeal 24 files first stack, represent 24 “female” files.\nsecond stack also 24 files, represent 24 “male” files.\nFigure 9.4 highlights shuffle reallocation sham sex groups.\nFigure 9.4: sex discrimination data shuffled reallocated new groups male female files.\n, original data, tabulate results determine fraction personnel files designated “male” “female” promoted.Since randomization files simulation independent promotion decisions, difference promotion rates due chance.\nTable 9.2 show results one simulation.\nTable 9.2: Simulation results, difference promotion rates male female purely due random chance.\ndifference promotion rates two simulated groups Table 9.2 ?\ncompare observed difference 29.2% actual study?78Figure 9.5 shows difference promotion rates much larger original data simulated groups (0.292 > 0.042).\nquantity interest throughout case study difference promotion rates.\ncall summary value observed statistic interest (often test statistic).\nencounter different data structures, type statistic likely change (e.g., might calculate average instead proportion), always want understand statistic varies sample sample.\nFigure 9.5: summarize randomized data produce one estimate difference proportions given sex discrimination. Note sort step used make easier visually calculate simulated sample proportions.\n","code":""},{"path":"foundations-randomization.html","id":"observed-statistic-vs.-null-statistics-1","chapter":"9 Hypothesis testing with randomization","heading":"9.2.3 Observed statistic vs. null statistics","text":"computed one possible difference null hypothesis Guided Practice, represents one difference due chance null hypothesis assumed true.\nfirst simulation, physically dealt files, much efficient perform simulation using computer.\nRepeating simulation computer, get another difference due chance assumption: -0.042.\nanother: 0.208.\nrepeat simulation enough times good idea shape distribution differences null hypothesis.\nFigure 9.6 shows plot differences found 100 simulations, dot represents simulated difference proportions male female files recommended promotion.\nFigure 9.6: stacked dot plot differences 100 simulations produced null hypothesis, \\(H_0,\\) simulated sex decision independent. Two 100 simulations difference least 29.2%, difference observed study, shown solid blue dots.\nNote distribution simulated differences proportions centered around 0.\nnull hypothesis simulations made distinction male female personnel files.\nThus, center 0 makes sense: expect differences chance alone fall around zero random fluctuation simulation.often observe difference least 29.2% (0.292) according Figure 9.6?\nOften, sometimes, rarely, never?appears difference least 29.2% null hypothesis happen 2% time according Figure 9.6.\nlow probability indicates observing large difference chance alone rare.difference 29.2% rare event really impact listing sex candidates’ files, provides us two possible interpretations study results:\\(H_0,\\) Null hypothesis true: Sex effect promotion decision, observed difference large happen rarely.\\(H_0,\\) Null hypothesis true: Sex effect promotion decision, observed difference large happen rarely.\\(H_A,\\) Alternative hypothesis true: Sex effect promotion decision, observed actually due equally qualified female candidates discriminated promotion decisions, explains large difference 29.2%.\\(H_A,\\) Alternative hypothesis true: Sex effect promotion decision, observed actually due equally qualified female candidates discriminated promotion decisions, explains large difference 29.2%.conduct formal studies, reject null position (idea data result chance ) data strongly conflict null position.79\nanalysis, determined \\(\\approx\\) 2% probability obtaining sample \\(\\geq\\) 29.2% male candidates female candidates get promoted null hypothesis, conclude data provide strong evidence sex discrimination female candidates male supervisors.\ncase, reject null hypothesis favor alternative.","code":""},{"path":"foundations-randomization.html","id":"HypothesisTesting","chapter":"9 Hypothesis testing with randomization","heading":"9.3 Hypothesis testing","text":"last two sections, utilized hypothesis test, formal technique evaluating two competing possibilities.\nscenario, described null hypothesis, represented either skeptical perspective perspective difference.\nalso laid alternative hypothesis, represented new perspective possibility relationship two variables treatment effect experiment.\nalternative hypothesis usually reason scientists set research first place.Null alternative hypotheses.observe effect sample, like determine observed effect represents actual effect population, whether simply due chance. label two competing claims, \\(H_0\\) \\(H_A\\), spoken “H-naught” “H_A”.null hypothesis (\\(H_0\\)) often represents either skeptical perspective claim tested.alternative hypothesis (\\(H_A\\)) represents alternative claim consideration often represented range possible values parameter interest.Martian alphabet example, two competing possibilities null hypothesis? alternative hypothesis?80The hypothesis testing framework general tool, often use without second thought. person makes somewhat unbelievable claim, initially skeptical. However, sufficient evidence supports claim, set aside skepticism. hallmarks hypothesis testing also found US court system.","code":""},{"path":"foundations-randomization.html","id":"the-us-court-system","chapter":"9 Hypothesis testing with randomization","heading":"9.3.1 The US court system","text":"US course system, jurors evaluate evidence see whether convincingly shows defendant guilty.\nDefendants considered innocent proven otherwise.US court considers two possible claims defendant: either innocent guilty.set claims hypothesis framework, null hypothesis alternative?jury considers whether evidence convincing (strong) reasonable doubt regarding person’s guilt.\n, skeptical perspective (null hypothesis) person innocent evidence presented convinces jury person guilty (alternative hypothesis).Jurors examine evidence see whether convincingly shows defendant guilty.\nNotice jury finds defendant guilty, necessarily mean jury confident person’s innocence.\nsimply convinced alternative, person guilty.\nalso case hypothesis testing: even fail reject null hypothesis, accept null hypothesis truth.Failing find evidence favor alternative hypothesis equivalent finding evidence null hypothesis true81.\nsee idea greater detail Section 10.2.","code":""},{"path":"foundations-randomization.html","id":"p-value-stat-signif","chapter":"9 Hypothesis testing with randomization","heading":"9.3.2 p-value and statistical significance","text":"Martian alphabet example, research question—can humans read Martian?—framed context hypotheses:\\(H_0\\): chance human chooses Bumba left 50%.\\(H_0\\): chance human chooses Bumba left 50%.\\(H_A\\): Humans preference choosing Bumba left.\\(H_A\\): Humans preference choosing Bumba left.null hypothesis (\\(H_0\\)) perspective effect (ability read Martian). student data provided point estimate 89.5% (\\(34/38 \\times 100\\)%) true probability choosing Bumba left. determined observing sample proportion chance alone (assuming \\(H_0\\)) rare—happen less 1 1000 samples. results like inconsistent \\(H_0\\), reject \\(H_0\\) favor \\(H_A\\). , concluded humans preference choosing Bumba left.less 1--1000 chance call p-value, probability quantifying strength evidence null hypothesis favor alternative.p-value.p-value probability observing data least favorable alternative hypothesis current data set, null hypothesis true. typically use summary statistic data, proportion difference proportions, help compute p-value evaluate hypotheses. summary value used compute p-value often called test statistic.interpreting p-value, remember definition p-value three components. (1) probability. probability ? probability (2) observed sample statistic one extreme. Assuming ? probability observed sample statistic one extreme, (3) assuming null hypothesis true:probabilitydata82null hypothesisWhat test statistic Martian alphabet example?test statistic Martian alphabet example sample proportion, \\(\\frac{34}{38} = 0.895\\) (89.5%). also point estimate true probability humans choose Bumba left.Since p-value probability, value always 0 1. closer p-value 0, stronger evidence null hypothesis. ? small p-value means data unlikely occur, null hypothesis true. take mean null hypothesis isn’t plausible assumption, reject . process mimics scientific method—easier disprove theory prove . scientists want find evidence new drug reduces risk stroke, assume doesn’t reduce risk stroke show observed data unlikely occur plausible explanation drug works.Think p-values continuum strength evidence null, 0 (extremely strong evidence) 1 (evidence). Beyond around 10%, data provide evidence null hypothesis. careful equate evidence null hypothesis, incorrect.\nFigure 9.7: Strength evidence null continuum p-values. p-value beyond around 0.10, data provide evidence null hypothesis.\np-value small, .e., less previously set threshold, say results statistically significant.\nmeans data provide strong evidence \\(H_0\\) reject null hypothesis favor alternative hypothesis.\nthreshold called significance level often represented \\(\\alpha\\) (Greek letter alpha).\nvalue \\(\\alpha\\) represents rare event needs order null hypothesis rejected.\nHistorically, many fields set \\(\\alpha = 0.05,\\) meaning results need occur less 5% time, null hypothesis rejected.\nvalue \\(\\alpha\\) can vary depending field application.Although everyday language “significant” indicate difference large meaningful, necessarily case .\nterm “statistically significant” indicates p-value study fell chosen significance level.\nexample, sex discrimination study, p-value found approximately 0.02.\nUsing significance level \\(\\alpha = 0.05,\\) say data provided statistically significant evidence null hypothesis.\nHowever, conclusion gives us information regarding size difference promotion rates!Statistical significance.say data provide statistically significant evidence null hypothesis p-value less predetermined threshold (e.g., 0.01, 0.05, 0.1).’s special 0.05?often use threshold 0.05 determine whether result statistically significant. 0.05? Maybe use bigger number, maybe smaller number. ’re little puzzled, probably means ’re reading critical eye—good job! OpenIntro authors video help clarify 0.05: Sometimes ’s also good idea deviate standard. ’ll discuss choose threshold different 0.05 Section 12.Statistical significance hot topic news, related “reproducibility crisis” scientific fields. encourage read debate use p-values statistical significance. good place start Nature article, “Scientists rise statistical significance,” March 20, 2019.","code":""},{"path":"foundations-randomization.html","id":"chp9-review","chapter":"9 Hypothesis testing with randomization","heading":"9.4 Chapter review","text":"","code":""},{"path":"foundations-randomization.html","id":"summary-6","chapter":"9 Hypothesis testing with randomization","heading":"Summary","text":"Regardless data structure analysis method, hypothesis testing framework always follows steps—details model randomness data change.General steps hypothesis test. Every hypothesis test follows general steps:Frame research question terms hypotheses.Collect summarize data using test statistic.Assume null hypothesis true, simulate mathematically model null distribution test statistic.Compare observed test statistic null distribution calculate p-value.Make conclusion based p-value, write conclusion context, plain language, terms alternative hypothesis.Add summary","code":""},{"path":"foundations-randomization.html","id":"terms-7","chapter":"9 Hypothesis testing with randomization","heading":"Terms","text":"introduced following terms chapter. ’re sure terms mean, recommend go back text review definitions. purposefully presenting alphabetical order, instead order appearance, little challenging locate. However able easily spot bolded text.","code":""},{"path":"foundations-randomization.html","id":"key-ideas-6","chapter":"9 Hypothesis testing with randomization","heading":"Key ideas","text":"Need update list distribute across chaptersIn chapter, introduced statistical inference methods — simulation-based theory-based — scenarios involving one two categorical variables.statistical inference revolves around idea sampling variability: take different samples population, value sample statistic vary. chapter, explored sampling variability single proportion difference two proportions. see sort effect sample, just due chance? indicative actual effect population? Statistical inference answer question.statistical inference revolves around idea sampling variability: take different samples population, value sample statistic vary. chapter, explored sampling variability single proportion difference two proportions. see sort effect sample, just due chance? indicative actual effect population? Statistical inference answer question.hypothesis test answers question “strong evidence effect?” confidence interval answers question “large effect?”hypothesis test answers question “strong evidence effect?” confidence interval answers question “large effect?”general steps hypothesis test :\nFrame research question terms hypotheses.\nCollect summarize data using test statistic.\nAssume null hypothesis true, simulate mathematically model null distribution test statistic.\nCompare observed test statistic null distribution calculate p-value.\nMake conclusion based p-value, write conclusion context, plain language, terms alternative hypothesis.\ngeneral steps hypothesis test :Frame research question terms hypotheses.Collect summarize data using test statistic.Assume null hypothesis true, simulate mathematically model null distribution test statistic.Compare observed test statistic null distribution calculate p-value.Make conclusion based p-value, write conclusion context, plain language, terms alternative hypothesis.p-value probability observing data like , data extreme effect, assumption null hypothesis. define “extreme” depends direction alternative hypothesis. p-value answer question “null hypothesis true, chances observing data like mine?”\nsmall p-value indicates observed data unusual null hypothesis true, thus evidence null hypothesis.\np-value small indicates observed data plausible assumption null hypothesis, thus evidence null hypothesis.\np-value probability observing data like , data extreme effect, assumption null hypothesis. define “extreme” depends direction alternative hypothesis. p-value answer question “null hypothesis true, chances observing data like mine?”small p-value indicates observed data unusual null hypothesis true, thus evidence null hypothesis.p-value small indicates observed data plausible assumption null hypothesis, thus evidence null hypothesis.Since decisions hypothesis testing based probabilities (.e., p-values), ’s possible make wrong decision. Type 1 error occurs reject true null hypothesis. fail reject false null hypothesis, committed Type 2 error.Since decisions hypothesis testing based probabilities (.e., p-values), ’s possible make wrong decision. Type 1 error occurs reject true null hypothesis. fail reject false null hypothesis, committed Type 2 error.power hypothesis test probability rejecting null hypothesis, varies depending true value parameter. Power typically increases sample size increases. Thus, small samples may show effect sample practically important — may matter real life — test high enough power reject null hypothesis, statistically significant. hand, large samples may show effect sample isn’t meaningful, practically important, statistically significant due high power.power hypothesis test probability rejecting null hypothesis, varies depending true value parameter. Power typically increases sample size increases. Thus, small samples may show effect sample practically important — may matter real life — test high enough power reject null hypothesis, statistically significant. hand, large samples may show effect sample isn’t meaningful, practically important, statistically significant due high power.simulation-based confidence interval takes percentiles bootstrap distribution sample statistics endpoints. example, 95% confidence interval interval 2.5th percentile 97.5th percentile — percentiles capture middle 95% bootstrap distribution.simulation-based confidence interval takes percentiles bootstrap distribution sample statistics endpoints. example, 95% confidence interval interval 2.5th percentile 97.5th percentile — percentiles capture middle 95% bootstrap distribution.theory-based confidence interval always form: statistic \\(\\pi\\) (multiplier) \\(\\times\\) (standard error statistic). amount add subtract statistic ((multiplier) \\(\\times\\) (standard error statistic)) called margin error. mathematical model sampling variability sample proportion difference sample proportions normal distribution, due Central Limit Theorem.theory-based confidence interval always form: statistic \\(\\pi\\) (multiplier) \\(\\times\\) (standard error statistic). amount add subtract statistic ((multiplier) \\(\\times\\) (standard error statistic)) called margin error. mathematical model sampling variability sample proportion difference sample proportions normal distribution, due Central Limit Theorem.statistical inference methods require certain validity conditions met; otherwise, methods valid. methods textbook require observations data set independent. Additionally, theory-based methods require large enough sample size Central Limit Theorem can apply. proportions, condition known success-failure condition.statistical inference methods require certain validity conditions met; otherwise, methods valid. methods textbook require observations data set independent. Additionally, theory-based methods require large enough sample size Central Limit Theorem can apply. proportions, condition known success-failure condition.","code":""},{"path":"foundations-bootstrapping.html","id":"foundations-bootstrapping","chapter":"10 Confidence intervals with bootstrapping","heading":"10 Confidence intervals with bootstrapping","text":"chapter, expand familiar idea using sample proportion estimate population proportion.\n, create called confidence interval, range plausible values may find true population value.\nprocess creating confidence interval based understanding statistic (sample proportion) varies around parameter (population proportion) many different statistics calculated many different samples., measure variability statistics repeatedly taking sample data population compute sample proportion.\n.\n.\ngood sense variability original estimate.variability across samples large, assume original statistic possibly far true population parameter interest (interval estimate wide).\nvariability across samples small, expect sample statistic close true parameter interest (interval estimate narrow).ideal world sampling data free extremely cheap almost never case, taking repeated samples population usually impossible., instead using “resample population” approach, bootstrapping uses “resample sample” approach.\nchapter provide examples details bootstrapping process.Chapter 9, explored use simulation methods\nmodel variability sample statistics assumption null hypothesis.\nSometimes, however, instead testing claim, goal estimate unknown\nvalue population parameter certain degree certainty.example,much less likely get malaria get vaccine?much faster (slower) can person tap finger, average, drink caffeine first?proportion vote go candidate ?, explore situation focus single proportion, introduce new simulation method: bootstrapping.Bootstrapping best suited modeling studies data generated random sampling population.randomization tests, goal bootstrapping understand variability statistic.Unlike randomization tests (modeled statistic change treatment allocated differently), bootstrap model statistic varies one sample another taken population.\nprovide information different statistic parameter interest.Quantifying variability statistic sample sample hard problem.Fortunately, sometimes mathematical theory statistic varies (across different samples) well-known; case sample proportion seen Chapter 11.However, statistics simple theory vary, bootstrapping provides computational approach providing interval estimates almost population parameter.\nchapter focus bootstrapping estimate single proportion, revisit bootstrapping Chapters 17 18, ’ll get plenty practice well exposure bootstrapping many different data settings.goal bootstrapping produce interval estimate (range plausible values) population parameter.","code":""},{"path":"foundations-bootstrapping.html","id":"case-study-med-consult","chapter":"10 Confidence intervals with bootstrapping","heading":"10.1 Case study: Medical consultant","text":"People providing organ donation sometimes seek help special medical consultant.\nconsultants assist patient aspects surgery, goal reducing possibility complications medical procedure recovery.\nPatients might choose consultant based part historical complication rate consultant’s clients.","code":""},{"path":"foundations-bootstrapping.html","id":"observed-data-2","chapter":"10 Confidence intervals with bootstrapping","heading":"10.1.1 Observed data","text":"One consultant tried attract patients noting average complication rate liver donor surgeries US 10%, clients 3 complications 62 liver donor surgeries facilitated.\nclaims strong evidence work meaningfully contributes reducing complications (therefore hired!).let \\(\\pi\\) represent true complication rate liver donors working consultant.\n(“true” complication rate referred parameter.) estimate \\(\\pi\\) using data, label estimate \\(\\hat{p}.\\)sample proportion complication rate 3 complications divided 62 surgeries consultant worked : \\(\\hat{p} = 3/62 = 0.048.\\)possible assess consultant’s claim (reduction complications due work) using data?.claim causal connection, data observational, must lookout confounding variables.example, maybe patients can afford medical consultant can afford better medical care, can also lead lower complication rate.possible assess causal claim, still possible understand consultant’s true rate complications.Parameter.parameter “true” value interest.typically estimate parameter using point estimate sample data.\npoint estimate also known statistic.example, estimate probability \\(\\pi\\) complication client medical consultant examining past complications rates clients:\\[\\hat{p} = 3 / 62 = 0.048~\\text{used estimate}~\\pi\\]","code":""},{"path":"foundations-bootstrapping.html","id":"variability-of-the-statistic-1","chapter":"10 Confidence intervals with bootstrapping","heading":"10.1.2 Variability of the statistic","text":"medical consultant case study, parameter \\(\\pi,\\) true probability complication client medical consultant.\nreason believe \\(\\pi\\) exactly \\(\\hat{p} = 3/62,\\) also reason believe \\(\\pi\\) particularly far \\(\\hat{p} = 3/62.\\)\nsampling replacement dataset (process called bootstrapping), variability possible \\(\\hat{p}\\) values can approximated.inferential procedures covered text grounded quantifying one dataset differ another taken population.\nmake sense take repeated samples population means take samples, larger sample size benefit separately evaluating two sample exact size.\nInstead, measure samples behave estimate population.Figure 10.1 shows unknown original population can estimated using sample approximate proportion successes failures (case, proportion complications complications medical consultant).\nFigure 10.1: unknown population estimated using observed sample data. Note can use sample create estimated bootstrapped population sample. observed data include three red four white marbles, estimated population contains 3/7 red marbles 4/7 white marbles.\ntaking repeated samples estimated population, variability sample sample can observed.\nFigure 10.2 repeated bootstrap samples obviously different original population.\nRecall bootstrap samples taken (estimated) population, differences due entirely natural variability sampling procedure.\nFigure 10.2: Bootstrap sampling provides measure sample sample variability. Note taking samples estimated population created observed data.\nsummarizing bootstrap samples (, using sample proportion), see, directly, variability sample proportion, \\(\\hat{p},\\) sample sample.\ndistribution \\(\\hat{p}_{boot}\\) example scenario shown Figure 10.3, full bootstrap distribution medical consultant data shown Figure 10.6.\nFigure 10.3: bootstrapped proportion estimated bootstrap sample. resulting bootstrap distribution (dotplot) provides measure proportions vary sample sample\nturns practice, difficult computers work infinite population (proportional breakdown sample).\nHowever, physical computational method produces equivalent bootstrap distribution sample proportion computationally efficient manner.Consider observed data bag marbles 3 success (red) 4 failures (white).\ndrawing marbles bag replacement, depict exact sampling process done infinitely large estimated population.\nFigure 10.4: Taking repeated resamples sample data process creating infinitely large estimate population. computationally feasible take resamples directly sample. Note resampling now done replacement (, original sample ever change) original sample estimated hypothetical population equivalent.\n\nFigure 10.5: comparison process sampling estimate infinite population resampling replacement original sample. Note dotplot bootstrapped proportions process statistics estimated equivalent.\napply bootstrap sampling process medical consultant example, consider client one marbles bag.\n59 white marbles (complication) 3 red marbles (complication).\nchoose 62 marbles bag (one time replacement) compute proportion simulated patients complications, \\(\\hat{p}_{boot},\\) “bootstrap” proportion represents single simulated proportion “resample sample” approach.simulation 62 patients, many expect complication? ?83One simulation isn’t enough get sense variability one bootstrap proportion another bootstrap proportion, repeat simulation 10,000 times using computer.Figure 10.6 shows distribution 10,000 bootstrap simulations.\nbootstrapped proportions vary zero 11.3%.\nvariability bootstrapped proportions leads us believe true probability complication (parameter, \\(\\pi\\)) likely fall somewhere 0 11.3%, numbers capture 95% bootstrap resampled values.range values true proportion called bootstrap percentile confidence interval, see throughout next sections chapters.\nFigure 10.6: original medical consultant data bootstrapped 10,000 times. simulation creates sample original data probability complication \\(\\hat{p} = 3/62.\\) bootstrap 2.5 percentile proportion 0 97.5 percentile 0.113. result : 95% confident , population, true probability complication 0% 11.3%.\noriginal claim consultant’s true rate complication national rate 10%.\ninterval estimate 0 11.3% true probability complication indicate surgical consultant lower rate complications national average?\nExplain..\ninterval overlaps 10%, might consultant’s work associated lower risk complications, might consultant’s work associated higher risk (.e., greater 10%) complications!\nAdditionally, previously mentioned, observational study, even association can measured, evidence consultant’s work cause complication rate (higher lower).","code":""},{"path":"foundations-bootstrapping.html","id":"ConfidenceIntervals","chapter":"10 Confidence intervals with bootstrapping","heading":"10.2 Confidence intervals","text":"point estimate provides single plausible value parameter. However, point estimate rarely perfect—usually error estimate. addition supplying point estimate parameter, next logical step provide plausible range values parameter.","code":""},{"path":"foundations-bootstrapping.html","id":"plausible-range-of-values-for-the-population-parameter","chapter":"10 Confidence intervals with bootstrapping","heading":"10.2.1 Plausible range of values for the population parameter","text":"plausible range values population parameter called confidence interval. Using single point estimate like fishing murky lake spear, using confidence interval like fishing net. can throw spear saw fish, probably miss. hand, toss net area, good chance catching fish.report point estimate, probably hit exact population parameter. hand, report range plausible values—confidence interval—good shot capturing parameter.reasoning also explains can never prove null hypothesis. Sample statistics vary sample sample. can quantify uncertainty (e.g., 95% sure statistic fall within 0.15 parameter), can never certain parameter exact value.\nexample, suppose want test whether coin fair coin, .e., \\(H_0: \\pi = 0.50\\) versus \\(H_0: \\pi \\neq 0.50\\), toss coin 10 times collect data. 10 tosses, 6 land heads 4 land tails, resulting p-value 0.75484. don’t enough evidence show coin biased, surely wouldn’t say just proved coin fair!want certain capture population parameter, use wider interval (e.g., 99%) smaller interval (e.g., 80%)?85","code":""},{"path":"foundations-bootstrapping.html","id":"bootstrap-confidence-interval","chapter":"10 Confidence intervals with bootstrapping","heading":"10.2.2 Bootstrap confidence interval","text":"saw , bootstrap sample sample original sample.\ncase medical complications data, proceed follows:Randomly sample one observation 62 patients (replace marble back bag keep population constant).Randomly sample second observation 62 patients. sample replacement (.e., actually remove marbles bag), 1--62 chance second observation one sampled first step!Keep going one sampled observation time …Randomly sample 62nd observation 62 patients.Bootstrap sampling often called sampling replacement.bootstrap sample behaves similarly actual sample population behave, compute point estimate interest (, compute \\(\\hat{p}_{boot}\\)).Due theory beyond text, know bootstrap proportions \\(\\hat{p}_{boot}\\) vary around \\(\\hat{p}\\) similar way different sample proportions (.e., values \\(\\hat{p}\\)) vary around true parameter \\(\\pi.\\)Therefore, interval estimate \\(\\pi\\) can produced using \\(\\hat{p}_{boot}\\) values .95% Bootstrap percentile confidence interval parameter \\(\\pi.\\)95% bootstrap confidence interval parameter \\(\\pi\\) can obtained directly using ordered \\(\\hat{p}_{boot}\\) values.Consider sorted \\(\\hat{p}_{boot}\\) values.\nCall 2.5% bootstrapped proportion value “lower”, call 97.5% bootstrapped proportion value “upper”.95% confidence interval given : (lower, upper)Need update two section references ’re ready:Section 14.1 discuss different percentages confidence level (e.g., 90% confidence interval 99% confidence interval).Section 14.1 also provides longer discussion “95% confidence” actually means.","code":""},{"path":"foundations-bootstrapping.html","id":"case-study-med-consult-test","chapter":"10 Confidence intervals with bootstrapping","heading":"10.3 Case study: Medical consultant revisited","text":"Add medical consultant example demonstrate shifted bootstrap distribution order simulate null distribution.","code":""},{"path":"foundations-bootstrapping.html","id":"chp10-review","chapter":"10 Confidence intervals with bootstrapping","heading":"10.4 Chapter review","text":"","code":""},{"path":"foundations-bootstrapping.html","id":"summary-7","chapter":"10 Confidence intervals with bootstrapping","heading":"Summary","text":"TODO","code":""},{"path":"foundations-bootstrapping.html","id":"terms-8","chapter":"10 Confidence intervals with bootstrapping","heading":"Terms","text":"introduced following terms chapter. ’re sure terms mean, recommend go back text review definitions. purposefully presenting alphabetical order, instead order appearance, little challenging locate. However able easily spot bolded text.","code":""},{"path":"foundations-bootstrapping.html","id":"key-ideas-7","chapter":"10 Confidence intervals with bootstrapping","heading":"Key ideas","text":"TODO","code":""},{"path":"foundations-mathematical.html","id":"foundations-mathematical","chapter":"11 Inference with mathematical models","heading":"11 Inference with mathematical models","text":"Chapters 9 10 questions population parameters addressed using computational techniques simulation.\nrandomization tests, data permuted assuming null hypothesis.\nbootstrapping, data resampled order measure variability.\nmany cases (indeed, sample proportions), variability statistic can described computational method (previous chapters) mathematical formula (chapter).normal distribution presented describe variability associated sample proportions taken either repeated samples repeated experiments.\nnormal distribution quite powerful describes variability many different statistics, encounter normal distribution throughout remainder book.now, however, focus parallels data can provide insight research question either computational methods mathematical models.two approaches modeling statistic, sample proportion, may vary sample sample.\nMartian alphabet example, used simulation-based approach model variability—simulating distribution sample proportions (Figure 9.2). Simulation-based methods include randomization tests bootstrapping methods discussed Chapters 9 10. can also use theory-based approach—one makes use mathematical modeling—involves normal \\(t\\) probability distributions.\nintroduce normal distribution chapter, focusing can use normal distribution model distribution statistics. \\(t\\) distribution introduced later chapters.theory-based methods discussed book work (certain conditions) important theorem Statistics called Central Limit Theorem.","code":""},{"path":"foundations-mathematical.html","id":"CLTsection","chapter":"11 Inference with mathematical models","heading":"11.1 Central Limit Theorem","text":"recent chapters, encountered several case studies.\ndiffer settings, outcomes, technique used analyze data, something common: general shape distribution statistics (called sampling distribution). may noticed distributions mostly symmetric bell-shaped.sure LaTex code isn’t working definition .Sampling distribution.sampling distribution distribution possible values sample statistic samples given sample size given population.\ncan think sample distribution describing sample statistics (e.g., sample proportion \\(\\hat{p}\\) sample mean \\(\\bar{x}\\)) vary one study another.sampling distribution contrasted data distribution86 shows variability observed data values.\ndata distribution can visualized observations .\nHowever, sampling distribution describes sample statistics computed many studies, visualized directly single data set.\nInstead, use either computational mathematical structures estimate sampling distribution hence describe expected variability sample statistic repeated studies.Figure 11.1 shows null distributions two case studies ran 10,000 simulations.\nNote null distribution sampling distribution statistic created setting null hypothesis true.\nTherefore, null distribution always centered value parameter given null hypothesis.\nFigure 11.1: null distribution two case studies presented previously. Note center distribution given value parameter set null hypothesis.\nDescribe shape distributions note anything find interesting.87The case study medical consultant slight right skew, whereas null distribution simulated sex discrimination case study symmetric.\nobserved Chapter 1, ’s common distributions skewed contain outliers.\nHowever, null distributions far encountered looked somewhat similar , part, symmetric.\nresemble bell-shaped curve.\nbell-shaped curve similarity coincidence, rather, guaranteed mathematical theory.Central Limit Theorem proportions.look proportion88 scenario satisfies certain conditions, sample proportion appear follow bell-shaped curve called normal distribution.example perfect normal distribution shown Figure 11.2.\nImagine laying normal curve two null distributions Figure 11.1.\nmean (center) standard deviation (width spread) may change plot, general shape remains roughly intact.\nFigure 11.2: normal curve.\nMathematical theory guarantees repeated samples taken sample proportion difference sample proportions follow something resembles normal distribution certain conditions met.\n(Note: typically take one sample, mathematical model lets us know expect taken repeated samples.) conditions fall two general categories describing independence observations need take sufficiently large sample size.Observations sample independent.\nIndependence can guaranteed take random sample large population.Observations sample independent.\nIndependence can guaranteed take random sample large population.sample large enough.\nsample size small.\nqualifies “small” differs one context next, ’ll provide suitable guidelines proportions Chapters 14 15.sample large enough.\nsample size small.\nqualifies “small” differs one context next, ’ll provide suitable guidelines proportions Chapters 14 15.quite amazing , sample size large enough, something like sample proportion—statistic summarizes categorical variable—bell-shaped sampling distribution!far need normal distribution.\n’ve able answer questions somewhat easily using simulation techniques.\nHowever, soon change.\nSimulating data can non-trivial.\nexample, scenarios encountered Chapter 7 introduced regression models multiple predictors require complex simulations order make inferential conclusions.\nInstead, normal distribution distributions like offer general framework statistical inference applies large number settings.Technical Conditions.order normal approximation describe sampling distribution sample proportion varies sample sample, two conditions must hold.\nconditions hold, unwise use normal distribution (related concepts like Z scores, probabilities normal curve, etc.) inferential analyses.Independent observations: Measurements taken one individual give information measurements taken another.Large enough sample: proportions, least 10 expected successes 10 expected failures sample.","code":""},{"path":"foundations-mathematical.html","id":"normal","chapter":"11 Inference with mathematical models","heading":"11.2 Normal distributions","text":"Among distributions see statistics, one overwhelmingly common. symmetric, unimodal, bell curve ubiquitous throughout statistics. common people know variety names including normal curve, normal model, normal distribution.89 certain conditions, sample proportions, sample means, sample differences can modeled using normal distribution—basis theory-based inference methods. Additionally, variables SAT scores heights US adult males closely follow normal distribution.Normal distribution facts.Many summary statistics variables nearly normal, none exactly normal. Thus normal distribution, perfect single problem, useful variety problems. use data exploration solve important problems statistics.section, discuss normal distribution context data become familiar normal distribution techniques.","code":""},{"path":"foundations-mathematical.html","id":"normal-distribution-model","chapter":"11 Inference with mathematical models","heading":"11.2.1 Normal distribution model","text":"normal distribution always describes symmetric, unimodal, bell-shaped curve. However, normal curves can look different depending details model. Specifically, normal model can adjusted using two parameters: mean standard deviation. can probably guess, changing mean shifts bell curve left right, changing standard deviation stretches constricts curve. Figure 11.3 shows normal distribution mean \\(0\\) standard deviation \\(1\\) (commonly referred standard normal distribution) top. normal distribution mean \\(19\\) standard deviation \\(4\\) shown bottom. Figure 11.4 shows two normal distributions axis.\nFigure 11.3: curves represent normal distribution; however, differ center spread. normal distribution mean 0 standard deviation 1 called standard normal distribution. distribution (green dashed line, right) mean 19 standard deviation 4.\n\nFigure 11.4: two normal models shown Figure 11.3 plotted together scale.\nnormal distribution mean \\(\\mu\\) standard deviation \\(\\sigma,\\) may write distribution \\(N(\\mu, \\sigma).\\) two distributions Figure 11.4 can written \\[ N(\\mu = 0, \\sigma = 1)\\quad\\text{}\\quad N(\\mu = 19, \\sigma = 4) \\]mean standard deviation describe normal distribution exactly, called distribution’s parameters.Write short-hand normal distribution following parameters.mean 5 standard deviation 3mean -100 standard deviation 10mean 2 standard deviation 9\\(N(\\mu = 5,\\sigma = 3)\\)\\(N(\\mu = -100, \\sigma = 10)\\)\\(N(\\mu = 2, \\sigma = 9)\\)","code":""},{"path":"foundations-mathematical.html","id":"standardizing-with-z-scores","chapter":"11 Inference with mathematical models","heading":"11.2.2 Standardizing with Z-scores","text":"SAT scores follow nearly normal distribution mean 1500 points standard deviation 300 points.\nACT scores also follow nearly normal distribution mean 21 points standard deviation 5 points.\nSuppose Nel scored 1800 points SAT Sian scored 24 points ACT.\nperformed better?90\nFigure 11.5: Nel’s Sian’s scores shown distributions SAT ACT scores.\nsolution previous example relies standardization technique called Z-score, method commonly employed nearly normal observations (may used distribution).\nZ-score observation defined number standard deviations falls mean.\nobservation one standard deviation mean, Z-score 1.\n1.5 standard deviations mean, Z-score -1.5.\n\\(x\\) observation distribution \\(N(\\mu, \\sigma)\\), define Z-score mathematically \\[ Z = \\frac{x-\\mu}{\\sigma} \\]Using \\(\\mu_{SAT}=1500,\\) \\(\\sigma_{SAT}=300,\\) \\(x_{Nel}=1800,\\) find Nel’s Z score:\\[ Z_{Nel} = \\frac{x_{Nel} - \\mu_{SAT}}{\\sigma_{SAT}} = \\frac{1800-1500}{300} = 1 \\]Z-score.Z-score observation number standard deviations falls mean.\ncompute Z-score observation \\(x\\) follows distribution mean \\(\\mu\\) standard deviation \\(\\sigma\\) first subtracting mean, dividing standard deviation:\\[Z = \\frac{x-\\mu}{\\sigma}\\]\nobservation \\(x\\) comes normal distribution centered \\(\\mu\\) standard deviation \\(\\sigma\\), Z score distributed according normal distribution center 0 standard deviation 1.\n, normality remains transforming \\(x\\) \\(Z\\) shift center well spread.Use Sian’s ACT score, 24, along ACT mean standard deviation compute Z score.91Observations mean always positive Z-scores mean negative Z-scores. observation equal mean (e.g., SAT score 1500), Z-score \\(0\\).Let \\(X\\) represent random variable \\(N(\\mu=3, \\sigma=2),\\) suppose observe \\(x=5.19.\\) Find Z score \\(x.\\) , use Z score determine many standard deviations mean \\(x\\) falls.Z score given \\(Z = \\frac{x-\\mu}{\\sigma} = \\frac{5.19 - 3}{2} = 2.19/2 = 1.095.\\) observation \\(x\\) 1.095 standard deviations mean.\nknow must mean since \\(Z\\) positive.Head lengths brushtail possums follow nearly normal distribution mean 92.6 mm standard deviation 3.6 mm.\nCompute Z scores possums head lengths 95.4 mm 85.8 mm.92We can use Z-scores roughly identify observations unusual others. One observation \\(x_1\\) said unusual another observation \\(x_2\\) absolute value Z-score larger absolute value observation’s Z-score: \\(|Z_1| > |Z_2|\\). technique especially insightful distribution symmetric.two brushtail possum observations previous guided practice unusual?93","code":""},{"path":"foundations-mathematical.html","id":"normal-probability-calculations-in-r","chapter":"11 Inference with mathematical models","heading":"11.2.3 Normal probability calculations in R","text":"Nel SAT Guided Practice earned score 1800 SAT corresponding \\(Z=1.\\) like know percentile fall among SAT test-takers.Nel’s percentile percentage people earned lower SAT score Nel.\nshade area representing individuals Figure 11.6.\ntotal area normal curve always equal 1, proportion people scored Nel SAT equal area shaded Figure 11.6: 0.8413.\nwords, Nel \\(84^{th}\\) percentile SAT takers.\nFigure 11.6: normal model SAT scores, shading area individuals scored Nel.\ncan use normal model find percentiles probabilities.\nnormal probability table, lists Z-scores corresponding percentiles, can used identify percentile based Z-score (vice versa).\nStatistical software can also used.Normal probabilities commonly found using statistical software show using R.\nuse software identify percentile corresponding particular Z-score.\nR, function calculate normal probabilities pnorm().\nnormTail() function available openintro R package draw associated curve helpful.\ncode , find percentile \\(Z=0.43\\) 0.6664, \\(66.64^{th}\\) percentile.can also find Z-score associated percentile. example, identify Z \\(80^{th}\\) percentile, use qnorm() identifies quantile given percentage.\nquantile represents cutoff value.94 determine Z-score \\(80^{th}\\) percentile using qnorm(): 0.84.can use functions normal distributions standard normal distribution specifying mean argument m standard deviation argument s. determine proportion ACT test takers scored worse Sian ACT: 0.73.Determine proportion SAT test takers scored better Nel SAT.95","code":"\npnorm(0.43, m = 0, s = 1)\n#> [1] 0.666\nopenintro::normTail(0.43, m = 0, s = 1)\nqnorm(0.80, m = 0, s = 1)\n#> [1] 0.842\nopenintro::normTail(0.80, m = 0, s = 1)\npnorm(24, m = 21, s = 5)\n#> [1] 0.726\nopenintro::normTail(24, m = 21, s = 5)"},{"path":"foundations-mathematical.html","id":"normal-probability-examples","chapter":"11 Inference with mathematical models","heading":"11.2.4 Normal probability examples","text":"Cumulative SAT scores approximated well normal model, \\(N(\\mu=1500, \\sigma=300)\\).Shannon randomly selected SAT taker, nothing known Shannon’s SAT aptitude. probability Shannon scores least 1630 SATs?First, always draw label picture normal distribution. (Drawings need exact useful.) interested chance scores 1630, shade upper tail. See normal curve .\\(x\\)-axis identifies mean values 2 standard deviations mean.\nsimplest way find shaded area curve makes use Z score cutoff value.\n\\(\\mu=1500,\\) \\(\\sigma=300,\\) cutoff value \\(x=1630,\\) Z score computed \\[ Z = \\frac{x - \\mu}{\\sigma} = \\frac{1630 - 1500}{300} = \\frac{130}{300} = 0.43. \\]\nuse software find percentile \\(Z=0.43\\), yields 0.6664.\nHowever, percentile describes Z-score lower 0.43. find area \\(Z=0.43\\), compute one minus area lower tail, seen .probability Shannon scores least 1630 SAT 0.3336.\ncalculation visualized Figure 11.7.\nFigure 11.7: Visual calculation probability Shannon scores least 1630 SAT.\nAlways draw picture first, find Z-score second.normal probability situation, always always always draw label normal curve shade area interest first. picture provide estimate probability.drawing figure represent situation, identify Z-score observation interest.probability Shannon scoring least 1630 0.3336, probability scores less 1630?\nDraw normal curve representing exercise, shading lower region instead upper one.96Edward earned 1400 SAT. percentile?First, picture needed. Edward’s percentile proportion people get high 1400. scores left 1400.mean \\(\\mu=1500,\\) standard deviation \\(\\sigma=300,\\) cutoff tail area \\(x=1400\\) used compute Z score:\\[ Z = \\frac{x - \\mu}{\\sigma} = \\frac{1400 - 1500}{300} = -0.33\\]Statistical software can used find proportion \\(N(0,1)\\) curve left \\(-0.33\\) 0.3707.\nEdward \\(37^{th}\\) percentile.Use results previous example compute proportion SAT takers better Edward.\nAlso draw new picture.Edward better 37% SAT takers, 63% must done better .Areas right.pnorm() function (normal probability table books) gives area left. like area right, first find area left subtract amount one.\nR, can also setting lower.tail argument FALSE.Stuart earned SAT score 2100.\nDraw picture part.\n() percentile?\n(b) percent SAT takers better Stuart?97Based sample 100 men,98 heights adults identify male, ages 20 62 US nearly normal mean 70.0’’ standard deviation 3.3’’.Kamron 5’7’’ (67 inches) Adrian 6’4’’ (76 inches).\n() Kamron’s height percentile?\n(b) Adrian’s height percentile?\nAlso draw one picture part.Numerical answers, calculated using statistical software (e.g., pnorm() R): () 18.17th percentile.\n(b) 96.55th percentile.last several problems focused finding probability percentile particular observation. like know observation corresponding particular percentile?Erik’s height \\(40^{th}\\) percentile. tall ?always, first draw picture.case, lower tail probability known (0.40), can shaded diagram. want find observation corresponds value. first step direction, determine Z-score associated \\(40^{th}\\) percentile.percentile 50%, know \\(Z\\) negative.\nStatistical software provides \\(Z\\) value \\(-0.25.\\)Knowing \\(Z_{Erik}=-0.25\\) population parameters \\(\\mu=70\\) \\(\\sigma=3.3\\) inches, Z-score formula can set determine Erik’s unknown height, labeled \\(x_{Erik}\\): \\[\\begin{eqnarray*}\n-0.25 = Z_{Erik} = \\frac{x_{Erik} - \\mu}{\\sigma} = \\frac{x_{Erik} - 70}{3.3}\n\\end{eqnarray*}\\] Solving \\(x_{Erik}\\) yields height 69.18 inches. , Erik 5’9’’ (notation 5-feet, 9-inches).adult male height \\(82^{nd}\\) percentile?, draw figure first.calculate Z value associated \\(82^{nd}\\) percentile:Next, want find Z-score \\(82^{nd}\\) percentile, positive value (percentile bigger 50%).\nUsing qnorm(), \\(82^{nd}\\) percentile corresponds \\(Z=0.92\\). Finally, height \\(x\\) found using Z-score formula known mean \\(\\mu\\), standard deviation \\(\\sigma\\), Z-score \\(Z=0.92\\): \\[\\begin{eqnarray*}\n0.92 = Z = \\frac{x-\\mu}{\\sigma} = \\frac{x - 70}{3.3}\n\\end{eqnarray*}\\] yields 73.04 inches 6’1’’ height \\(82^{nd}\\) percentile.\\(95^{th}\\) percentile SAT scores?\\(97.5^{th}\\) percentile male heights? always normal probability problems, first draw picture.99What probability randomly selected male adult least 6’2’’ (74 inches)?probability male adult shorter 5’9’’ (69 inches)?100\n:::probability randomly selected adult male 5’9’’ 6’2’’?heights correspond 69 inches 74 inches.\nFirst, draw figure.\narea interest longer upper lower tail.total area curve 1.\nfind area two tails shaded (previous Guided Practice, areas \\(0.3821\\) \\(0.1131\\)), can find middle area:, probability 5’9’’ 6’2’’ 0.5048.percent SAT takers get 1500 2000?101What percent adult males 5’5’’ 5’7’’?102","code":"\nqnorm(0.4, mean = 0, sd = 1)\n#> [1] -0.253\nqnorm(0.82, m = 0, s = 1)\n#> [1] 0.915"},{"path":"foundations-mathematical.html","id":"rule","chapter":"11 Inference with mathematical models","heading":"11.2.5 68-95-99.7 rule","text":", present useful general rule probability falling within 1, 2, 3 standard deviations mean normal distribution. rule useful wide range practical settings, especially trying make quick estimate without calculator Z table.\nFigure 11.8: Probabilities falling within 1, 2, 3 standard deviations mean normal distribution.\nUse pnorm() confirm 68%, 95%, 99.7% observations fall within 1, 2, 3, standard deviations mean normal distribution, respectively. instance, first find area falls \\(Z=-1\\) \\(Z=1\\), area 0.68. Similarly area 0.95 \\(Z=-2\\) \\(Z=2\\).103It possible normal random variable fall 4, 5, even standard deviations mean. However, occurrences rare data nearly normal. probability 4 standard deviations mean 1--30,000. 5 6 standard deviations, 1--3.5 million 1--1 billion, respectively.SAT scores closely follow normal model mean \\(\\mu = 1500\\) standard deviation \\(\\sigma = 300.\\) percent test takers score 900 2100?\npercent score 1500 2100 ?104","code":""},{"path":"foundations-mathematical.html","id":"quantifying-the-variability-of-a-statistic","chapter":"11 Inference with mathematical models","heading":"11.3 Quantifying the variability of a statistic","text":"seen later chapters, turns many statistics used summarize data (e.g., sample proportion, sample mean, differences two sample proportions, differences two sample means, sample slope linear model, etc.) vary according normal distribution seen .\nmathematical models derived normal theory, even computational methods (intuitive thinking behind approaches) use general bell-shaped variability seen distributions constructed far.","code":""},{"path":"foundations-mathematical.html","id":"standard-error","chapter":"11 Inference with mathematical models","heading":"11.3.1 Standard error","text":"Theory-based methods also give us mathematical expressions standard deviation sampling distribution. instance, true population proportion \\(\\pi\\), standard deviation sampling distribution sample proportions—far away expect sample proportion away population proportion—is105 \\[\nSD(\\hat{p}) = \\sqrt{\\frac{\\pi(1-\\pi)}{n}}.\n\\] Typically, values parameters \\(\\pi\\) unknown, unable calculate standard deviations. case, substitute “best guess” \\(\\pi\\) formulas, either hypothesis point estimate.Standard error.standard deviation sampling distribution statistic, denoted \\(SD\\)(statistic), represents far away expect statistic land parameter.Since formulas standard deviations depend unknown parameters, substitute “best guess” unknown parameters (observed statistics) formulas, either hypothesis point estimate. resulting estimated standard deviation called standard error statistic, denoted \\(SE\\)(statistic).","code":""},{"path":"foundations-mathematical.html","id":"moe","chapter":"11 Inference with mathematical models","heading":"11.3.2 Margin of error","text":"explore simulation-based methods (bootstrapping) theory-based methods creating confidence intervals text. Though details change different scenarios, theory-based confidence intervals always take form:\n\\[\n\\mbox{statistic} \\pm (\\mbox{multiplier}) \\times (\\mbox{standard error statistic})\n\\]\nstatistic best guess value parameter, makes sense build confidence interval around value. standard error, measure uncertainty associated statistic, provides guide large make confidence interval. multiplier determined confident ’d like , tells us many standard errors need add subtract statistic. amount add subtract statistic called margin error.General form confidence interval.general form theory-based confidence interval unknown parameter \n\\[\n\\mbox{statistic} \\pm (\\mbox{multiplier}) \\times (\\mbox{standard error statistic})\n\\]\namount add subtract statistic calculate confidence interval called margin error.\n\\[\n\\mbox{margin error} = (\\mbox{multiplier}) \\times (\\mbox{standard error statistic})\n\\]margin error describes variability associated given percentage sampled statistics.\nexample, describe (.e., 95%) observations lie, say margin error approximately \\(2 \\times SE\\).\n, 95% sampled statistics within one margin error mean.\nNotice spread observations goes lower bound upper bound, rough approximation SE divide range 4.\n, notice sample proportions go 0.1 0.4, SE can approximated 0.075.Section 14.3 discuss different percentages confidence interval (e.g., 90% confidence interval 99% confidence interval). Section 15.2 provides longer discussion “95% confidence” actually means.","code":""},{"path":"foundations-mathematical.html","id":"martian-alphabet-revisited","chapter":"11 Inference with mathematical models","heading":"11.4 Martian alphabet revisited","text":"::: {.underconstruction}\nEdit following (copied old categorical inference chapter):Section 9.1, simulation assumption students just guessing, found evidence students tend prefer Bumba left. much? answer , need confidence interval—interval plausible values true probability humans select Bumba left letter. width interval determined variable sample proportions sample sample. turns , mathematical model variability explore later chapter. now, let’s take standard deviation simulated sample proportions estimate variability: 0.08. Since simulated distribution proportions bell-shaped, know 95% sample proportions fall within two standard deviations true proportion, can add subtract margin error sample proportion calculate approximate 95% confidence interval106: \\[\n\\frac{34}{38} \\pm 2\\times 0.08 = 0.89 \\pm 0.16 = (0.73, 1)\n\\] Thus, based data, 95% confident probability human guesses Bumba left somewhere 73% 100%.","code":""},{"path":"foundations-mathematical.html","id":"case-study-test-medical-consultant","chapter":"11 Inference with mathematical models","heading":"11.5 Case study (test): Medical consultant","text":"Model Section 13.5 IMS: https://openintro-ims.netlify.app/foundations-mathematical.html#casemed","code":""},{"path":"foundations-mathematical.html","id":"case-study-interval-stents","chapter":"11 Inference with mathematical models","heading":"11.6 Case study (interval): Stents","text":"Model Section 13.6 IMS: https://openintro-ims.netlify.app/foundations-mathematical.html#casestent","code":""},{"path":"foundations-mathematical.html","id":"chp11-review","chapter":"11 Inference with mathematical models","heading":"11.7 Chapter review","text":"","code":""},{"path":"foundations-mathematical.html","id":"summary-8","chapter":"11 Inference with mathematical models","heading":"Summary","text":"TODO","code":""},{"path":"foundations-mathematical.html","id":"terms-9","chapter":"11 Inference with mathematical models","heading":"Terms","text":"introduced following terms chapter. ’re sure terms mean, recommend go back text review definitions. purposefully presenting alphabetical order, instead order appearance, little challenging locate. However able easily spot bolded text.","code":""},{"path":"foundations-mathematical.html","id":"key-ideas-8","chapter":"11 Inference with mathematical models","heading":"Key ideas","text":"TODO","code":""},{"path":"foundations-errors.html","id":"foundations-errors","chapter":"12 Errors, power, and practical importance","heading":"12 Errors, power, and practical importance","text":"Using data make inferential decisions larger populations perfect process.\nseen Chapter 9, small p-value typically leads researcher decision reject null claim hypothesis.\nSometimes, however, data can produce small p-value null hypothesis actually true data just inherently variable.\ndescribe errors can arise hypothesis testing, define quantify different errors, suggestions mitigating errors possible.","code":""},{"path":"foundations-errors.html","id":"decerr","chapter":"12 Errors, power, and practical importance","heading":"12.1 Decision errors","text":"Chapter 9, explored concept p-value continuum strength evidence null hypothesis, 0 (extremely strong evidence null) 1 (evidence null). cases, however, decision hypothesis test needed, two possible decisions follows:Reject null hypothesisFail reject null hypothesisFor values p-value “reject” null hypothesis? “fail reject” null hypothesis?Since smaller p-value gives stronger evidence null hypothesis, reject \\(H_0\\) p-value small, fail reject \\(H_0\\) p-value small.Hypothesis tests flawless. Just think court system: innocent people sometimes wrongly convicted guilty sometimes walk free. Similarly, data can point wrong conclusion. However, distinguishes statistical hypothesis tests court system framework allows us quantify control often data lead us incorrect conclusion.hypothesis test, two competing hypotheses: null alternative. make statement one might true, might choose incorrectly.\nfour possible scenarios hypothesis test, summarized Table 12.1.\nTable 12.1: Four different scenarios hypothesis tests.\nType 1 Error rejecting null hypothesis \\(H_0\\) actually true.\nSince rejected null hypothesis Martian alphabet example sex discrimination case study, possible made Type 1 Error one studies.Type 2 Error failing reject null hypothesis alternative actually true. Since failed reject null hypothesis medical consultant, possible made Type 2 Error study.US court, defendant either innocent (\\(H_0\\)) guilty (\\(H_A\\)). Type 1 Error represent context? Type 2 Error represent? Table 12.1 may useful.court makes Type 1 Error, means defendant innocent (\\(H_0\\) true) wrongly convicted. Type 2 Error means court failed reject \\(H_0\\) (.e., failed convict person) fact guilty (\\(H_A\\) true).Consider Martian alphabet study concluded students likely say Bumba figure left. Type 1 Error represent context?107How reduce Type 1 Error rate US courts? influence Type 2 Error rate?lower Type 1 Error rate, might raise standard conviction “beyond reasonable doubt” “beyond conceivable doubt” fewer people wrongly convicted. However, also make difficult convict people actually guilty, make Type 2 Errors.reduce Type 1 Error rate US courts?\ninfluence Type 2 Error rate?lower Type 1 Error rate, might raise standard conviction “beyond reasonable doubt” “beyond conceivable doubt” fewer people wrongly convicted.\nHowever, also make difficult convict people actually guilty, make Type 2 Errors.reduce Type 2 Error rate US courts?\ninfluence Type 1 Error rate?108The example guided practice provide important lesson: reduce often make one type error, generally make type.","code":""},{"path":"foundations-errors.html","id":"significance-level","chapter":"12 Errors, power, and practical importance","heading":"12.2 Significance level","text":"significance level provides cutoff p-value lead decision “reject null hypothesis”.\np-value less significance level, say results statistically significant.\nmeans data provide strong evidence \\(H_0\\) reject null hypothesis favor alternative hypothesis.Significance level = probability making Type 1 error.reject null hypothesis p-value less chosen significance level, \\(\\alpha\\). Therefore, null hypothesis true, end really unusual data just chance—p-value less \\(\\alpha\\)—mistakenly reject null hypothesis, making Type 1 error.significance level chosen depending field application real-life consequences incorrect decision.\ntraditional level 0.05, , discussed Section 9.3.2, choice somewhat arbitrary—nothing special particular value.\nselect level smaller larger 0.05 depending consequences conclusions reached test.making Type 1 Error dangerous especially costly, choose small significance level (e.g., 0.01 0.001).\nwant cautious rejecting null hypothesis, demand strong evidence favoring alternative \\(H_A\\) reject \\(H_0.\\)Type 2 Error relatively dangerous much costly Type 1 Error, choose higher significance level (e.g., 0.10).\nwant cautious failing reject \\(H_0\\) null actually false.Significance levels reflect consequences errors.significance level selected test reflect real-world consequences associated making Type 1 Type 2 Error.","code":""},{"path":"foundations-errors.html","id":"two-sided-tests","chapter":"12 Errors, power, and practical importance","heading":"12.3 Two-sided hypotheses","text":"Chapter 9 explored whether women discriminated .\ncase study, however, ignored possibility men actually discriminated . possibility wasn’t considered original hypotheses analyses.\ndisregard extra alternatives may seemed natural since expected data point direction framed problem.\nHowever, two dangers ignore possibilities disagree prior beliefs conflict world view:Framing alternative hypothesis simply match direction data expected point generally inflate Type 1 Error rate.\nwork done (continue ) rigorously control error rates hypothesis tests, careless construction alternative hypotheses can disrupt hard work.Framing alternative hypothesis simply match direction data expected point generally inflate Type 1 Error rate.\nwork done (continue ) rigorously control error rates hypothesis tests, careless construction alternative hypotheses can disrupt hard work.use alternative hypotheses agree worldview, going subjecting confirmation bias, means looking data supports ideas.\n’s scientific, can better!use alternative hypotheses agree worldview, going subjecting confirmation bias, means looking data supports ideas.\n’s scientific, can better!hypotheses seen past two chapters called one-sided hypothesis tests explored one direction possibilities.\nhypotheses appropriate exclusively interested single direction, usually want consider possibilities.Consider situation medical consultant. setting framed context consultant helpful. original hypothesis one-sided hypothesis test explored whether consultant’s patients complication rate 10%.consultant actually performed worse average? care? ever! Since turns care finding either direction, run two-sided hypothesis test.Form hypotheses conduct two-sided test medical consultant case study plain statistical language. Let \\(\\pi\\) represent true complication rate organ donors work medical consultant.want understand whether medical consultant helpful harmful. ’ll consider possibilities using two-sided hypothesis test.\\(H_0\\): association consultant’s contributions clients’ complication rate, .e., \\(\\pi = 0.10\\)\\(H_0\\): association consultant’s contributions clients’ complication rate, .e., \\(\\pi = 0.10\\)\\(H_A\\): association, either positive negative, consultant’s contributions clients’ complication rate, .e., \\(\\pi \\neq 0.10\\).\\(H_A\\): association, either positive negative, consultant’s contributions clients’ complication rate, .e., \\(\\pi \\neq 0.10\\).Compare one-sided hypothesis test, hypotheses :\\(H_0\\): association consultant’s contributions clients’ complication rate, .e., \\(\\pi = 0.10\\).\\(H_0\\): association consultant’s contributions clients’ complication rate, .e., \\(\\pi = 0.10\\).\\(H_A\\): Patients work consultant tend complication rate lower 10%, .e., \\(\\pi < 0.10\\).\\(H_A\\): Patients work consultant tend complication rate lower 10%, .e., \\(\\pi < 0.10\\).62 patients worked medical consultant, 3 developed complications organ donation, point estimate \\(\\hat{p} = \\frac{3}{62} = 0.048\\).According point estimate, complication rate clients medical consultant 5.2% expected complication rate 10%. However, wonder difference easily explainable chance.Recall Section 10.3, simulated proportions might see chance alone null hypothesis. using marbles, cards, spinner reflect null hypothesis, can simulate happen 62 ‘patients’ true complication rate 10%. repeating simulation 10,000 times, can build null distribution sample proportions shown Figure 12.1.\nFigure 12.1: null distribution \\(\\hat{p}\\), created 10,000 simulated studies.\noriginal hypothesis, investigating medical consultant helpful, one-sided hypothesis test (\\(H_A: \\pi < 0.10\\)) counted simulations observed proportion 0.048 order calculate p-value 0.1222 12.22%. However, p-value two-sided hypothesis test investigating medical consultant helpful harmful 0.1222!p-value defined chance observe result least favorable alternative hypothesis result (.e., proportion) observe. two-sided hypothesis test, means finding proportion simulations either tail observed result, beyond point equi-distant null hypothesis observed result.case, observed proportion 0.048 0.052 null hypothesized value 0.10. continue count 0.1222 simulations 0.048, must add proportion simulations \\(0.10 + 0.052 = 0.152\\) order obtain p-value.Figure 12.2 ’ve also shaded differences right tail distribution. two shaded tails provide visual representation p-value two-sided test.\nFigure 12.2: null distribution \\(\\hat{p}\\), created 10,000 simulated studies. simulations least far null value 0.10 observed proportion (.e., 0.048 0.152) shaded.\nprevious simulation, know 12.22% simulations lie observed proportion 0.048. Figure 12.2 shows additional 0.0811 8.11% simulations fall 0.152. indicates p-value two-sided test \\(0.1222 + 0.0811 = 0.2033\\). large p-value, find statistically significant evidence medical consultant’s patients complication rate different 10%.Section 11.1, learned null distribution symmetric certain conditions. null distribution symmetric, can find two-sided p-value merely taking single tail (case, 0.1222) double get two-sided p-value: 0.2444. Note example satisfy conditions null distribution Figure 12.2 symmetric. Thus, result ‘doubled’ one-sided p-value 0.2444 good estimate actual two-sided p-value 0.2033.Default two-sided test.want rigorous keep open mind analyze data evidence. Use one-sided hypothesis test truly interest one direction.Computing p-value two-sided test.null distribution symmetric, first compute p-value one tail distribution, double value get two-sided p-value. ’s !109Generally, find two-sided p-value double single tail area, remains reasonable approach even sampling distribution asymmetric. However, approach can result p-values larger 1 point estimate near mean null distribution; cases, write p-value 1. Also, large p-values computed way (e.g., 0.85), may also slightly inflated. Typically, worry much precision large p-values lead analysis conclusion, even value slightly .","code":""},{"path":"foundations-errors.html","id":"controlling-the-type-1-error-rate","chapter":"12 Errors, power, and practical importance","heading":"12.4 Controlling the Type 1 error rate","text":"Now understand difference one-sided two-sided tests, must recognize use type test.\nresult increased error rates, never okay change two-sided tests one-sided tests observing data.\nexplore consequences ignoring advice next example.Using \\(\\alpha=0.05,\\) show freely switching two-sided tests one-sided tests lead us make twice many Type 1 Errors intended.Suppose interested finding difference 0.\n’ve created smooth-looking null distribution representing differences due chance Figure 12.3.Suppose sample difference larger 0.\ncan flip one-sided test, use \\(H_A:\\) difference \\(> 0.\\) Now obtain observation upper 5% distribution, reject \\(H_0\\) since p-value just single tail.\nThus, null hypothesis true, incorrectly reject null hypothesis 5% time sample mean null value, shown Figure 12.3.Suppose sample difference smaller 0.\nchange one-sided test, use \\(H_A:\\) difference \\(< 0.\\) observed difference falls lower 5% figure, reject \\(H_0.\\) , null hypothesis true, observe situation 5% time.examining two scenarios, can determine make Type 1 Error \\(5\\%+5\\%=10\\%\\) time allowed swap “best” one-sided test data.\ntwice error rate prescribed significance level: \\(\\alpha=0.05\\) (!).\nFigure 12.3: shaded regions represent areas reject \\(H_0\\) bad practices considered \\(\\alpha = 0.05.\\)\nHypothesis tests set seeing data.observing data, tempting turn two-sided test one-sided test.\nAvoid temptation.\nHypotheses set observing data.","code":""},{"path":"foundations-errors.html","id":"power","chapter":"12 Errors, power, and practical importance","heading":"12.5 Power","text":"Although won’t go extensive detail , power important topic follow-consideration understanding basics hypothesis testing.\ngood power analysis vital preliminary step study inform whether data collect sufficient able conclude research broadly.Often times experiment planning, two competing considerations:want collect enough data can detect important effects.Collecting data can expensive, , experiments involving people, may risk patients.planning study, want know likely detect effect care .\nwords, real effect, effect large enough practical value, probability detect effect?\nprobability called power, can compute different sample sizes different effect sizes.Power.power test probability rejecting null claim alternative claim true.easy detect effect depends big effect (e.g., good medical treatment ) well sample size.think power probability become rich famous science.\norder science make splash, need good ideas!\n, won’t become famous happen find single Type 1 error rejects null hypothesis.\nInstead, ’ll become famous science good important (, alternative hypothesis true).\nbetter science (.e., better medical treatment), larger effect size easier convince people work.science need solid, also need evidence (.e., data) shows effect.\nobservations (e.g., \\(n = 2)\\) unlikely convincing well known ideas natural variability.\nIndeed, larger data set provides evidence scientific claim, likely convince community idea correct.TODO - sections old Chapter 5 - go cut/moveWhen null hypothesis true, probability Type 1 error chosen significance level, \\(\\alpha\\), means probability correct decision \\(1 - \\alpha\\).alternative hypothesis true, probability Type 2 error, denote \\(\\beta\\), depends several components:significance levelsample sizewhether alternative hypothesis one-sided two-sidedstandard deviation statistichow far alternative parameter value null valueOnly first three components within control researcher.probability correct decision alternative hypothesis true, \\(1 - \\beta\\), called power test. Higher power means likely detect effect actually exists.Power.power test probability rejecting false null hypothesis.Suppose like test whether less 65% large population approves new law: \\(H_0: \\pi = 0.65\\) versus \\(H_A: \\pi < 0.65\\). collect random sample \\(n = 200\\) individuals population. values sample proportion, \\(\\hat{p}\\), reject \\(H_0\\) using significance level \\(\\alpha = 0.05\\)?assumption null hypothesis, standard deviation \\(\\hat{p}\\) \\(\\sqrt{0.65(1-0.65)/200} = 0.0337\\). Thus, Central Limit Theorem, sample proportions vary according approximate normal distribution mean 0.65 standard deviation 0.0337. reject null hypothesis true proportion 0.65 sample proportion low probability less 0.05, shown Figure 12.4.precise, reject \\(H_0\\) \\(\\hat{p}\\) less 5th percentile null distribution: qnorm(0.05, 0.65, 0.0337) = 0.59.\nFigure 12.4: Shaded area null distribution reject null hypothesis. area equal significance level.\ncalculate power, need know true value parameter. previous example, alternative \\(H_0: \\pi < 0.65\\), just say alternative hypothesis true, still know value \\(\\pi\\). Thus, power calculations done specific value parameter, power changes value parameter changes.Consider test whether less 65% large population approves new law: \\(H_0: \\pi = 0.65\\) versus \\(H_A: \\pi < 0.65\\). Suppose population approval rate actually \\(\\pi = 0.58\\). probability detect effect?example asks us calculate power – probability test provide evidence \\(\\pi < 0.65\\) true value \\(\\pi\\) 0.58. Recall previous example reject null \\(\\hat{p} < 0.59\\). Thus, power probability \\(\\hat{p}\\) less 0.59 true proportion 0.58: pnorm(0.59, 0.58, 0.0337) = 0.62. 62% chance data collect provide strong enough evidence conclude \\(\\pi < 0.65\\). probability represented red area Figure 12.5.\nFigure 12.5: blue distribution distribution sample proportions null hypothesis true, \\(\\pi = 0.65\\) – blue shaded area represents probability reject true null hypothesis. red distribution distribution sample proportions particular alternative hypothesis, \\(\\pi = 0.58\\) – red shaded area represents power.\nIncreasing power.power test increase :significance level increasesthe sample size increaseswe change two-sided one-sided testthe standard deviation statistic decreaseshow far alternative parameter value null value increases","code":""},{"path":"foundations-errors.html","id":"statistical-significance-vs.-practical-importance","chapter":"12 Errors, power, and practical importance","heading":"12.6 Statistical significance vs. practical importance","text":"Austrian study heights 507,125 military recruits reported men born spring statistically significantly taller men born fall (p-value < 0.0001). confidence interval true difference mean height men born spring men born fall (0.598, 0.602) cm. result practically important?, results don’t mean much context – difference average height around 0.6 cm even noticeable human eye! Just result statistically significant mean necessarily practically important – meaningful context problem.previous example, saw two groups men differed average height, difference statistically significant – , observed difference sample means 0.6 cm unlikely occur true difference average height zero. , difference 0.6 cm height meaningful – practically important.happen? Recall variability sample statistics decreases sample size increases. example, unknown , suppose slight majority population, say 50.5%, support new ballot measure. want test \\(H_0: \\pi = 0.50\\) versus \\(H_0: \\pi > 0.50\\) population. Since true proportion exactly 0.50, can make p-value smaller given significance level long choose large enough sample size!Figure 12.6 displays scenario. distribution possible sample proportions support new ballot measure samples size \\(n = 100,000\\) 50.5% population supports measure represented black normal curve. dotted red normal curve null distribution sample proportions \\(H_0: \\pi = 0.5\\). little overlap two distributions due large sample size. shaded blue area represents power test \\(H_0: \\pi = 0.5\\) versus \\(H_A: \\pi > 0.5\\) \\(\\alpha = 0.05\\) – 0.885! , 88.5% chance p-value less 0.05, even though true proportion 0.05 0.5!\nFigure 12.6: Black curve: sampling distribution sample proportions samples size 100,000 true proportion 0.505. Red curve: null distribution sample proportions null value 0.50.\np-values can made arbitrarily small large sample sizes, might tend happen small sample sizes? small sample sizes likely give practically important results statistically significant? statistically significant results practically important?110Consider opposite scenario – small sample sizes meaningful difference. Suppose like determine majority population support new ballot measure. However, time money survey 20 people community. Unknown , 65% population support measure.Examine Figure 12.7. distribution possible sample proportions support new ballot measure samples size \\(n = 20\\) 65% population supports measure represented black normal curve. dotted red normal curve null distribution sample proportions \\(H_0: \\pi = 0.5\\). Even though 0.65 quite bit higher 0.50, still lot overlap two distributions due small sample size. shaded blue area represents power test \\(H_0: \\pi = 0.5\\) versus \\(H_A: \\pi > 0.5\\) \\(\\alpha = 0.05\\) – 0.29! , even though 65% population supports measure (much higher 50%), 29% chance detecting difference small sample size.\nFigure 12.7: Black curve: approximate sampling distribution sample proportions samples size 20 true proportion 0.65. Red curve: approximate null distribution sample proportions null value 0.50.\nStatistical significance versus practical importance.large sample sizes, results may statistically significant, practically important. Since sample statistics vary little among samples large sample sizes, easy hypothesis test result small p-value, even observed effect practically meaningless.small sample sizes, results may practically important, statistically significant. Since studies small sample sizes tend low power, difficult hypothesis test result small p-value, even observed effect quite large.","code":""},{"path":"foundations-errors.html","id":"chp12-review","chapter":"12 Errors, power, and practical importance","heading":"12.7 Chapter review","text":"","code":""},{"path":"foundations-errors.html","id":"summary-9","chapter":"12 Errors, power, and practical importance","heading":"Summary","text":"TODO","code":""},{"path":"foundations-errors.html","id":"terms-10","chapter":"12 Errors, power, and practical importance","heading":"Terms","text":"introduced following terms chapter. ’re sure terms mean, recommend go back text review definitions. purposefully presenting alphabetical order, instead order appearance, little challenging locate. However able easily spot bolded text.","code":""},{"path":"foundations-errors.html","id":"key-ideas-9","chapter":"12 Errors, power, and practical importance","heading":"Key ideas","text":"TODO","code":""},{"path":"foundations-applications.html","id":"foundations-applications","chapter":"13 Applications: Foundations","heading":"13 Applications: Foundations","text":"TODOTODO","code":""},{"path":"inference-one-prop.html","id":"inference-one-prop","chapter":"14 Inference for a single proportion","heading":"14 Inference for a single proportion","text":"TODOOld content - revise neededBelow summarize notation used throughout chapter.Notation.\\(n\\) = sample size (number observational units data set)\\(\\hat{p}\\) = sample proportion (number “successes” divided sample size)\\(\\pi\\) = population proportion111A single proportion used summarize data measured single categorical variable observational unit—single variable measured either success failure (e.g., “surgical complication” vs. “surgical complication”)112.","code":""},{"path":"inference-one-prop.html","id":"one-prop-null-boot","chapter":"14 Inference for a single proportion","heading":"14.1 Simulation-based test for \\(H_0: \\pi = \\pi_0\\)","text":"Need revise section - medical consultant case study introduced Chapter 10. General steps hypothesis test introduced Summary Ch. 9. Move ? end Chapter 9, introduced general steps hypothesis test:General steps hypothesis test. Every hypothesis test follows general steps:Frame research question terms hypotheses.Collect summarize data using test statistic.Assume null hypothesis true, simulate mathematically model null distribution test statistic.Compare observed test statistic null distribution calculate p-value.Make conclusion based p-value, write conclusion context, plain language, terms alternative hypothesis.People providing organ donation sometimes seek help special medical consultant. consultants assist patient aspects surgery, goal reducing possibility complications medical procedure recovery. Patients might choose consultant based part historical complication rate consultant’s clients.One consultant tried attract patients noting average complication rate liver donor surgeries US 10%, clients 3 complications 62 liver donor surgeries facilitated. claims strong evidence work meaningfully contributes reducing complications (therefore hired!).Using data, possible assess consultant’s claim work meaningfully contributes reducing complications?. claim causal connection, data observational, must lookout confounding variables. example, maybe patients can afford medical consultant can afford better medical care, can also lead lower complication rate.possible assess causal claim, still possible understand consultant’s true rate complications.","code":""},{"path":"inference-one-prop.html","id":"steps-1-and-2-hypotheses-and-test-statistic","chapter":"14 Inference for a single proportion","heading":"Steps 1 and 2: Hypotheses and test statistic","text":"Regardless use simulation-based methods theory-based methods, first two steps hypothesis test start : setting hypotheses summarizing data test statistic. let \\(\\pi\\) represent true complication rate liver donors working consultant. “true” complication probability called parameter interest113. sample proportion complication rate 3 complications divided 62 surgeries consultant worked : \\(\\hat{p} = 3/62 = 0.048\\). Since value estimated sample data, called statistic. statistic \\(\\hat{p}\\) also point estimate, “best guess,” \\(\\pi\\), use test statistic.Parameters statistics.parameter “true” value interest. typically estimate parameter using statistic sample data. statistic used estimate parameter, called point estimate.example, estimate probability \\(\\pi\\) complication client medical consultant examining past complications rates clients:\\[\\hat{p} = 3 / 62 = 0.048\\qquad\\text{used estimate}\\qquad \\pi\\]Summary measures summarize sample data, \\(\\hat{p}\\), called statistics. Numbers summarize entire population, \\(\\pi\\), called parameters. can remember distinction looking first letter term:Statistics summarize Samples.Parameters summarize Populations.typically use Roman letters symbolize statistics (e.g., \\(\\bar{x}\\), \\(\\hat{p}\\)), Greek letters symbolize parameters (e.g., \\(\\mu\\), \\(\\pi\\)). Since rarely can measure entire population, thus rarely know actual parameter values, like say, “don’t know Greek, don’t know parameters!”Write hypotheses plain statistical language test association consultant’s work true complication rate, \\(\\pi\\), consultant’s clients.words:\\(H_0\\): association consultant’s contributions clients’ complication rate.\\(H_A\\): Patients work consultant tend complication rate lower 10%.statistical language:\\(H_0: \\pi=0.10\\)\\(H_A: \\pi<0.10\\)","code":""},{"path":"inference-one-prop.html","id":"steps-3-and-4-null-distribution-and-p-value","chapter":"14 Inference for a single proportion","heading":"Steps 3 and 4: Null distribution and p-value","text":"assess hypotheses, need evaluate possibility getting sample proportion far null value, \\(0.10\\), observed (\\(0.048\\)), null hypothesis true.Null value hypothesis test.null value reference value parameter \\(H_0\\), sometimes represented parameter’s label subscript 0 (“null”), e.g., \\(\\pi_0\\) (just like \\(H_0\\)).deviation sample statistic null hypothesized parameter usually quantified p-value114. p-value computed based null distribution, distribution test statistic null hypothesis true. Supposing null hypothesis true, can compute p-value identifying chance observing test statistic favors alternative hypothesis least strongly observed test statistic.Null distribution.null distribution test statistic sampling distribution statistic assumption null hypothesis. describes statistic vary sample sample, null hypothesis true.null distribution can estimated simulation (simulation-based methods), section, can modeled mathematical function (theory-based methods), Section 14.3.want identify sampling distribution test statistic (\\(\\hat{p}\\)) null hypothesis true. words, want see sample proportion changes due chance alone. plan use information decide whether enough evidence reject null hypothesis.null hypothesis, 10% liver donors complications surgery. Suppose rate really different consultant’s clients (consultant’s clients, just 62 previously measured). case, simulate 62 clients get sample proportion complication rate null distribution.similar scenario one encountered Section 9.1, one important difference—null value 0.10, 0.50. Thus, flipping coin simulate whether client complications simulating correct null hypothesis.physical object use simulate random sample 62 clients 10% chance complications? use object?115Assuming true complication rate consultant’s clients 10%, client can simulated using bag marbles 10% red marbles 90% white marbles. Sampling marble bag (10% red marbles) one way simulating whether patient complication true complication rate 10% data. select 62 marbles compute proportion patients complications simulation, \\(\\hat{p}_{sim}\\), resulting sample proportion calculated exactly sample null distribution.undergraduate student paid $2 complete simulation. 5 simulated cases complication 57 simulated cases without complication, .e., \\(\\hat{p}_{sim} = 5/62 = 0.081\\).one simulation enough determine whether reject null hypothesis?. assess hypotheses, need see distribution many \\(\\hat{p}_{sim}\\), just single draw sampling distribution.One simulation isn’t enough get sense null distribution; many simulation studies needed. Roughly 10,000 seems sufficient. However, paying someone simulate 10,000 studies hand waste time money. Instead, simulations typically programmed computer, much efficient.Figure 14.1 shows results 10,000 simulated studies. proportions equal less \\(\\hat{p}=0.048\\) shaded. shaded areas represent sample proportions null distribution provide least much evidence \\(\\hat{p}\\) favoring alternative hypothesis. 1222 simulated sample proportions \\(\\hat{p}_{sim} \\leq 0.048\\). use construct null distribution’s left-tail area find p-value: \\[\\begin{align}\n\\text{left tail area }\\label{estOfPValueBasedOnSimulatedNullForSingleProportion}\n    &= \\frac{\\text{Number observed simulations }\\hat{p}_{sim}\\leq\\text{ 0.048}}{10000}\n\\end{align}\\] 10,000 simulated \\(\\hat{p}_{sim}\\), 1222 equal smaller \\(\\hat{p}\\). Since hypothesis test one-sided, estimated p-value equal tail area: 0.1222.\nFigure 14.1: null distribution \\(\\hat{p}\\), created 10,000 simulated studies. left tail, representing p-value hypothesis test, contains 12.22% simulations.\n","code":""},{"path":"inference-one-prop.html","id":"step-5-conclusion-and-scope-of-inference","chapter":"14 Inference for a single proportion","heading":"Step 5: Conclusion and scope of inference","text":"estimated p-value 0.1222, small, little evidence null hypothesis. Explain means plain language context problem.116Does conclusion previous Guided Practice imply real association surgical consultant’s work risk complications? Explain.117Add scope inference","code":""},{"path":"inference-one-prop.html","id":"boot-ci-prop","chapter":"14 Inference for a single proportion","heading":"14.2 Bootstrap confidence interval for \\(\\pi\\)","text":"confidence interval provides range plausible values parameter \\(\\pi\\). goal produce range possible values population value, ideal world, sample data population recompute sample proportion. . . good sense variability original estimate. ideal world sampling data free extremely cheap almost never case, taking repeated samples population usually impossible. , instead using “resample population” approach, bootstrapping uses “resample sample” approach.Let’s revisit medical consultant example Section 14.1. consultant tried attract patients noting average complication rate liver donor surgeries US 10%, clients 3 complications 62 liver donor surgeries facilitated. data, however, provide sufficient evidence consultant’s complication rate less 10%, since p-value approximately 0.122. mean can conclude consultant’s complication rate equal 10%?! Though decision fail reject null hypothesis, mean evidence null hypothesis—“accept” null. sample proportion \\(\\hat{p} = 3/62 = 0.048\\), point estimate—“best guess”—\\(\\pi\\). wouldn’t make sense sample complication rate 4.8% gives us evidence true complication rate exactly 10%. ’s plausible true complication rate 10%, range plausible values \\(\\pi\\). section, use simulation-based method called bootstrapping generate range plausible values \\(\\pi\\) using observed data.medical consultant case study, parameter \\(\\pi\\), true probability complication client medical consultant. reason believe \\(\\pi\\) exactly \\(\\hat{p} = 3/62\\), also reason believe \\(\\pi\\) particularly far \\(\\hat{p} = 3/62\\). sampling replacement data set (process called bootstrapping),118 variability possible \\(\\hat{p}\\) values can approximated, allow us generate range plausible values \\(\\pi\\), .e., confidence interval.inferential procedures covered text grounded quantifying one data set differ another taken population. doesn’t make sense take repeated samples population means take samples, larger sample size benefit exact sample twice. Instead, measure samples behave estimate population. Figure 10.1 shows unknown original population red white marbles can estimated using multiple copies sample seven marbles.\nFigure 10.1: unknown population red white marbles. estimated population right many copies observed sample.\ntaking repeated samples estimated population, variability sample sample can observed. Figure 10.2 repeated bootstrap samples obviously different , original sample, original population. Recall bootstrap samples taken (estimated) population, differences due entirely natural variability sampling procedure.\nFigure 10.2: Selecting \\(k\\) random samples estimated population created copies observed sample.\nsummarizing bootstrap samples (, using sample proportion), see, directly, variability sample proportion red marbles, \\(\\hat{p}\\), sample sample. distribution bootstrapped \\(\\hat{p}\\)’s example scenario shown Figure 10.3, bootstrap distribution medical consultant data shown Figure 10.6.\nFigure 10.3: Calculate sample proportion red marbles bootstrap resample, plot simulated sample proportions dot plot. dot plot sample proportion provides us sense sample proportions vary sample sample take many samples original population.\nturns practice, difficult computers work infinite population (proportional breakdown sample). However, physical computational model produces equivalent bootstrap distribution sample proportion computationally efficient manner. Consider observed data bag marbles 3 red 4 white. drawing marbles bag replacement, depict sampling process done infinitely large estimated population. Note sampling original observations replacement, particular marble may end new sample one time, multiple times, .Bootstrapping one sample.Take random sample size \\(n\\) original sample, replacement. called bootstrapped resample.Record sample proportion (statistic interest) bootstrapped resample. called bootstrapped statistic.Repeat steps (1) (2) 1000s times create distribution bootstrapped statistics.apply bootstrap sampling process medical consultant example, consider client one marbles bag. 59 white marbles (complication) 3 red marbles (complication). choose 62 marbles bag (one time), replacing chosen marble color recorded, compute proportion simulated patients complications, \\(\\hat{p}_{bs}\\), “bootstrap” proportion represents single simulated proportion “resample sample” approach.simulation 62 patients conducted sampling replacement original sample, many expect complication?119One simulated bootstrap resample isn’t enough get sense variability one bootstrap proportion another bootstrap proportion, repeated simulation 10,000 times using computer. Figure 10.6 shows distribution 10,000 bootstrap simulations. bootstrapped proportions vary zero 0.15. taking range middle 95% distribution, can construct 95% bootstrapped confidence interval \\(\\pi\\). 2.5th percentile 0, 97.5th percentile 0.113, middle 95% distribution range (0, 0.113). variability bootstrapped proportions leads us believe true risk complication (parameter, \\(\\pi\\)) somewhere 0 11.3%.\nFigure 10.6: original medical consultant data bootstrapped 10,000 times. simulation creates sample original data probability complication \\(\\hat{p} = 3/62\\). bootstrap 2.5 percentile proportion 0 97.5 percentile 0.113. result : confident , population, true probability complication 0% 11.3%.\n95% Bootstrap confidence interval population proportion \\(\\pi\\).95% bootstrap confidence interval parameter \\(\\pi\\) can obtained directly using ordered values \\(\\hat{p}_{boot}\\) values — bootstrapped sample proportions. Consider sorted \\(\\hat{p}_{boot}\\) values, let \\(\\hat{p}_{boot, 0.025}\\) 2.5th percentile value \\(\\hat{p}_{boot, 0.975}\\) 97.5th percentile. 95% confidence interval given :can find confidence intervals difference confidence levels changing percent distribution take, e.g., locate middle 90% bootstrapped statistics 90% confidence interval.find middle 90% distribution, two percentiles form boundaries?120The original claim consultant’s true rate complication national rate 10%. interval estimate 0 11.3% true probability complication indicate surgical consultant lower rate complications national average? Explain.. interval overlaps 10%, might consultant’s work associated lower risk complciations, might consulant’s work associated higher risk (.e., greater 10%) complications! Additionally, previously mentioned, observational study, even association can measured, evidence consultant’s work cause complication rate (higher lower).","code":""},{"path":"inference-one-prop.html","id":"theory-prop","chapter":"14 Inference for a single proportion","heading":"14.3 Theory-based inferential methods for \\(\\pi\\)","text":"Chapter 11, introduced normal distribution showed can used mathematical model describe variability sample mean sample proportion result Central Limit Theorem. Theory-based hypothesis tests confidence intervals proportions use normal distribution calculate p-value determine width confidence interval.Central Limit Theorem sample proportion.collect sufficiently large sample \\(n\\) independent observations categorical variable population \\(\\pi\\) proportion successes, sampling distribution \\(\\hat{p}\\) nearly normal \\[\\begin{align*}\n  &\\text{Mean}=\\pi\n  &&\\text{Standard Deviation }(SD) = \\sqrt{\\frac{\\pi(1-\\pi)}{n}}\n  \\end{align*}\\]","code":""},{"path":"inference-one-prop.html","id":"evaluating-the-two-conditions-required-for-modeling-hatp-using-theory-based-methods","chapter":"14 Inference for a single proportion","heading":"14.3.1 Evaluating the two conditions required for modeling \\(\\hat{p}\\) using theory-based methods","text":"two conditions required apply Central Limit Theorem sample proportion \\(\\hat{p}\\). sample observations independent sample size sufficiently large, normal model describe variability sample proportions quite well; observations violate conditions, normal model can inaccurate.Conditions sampling distribution \\(\\hat{p}\\) approximately normal.sampling distribution \\(\\hat{p}\\) based sample size \\(n\\) population true proportion \\(\\pi\\) can modeled using normal distribution :Independence. sample observations independent, .e., outcome one observation influence outcome another. condition met data come simple random sample target population.Independence. sample observations independent, .e., outcome one observation influence outcome another. condition met data come simple random sample target population.Success-failure condition. expected see least 10 successes 10 failures sample, .e., \\(n\\pi\\geq10\\) \\(n(1-\\pi)\\geq10\\). condition met least 10 successes 10 failures observed data.Success-failure condition. expected see least 10 successes 10 failures sample, .e., \\(n\\pi\\geq10\\) \\(n(1-\\pi)\\geq10\\). condition met least 10 successes 10 failures observed data.conditions satisfied, sampling distribution \\(\\hat{p}\\) approximately normal mean \\(\\pi\\) standard deviation \\(\\sqrt{\\frac{\\ \\pi(1-\\pi)\\ }{n}}\\). success-failure condition listed necessary sampling distribution \\(\\hat{p}\\) approximately normal. mean sampling distribution \\(\\hat{p}\\) \\(\\pi\\), standard deviation \\(\\sqrt{\\frac{\\ \\pi(1-\\pi)\\ }{n}}\\), regardless sample size.Typically don’t know true proportion \\(\\pi\\), substitute value check success-failure condition estimate standard deviation sampling distribution \\(\\hat{p}\\). independence condition nuanced requirement. isn’t met, important understand isn’t met. example, exist statistical methods available truly correct inherent biases data convenience sample. hand, took cluster random sample (see Section 2.1.5), observations wouldn’t independent, suitable statistical methods available analyzing data (beyond scope even second third courses statistics)121.examples based large sample theory, modeled \\(\\hat{p}\\) using normal distribution. appropriate study medical consultant?independence assumption may reasonable surgeries different surgical team. However, success-failure condition satisfied. null hypothesis, anticipate seeing \\(62\\times 0.10=6.2\\) complications, 10 required normal approximation.Since theory-based methods used medical consultant example, ’ll turn another example demonstrate methods, conditions approximating distribution \\(\\hat{p}\\) normal distribution met.","code":""},{"path":"inference-one-prop.html","id":"payday-lenders","chapter":"14 Inference for a single proportion","heading":"14.3.2 Hypothesis test for \\(H_0: \\pi = \\pi_0\\)","text":"One possible regulation payday lenders required credit check evaluate debt payments borrower’s finances. like know: borrowers support form regulation?Set hypotheses evaluate whether borrowers majority support type regulation. take “majority” mean greater 50% population.words,\\(H_0\\): majority support regulation\\(H_A\\): majority borrowers support regulationIn statistical notation,\\(H_0\\): \\(\\pi = 0.50\\)\\(H_A\\): \\(\\pi > 0.50\\),\\(\\pi\\) represents proportion payday loan borrowers support regulation.Note null hypothesis stated \\(H_0: \\pi = 0.50\\), even though saying “majority support” imply \\(\\pi \\leq 0.50\\). Indeed, textbooks write \\(H_0: \\pi \\leq 0.50\\) case, incorrect statement. However, calculating p-value, need assume particular value \\(\\pi\\) null hypothesis, textbook, null hypothesis always form:\\[\nH_0: \\mbox{ parameter } = \\mbox{ null value}\n\\]apply normal distribution model null distribution, independence success-failure conditions must satisfied. hypothesis test, success-failure condition checked using null proportion: verify \\(n\\pi_0\\) \\(n(1-\\pi_0)\\) least 10, \\(\\pi_0\\) null value.payday loan borrowers support regulation require lenders pull credit report evaluate debt payments? random sample 826 borrowers, 51% said support regulation. reasonable use normal distribution model \\(\\hat{p}\\) hypothesis test ?122Continuing previous Example, evaluate whether poll lending regulations provides convincing evidence majority payday loan borrowers support new regulation require lenders pull credit reports evaluate debt payments.hypotheses already set conditions checked, can move onto calculations. null standard error context one proportion hypothesis test computed using null value, \\(\\pi_0\\): \\[\\begin{align*}\n  SE_0(\\hat{p}) = \\sqrt{\\frac{\\pi_0 (1 - \\pi_0)}{n}}\n      = \\sqrt{\\frac{0.5 (1 - 0.5)}{826}}\n      = 0.017\n  \\end{align*}\\] picture normal model null distribution sample proportions scenario shown Figure 14.2, p-value represented shaded region. Note null distribution centered 0.50, null value, standard deviation 0.017.\\(H_0\\), probability observing \\(\\hat{p} = 0.51\\) higher 0.278, area 0.51 null distribution.p-value 0.278, poll provide convincing evidence majority payday loan borrowers support regulations around credit checks evaluation debt payments.’ll note conclusion somewhat unsatisfactory conclusion, case larger p-values. , resolution one way public opinion. claim exactly 50% people support regulation, claim majority support either.\nFigure 14.2: Approximate sampling distribution \\(\\hat{p}\\) across possible samples assuming \\(\\pi = 0.50\\). shaded area represents p-value corresponding observed sample proportion 0.51.\nOften, theory-based methods, use standardized statistic rather original statistic test statistic. standardized statistic computed subtracting mean null distribution original statistic, dividing standard error: \\[\n\\mbox{standardized statistic} = \\frac{\\mbox{observed statistic} - \\mbox{null value}}{\\mbox{null standard error}}\n\\] null standard error (\\(SE_0(\\text{statistic})\\)) observed statistic estimated standard deviation assuming null hypothesis true. can interpret standardized statistic number standard errors observed statistic (positive) (negative) null value. modeling null distribution normal distribution, standardized statistic called \\(Z\\), since Z-score sample proportion.Standardized sample proportion.standardized statistic theory-based methods one proportion \\[\nZ = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}} = \\frac{\\hat{p} - \\pi_0}{SE_0(\\hat{p})}\n\\] \\(\\pi_0\\) null value. denominator, \\(SE_0(\\hat{p}) = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\), called null standard error sample proportion.standardized statistic test statistic, can find p-value area standard normal distribution extreme observed \\(Z\\) value.payday loan borrowers support regulation require lenders pull credit report evaluate debt payments? random sample 826 borrowers, 51% said support regulation. set hypotheses checked conditions previously. Now calculate interpret standardized statistic, use standard normal distribution calculate approximate p-value.sample proportion \\(\\hat{p} = 0.51\\). Since null value \\(\\pi_0 = 0.50\\),\nnull standard error \\[\\begin{align*}\n  SE_0(\\hat{p}) = \\sqrt{\\frac{\\pi_0 (1 - \\pi_0)}{n}}\n      = \\sqrt{\\frac{0.5 (1 - 0.5)}{826}}\n      = 0.017\n  \\end{align*}\\]standardized statistic \\[\\begin{align*}\nZ = \\frac{0.51 - 0.50}{0.017} = 0.59\n\\end{align*}\\]Interpreting value, can say sample proportion 0.51 0.59 standard errors null value 0.50.Shown Figure 14.3, p-value area \\(Z = 0.59\\) standard normal distribution—0.278—p-value obtain finding area \\(\\hat{p} = 0.51\\) normal distribution mean 0.50 standard deviation 0.017, Figure 14.2.\nFigure 14.3: Approximate sampling distribution \\(Z\\) across possible samples assuming \\(\\pi = 0.50\\). shaded area represents p-value corresponding observed standardized statistic 0.59. Compare Figure 14.2.\nTheory-based hypothesis test proportion: one-sample \\(Z\\)-test.Frame research question terms hypotheses.Using null value, \\(\\pi_0\\), verify conditions using normal distribution approximate null distribution.Calculate test statistic: \\[\nZ = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}} = \\frac{\\hat{p} - \\pi_0}{SE_0(\\hat{p})}\n\\]Use test statistic standard normal distribution calculate p-value.Make conclusion based p-value, write conclusion context, plain language, terms alternative hypothesis.Regardless statistical method chosen, p-value always derived analyzing null distribution test statistic. normal model poorly approximates null distribution \\(\\hat{p}\\) success-failure condition satisfied. substitute, can generate null distribution using simulated sample proportions use distribution compute tail area, .e., p-value. Neither p-value approximated normal distribution simulated p-value exact, normal distribution simulated null distribution exact, close approximation. exact p-value can generated using binomial distribution, method covered text.","code":""},{"path":"inference-one-prop.html","id":"confidence-interval-for-pi","chapter":"14 Inference for a single proportion","heading":"14.3.3 Confidence interval for \\(\\pi\\)","text":"confidence interval provides range plausible values parameter \\(\\pi\\). point estimate best guess value parameter, makes sense build confidence interval around value. standard error, measure uncertainty associated point estimate, provides guide large make confidence interval. \\(\\hat{p}\\) can modeled using normal distribution, 68-95-99.7 rule tells us , general, 95% observations within 2 standard errors mean. , use value 1.96 slightly precise. confidence interval \\(\\pi\\) takes form \\[\\begin{align*}\n\\hat{p} \\pm z^{\\star} \\times SE(\\hat{p}).\n\\end{align*}\\]seen \\(\\hat{p}\\) sample proportion. value \\(z^{\\star}\\) comes standard normal distribution determined chosen confidence level. value standard error \\(\\hat{p}\\), \\(SE(\\hat{p})\\), approximates far expect sample proportion fall \\(\\pi\\), depends heavily sample size.Standard error one proportion, \\(\\hat{p}\\).conditions met distribution \\(\\hat{p}\\) nearly normal, variability single proportion, \\(\\hat{p}\\) well described standard deviation:\\[SD(\\hat{p}) = \\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\]Note almost never know true value \\(\\pi\\), can substitute best guess \\(\\pi\\) obtain approximate standard deviation, called standard error \\(\\hat{p}\\):\\[SD(\\hat{p}) \\approx \\hspace{3mm} SE(\\hat{p}) = \\sqrt{\\frac{(\\mbox{best guess }\\pi)(1 - \\mbox{best guess }\\pi)}{n}}\\]hypothesis testing, often use \\(\\pi_0\\) best guess \\(\\pi\\). confidence intervals, typically use \\(\\hat{p}\\) best guess \\(\\pi\\).Consider taking many polls registered voters (.e., random samples) size 300 asking support legalized marijuana. suspected 2/3 voters support legalized marijuana. understand sample proportion (\\(\\hat{p}\\)) vary across samples, calculate standard error \\(\\hat{p}\\).123A simple random sample 826 payday loan borrowers surveyed better understand interests around regulation costs. 51% responses supported new regulations payday lenders.reasonable model variability \\(\\hat{p}\\) sample sample using normal distribution?reasonable model variability \\(\\hat{p}\\) sample sample using normal distribution?Calculate standard error \\(\\hat{p}\\).Calculate standard error \\(\\hat{p}\\).Construct 95% confidence interval \\(\\pi\\), proportion payday borrowers support increased regulation payday lenders.Construct 95% confidence interval \\(\\pi\\), proportion payday borrowers support increased regulation payday lenders.data random sample, observations independent representative population interest.\nalso must check success-failure condition, using \\(\\hat{p}\\) place \\(\\pi\\) computing confidence interval:\nSupport: \\(n \\hat{p} = 826 \\times 0.51 \\approx 421 > 10\\)\n: \\(n (1 - \\hat{p}) = 826 \\times (1 - 0.51) \\approx 405 > 10\\)\nSince values least 10, can use normal distribution model sampling distribution \\(\\hat{p}\\).data random sample, observations independent representative population interest.also must check success-failure condition, using \\(\\hat{p}\\) place \\(\\pi\\) computing confidence interval:Support: \\(n \\hat{p} = 826 \\times 0.51 \\approx 421 > 10\\): \\(n (1 - \\hat{p}) = 826 \\times (1 - 0.51) \\approx 405 > 10\\)Since values least 10, can use normal distribution model sampling distribution \\(\\hat{p}\\).\\(\\pi\\) unknown standard error confidence interval, use \\(\\hat{p}\\) best guess \\(\\pi\\) formula.\n\\(SE(\\hat{p}) = \\sqrt{\\frac{0.51 (1 - 0.51)} {826}} = 0.017\\).\\(\\pi\\) unknown standard error confidence interval, use \\(\\hat{p}\\) best guess \\(\\pi\\) formula.\\(SE(\\hat{p}) = \\sqrt{\\frac{0.51 (1 - 0.51)} {826}} = 0.017\\).Using point estimate \\(0.51\\), \\(z^{\\star} = 1.96\\) 95% confidence interval, standard error \\(SE = 0.017\\) previous Guided Practice, confidence interval \\[\\begin{align*}\n  \\text{point estimate} &\\pm\\ z^{\\star} \\times SE \\\\\n   \\quad\\\\quad\n   0.51 \\ &\\pm\\ 1.96 \\times 0.017 \\\\\n   \\quad\\\\quad\n   (0.477, &0.543)\n  \\end{align*}\\] 95% confident true proportion payday borrowers supported regulation time poll 0.477 0.543.Using point estimate \\(0.51\\), \\(z^{\\star} = 1.96\\) 95% confidence interval, standard error \\(SE = 0.017\\) previous Guided Practice, confidence interval \\[\\begin{align*}\n  \\text{point estimate} &\\pm\\ z^{\\star} \\times SE \\\\\n   \\quad\\\\quad\n   0.51 \\ &\\pm\\ 1.96 \\times 0.017 \\\\\n   \\quad\\\\quad\n   (0.477, &0.543)\n  \\end{align*}\\] 95% confident true proportion payday borrowers supported regulation time poll 0.477 0.543.Constructing confidence interval single proportion.four steps constructing confidence interval \\(p\\).Check independence success-failure condition using \\(\\hat{p}\\). conditions met, sampling distribution \\(\\hat{p}\\) may well-approximated normal model.Construct standard error: \\[\nSE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n  \\]Use statistical software find multiplier \\(z^{\\star}\\) corresponding confidence level.Apply general confidence interval formula \\(\\mbox{statistic} \\pm (\\mbox{multiplier}) \\times SE\\): \\[\n\\hat{p} \\pm z^{\\star}\\times \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n  \\]","code":""},{"path":"inference-one-prop.html","id":"zstar-and-the-confidence-level","chapter":"14 Inference for a single proportion","heading":"\\(z^{\\star}\\) and the confidence level","text":"Suppose want consider confidence intervals confidence level somewhat higher 95%: perhaps like confidence level 99%. Think back analogy trying catch fish: want sure catch fish, use wider net. create 99% confidence level, must also widen 95% interval. hand, want interval lower confidence, 90%, make original 95% interval slightly slimmer.95% confidence interval structure provides guidance make intervals new confidence levels. general 95% confidence interval parameter whose point estimate nearly normal distribution: \\[\\begin{eqnarray}\n\\text{point estimate}\\ \\pm\\ 1.96\\times SE\n\\end{eqnarray}\\] three components interval: point estimate, “1.96”, standard error. choice \\(1.96\\times SE\\) based capturing 95% sampling distribution statistics since point estimate within 1.96 standard errors true parameter 95% time. choice 1.96 corresponds 95% confidence level.\\(X\\) normally distributed random variable, often \\(X\\) within 2.58 standard deviations mean?124\nFigure 14.4: area -\\(z^{\\star}\\) \\(z^{\\star}\\) increases \\(|z^{\\star}|\\) becomes larger. confidence level 99%, choose \\(z^{\\star}\\) 99% normal curve -\\(z^{\\star}\\) \\(z^{\\star}\\), corresponds 0.5% lower tail 0.5% upper tail: \\(z^{\\star}=2.58\\).\ncreate 99% confidence interval, change 1.96 95% confidence interval formula 2.58. previous Guided Practice highlights 99% time normal random variable within 2.58 standard deviations mean. approach—using Z-scores normal model compute confidence levels—appropriate point estimate associated normal distribution can properly compute standard error. Thus, formula 99% confidence interval :normal approximation crucial precision \\(z^\\star\\) confidence intervals. normal model good fit, use alternative distributions better characterize sampling distribution use bootstrapping procedures.Create 99% confidence interval impact stent risk stroke using data Section 1.1. point estimate 0.090, standard error \\(SE = 0.028\\). verified point estimate can reasonably modeled normal distribution.125Theory-based \\((1-\\alpha)\\times 100\\)% confidence interval.statistic follows normal model standard error \\(SE\\), confidence interval population parameter \\[\\begin{eqnarray*}\n\\text{statistic}\\ \\pm\\ z^{\\star} \\times SE\n\\end{eqnarray*}\\] \\(z^{\\star}\\) corresponds confidence level selected: middle \\((1-\\alpha)\\times 100\\)% standard normal distribution lies \\(-z^{\\star}\\) \\(z^{\\star}\\).","code":""},{"path":"inference-one-prop.html","id":"using-r-to-find-zstar","chapter":"14 Inference for a single proportion","heading":"Using R to find \\(z^{\\star}\\)","text":"Figure 14.4 provides picture identify \\(z^{\\star}\\) based confidence level. select \\(z^{\\star}\\) area -\\(z^{\\star}\\) \\(z^{\\star}\\) normal model corresponds confidence level. R, can find \\(z^{\\star}\\) using qnorm() function:Previously, found implanting stent brain patient risk stroke increased risk stroke. study estimated 9% increase number patients stroke, standard error estimate \\(SE = 2.8%\\). Compute 90% confidence interval effect.126","code":"\n# z* for 90% --> alpha = 0.15 --> need 5% on each side:\nqnorm(.90 + .05)\n#> [1] 1.645\n\n# z* for 95% --> alpha = 0.05 --> need 2.5% on each side:\nqnorm(.95 + .025)\n#> [1] 1.96\n\n# z* for 99% --> alpha = 0.01 --> need .5% on each side:\nqnorm(.99 + .005)\n#> [1] 2.576"},{"path":"inference-one-prop.html","id":"violating-conditions","chapter":"14 Inference for a single proportion","heading":"14.3.4 Violating conditions","text":"’ve spent lot time discussing conditions \\(\\hat{p}\\) can reasonably modeled normal distribution. happens success-failure condition fails? independence condition fails? either case, general ideas confidence intervals hypothesis tests remain , strategy technique used generate interval p-value change.success-failure condition isn’t met hypothesis test, can simulate null distribution \\(\\hat{p}\\) using null value, \\(\\pi_0\\), seen Section 14.1. Unfortunately, methods dealing observations independent outside scope book.","code":""},{"path":"inference-one-prop.html","id":"chp14-review","chapter":"14 Inference for a single proportion","heading":"14.4 Chapter review","text":"","code":""},{"path":"inference-one-prop.html","id":"summary-10","chapter":"14 Inference for a single proportion","heading":"Summary","text":"TODO","code":""},{"path":"inference-one-prop.html","id":"terms-11","chapter":"14 Inference for a single proportion","heading":"Terms","text":"introduced following terms chapter. ’re sure terms mean, recommend go back text review definitions. purposefully presenting alphabetical order, instead order appearance, little challenging locate. However able easily spot bolded text.","code":""},{"path":"inference-one-prop.html","id":"key-ideas-10","chapter":"14 Inference for a single proportion","heading":"Key ideas","text":"TODO","code":""},{"path":"inference-two-props.html","id":"inference-two-props","chapter":"15 Inference for comparing two proportions","heading":"15 Inference for comparing two proportions","text":"TODOSex discrimination case study now chapter 11.Old content - revise neededBelow summarize notation used throughout chapter.Notation.\\(n_1\\), \\(n_2\\) = sample sizes two independent samples\\(\\hat{p}_1\\), \\(\\hat{p}_2\\) = sample proportions two independent samples\\(\\pi_1\\), \\(\\pi_2\\) = population proportions two independent samplesWe now extend methods Chapter 14 apply confidence intervals hypothesis tests differences population proportions come two groups: \\(\\pi_1 - \\pi_2\\).investigations, ’ll identify reasonable point estimate \\(\\pi_1 - \\pi_2\\) based sample, may already guessed form: \\(\\hat{p}_1 - \\hat{p}_2\\). ’ll look statistical inference difference proportions two ways: simulation-based methods randomization test bootstrap confidence interval, theory-based methods two sample \\(z\\)-test \\(z\\)-interval.","code":""},{"path":"inference-two-props.html","id":"two-prop-errors","chapter":"15 Inference for comparing two proportions","heading":"15.1 Randomization test for \\(H_0: \\pi_1 - \\pi_2 = 0\\)","text":"learned Chapter 1, randomized experiment done assess whether one variable (explanatory variable) causes changes second variable (response variable). Every data set variability , decide whether variability data due (1) causal mechanism (randomized explanatory variable experiment) instead (2) natural variability inherent data, set sham randomized experiment comparison. , assume observational unit gotten exact response value regardless treatment level. reassigning treatments many many times, can compare actual experiment sham experiment. actual experiment extreme results sham experiments, led believe explanatory variable causing result inherent data variability. Using different studies, let’s look carefully idea randomization test.","code":""},{"path":"inference-two-props.html","id":"caseStudySexDiscrimination-revisited","chapter":"15 Inference for comparing two proportions","heading":"15.1.1 Case study: Sex discrimination revisited","text":"Need revise section - sex discrimination case study introduced Chapter 9We consider study investigating gender discrimination 1970s, set context personnel decisions within bank.127 research question hope answer , “females discriminated promotion decisions made male managers?”Add subsections steps hypotheses:Steps 1 2: Hypotheses test statisticSteps 3 4: Null distribution p-valueStep 5: ConclusionThe participants study 48 male bank supervisors attending management institute University North Carolina 1972. asked assume role personnel director bank given personnel file judge whether person promoted branch manager position. files given participants identical, except half indicated candidate male half indicated candidate female. files randomly assigned subjects.observational study experiment? type study impact can inferred results?128For supervisor recorded gender associated assigned file promotion decision. Using results study summarized Table 15.1, like evaluate females unfairly discriminated promotion decisions. study, smaller proportion females promoted males (0.583 versus 0.875), unclear whether difference provides convincing evidence females unfairly discriminated .\nTable 15.1: Summary results gender discrimination study.\ndata visualized Figure 15.1. Note promoted decision colored red (promoted) white(promoted). Additionally, observations broken male female groups.\nFigure 15.1: gender descrimination study can thought 48 red black cards.\nStatisticians sometimes called upon evaluate strength evidence. looking rates promotion males females study, might tempted immediately conclude females discriminated ?large difference promotion rates (58.3% females versus 87.5% males) suggest might discrimination women promotion decisions. However, yet sure observed difference represents discrimination just random chance. Generally little bit fluctuation sample data, wouldn’t expect sample proportions exactly equal, even truth promotion decisions independent gender.Additionally, researchers used convenience sample—48 male bank supervisors attending management institute—need think carefully population can generalize results.previous example reminder observed outcomes sample may perfectly reflect true relationships variables underlying population. Table 15.1 shows 7 fewer promotions female group male group, difference promotion rates 29.2%: \\[\n\\hat{p}_M - \\hat{p}_F = \\frac{21}{24} - \\frac{14}{24} = 0.292.\n\\] point estimate true difference large, sample size study small, making unclear observed difference represents discrimination whether simply due chance. two competing claims null alternative hypotheses:\\(H_0\\): Null hypothesis. variables gender decision independent. relationship, observed difference proportion males females promoted, 29.2%, due chance.\\(H_0\\): Null hypothesis. variables gender decision independent. relationship, observed difference proportion males females promoted, 29.2%, due chance.\\(H_A\\): Alternative hypothesis. variables gender decision independent. difference promotion rates 29.2% due chance, equally qualified females less likely promoted males.\\(H_A\\): Alternative hypothesis. variables gender decision independent. difference promotion rates 29.2% due chance, equally qualified females less likely promoted males.statistical notation:\\(H_0: \\pi_M - \\pi_F = 0\\)\\(H_0: \\pi_M - \\pi_F = 0\\)\\(H_A: \\pi_M - \\pi_F > 0\\)\\(H_A: \\pi_M - \\pi_F > 0\\)mean null hypothesis, says variables gender decision unrelated, true? mean banker decide whether promote candidate without regard gender indicated file. , difference promotion percentages due way files randomly divided bankers, randomization just happened give rise relatively large difference 29.2%.Consider alternative hypothesis: bankers influenced gender listed personnel file. true, especially influence substantial, expect see difference promotion rates male female candidates. gender bias females, expect smaller fraction promotion recommendations female personnel files relative male files.choose two competing claims assessing data conflict much \\(H_0\\) null hypothesis deemed reasonable. case, data support \\(H_A\\), reject notion independence conclude data provide strong evidence discrimination.Table 15.1 shows 35 bank supervisors recommended promotion 13 . Now, suppose bankers’ decisions independent gender. , conducted experiment different random assignment gender files, differences promotion rates based random fluctuation. can actually perform randomization, simulates happened bankers’ decisions independent gender distributed file genders differently.129In simulation, thoroughly shuffle 48 personnel files, 35 labeled promoted 13 labeled promoted, deal files two stacks. Note keeping 35 promoted 13 promoted, assuming 35 bank managers promoted individual whose content contained file (independent gender). deal 24 files first stack, represent 24 “female” files. second stack also 24 files, represent 24 “male” files. Figure 15.2 highlights shuffle reallocation sham gender groups.\nFigure 15.2: gender descrimination data shuffled reallocated gender groups.\n, original data, tabulate results determine fraction male female promoted.Since randomization files simulation independent promotion decisions, difference two promotion rates entirely due chance. Table 15.2 show results one simulation.\nTable 15.2: Simulation results, difference promotion rates male female purely due chance.\ndifference promotion rates two simulated groups Table 15.2 ? compare observed difference 29.2% actual study?130Figure 15.3 shows difference promotion rates much larger original data simulated groups (0.292 >>> 0.042). quantity interest throughout case study difference promotion rates. summary value statistic interest (often test statistic).\nFigure 15.3: summarize randomized data produce one estimate difference proportions given gender discrimination.\ncomputed one possible sample difference proportions null hypothesis Guided Practice , represents one difference due chance. first simulation, physically dealt files, much efficient perform simulation using computer. Repeating simulation computer, get another difference due chance: -0.042. another: 0.208. repeat simulation enough times good idea represents distribution differences sample proportions chance alone. Figure 15.4 shows plot differences found 100 simulations, dot represents simulated difference proportions male female files recommended promotion.\nFigure 15.4: dot plot differences 100 simulations produced null hypothesis, \\(H_0\\), gender_simulated decision independent. Two 100 simulations difference least 29.2%, difference observed study, shown solid red dots.\nNote distribution simulated differences proportions centered around 0. simulated differences way made distinction men women, makes sense: expect differences chance alone fall around zero random fluctuation simulation.often observe difference least 29.2% (0.292) according Figure 15.4? Often, sometimes, rarely, never?appears difference least 29.2% due chance alone happen 2% time according Figure 15.4. low probability indicates observing large difference chance rare.difference 29.2% rare event really impact listing gender candidates’ files, provides us two possible interpretations study results:\\(H_0\\): Null hypothesis. Gender effect promotion decision, observed difference large happen rarely.\\(H_0\\): Null hypothesis. Gender effect promotion decision, observed difference large happen rarely.\\(H_A\\): Alternative hypothesis. Gender effect promotion decision, observed actually due equally qualified women discriminated promotion decisions, explains large difference 29.2%.\\(H_A\\): Alternative hypothesis. Gender effect promotion decision, observed actually due equally qualified women discriminated promotion decisions, explains large difference 29.2%.conduct formal studies, reject null position (idea data result chance ) data strongly conflict null position.131 analysis, determined \\(\\approx\\) 2% probability obtaining sample \\(\\geq\\) 29.2% males females get promoted chance alone, conclude data provide strong evidence gender discrimination women supervisors.statistical term given 2% probability obtaining sample \\(\\geq\\) 29.2% males females get promoted chance alone?132Since study randomized experiment, can conclude effect due gender discrimination—gender application caused lower rate promotion. However, since study convenience sample, can generalize result individuals similar study. Thus, evidence gender discrimination, among male bank supervisors attending management institute University North Carolina 1972 similar study.","code":""},{"path":"inference-two-props.html","id":"cpr","chapter":"15 Inference for comparing two proportions","heading":"15.1.2 Case study: CPR and blood thinner","text":"Cardiopulmonary resuscitation (CPR) procedure used individuals suffering heart attack emergency resources unavailable. procedure helpful providing blood circulation keep person alive, CPR chest compressions can also cause internal injuries. Internal bleeding injuries can result CPR complicate additional treatment efforts. instance, blood thinners may used help release clot causing heart attack patient arrives hospital. However, blood thinners negatively affect internal injuries.consider experiment patients underwent CPR heart attack subsequently admitted hospital.133 patient randomly assigned either receive blood thinner (treatment group) receive blood thinner (control group). outcome variable interest whether patient survived least 24 hours.Form hypotheses study plain statistical language. Let \\(\\pi_c\\) represent true survival rate people receive blood thinner (corresponding control group) \\(\\pi_t\\) represent true survival rate people receiving blood thinner (corresponding treatment group).want understand whether blood thinners helpful harmful. ’ll consider possibilities using two-sided hypothesis test.\\(H_0\\): Blood thinners overall survival effect, .e., \\(\\pi_t - \\pi_c = 0\\).\\(H_0\\): Blood thinners overall survival effect, .e., \\(\\pi_t - \\pi_c = 0\\).\\(H_A\\): Blood thinners impact survival, either positive negative, zero, .e., \\(\\pi_t - \\pi_c \\neq 0\\).\\(H_A\\): Blood thinners impact survival, either positive negative, zero, .e., \\(\\pi_t - \\pi_c \\neq 0\\).Note done one-sided hypothesis test, resulting hypotheses :\\(H_0\\): Blood thinners positive overall survival effect, .e., \\(\\pi_t - \\pi_c = 0\\).\\(H_0\\): Blood thinners positive overall survival effect, .e., \\(\\pi_t - \\pi_c = 0\\).\\(H_A\\): Blood thinners positive impact survival, .e., \\(\\pi_t - \\pi_c > 0\\).\\(H_A\\): Blood thinners positive impact survival, .e., \\(\\pi_t - \\pi_c > 0\\).50 patients experiment receive blood thinner 40 patients . study results shown Table 15.3.\nTable 15.3: Results CPR study. Patients treatment group given blood thinner, patients control group .\nobserved survival rate control group? treatment group? Also, provide point estimate difference survival proportions two groups (\\(\\hat{p}_t - \\hat{p}_c\\)) relative “risk” survival (\\(\\hat{p}_t/\\hat{p}_c\\)).134According point estimate, patients undergone CPR outside hospital, additional 13% patients survive treated blood thinners. Interpreting relative risk, patients sample undergone CPR outside hospital 59% higher survival rate treated blood thinners. However, wonder difference easily explainable chance.past studies chapter, simulate type differences might see chance alone null hypothesis. randomly assigning “simulated treatment” “simulated control” stickers patients’ files, get new grouping. repeat simulation 10,000 times, can build null distribution differences sample proportions shown Figure 15.5.\nFigure 15.5: Null distribution point estimate difference proportions, \\(\\hat{p}_t - \\hat{p}_c\\). shaded right tail shows observations least large observed difference, 0.13.\nright tail area 0.131.135 However, contrary calculated p-value previous studies, p-value test 0.131!p-value defined chance observe result least favorable alternative hypothesis result (.e., difference) observe. case, differences less equal \\(-0.13\\) also provide equally strong evidence favoring alternative hypothesis difference \\(+0.13\\) . difference \\(-0.13\\) correspond survival rate control group 0.13 higher treatment group.136 Figure 15.6 ’ve also shaded differences left tail distribution. two shaded tails provide visual representation p-value two-sided test.\nFigure 15.6: Null distribution point estimate difference proportions, \\(\\hat{p}_t - \\hat{p}_c\\). values least extreme +0.13 either direction away 0 shaded.\ntwo-sided test, since null distribution symmetric, take single tail (case, 0.131) double get p-value: 0.262. large p-value, find statistically significant evidence blood thinner influence survival patients undergo CPR prior arriving hospital.","code":""},{"path":"inference-two-props.html","id":"caseStudyOpportunityCost","chapter":"15 Inference for comparing two proportions","heading":"15.1.3 Case study: Opportunity cost","text":"rational consistent behavior typical American college student? section, ’ll explore whether college student consumers always consider following: money spent now can spent later.particular, interested whether reminding students well-known fact money causes little thriftier. skeptic might think reminder impact. can summarize two different perspectives using null alternative hypothesis framework.\\(H_0\\): Null hypothesis. Reminding students can save money later purchases impact students’ spending decisions.\\(H_0\\): Null hypothesis. Reminding students can save money later purchases impact students’ spending decisions.\\(H_A\\): Alternative hypothesis. Reminding students can save money later purchases reduce chance continue purchase.\\(H_A\\): Alternative hypothesis. Reminding students can save money later purchases reduce chance continue purchase.design randomized experiment test two hypotheses?137In statistical notation, can define parameters \\(\\pi_{ctrl}\\) = probability student control condition (reminding can save money later purchases) refrains making purchase, \\(\\pi_{trmt}\\) = probability student treatment condition (reminding can save money later purchases) refrains makes purchase. hypotheses \\(H_0: \\pi_{trmt} - \\pi_{ctrl} = 0\\)\\(H_0: \\pi_{trmt} - \\pi_{ctrl} = 0\\)\\(H_A: \\pi_{trmt} - \\pi_{ctrl} > 0\\)\\(H_A: \\pi_{trmt} - \\pi_{ctrl} > 0\\)section, ’ll explore experiment conducted researchers investigates question students university southwestern United States.138One-hundred fifty students recruited study, given following statement:Imagine saving extra money side make purchases, recent visit video store come across special sale new video. video one favorite actor actress, favorite type movie (comedy, drama, thriller, etc.). particular video considering one thinking buying long time. available special sale price $14.99.situation? Please circle one options .Half 150 students randomized control group given following two options:Buy entertaining video.buy entertaining video.remaining 75 students placed treatment group, saw slightly modified option (B):Buy entertaining video.buy entertaining video. Keep $14.99 purchases.extra statement reminding students obvious fact impact purchasing decision? Table 15.4 summarizes study results.\nTable 15.4: Summary student choices opportunity cost study.\nmight little easier review results using row proportions, specifically considering proportion participants group said buy buy DVD. summaries given Table 15.5, segmented bar plot provided Figure 15.7.\nTable 15.5: data now summarized using row proportions. Row proportions particularly useful since can view proportion buy buy decisions group.\n\nFigure 15.7: Segmented bar plot comparing proportion bought buy DVD control treatment groups.\ndefine success study student chooses buy DVD.139 , value interest change DVD purchase rates results reminding students spending money now means can spend money later.Check define success previous chapter refer .can construct point estimate difference \\[\\begin{align*}\n\\hat{p}_{trmt} - \\hat{p}_{ctrl}\n= \\frac{34}{75} - \\frac{19}{75}\n= 0.453 - 0.253\n= 0.200\n\\end{align*}\\] proportion students chose buy DVD 20% higher treatment group control group. However, result statistically significant? words, 20% difference two groups prominent unlikely occurred chance alone?primary goal data analysis understand sort differences might see null hypothesis true, .e., treatment effect students. , ’ll use procedure applied Section 9.2: randomization.Let’s think data context hypotheses. null hypothesis (\\(H_0\\)) true treatment impact student decisions, observed difference two groups 20% attributed entirely chance. , hand, alternative hypothesis (\\(H_A\\)) true, difference indicates reminding students saving later purchases actually impacts buying decisions.Just like gender discrimination study, can perform statistical analysis. Using randomization technique last section, let’s see happens simulate experiment scenario effect treatment.reality simulation computer, might useful think go carrying simulation without computer. start 150 index cards label card indicate distribution response variable: decision. , 53 cards labeled “buy DVD” represent 53 students opted buy, 97 labeled “buy DVD” 97 students. shuffle cards thoroughly divide two stacks size 75, representing simulated treatment control groups. observed difference proportions “buy DVD” cards (earlier defined success) can attributed entirely chance.randomly assigning cards simulated treatment control groups, many “buy DVD” cards expect end simulated group? expected difference proportions “buy DVD” cards group?Since simulated groups equal size, expect \\(53 / 2 = 26.5\\), .e., 26 27, “buy DVD” cards simulated group, yielding simulated point estimate 0% . However, due random fluctuations, might actually observe number little 26 27.results single randomization chance alone shown Table 15.6. table, can compute difference occurred chance alone: \\[\\begin{align*}\n\\hat{p}_{trmt, simulated} - \\hat{p}_{ctrl, simulated}\n= \\frac{24}{75} - \\frac{29}{75}\n= 0.32 - 0.387\n= - 0.067\n\\end{align*}\\] \nTable 15.6: Summary student choices simulated groups. group assignment connection student decisions, difference two groups due chance.\nJust one simulation enough get sense sorts differences happen chance alone. ’ll simulate another set simulated groups compute new difference: 0.013. : 0.067. : -0.173. ’ll 1,000 times. results summarized dot plot Figure 15.8, point represents simulation. Since many points, convenient summarize results histogram one Figure 15.9, height histogram bar represents fraction observations group.\nFigure 15.8: stacked dot plot 1,000 chance differences produced null hypothesis, \\(H_0\\). Six 1,000 simulations difference least 20% , difference observed study.\n\nFigure 15.9: histogram 1,000 chance differences produced null hypothesis, \\(H_0\\). Histograms like one convenient representation data results large number observations.\ntreatment effect, ’d observe difference least +20% 0.6% time, 1--150 times. really rare! Instead, conclude data provide strong evidence treatment effect: reminding students purchase instead spend money later something else lowers chance continue purchase. Notice able make causal statement study since study experiment.Since study randomized experiment, can conclude effect due reminder saving money purchases—reminder caused lower rate purchase. However, since study used volunteer sample (students “recruited”), can generalize result individuals similar study. Thus, evidence reminding students can save money later purchases reduce chance continue purchase, among students similar study.","code":""},{"path":"inference-two-props.html","id":"caseStudyMalaria","chapter":"15 Inference for comparing two proportions","heading":"15.1.4 Case study: Malaria vaccine","text":"consider study new malaria vaccine called PfSPZ. study, volunteer patients randomized one two experiment groups: 14 patients received experimental vaccine 6 patients received placebo vaccine. Nineteen weeks later, 20 patients exposed drug-sensitive malaria virus strain; motivation using drug-sensitive strain virus ethical considerations, allowing infections treated effectively. results summarized Table 15.7, 9 14 treatment patients remained free signs infection 6 patients control group patients showed baseline signs infection.\nTable 15.7: Summary results malaria vaccine experiment.\nobservational study experiment? implications study type can inferred results?140In study, smaller proportion patients received vaccine showed signs infection (35.7% versus 100%). However, sample small, unclear whether difference provides convincing evidence vaccine effective. determine , need perform statistical inference.Instead using difference proportion infected summary measure, let’s use relative risk infection case study. Thus, parameter interest \\(\\pi_{Vac} / \\pi_{Pla}\\), point estimate parameter \\[\n\\frac{\\hat{p}_{Vac}}{\\hat{p}_{Pla}} = \\frac{5/14}{6/6} = 0.357.\n\\]Converting percent decrease141, see patients vaccine group 64.3% reduced risk infection compared placebo group.142In terms relative risk, null alternative hypotheses areIndependence model \\(H_0: \\dfrac{\\pi_{Vac}}{\\pi_{Pla}} = 1\\) Alternative model \\(H_A: \\dfrac{\\pi_{Vac}}{\\pi_{Pla}} < 1\\)Whether write hypotheses terms difference proportions ratio proportions (relative risk), hypotheses still interpretation. example, three null hypotheses \\(H_0: \\pi_{Vac} = \\pi_{Pla}\\), \\(H_0: \\pi_{Vac} - \\pi_{Pla} = 0\\), \\(H_0: \\pi_{Vac}/\\pi_{Pla} = 1\\), algebraically equivalent.mean independence model, says vaccine influence rate infection, true? mean 11 patients going develop infection matter group randomized , 9 patients develop infection matter group randomized . , vaccine affect rate infection, difference infection rates due chance alone patients randomized.Now consider alternative model: infection rates influenced whether patient received vaccine . true, especially influence substantial, expect see difference infection rates patients groups.choose two competing claims assessing data conflict much \\(H_0\\) independence model deemed reasonable. case, data support \\(H_A\\), reject notion independence conclude vaccine effective.’re going implement simulation, pretend know malaria vaccine tested work. Ultimately, want understand large difference observed common simulations. common, maybe difference observed purely due chance. uncommon, possibility vaccine helpful seems plausible.can randomize responses (infection infection) treatment conditions null hypothesis independence, time, ’ll compute sample relative risks simulated sample.use cards re-randomize one sample groups? Remember, hypothetical world, believe patient got infection going get regardless group , like see happens randomly assign patients treatment control groups .143Figure 15.10 shows histogram relative risks found 1,000 randomization simulations, dot represents simulated relative risk infection (treatment rate divided control rate).\nFigure 15.10: histogram relative risks infection 1,000 simulations produced independence model \\(H_0\\), simulations infections unaffected vaccine. Seventeen 1,000 simulations (shaded red) relative risk 0.357, relative risk observed study.\nNote distribution simulated differences centered around 1. simulated relative risks assuming independence model true, condition, expect difference near one random fluctuation, near pretty generous case since sample sizes small study.often observe sample relative risk 0.357 (least 64.3% reduction risk vaccine) according Figure 15.10? Often, sometimes, rarely, never?appears 64.3% reduction risk due chance alone happen 2% time according Figure 15.10. low probability indicates rare event.Based simulations, two options:conclude study results provide strong evidence independence model. , sufficiently strong evidence conclude vaccine effect clinical setting.conclude study results provide strong evidence independence model. , sufficiently strong evidence conclude vaccine effect clinical setting.conclude evidence sufficiently strong reject \\(H_0\\) assert vaccine useful. conduct formal studies, usually reject notion just happened observe rare event.144We conclude evidence sufficiently strong reject \\(H_0\\) assert vaccine useful. conduct formal studies, usually reject notion just happened observe rare event.144In case, reject independence model favor alternative. , concluding data provide strong evidence vaccine provides protection malaria clinical setting.Statistical inference built evaluating whether differences due chance. statistical inference, data scientists evaluate model reasonable given data. Errors occur, just like rare events, might choose wrong model. always choose correctly, statistical inference gives us tools control evaluate often errors occur.","code":""},{"path":"inference-two-props.html","id":"two-prop-boot-ci","chapter":"15 Inference for comparing two proportions","heading":"15.2 Bootstrap confidence interval for \\(\\pi_1 - \\pi_2\\)","text":"Section 15.1, worked randomization distribution understand distribution \\(\\hat{p}_1 - \\hat{p}_2\\) null hypothesis \\(H_0: \\pi_1 - \\pi_2 = 0\\) true. Now, bootstrapping, study variability \\(\\hat{p}_1 - \\hat{p}_2\\) without null assumption.","code":""},{"path":"inference-two-props.html","id":"observed-data-3","chapter":"15 Inference for comparing two proportions","heading":"15.2.1 Observed data","text":"Reconsider CPR data Section 15.1 provided Table 15.3. experiment consisted two treatments patients underwent CPR heart attack subsequently admitted hospital. patient randomly assigned either receive blood thinner (treatment group) receive blood thinner (control group). outcome variable interest whether patient survived least 24 hours., use difference sample proportions observed statistic interest. , value statistic : \\(\\hat{p}_t - \\hat{p}_c = 0.35 - 0.22 = 0.13\\).","code":""},{"path":"inference-two-props.html","id":"variability-of-the-statistic-2","chapter":"15 Inference for comparing two proportions","heading":"15.2.2 Variability of the statistic","text":"bootstrap method applied two samples extension method described Section 14.2. Now, two samples, sample estimates population came. CPR setting, treatment sample estimates population individuals gotten (get) treatment; control sample estimate population individuals get treatment controls. Figure 15.11 extends Figure 10.1 show bootstrapping process two samples simultaneously.\nFigure 15.11: Creating two estimated populations two different samples different populations.\n, population estimated, can randomly resample observations create bootstrap samples, seen Figure 15.12. Computationally, bootstrap resample created randomly sampling replacement original sample.\nFigure 15.12: Bootstrapped resamples two separate estimated populations.\nvariability statistic (difference sample proportions) can calculated taking one treatment bootstrap sample one control bootstrap sample calculating difference bootstrap survival proportions. Figure 15.13 displays one bootstrap resample estimated populations, difference sample proportions calculated treatment bootstrap sample control bootstrap sample.\nFigure 15.13: bootstrap resample left first estimated population; one right second. case, value simulated bootstrap statistic \\(\\hat{p}_1 - \\hat{p}_2 = \\frac{2}{7}-\\frac{1}{7}\\).\nalways, variability difference proportions can estimated repeated simulations, case, repeated bootstrap samples. Figure 15.14 shows multiple bootstrap differences calculated repeated bootstrap samples.\nFigure 15.14: pair bootstrap samples, calculate difference sample proportions.\nRepeated bootstrap simulations lead bootstrap sampling distribution statistic interest, difference sample proportions. Figure 15.15 visualizes process toy example, Figure 15.16 shows 1000 bootstrap differences proportions CPR data. Note CPR data includes 40 50 people respective groups, toy example includes 7 9 people two groups. Accordingly, variability distribution sample proportions higher toy example. turns standard error sample difference proportions inversely related sample sizes.\nFigure 15.15: process repeatedly resampling estimated population (sampling replacement original sample), computing difference sample proportions pair samples, plotting distribution.\n\nFigure 15.16: histogram differences proportions (treatment \\(-\\) control) 1000 bootstrap simulations using CPR data.\nFigure 15.16 provides estimate variability difference survival proportions sample sample. Section 14.2, bootstrap confidence interval can calculated directly bootstrapped differences Figure 15.16 finding percentiles distribution correspond confidence level. example, calculate 90% confidence interval finding 5th 95th percentile values bootstrapped differences. bootstrap 5th percentile proportion -0.03 95th percentile 0.28. result : 90% confident , population, true difference probability survival (treatment \\(-\\) control) -0.03 0.28. clearly, 90% confident probability survival heart attack patients underwent CPR blood thinners 0.03 less 0.28 patients given blood thinners. interval shows much definitive evidence affect blood thinners, one way another.\nFigure 15.17: CPR data bootstrapped 1000 times. simulation creates sample original data proportion survived treatment group \\(\\hat{p}_{t} = 14/40\\) proportion survived control group \\(\\hat{p}_{c} = 11/50\\).\n","code":""},{"path":"inference-two-props.html","id":"what-does-95-mean","chapter":"15 Inference for comparing two proportions","heading":"15.2.3 What does 95% mean?","text":"Recall goal confidence interval find plausible range values parameter interest. estimated statistic value interest, typically best guess unknown parameter. confidence level (often 95%) number takes get used . Surprisingly, percentage doesn’t describe data set hand, describes many possible data sets. One way understand confidence interval think confidence intervals ever made ever make scientist, confidence level describes intervals.Figure 15.18 demonstrates hypothetical situation 25 different studies performed exact population (goal estimating true parameter value \\(\\pi_1 - \\pi_2 = 0.47\\)). study hand represents one point estimate (dot) corresponding interval. possible know whether interval hand right unknown true parameter value (black line) left line. also impossible know whether interval captures true parameter (blue) doesn’t (red). making 95% intervals, 5% intervals create lifetime capture parameter interest (e.g., red Figure 15.18 ). know lifetimes scientists, 95% intervals created reported capture parameter value interest: thus language “95% confident.”\nFigure 15.18: One hypothetical population, parameter value : \\(\\pi_1 - \\pi_2 = 0.47\\). Twenty-five different studies led different point estimate, SE, confidence interval. study hand one horizontal lines (hopefully blue line!).\nchoice 95% 90% even 99% confidence level admittedly somewhat arbitrary; however, related logic used deciding p-value declared significant lower 0.05 (0.10 0.01, respectively). Indeed, one can show mathematically, 95% confidence interval two-sided hypothesis test cutoff 0.05 provide conclusion data mathematical tools applied analysis. full derivation explicit connection confidence intervals hypothesis tests beyond scope text.","code":""},{"path":"inference-two-props.html","id":"math-2prop","chapter":"15 Inference for comparing two proportions","heading":"15.3 Theory-based inferential methods for \\(\\pi_1 - \\pi_2\\)","text":"Like \\(\\hat{p}\\), difference two sample proportions \\(\\hat{p}_1 - \\hat{p}_2\\) can modeled using normal distribution certain conditions met.","code":""},{"path":"inference-two-props.html","id":"evaluating-the-two-conditions-required-for-modeling-pi_1---pi_2-using-theory-based-methods","chapter":"15 Inference for comparing two proportions","heading":"15.3.1 Evaluating the two conditions required for modeling \\(\\pi_1 - \\pi_2\\) using theory-based methods","text":"First, require broader independence condition, secondly, success-failure condition must met groups.Conditions sampling distribution \\(\\hat{p}_1 -\\hat{p}_2\\) normal.difference \\(\\hat{p}_1 - \\hat{p}_2\\) can modeled using normal distribution whenIndependence (extended). data independent within two groups. Generally satisfied data come two independent random samples data come randomized experiment.Success-failure condition. success-failure condition holds groups, check successes failures group separately. condition met least 10 successes 10 failures sample. data displayed two-way table, equivalent checking cells table least 10 observations.conditions satisfied, sampling distribution \\(\\hat{p}_1 - \\hat{p}_2\\) approximately normal mean \\(\\pi_1 - \\pi_2\\) standard deviation\\[\\begin{eqnarray*}\n  SD(\\hat{p}_1 - \\hat{p}_2) = \\sqrt{\\frac{\\pi_1(1-\\pi_1)}{n_1} + \\frac{\\pi_2(1-\\pi_2)}{n_2}}\n  \\end{eqnarray*}\\] \\(\\pi_1\\) \\(\\pi_2\\) represent population proportions, \\(n_1\\) \\(n_2\\) represent sample sizes.success-failure condition listed necessary sampling distribution \\(\\hat{p}_1 - \\hat{p}_2\\) approximately normal. mean sampling distribution \\(\\hat{p}_1 - \\hat{p}_2\\) \\(\\pi_1 - \\pi_2\\), standard deviation \\(\\sqrt{\\frac{\\pi_1(1-\\pi_1)}{n_1}+\\frac{\\pi_2(1-\\pi_2)}{n_2}}\\), regardless two sample sizes.case one proportion, typically don’t know true proportions \\(\\pi_1\\) \\(\\pi_2\\), substitute value check success-failure condition estimate standard deviation sampling distribution \\(\\hat{p}_1 - \\hat{p}_2\\).","code":""},{"path":"inference-two-props.html","id":"confidence-interval-for-pi_1---pi_2","chapter":"15 Inference for comparing two proportions","heading":"15.3.2 Confidence interval for \\(\\pi_1 - \\pi_2\\)","text":"Standard error difference two proportions, \\(\\hat{p}_1 -\\hat{p}_2\\): confidence intervals.computing theory-based confidence interval \\(\\pi_1 - \\pi_2\\), substitute \\(\\hat{p}_1\\) \\(\\pi_1\\) \\(\\hat{p}_2\\) \\(\\pi_2\\) expression standard deviation statistic, resulting standard error:standard error formula use computing confidence intervals difference two proportions.conditions sampling distribution \\(\\hat{p}_1 - \\hat{p}_2\\) normal met, can apply generic confidence interval formula difference two proportions, use \\(\\hat{p}_1 - \\hat{p}_2\\) point estimate substitute \\(SE\\) formula : \\[\\begin{align*}\n\\text{point estimate} \\ &\\pm\\  z^{\\star} \\times SE  \\quad\\\\\\\n\\hat{p}_1 - \\hat{p}_2 \\ &\\pm\\\n    z^{\\star} \\times\n   \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\n\\end{align*}\\]ac{_1(1-_1)}{n_1} + }\n\\end{align*}ac{_1(1-_1)}{n_1} + } \\end{align*}ac{_1(1-_1)}{n_1} + } \\end{align*}reconsider experiment patients underwent cardiopulmonary resuscitation (CPR) heart attack subsequently admitted hospital. patients randomly divided treatment group received blood thinner control group receive blood thinner. outcome variable interest whether patients survived least 24 hours. results shown Table 15.3. Check whether can model difference sample proportions using normal distribution.first check independence: since randomized experiment, condition satisfied.Next, check success-failure condition group. least 10 successes 10 failures experiment arm (11, 14, 39, 26), condition also satisfied.conditions satisfied, difference sample proportions can reasonably modeled using normal distribution data.Create interpret 90% confidence interval difference survival rates CPR study.’ll use \\(\\pi_t\\) true survival rate treatment group \\(\\pi_c\\) control group. point estimate \\(\\pi_t - \\pi_c\\) : \\[\\begin{align*}\n  \\hat{p}_{t} - \\hat{p}_{c}\n    = \\frac{14}{40} - \\frac{11}{50}\n    = 0.35 - 0.22\n    = 0.13\n  \\end{align*}\\] use standard error formula previously provided. one-sample proportion case, use sample estimates proportion formula confidence interval context: \\[\\begin{align*}\n  SE \\approx \\sqrt{\\frac{0.35 (1 - 0.35)}{40} +\n      \\frac{0.22 (1 - 0.22)}{50}}\n    = 0.095\n  \\end{align*}\\] 90% confidence interval, use \\(z^{\\star} = 1.65\\): \\[\\begin{align*}\n  \\text{point estimate} &\\pm\\ z^{\\star} \\times SE \\quad \\\\\\\n   0.13 \\ &\\pm\\ 1.65 \\times  0.095 \\quad = \\quad (-0.027, 0.287)\n  \\end{align*}\\] 90% confident survival probability patients given blood thinners 0.027 lower 0.287 higher patients given blood thinners, among patients like study. 0% contained interval, enough information say whether blood thinners help harm heart attack patients admitted undergone CPR.Note, problem set 90% indicate need high level confidence (95% 99%). lower degree confidence increases potential error, also produces narrow interval.5-year experiment conducted evaluate effectiveness fish oils reducing cardiovascular events, subject randomized one two treatment groups. consider heart attack outcomes patients listed Table 15.8.Create 95% confidence interval effect fish oils heart attacks patients well-represented study. Also interpret interval context study.145\nTable 15.8: Results study n-3 fatty acid supplement related health benefits.\n","code":""},{"path":"inference-two-props.html","id":"hypothesis-test-for-h_0-pi_1---pi_2-0","chapter":"15 Inference for comparing two proportions","heading":"15.3.3 Hypothesis test for \\(H_0: \\pi_1 - \\pi_2 = 0\\)","text":" mammogram X-ray procedure used check breast cancer. Whether mammograms used part controversial discussion, ’s topic next example learn two proportion hypothesis tests \\(H_0\\) \\(\\pi_1 - \\pi_2 = 0\\) (equivalently, \\(\\pi_1 = \\pi_2\\)).30-year study conducted nearly 90,000 female participants. 5-year screening period, woman randomized one two groups: first group, women received regular mammograms screen breast cancer, second group, women received regular non-mammogram breast cancer exams. intervention made following 25 years study, ’ll consider death resulting breast cancer full 30-year period. Results study summarized Figure 15.9.mammograms much effective non-mammogram breast cancer exams, expect see additional deaths breast cancer control group. hand, mammograms effective regular breast cancer exams, expect see increase breast cancer deaths mammogram group.\nTable 15.9: Summary results breast cancer study.\nstudy experiment observational study?146Set hypotheses test whether difference breast cancer deaths mammogram control groups.147The research question describing mammograms set address specific hypotheses (contrast confidence interval parameter). order fully take advantage hypothesis testing structure, assess randomness condition null hypothesis true (always hypothesis testing). Using data Table 15.9, check conditions using normal distribution analyze results study using hypothesis test. details checking conditions similar confidence intervals. However, null hypothesis \\(\\pi_1 - \\pi_2 = 0\\), use special proportion called pooled proportion check success-failure condition computing standard error: \\[\\begin{align*}\n\\hat{p}_{\\textit{pool}}\n    &= \\frac\n        {\\text{# patients died breast cancer entire study}}\n        {\\text{# patients entire study}} \\\\\n        &\\\\\n    &= \\frac{500 + 505}{500 + \\text{44,425} + 505 + \\text{44,405}} \\\\\n    &\\\\\n    &= 0.0112\n\\end{align*}\\] proportion estimate breast cancer death rate across entire study, ’s best estimate death rates \\(\\pi_{mgm}\\) \\(\\pi_{ctrl}\\) null hypothesis true \\(\\pi_{mgm} = \\pi_{ctrl}\\).Use pooled proportion \\(H_0\\) \\(\\pi_1 - \\pi_2 = 0\\).null hypothesis proportions equal, use pooled proportion (\\(\\hat{p}_{\\textit{pool}}\\)) verify success-failure condition estimate standard error: \\[\\begin{eqnarray*}\n  \\hat{p}_{\\textit{pool}}\n    = \\frac{\\text{total number successes}}\n      {\\text{total number cases}}\n    = \\frac{\\hat{p}_1 n_1 + \\hat{p}_2 n_2}{n_1 + n_2}\n  \\end{eqnarray*}\\] \\(\\hat{p}_1 n_1\\) represents number successes sample 1 since \\[\\begin{eqnarray*}\n  \\hat{p}_1\n    = \\frac{\\text{number successes sample 1}}{n_1}\n  \\end{eqnarray*}\\] Similarly, \\(\\hat{p}_2 n_2\\) represents number successes sample 2.reasonable model difference proportions using normal distribution study?patients randomized, can treated independent, within groups. also must check success-failure condition group. null hypothesis, proportions \\(\\pi_{mgm}\\) \\(\\pi_{ctrl}\\) equal, check success-failure condition best estimate values \\(H_0\\), pooled proportion two samples, \\(\\hat{p}_{\\textit{pool}} = 0.0112\\): \\[\\begin{align*}\n  \\hat{p}_{\\textit{pool}} \\times n_{mgm}\n      &= 0.0112 \\times \\text{44,925} = 503 \\\\\n   (1 - \\hat{p}_{\\textit{pool}}) \\times n_{mgm}\n      &= 0.9888 \\times \\text{44,925} = \\text{44,422} \\\\\n  & \\\\\n  \\hat{p}_{\\textit{pool}} \\times n_{ctrl}\n      &= 0.0112 \\times \\text{44,910} = 503 \\\\\n   (1 - \\hat{p}_{\\textit{pool}}) \\times n_{ctrl}\n      &= 0.9888 \\times \\text{44,910} = \\text{44,407}\n  \\end{align*}\\] success-failure condition satisfied since values least 10. conditions satisfied, can safely model difference proportions using normal distribution.used pooled proportion check success-failure condition148. next use standard error calculation.Standard error difference two proportions, \\(\\hat{p}_1 -\\hat{p}_2\\): hypothesis tests.Since assume \\(\\pi_1 = \\pi_2\\) conduct theory-based hypothesis test \\(H_0: \\pi_1 - \\pi_2 = 0\\), substitute pooled sample proportion, \\(\\hat{p}_{pool}\\) \\(\\pi_1\\) \\(\\pi_2\\) expression standard deviation statistic, resulting null standard error:standard error formula use computing test statistic hypothesis test \\(H_0: \\pi_1 - \\pi_2 = 0\\).Compute point estimate difference breast cancer death rates two groups, use pooled proportion \\(\\hat{p}_{\\textit{pool}} = 0.0112\\) calculate standard error.point estimate difference breast cancer death rates \\[\\begin{align*}\n  \\hat{p}_{mgm} - \\hat{p}_{ctrl}\n    &= \\frac{500}{500 + 44,425} - \\frac{505}{505 + 44,405} \\\\\n  & \\\\\n    &= 0.01113 - 0.01125 \\\\\n  & \\\\\n    &= -0.00012\n  \\end{align*}\\] breast cancer death rate mammogram group 0.00012 less control group.Next, standard error \\(\\hat{p}_{mgm} - \\hat{p}_{ctrl}\\) calculated using pooled proportion, \\(\\hat{p}_{\\textit{pool}}\\): \\[\\begin{align*}\nSE_0 = \\sqrt{\n      \\frac{\\hat{p}_{\\textit{pool}}(1-\\hat{p}_{\\textit{pool}})}\n          {n_{mgm}}\n      + \\frac{\\hat{p}_{\\textit{pool}}(1-\\hat{p}_{\\textit{pool}})}\n          {n_{ctrl}}\n    }\n    = 0.00070\n\\end{align*}\\]Using point estimate \\(\\hat{p}_{mgm} - \\hat{p}_{ctrl} = -0.00012\\) standard error \\(SE = 0.00070\\), calculate p-value hypothesis test write conclusion.Just like past tests, first compute test statistic draw picture: \\[\\begin{align*}\nZ = \\frac{\\text{point estimate} - \\text{null value}}{\\mbox{Null }SE}\n    = \\frac{-0.00012 - 0}{0.00070}\n    = -0.17\n\\end{align*}\\]lower tail area -0.17 standard normal distribution 0.4325, double get p-value: 0.8650 (see Figure 15.19). large p-value, difference breast cancer death rates reasonably explained chance, significant evidence mammograms either decrease increase risk death breast cancer compared regular breast exams, among women similar study.\nFigure 15.19: Standard normal distribution p-value shaded. shaded area represents probability observing difference sample proportions -0.17 away zero, true proportions equal.\nCan conclude mammograms benefits harm? considerations keep mind reviewing mammogram study well medical study:accept null hypothesis. can say don’t sufficient evidence conclude mammograms reduce breast cancer deaths, don’t sufficient evidence conclude mammograms increase breast cancer deaths.mammograms helpful harmful, data suggest effect isn’t large.mammograms less expensive non-mammogram breast exam? one option much expensive doesn’t offer clear benefits, lean towards less expensive option.study’s authors also found mammograms led -diagnosis breast cancer, means breast cancers found (thought found) cancers cause symptoms patients’ lifetimes. , something else kill patient breast cancer symptoms appeared. means patients may treated breast cancer unnecessarily, treatment another cost consider. also important recognize -diagnosis can cause unnecessary physical emotional harm patients.considerations highlight complexity around medical care treatment recommendations. Experts medical boards study medical treatments use considerations like provide best recommendation based current evidence. ","code":""},{"path":"inference-two-props.html","id":"chp15-review","chapter":"15 Inference for comparing two proportions","heading":"15.4 Chapter review","text":"","code":""},{"path":"inference-two-props.html","id":"summary-11","chapter":"15 Inference for comparing two proportions","heading":"Summary","text":"TODO","code":""},{"path":"inference-two-props.html","id":"summary-of-z-procedures","chapter":"15 Inference for comparing two proportions","heading":"15.4.1 Summary of Z-procedures","text":"far chapter, seen normal distribution applied appropriate mathematical model two distinct settings. Although two data structures different, similarities differences worth pointing . provide Table 15.10 partly mechanism understanding \\(z\\)-procedures partly highlight extremely common usage normal distribution practice. often hear following two \\(z\\)-procedures referred one sample \\(z\\)-test (\\(z\\)-interval) two sample \\(z\\)-test (\\(z\\)-interval).\nTable 15.10: Similarities \\(z\\)-methods across one sample two independent samples analysis categorical response variable.\nindependence, 2. large samples (least 10 successes 10 failures)\nindependence, 2. large samples (least 10 successes 10 failures sample)\nHypothesis tests. applying normal distribution hypothesis test, proceed follows:Write appropriate hypotheses.Verify conditions using normal distribution.\nOne-sample: observations must independent, must least 10 successes 10 failures.\ndifference proportions: sample must separately satisfy one-sample conditions normal distribution, data groups must also independent.\nOne-sample: observations must independent, must least 10 successes 10 failures.difference proportions: sample must separately satisfy one-sample conditions normal distribution, data groups must also independent.Compute statistic interest, null standard error, degrees freedom. \\(df\\), use \\(n-1\\) one sample, two samples use either statistical software smaller \\(n_1 - 1\\) \\(n_2 - 1\\).Compute Z-score using general formula: \\[\nZ = \\frac{\\mbox{statistic} - \\mbox{null value}}{\\mbox{null standard error statistic}} = \\frac{\\mbox{statistic} - \\mbox{null value}}{SE_0(\\mbox{statistic})}\n\\]Use statistical software find p-value using standard normal distribution:\nSign \\(H_A\\) \\(<\\): p-value = area Z-score\nSign \\(H_A\\) \\(>\\): p-value = area Z-score\nSign \\(H_A\\) \\(\\neq\\): p-value = 2 \\(\\times\\) area \\(-|\\mbox{Z-score}|\\)\nSign \\(H_A\\) \\(<\\): p-value = area Z-scoreSign \\(H_A\\) \\(>\\): p-value = area Z-scoreSign \\(H_A\\) \\(\\neq\\): p-value = 2 \\(\\times\\) area \\(-|\\mbox{Z-score}|\\)Make conclusion based p-value, write conclusion context, plain language, terms alternative hypothesis.Confidence intervals. Similarly, following generally compute confidence interval using normal distribution:Verify conditions using normal distribution. (See .)Compute statistic interest, standard error, \\(z^{\\star}\\).Calculate confidence interval using general formula: \\[\n\\mbox{statistic} \\pm\\ z^{\\star} SE(\\mbox{statistic}).\n\\]Put conclusions context plain language even non-data scientists can understand results.","code":""},{"path":"inference-two-props.html","id":"terms-12","chapter":"15 Inference for comparing two proportions","heading":"Terms","text":"introduced following terms chapter. ’re sure terms mean, recommend go back text review definitions. purposefully presenting alphabetical order, instead order appearance, little challenging locate. However able easily spot bolded text.","code":""},{"path":"inference-two-props.html","id":"key-ideas-11","chapter":"15 Inference for comparing two proportions","heading":"Key ideas","text":"TODO","code":""},{"path":"inference-categ-applications.html","id":"inference-categ-applications","chapter":"16 Applications: Infer categorical","heading":"16 Applications: Infer categorical","text":"TODOOld content - revise needed","code":""},{"path":"inference-categ-applications.html","id":"inference-for-categorical-data-using-r-and-catstats","chapter":"16 Applications: Infer categorical","heading":"16.1 Inference for categorical data using R and catstats","text":"","code":""},{"path":"inference-categ-applications.html","id":"making-tables-from-raw-data","chapter":"16 Applications: Infer categorical","heading":"Making tables from raw data","text":"collecting data, analysis starts raw data frame, one observational unit per row one variable per column. case, first need find counts (frequencies) observations category prior inference. can use table() function R raw data create contingency table counts categorical variable.one-proportion case, suppose data frame called loans variable regulate contains Yes/response 826 payday loan borrowers Section 14.3 regarding support regulation require lenders pull credit report evaluate debt payments. can obtain counts borrowers response using table() function R:R code , loans name data set R, regulate name variable. $ tells R select regulate variable loans data set. also used pipe command generate table:can also use table() compute proportions group:comparisons two proportions, get two-way table. Consider case study effect blood thinners survival receiving CPR Section 15.1. Data study stored data frame called cpr, variables survival – giving patient’s outcome decision – group – indicating whether treatment (blood thinner) control (blood thinner) group. glimpse() summary() functions can help us see variables data set values take :obtain two-way table choices group, use table() function R. key thing remember put response variable (outcome) first argument explanatory variable (grouping) second. example, survival response variable, group explanatory variable.set table useful making segmented bar plots using column percentages compute test statistics. order either things, need store table R object can manipulate :get column percentages, use prop.table() function:","code":"\ntable(loans$regulate)#> \n#>  No Yes \n#> 404 422\nloans %>% select(regulate) %>% table()#> regulate\n#>  No Yes \n#> 404 422\n#If we know the number of observations:\ntable(loans$regulate)/826#> \n#>    No   Yes \n#> 0.489 0.511\n#If we don't know the number of observations:\ntable(loans$regulate)/length(loans$regulate)#> \n#>    No   Yes \n#> 0.489 0.511\nglimpse(cpr)#> Rows: 90\n#> Columns: 2\n#> $ survival <fct> Survived, Survived, Survived, Survived, Survived, Survived, S…\n#> $ group    <fct> control, control, control, control, control, control, control…\nsummary(cpr)#>      survival        group   \n#>  Died    :65   control  :50  \n#>  Survived:25   treatment:40\ntable(cpr$survival, cpr$group)#>           \n#>            control treatment\n#>   Died          39        26\n#>   Survived      11        14\ndata_tbl <- table(cpr$survival, cpr$group)\nprop.table(data_tbl,  #Feed in your two-way table\n           margin = 2)  #Tell it to compute percentages for columns#>           \n#>            control treatment\n#>   Died        0.78      0.65\n#>   Survived    0.22      0.35"},{"path":"inference-categ-applications.html","id":"simulation-based-inference-for-one-proportion","chapter":"16 Applications: Infer categorical","heading":"Simulation-based inference for one proportion","text":"simulation-based inference, use functions included catstats package, created MSU Statistics courses. See Statistical computing section beginning textbook instructions install catstats haven’t already. package installed, can load R session make functions available using library() function:loaded package, able use functions simulation-based inference. one-proportion inference, one_proportion_test() function one_proportion_bootstrap_CI() function. Returning payday loan regulation example, can obtain simulation distribution p-value using following function call:Note observed statistic (as_extreme_as) summary_measure input need match; since put observed statistic proportion, need tell function report proportion successes. don’t match, almost certainly get p-value 0 1 – can happen strong evidence null, always good check function inputs get extreme outcome make sure seeing.resulting graph, see null distribution simulated proportions, greater observed value 0.51 highlighted blue. figure caption, see 308 1000 simulations resulted proportion successes least large observed value, yielding approximate p-value 0.308.find confidence interval true proportion payday loan borrowers support regulation, use one_proportion_bootstrap_CI() function:produces plot bootstrapped proportions upper lower bounds confidence interval marked, gives interval figure caption: case, 95% confident true proportion payday loan borrowers support proposed regulation 0.479 0.546.","code":"\nlibrary(catstats)\none_proportion_test(\n  probability_success = 0.5, #null hypothesis probability of success\n  sample_size = 826,  #number of observations\n  number_repetitions = 1000,  #number of simulations to create\n  as_extreme_as = 0.51,  #observed statistic\n  direction = \"greater\",  #alternative hypothesis direction\n  summary_measure = \"proportion\"  #Report number or proportion of successes?\n)\none_proportion_bootstrap_CI(\n  sample_size = 826,  #Number of observations\n  number_successes = 422,  #Number of observed successes \n  number_repetitions = 1000,  #Number of bootstrap draws\n  confidence_level = 0.95  #Confidence level, as a proportion\n)"},{"path":"inference-categ-applications.html","id":"simulation-based-inference-for-difference-in-two-proportions","chapter":"16 Applications: Infer categorical","heading":"Simulation-based inference for difference in two proportions","text":"inference difference two proportions, use two_proportion_test() two_proportion_bootstrap_CI() functions. functions assume data frame group outcome included variables. Using CPR blood thinner study, call two_proportion_test() look like :results give segmented bar plot data — can check formula correct making sure explanatory variable \\(x\\)-axis response variable \\(y\\)-axis. Look top right bar plot check correct order subtraction. Next bar plot, null distribution simulated differences proportions, observed statistic marked vertical line values extreme observed statistic colored red. figure caption gives approximate p-value: case 181/1000 = 0.181.couple things note using two_proportion_test function:need identify variable outcome group using formula argument.Specify order subtraction using first_in_subtraction putting EXACTLY category explanatory variable want first, quotes — must match capitalization, spaces, etc. text values!Specify success using response_value_numerator putting EXACTLY category response consider success, quotes. , capitalization, spaces, etc. matter !produce confidence interval true difference proportion patients survive receiving CPR, use two_proportion_bootstrap_CI(), uses arguments two_proportion_test() function:produces distribution bootstrapped statistics bounds confidence interval marked, value included caption. , 99% confident true proportion patients survive receiving CPR 0.1 lower 0.35 higher patients given blood thinners compared .","code":"\ntwo_proportion_test(\n  formula = survival ~ group,  #Always do response ~ explanatory\n  data = cpr,   #Name of the data set\n  first_in_subtraction = \"treatment\", #Value of the explanatory variable that is first in order of subtraction\n  response_value_numerator = \"Survived\",  #Value of response that is a \"success\"\n  number_repetitions = 1000,\n  as_extreme_as = 0.13,  #Observed statistic\n  direction = \"two-sided\"  #Direction of the alternative hypothesis\n)\ntwo_proportion_bootstrap_CI(\n  formula = survival ~ group,  #Always do response ~ explanatory\n  data = cpr,   #Name of the data set\n  first_in_subtraction = \"treatment\", #Value of the explanatory variable that is first in order of subtraction\n  response_value_numerator = \"Survived\",  #Value of response that is a \"success\"\n  number_repetitions = 1000, #Number of bootstrap samples\n  confidence_level = 0.99  #Confidence level, as a proportion\n)"},{"path":"inference-categ-applications.html","id":"theory-based-inference-for-one-proportion","chapter":"16 Applications: Infer categorical","heading":"Theory-based inference for one proportion","text":"theory-based inference, can use built-R function prop.test().149 one-proportion test, need tell number successes, number trials (sample size), null value. Using payday loan regulation example, call look like :output, can get observed statistic \\(\\hat{p} = 0.51\\) last line output (sample estimates: p), p-value 0.2656 second line output. always check null value alternative hypothesis output matches problem.might noticed test statistic reported output X-squared rather Z-statistic. test statistic equal Z-statistic squared, p-value found called \\(\\chi^2\\) distribution (pronounced “kai squared”). get Z-statistic X-squared statistic, take positive square root \\(\\hat{p} > \\pi_0\\), negative square root \\(\\hat{p} < \\pi_0\\).Use R output find value Z-statistic.150The prop.test() function also produce theory-based confidence interval, get correct confidence interval, need run function two-sided alternative:151From output, obtain 95% confidence interval true proportion payday loan borrowers support new regulation (0.477, 0.545).","code":"\nprop.test(x = 422,  #Number of successes\n          n = 826, #Number of trials\n          p = .5, #Null hypothesis value\n          alternative = \"greater\", #Direction of alternative,\n          conf.level = 0.95) #Confidence level as a proportion#> \n#>  1-sample proportions test with continuity correction\n#> \n#> data:  422 out of 826\n#> X-squared = 0.3, df = 1, p-value = 0.3\n#> alternative hypothesis: true p is greater than 0.5\n#> 95 percent confidence interval:\n#>  0.482 1.000\n#> sample estimates:\n#>     p \n#> 0.511\nprop.test(x = 422,  #Number of successes\n          n = 826, #Number of trials\n          p = .5, #Null hypothesis value\n          alternative = \"two.sided\", #Direction of alternative,\n          conf.level = 0.95, #Confidence level as a proportion\n          correct = FALSE) #We will not use  a continuity correction#> \n#>  1-sample proportions test without continuity correction\n#> \n#> data:  422 out of 826\n#> X-squared = 0.4, df = 1, p-value = 0.5\n#> alternative hypothesis: true p is not equal to 0.5\n#> 95 percent confidence interval:\n#>  0.477 0.545\n#> sample estimates:\n#>     p \n#> 0.511"},{"path":"inference-categ-applications.html","id":"theory-based-inference-for-a-difference-in-two-proportions","chapter":"16 Applications: Infer categorical","heading":"Theory-based inference for a difference in two proportions","text":"comparing two proportions, use function, prop.test, inputs now “vectors” rather “scalars”. example, use CPR blood thinner study Section 12.3.First, use table() function determine number successes sample size category explanatory variable:results (example Section 12.3), see Treatment group (call group 1) 14 survive (successes) 40 patients (sample size); Control group (call group 2) 11 survive 50 patients. counts input prop.test function follows:x = vector number successes = c(num successes group 1, num successes group 2)n = vector sample sizes = c(sample size group 1, sample size group 2)R creates vector using c() (“combine”) function. R take order subtraction group 1 \\(-\\) group 2.observed proportions group given sample estimates: prop 1 prop 2. R always take (prop 1 - prop 2) order subtraction. observed proportions don’t match calculations proportion successes wrong order subtraction, go back check inputs x n arguments., obtain p-value 0.1712 — little evidence null hypothesis difference two groups.Since used two-sided alternative prop.testc, call also produces correct confidence interval. 99% confidence interval true difference proportion patients survive \\((-0.116, 0.376)\\). 99% confident true proportion patients survive treatment 0.116 lower 0.376 higher true proportion patients survive control condition. fact confidence interval contains zero also shows us little evidence difference probability survival two groups.function prop.test also take table created table function. First, need create table counts raw data, becomes primary input prop.test(). one important step take creating table: R always put categories categorical variable alphabetical order building tables, unless told otherwise.However, prop.test() assume top row “success” order subtraction (column 1 - column 2). Without re-arranging table, get wrong order subtraction /wrong proportion successes group. fix , need relevel() inputs tell R put order want:re-arranging table, can use data_tbl first argument prop.test() function:","code":"\ntable(cpr$survival, cpr$group)#>           \n#>            control treatment\n#>   Died          39        26\n#>   Survived      11        14\nprop.test(x = c(14, 11), #Number successes in group 1 and group 2\n          n = c(40, 50), #Sample size of group 1 and group 2\n          alternative = \"two.sided\", #Match sign of alternative\n                                     #for order of subtraction \n                                     #group 1 - group 2\n          conf.level = 0.99, #Confidence level as a proportion\n          correct = FALSE)  #No continuity correction#> \n#>  2-sample test for equality of proportions without continuity correction\n#> \n#> data:  c out of c14 out of 4011 out of 50\n#> X-squared = 2, df = 1, p-value = 0.2\n#> alternative hypothesis: two.sided\n#> 99 percent confidence interval:\n#>  -0.116  0.376\n#> sample estimates:\n#> prop 1 prop 2 \n#>   0.35   0.22\ndata_tbl <- table(cpr$survival, cpr$group)\ndata_tbl#>           \n#>            control treatment\n#>   Died          39        26\n#>   Survived      11        14\n#Switch order of subtraction:\ncpr$group <- relevel(cpr$group, ref = \"treatment\")\ntable(cpr$survival, cpr$group)#>           \n#>            treatment control\n#>   Died            26      39\n#>   Survived        14      11\n#Switch \"success\":\ncpr$survival <- relevel(cpr$survival, ref = \"Survived\")\ntable(cpr$survival, cpr$group)#>           \n#>            treatment control\n#>   Survived        14      11\n#>   Died            26      39\ndata_tbl <- table(cpr$survival, cpr$group)\n\nstats::prop.test(x = data_tbl,\n          alternative = \"two.sided\",\n          conf.level = 0.99, #Confidence level as a proportion\n          correct = FALSE)  #No continuity correction#> \n#>  2-sample test for equality of proportions without continuity correction\n#> \n#> data:  data_tbl\n#> X-squared = 2, df = 1, p-value = 0.2\n#> alternative hypothesis: two.sided\n#> 99 percent confidence interval:\n#>  -0.14  0.46\n#> sample estimates:\n#> prop 1 prop 2 \n#>   0.56   0.40"},{"path":"inference-categ-applications.html","id":"catstats-function-summary","chapter":"16 Applications: Infer categorical","heading":"16.2 catstats function summary","text":"previous section, introduced four new R functions catstats library. provide summary functions. can also access help files functions using ? command. example, type ?one_proportion_test R console bring help file one_proportion_test function. one_proportion_test: Simulation-based hypothesis test single proportion.\nprobability_success = null value\nsample_size = sample size (\\(n\\))\nsummary_measure = one \"number\" \"proportion\" (quotations important !) simulate either sample counts sample proportions (needs match form observed statistic)\ndirection = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)\nas_extreme_as = value observed statistic\nnumber_repetitions = number simulated samples generate (least 1000!) \none_proportion_test: Simulation-based hypothesis test single proportion.probability_success = null valuesample_size = sample size (\\(n\\))summary_measure = one \"number\" \"proportion\" (quotations important !) simulate either sample counts sample proportions (needs match form observed statistic)direction = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)as_extreme_as = value observed statisticnumber_repetitions = number simulated samples generate (least 1000!) one_proportion_bootstrap_CI: Bootstrap confidence interval one proportion.\nsample_size = sample size (\\(n\\))\nnumber_successes = number successes (note \\(\\hat{p}\\) = number_successes/sample_size)\nconfidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)\nnumber_repetitions = number simulated samples generate (least 1000!) \none_proportion_bootstrap_CI: Bootstrap confidence interval one proportion.sample_size = sample size (\\(n\\))number_successes = number successes (note \\(\\hat{p}\\) = number_successes/sample_size)confidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)number_repetitions = number simulated samples generate (least 1000!) two_proportion_test: Simulation-based hypothesis test single proportion.\nformula = y ~ x y name response variable data set x name explanatory variable\ndata = name data set\nfirst_in_subtraction = category explanatory variable first subtraction, written quotations\nresponse_value_numerator = category response variable counting “success”, written quotations\ndirection = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)\nas_extreme_as = value observed difference proportions\nnumber_repetitions = number simulated samples generate (least 1000!) \ntwo_proportion_test: Simulation-based hypothesis test single proportion.formula = y ~ x y name response variable data set x name explanatory variabledata = name data setfirst_in_subtraction = category explanatory variable first subtraction, written quotationsresponse_value_numerator = category response variable counting “success”, written quotationsdirection = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)as_extreme_as = value observed difference proportionsnumber_repetitions = number simulated samples generate (least 1000!) two_proportion_bootstrap_CI: Bootstrap confidence interval one proportion.\nformula = y ~ x y name response variable data set x name explanatory variable\ndata = name data set\nfirst_in_subtraction = category explanatory variable first subtraction, written quotations\nresponse_value_numerator = category response variable counting “success”, written quotations\nconfidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)\nnumber_repetitions = number simulated samples generate (least 1000!)\ntwo_proportion_bootstrap_CI: Bootstrap confidence interval one proportion.formula = y ~ x y name response variable data set x name explanatory variabledata = name data setfirst_in_subtraction = category explanatory variable first subtraction, written quotationsresponse_value_numerator = category response variable counting “success”, written quotationsconfidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)number_repetitions = number simulated samples generate (least 1000!)","code":""},{"path":"inference-one-mean.html","id":"inference-one-mean","chapter":"17 Inference for a single mean","heading":"17 Inference for a single mean","text":"Focusing now statistical inference quantitative data, revisit expand upon foundational aspects hypothesis testing Chapter 9.important data structure chapter quantitative response variable (, outcome numerical).\nthree data structures detail :one quantitative response variable, summarized single mean,one quantitative response variable difference across pair observations, summarized paired mean difference, anda quantitative response variable broken binary explanatory variable, summarized difference means.appropriate, data structures analyzed using three methods Chapters 9, 10, 11: randomization test, bootstrapping, mathematical models, respectively.build inferential ideas, visit new foundational concepts statistical inference. One key new idea rests estimating sample mean (opposed sample proportion) varies sample sample; resulting value referred standard error mean. also introduce new important mathematical model, \\(t\\)-distribution (foundation \\(t\\)-test).summarize quantitative response variable, focus sample mean (instead , example, sample median range observations) well-studied mathematical model describes behavior sample mean.\nsample mean calculated one group, two paired groups, two independent groups. cover mathematical models describe statistics, bootstrap randomization techniques described immediately extendable function observed data.\ntechniques described setting vary slightly, well served find structural similarities across different settings.Similar can model behavior sample proportion \\(\\hat{p}\\) using normal distribution, sample mean \\(\\bar{x}\\) can also modeled using normal distribution certain conditions met.\nHowever, ’ll soon learn new distribution, called \\(t\\)-distribution, tends useful working sample mean.\n’ll first learn new distribution, ’ll use construct confidence intervals conduct hypothesis tests mean.summarize notation used throughout chapter.Notation.\\(n\\) = sample size\\(\\bar{x}\\) = sample mean\\(s\\) = sample standard deviation\\(\\mu\\) = population mean\\(\\sigma\\) = population standard deviationA single mean used summarize data measured single quantitative variable observational unit, e.g., GPA, age, salary. Aside slight differences notation, inferential methods presented section identical paired mean difference, see Chapter 18.","code":""},{"path":"inference-one-mean.html","id":"bootstrap-confidence-interval-for-mu","chapter":"17 Inference for a single mean","heading":"17.1 Bootstrap confidence interval for \\(\\mu\\)","text":"section, use bootstrapping, first introduced Chapter 10, construct confidence interval population mean. Recall bootstrapping best suited modeling studies data generated random sampling population. bootstrapped distribution sample means mimic process randomly sampling population give us sense sample means vary sample sample.","code":""},{"path":"inference-one-mean.html","id":"observed-data-4","chapter":"17 Inference for a single mean","heading":"17.1.1 Observed data","text":"employer subsidizes housing employees, need know average monthly rental price three bedroom flat Edinburgh.\norder walk example clearly, let’s say able randomly sample five Edinburgh flats (real example, surely able take much larger sample size, possibly even able measure entire population!).Figure 17.1 presents details random sample observations monthly rent five flats recorded.\nFigure 17.1: Five randomly sampled flats Edinburgh.\nsample average monthly rent £1648 first guess price three bedroom flats. However, student statistics, understand one sample mean based sample five observations necessarily equal true population average rent three bedroom flats Edinburgh.\nIndeed, can see observed rent prices vary standard deviation £340.232, surely average monthly rent different different sample size five taken population.\nFortunately, can use bootstrapping approximate variability sample mean sample sample.","code":""},{"path":"inference-one-mean.html","id":"variability-of-the-statistic-3","chapter":"17 Inference for a single mean","heading":"17.1.2 Variability of the statistic","text":"inferential ideas covered previous chapters, inferential analysis methods chapter grounded quantifying one data set differs another taken population.Figure 17.2 shows unknown original population three bedroom flats Edinburgh can estimated using many duplicates sample. estimated population—consisting infinitely many copies original sample—can used generate bootstrapped resamples.\nFigure 17.2: Using original sample five Edinburgh flats generate estimated population, used generate bootstrapped resamples. process generating bootstrapped sample equivalent sampling five flats original sample, replacement.\nFigure 17.2, repeated bootstrap resamples obviously different original sample.\nSince bootstrap resamples taken (estimated) population, differences due entirely natural variability sampling procedure.\nsummarizing bootstrap resamples (, using sample mean), see, directly, variability sample mean sample sample.\ndistribution \\(\\bar{x}_{boot}\\), bootstrapped sample means, Edinburgh flats shown Figure 17.3.\nFigure 17.3: Distribution bootstrapped means 1,000 simulated bootstrapped samples generated sampling replacement original sample five Edinburgh flats. histogram provides sense variability average rent values sample sample samples size 5.\nbootstrapped average rent prices vary £1250 £1995 (small observed sample size 5, bootstrap resample can sometimes, although rarely, include repeated measurements observation).Bootstrapping one sample.Take random sample size \\(n\\) original sample, replacement. called bootstrapped resample.Record sample mean (statistic interest) bootstrapped resample. called bootstrapped statistic.Repeat steps (1) (2) 1000s times.Due theory beyond text, know bootstrap means \\(\\bar{x}_{boot}\\) vary around original sample mean, \\(\\bar{x}\\), similar way different sample (.e., different data sets produce different \\(\\bar{x}\\) values) means vary around true parameter \\(\\mu\\).\nTherefore, interval estimate \\(\\mu\\) can produced using \\(\\bar{x}_{boot}\\) values . 95% bootstrap confidence interval \\(\\mu\\), population mean rent price three bedroom flats Edinburgh, found locating middle 95% bootstrapped sample means Figure 17.3.95% Bootstrap confidence interval population mean \\(\\mu\\).can find confidence intervals difference confidence levels changing percent distribution take, e.g., locate middle 90% bootstrapped statistics 90% confidence interval.Using Figure 17.3, find 90% 95% confidence intervals true mean monthly rental price three bedroom flat Edinburgh.90% confidence interval given (£1429, £1876). conclusion 90% confident true average rental price three bedroom flats Edinburgh lies somewhere £1429 £1876.95% confidence interval given (£1389.75, £1916). conclusion 95% confident true average rental price three bedroom flats Edinburgh lies somewhere £1389.75 £1916.","code":""},{"path":"inference-one-mean.html","id":"bootstrap-percentile-confidence-interval-for-sigma-special-topic","chapter":"17 Inference for a single mean","heading":"17.1.3 Bootstrap percentile confidence interval for \\(\\sigma\\) (special topic)","text":"Suppose research question hand seeks understand variable rental price three bedroom flats Edinburgh.\n, interest longer average rental price flats standard deviation rental prices three bedroom flats Edinburgh, \\(\\sigma\\).\nmay already realized sample standard deviation, \\(s\\), work good point estimate parameter interest: population standard deviation, \\(\\sigma\\).\npoint estimate five observations calculated \\(s =\\) £340.23.\n\\(s =\\) £340.23 might good guess \\(\\sigma\\), prefer interval.\nAlthough mathematical model describes \\(s\\) varies sample sample, mathematical model presented text.\neven without mathematical model, bootstrapping can used find confidence interval parameter \\(\\sigma\\).Describe bootstrap distribution standard deviation shown Figure 17.4.distribution skewed left centered near £340.23, point estimate original data. observations distribution lie £0 £408.1.Using Figure 17.4, find interpret 90% confidence interval population standard deviation three bedroom flat prices Edinburgh.152\nFigure 17.4: original Edinburgh data bootstrapped 1,000 times. histogram provides sense variability standard deviation rent values sample sample.\n","code":""},{"path":"inference-one-mean.html","id":"bootstrapping-is-not-a-solution-to-small-sample-sizes","chapter":"17 Inference for a single mean","heading":"17.1.4 Bootstrapping is not a solution to small sample sizes!","text":"example presented done sample five observations.\nanalysis techniques build mathematical models, bootstrapping works best large random sample taken population.\nBootstrapping method capturing variability statistic mathematical model unknown — method navigating small samples.\nmight guess, larger random sample, accurately sample represent target population.","code":""},{"path":"inference-one-mean.html","id":"one-mean-null-boot","chapter":"17 Inference for a single mean","heading":"17.2 Shifted bootstrap test for \\(H_0: \\mu = \\mu_0\\)","text":"can also use bootstrapping conduct simulation-based test null hypothesis population mean equal specified value, \\(\\mu_0\\), called null value. case, first shift value data set sample distribution centered \\(\\mu_0\\). , bootstrap shifted data order generate null distribution sample means. Consider following example.1851, Carl Wunderlich, German physician, measured body temperatures around 25,000 adults found average body temperature 98.6\\(^{\\circ}\\)F, ’ve believed ever since. However, recent study conducted Stanford University suggests average body temperature may actually lower 98.6\\(^{\\circ}\\)F.153","code":""},{"path":"inference-one-mean.html","id":"observed-data-5","chapter":"17 Inference for a single mean","heading":"17.2.1 Observed data","text":"Curious average body temperature decreased since 1851, decided collect data random sample twenty Montana State University students. mean body temperature sample \\(\\bar{x}\\) = 97.47\\(^{\\circ}\\)F, standard deviation \\(s\\) = 0.35\\(^{\\circ}\\)F. dot plot data shown Figure 17.5, summary statistics displayed .\nFigure 17.5: Distribution body temperatures random sample twenty Montana State University students.\n","code":"\nfavstats(temperatures)#>   min   Q1 median   Q3  max mean    sd  n missing\n#>  96.7 97.3   97.5 97.7 98.1 97.5 0.353 20       0"},{"path":"inference-one-mean.html","id":"shifted-bootstrapped-null-distribution","chapter":"17 Inference for a single mean","heading":"17.2.2 Shifted bootstrapped null distribution","text":"like test set hypotheses \\(H_0: \\mu = 98.6\\) versus \\(H_A: \\mu < 98.6\\), \\(\\mu\\) true mean body temperature among adults (degrees F). simulate sample mean body temperatures \\(H_0\\), expect null distribution centered \\(\\mu_0\\) = 98.6\\(^\\circ\\)F. However, bootstrap sample means observed sample, bootstrap distribution centered sample mean body temperature \\(\\bar{x}\\) = 97.5\\(^\\circ\\)F.use bootstrapping generate null distribution sample means, first shift data centered null value. adding \\(\\mu_0 - \\bar{x} = 98.6 - 97.5 = 1.1^\\circ\\)F body temperature sample. process displayed Figure 17.6.\nFigure 17.6: Distribution body temperatures random sample twenty Montana State University students (blue) shifted body temperatures (red), found adding 1.1 degree F original body temperature.\nbootstrapped null distribution generated sampling 20 shifted temperatures, replacement, shifted data 1,000 times shown Figure 17.7.\nFigure 17.7: Bootstrapped null distribution sample mean temperatures assuming true mean temperature 98.6 degrees F.\nShifted bootstrap null distribution sample mean.simulate null distribution sample means null hypothesis \\(H_0: \\mu = \\mu_0\\):Add \\(\\mu_0 - \\bar{x}\\) value original sample:\n\\[\nx_1 + \\mu_0 - \\bar{x}, \\hspace{2.5mm} x_2 + \\mu_0 - \\bar{x}, \\hspace{2.5mm}  x_3 + \\mu_0 - \\bar{x}, \\hspace{2.5mm}  \\ldots, \\hspace{2.5mm}  x_n + \\mu_0 - \\bar{x}.\n  \\]\nNote \\(\\bar{x}\\) larger \\(\\mu\\), quantity \\(\\mu_0 - \\bar{x}\\) negative, subtracting distance \\(\\mu\\) \\(\\bar{x}\\) value.Generate 1000s bootstrap resamples shifted distribution, plotting shifted bootstrap sample mean time.calculate p-value, since \\(H_A: \\mu < 98.6\\), find proportion simulated sample means less equal original sample mean, \\(\\bar{x}\\) = 97.47. shown Figure 17.7, none simulated sample means 97.5\\(^\\circ\\)F lower, giving us strong evidence true mean body temperature among Montana State University students less commonly accepted 98.6\\(^\\circ\\)F average temperature.","code":""},{"path":"inference-one-mean.html","id":"one-mean-math","chapter":"17 Inference for a single mean","heading":"17.3 Theory-based inferential methods for \\(\\mu\\)","text":"sample proportion, variability sample mean well described mathematical theory given Central Limit Theorem. Similar can model behavior sample proportion \\(\\hat{p}\\) using normal distribution, sample mean \\(\\bar{x}\\) can also modeled using\nnormal distribution certain conditions met.\nHowever, missing information inherent variability population, \\(t\\)-distribution used place standard normal performing hypothesis test confidence interval analyses.Central Limit Theorem sample mean.\ncollect sufficiently large sample \n\\(n\\) independent observations population \nmean \\(\\mu\\) standard deviation \\(\\sigma\\),\nsampling distribution \\(\\bar{x}\\) nearly\nnormal \n\\[\\begin{align*}\n  &\\text{Mean}=\\mu\n  &&\\text{Standard Deviation }(SD) = \\frac{\\sigma}{\\sqrt{n}}\n  \\end{align*}\\]diving confidence intervals hypothesis\ntests using \\(\\bar{x}\\), first need cover two topics:modeled \\(\\hat{p}\\) using normal distribution,\ncertain conditions satisfied.\nconditions working \\(\\bar{x}\\)\nlittle complex, , discuss\ncheck conditions inference using mathematical model.standard deviation sample mean dependent population\nstandard deviation, \\(\\sigma\\).\nHowever, rarely know \\(\\sigma\\), instead\nmust estimate .\nestimation imperfect,\nuse new distribution called \n\\(t\\)-distribution\nfix problem.","code":""},{"path":"inference-one-mean.html","id":"evaluating-the-two-conditions-required-for-modeling-barx-using-theory-based-methods","chapter":"17 Inference for a single mean","heading":"17.3.1 Evaluating the two conditions required for modeling \\(\\bar{x}\\) using theory-based methods","text":"two conditions required apply \nCentral Limit Theorem\nsample mean \\(\\bar{x}\\).\nsample observations\nindependent sample size sufficiently\nlarge, normal model describe variability sample means quite well; observations violate conditions, normal model can inaccurate.Conditions modeling\n\\(\\bar{x}\\) using theory-based methods.sampling distribution \\(\\bar{x}\\) based \nsample size \\(n\\) population true\nmean \\(\\mu\\) true standard deviation \\(\\sigma\\) can modeled\nusing normal distribution :Independence. sample observations must independent,\ncommon way satisfy condition \nsample simple random sample \npopulation.\ndata come random process,\nanalogous rolling die,\nalso satisfy independence condition.Independence. sample observations must independent,\ncommon way satisfy condition \nsample simple random sample \npopulation.\ndata come random process,\nanalogous rolling die,\nalso satisfy independence condition.Normality. sample small,\nalso require sample observations\ncome normally distributed population.\ncan relax condition \nlarger larger sample sizes.\ncondition obviously vague,\nmaking difficult evaluate,\nnext introduce couple rules thumb\nmake checking condition easier.\nconditions satisfied, sampling\ndistribution \\(\\bar{x}\\) approximately normal mean\n\\(\\mu\\) standard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\).Normality. sample small,\nalso require sample observations\ncome normally distributed population.\ncan relax condition \nlarger larger sample sizes.\ncondition obviously vague,\nmaking difficult evaluate,\nnext introduce couple rules thumb\nmake checking condition easier.conditions satisfied, sampling\ndistribution \\(\\bar{x}\\) approximately normal mean\n\\(\\mu\\) standard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\).General rule: perform normality check.perfect way check normality condition,\ninstead use two general rules:\\(\\mathbf{n < 30}\\): sample size \\(n\\) less 30\nclear outliers data,\ntypically assume data come \nnearly normal distribution satisfy \ncondition.\\(\\mathbf{n \\geq 30}\\): sample size \\(n\\) least 30\nparticularly extreme outliers,\ntypically assume sampling distribution\n\\(\\bar{x}\\) nearly normal, even underlying\ndistribution individual observations .\\(\\mathbf{n > 100}\\): sample size \\(n\\) least 100\n(regardless presence skew outliers),\ntypically assume sampling distribution\n\\(\\bar{x}\\) nearly normal, even underlying\ndistribution individual observations .first course statistics, aren’t expected\ndevelop perfect judgement normality condition.\nHowever, expected able handle\nclear cut cases based rules thumb.154Example 17.1  Consider following two plots\ncome simple random samples \ndifferent populations.\nsample sizes \\(n_1 = 15\\) \\(n_2 = 50\\).independence normality conditions met\ncase?sample simple random sample \nrespective population, independence condition\nsatisfied.\nLet’s next check normality condition \nusing rule thumb.first sample fewer 30 observations,\nwatching clear outliers.\nNone present; small gap \nhistogram right, gap small \n20% observations small sample\nrepresented far right bar histogram,\ncan hardly call clear outliers.\nclear outliers, normality condition\nreasonably met.second sample sample size greater 30 \nincludes outlier appears roughly 5 times\ncenter distribution \nnext furthest observation.\nexample particularly extreme outlier,\nnormality condition satisfied.practice, ’s typical also mental check evaluate\nwhether reason believe underlying population\nmoderate skew (\\(n < 30\\))\nparticularly extreme outliers \\((n \\geq 30)\\)\nbeyond observe data.\nexample, consider number followers\nindividual account Twitter,\nimagine distribution.\nlarge majority accounts built \ncouple thousand followers fewer,\nrelatively tiny fraction amassed\ntens millions followers,\nmeaning distribution extremely skewed.\nknow data come extremely\nskewed distribution,\ntakes effort understand sample\nsize large enough normality condition\nsatisfied.","code":""},{"path":"inference-one-mean.html","id":"introducing-the-t-distribution","chapter":"17 Inference for a single mean","heading":"17.3.2 Introducing the \\(t\\)-distribution","text":"\npractice, directly calculate standard deviation\n\\(\\bar{x}\\) since know population standard\ndeviation, \\(\\sigma\\).\nencountered similar issue computing standard\nerror sample proportion, relied population\nproportion, \\(\\pi\\).\nsolution proportion context use sample\nvalue place\npopulation value calculate standard error.\n’ll employ similar strategy compute standard\nerror \\(\\bar{x}\\), using sample\nstandard deviation \\(s\\) place \\(\\sigma\\):\n\\[\\begin{align*}\nSE(\\bar{x}) = \\frac{s}{\\sqrt{n}} \\approx SD(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}.\n\\end{align*}\\]\nstandard error \\(\\bar{x}\\) provides estimate standard deviation \\(\\bar{x}\\). strategy tends work well \nlot data can estimate \\(\\sigma\\) using \\(s\\) accurately.\nHowever, estimate less precise smaller samples,\nleads problems using normal\ndistribution model \\(\\bar{x}\\) know \\(\\sigma\\).’ll find useful use new distribution \ninference calculations called \\(t\\)-distribution.\n\\(t\\)-distribution, shown solid line \nFigure 17.8, bell shape.\nHowever, tails thicker normal distribution’s,\nmeaning observations likely fall beyond two\nstandard deviations mean normal\ndistribution.extra thick tails \\(t\\)-distribution exactly\ncorrection needed resolve problem (due extra variability test statistic) using \\(s\\)\nplace \\(\\sigma\\) \\(SE(\\bar{x})\\) calculation.\nFigure 17.8: Comparison \\(t\\)-distribution normal distribution.\n\\(t\\)-distribution always centered zero \nsingle parameter: degrees freedom (\\(df\\)).\ndegrees freedom\ndescribes precise form bell-shaped \\(t\\)-distribution.\nSeveral \\(t\\)-distributions shown \nFigure 17.9\ncomparison normal distribution.inference single mean, ’ll use \\(t\\)-distribution\n\\(df = n - 1\\) model sample mean\nsample size \\(n\\).\n, observations,\ndegrees freedom larger \n\\(t\\)-distribution look like \nstandard normal distribution;\ndegrees freedom 30 ,\n\\(t\\)-distribution nearly indistinguishable\nnormal distribution.\nFigure 17.9: larger degrees freedom, closely \\(t\\)-distribution resembles standard normal distribution.\nDegrees freedom: \\(df\\).degrees freedom describes shape \n\\(t\\)-distribution.\nlarger degrees freedom, closely\ndistribution approximates normal model.modeling \\(\\bar{x}\\) using \\(t\\)-distribution,\nuse \\(df = n - 1\\).\\(t\\)-distribution allows us greater flexibility \nnormal distribution analyzing numerical data.\npractice, ’s common use statistical software,\nR, Python, SAS analyses.\nR, function used calculating probabilities \\(t\\)-distribution pt() (seem similar previous R function pnorm()).\nDon’t forget \\(t\\)-distribution, degrees freedom must always specified!examples guided practices , use R find answers. recommend trying problems get sense \\(t\\)-distribution can vary width depending degrees freedom, confirm working\nunderstanding \\(t\\)-distribution.Example 17.2  proportion \\(t\\)-distribution\n18 degrees freedom falls -2.10?Just like normal probability problem, first draw\npicture Figure 17.10\nshade area -2.10.Using statistical software, can obtain precise\nvalue: 0.0250.\nFigure 17.10: \\(t\\)-distribution 18 degrees freedom. area -2.10 shaded.\nExample 17.3  \\(t\\)-distribution 20 degrees freedom\nshown top panel \nFigure 17.11.\nEstimate proportion distribution falling\n1.65.Note 20 degrees freedom, \\(t\\)-distribution relatively close normal distribution.\nnormal distribution, correspond \n0.05, expect \\(t\\)-distribution\ngive us value neighborhood.\nUsing statistical software: 0.0573.\nFigure 17.11: Top: \\(t\\)-distribution 20 degrees freedom, area 1.65 shaded. Bottom: \\(t\\)-distribution 2 degrees freedom, area 3 units 0 shaded.\nExample 17.4  \\(t\\)-distribution 2 degrees freedom\nshown bottom panel \nFigure 17.11.\nEstimate proportion distribution falling \n3 units mean ().degrees freedom, \\(t\\)-distribution \ngive notably different value normal\ndistribution.\nnormal distribution, area \n0.003 using 68-95-99.7 rule.\n\\(t\\)-distribution \\(df = 2\\), area \ntails beyond 3 units totals 0.0955.\narea dramatically different \nobtain normal distribution.proportion \\(t\\)-distribution 19 degrees\nfreedom falls -1.79 units?155\n","code":"\n# using pt() to find probability under the $t$-distribution\npt(-2.10, df = 18)\n#> [1] 0.025\n# using pt() to find probability under the $t$-distribution\npt(1.65, df = 20, lower.tail=FALSE)\n#> [1] 0.0573\n# or\n1 - pt(1.65, df = 20)\n#> [1] 0.0573\n# using pt() to find probability under the $t$-distribution\n2 * pt(-3, df = 2)\n#> [1] 0.0955"},{"path":"inference-one-mean.html","id":"one-sample-t-confidence-intervals","chapter":"17 Inference for a single mean","heading":"17.3.3 One sample \\(t\\)-confidence intervals","text":"Let’s get first taste applying \\(t\\)-distribution\ncontext example mercury content\ndolphin muscle.\nElevated mercury concentrations important problem\ndolphins\nanimals, like humans, occasionally eat .\nFigure 17.12: Risso’s dolphin. Photo Mike Baird, www.bairdphotos.com.\n","code":""},{"path":"inference-one-mean.html","id":"observed-data-6","chapter":"17 Inference for a single mean","heading":"Observed data","text":"identify confidence interval average mercury content dolphin muscle using sample 19 Risso’s dolphins Taiji area Japan. data summarized Table 17.1. minimum maximum observed values can used evaluate whether clear outliers.\nTable 17.1: Summary mercury content muscle 19 Risso’s dolphins Taiji area. Measurements micrograms mercury per wet gram\nmuscle (\\(\\mu\\)g/wet g).\nExample 17.5  independence \nnormality conditions satisfied data set?observations simple random sample,\ntherefore independence reasonable.\nsummary statistics \nTable 17.1\nsuggest clear outliers, \nobservations within 2.5 standard deviations\nmean.\nBased evidence, normality condition\nseems reasonable.normal model, used \\(z^{\\star}\\) standard error determine width confidence interval. revise confidence interval formula slightly using \\(t\\)-distribution:\n\\[\\begin{align*}\n&\\text{point estimate} \\ \\pm\\  t^{\\star}_{df} \\times SE(\\text{point estimate})\n&&\\\n&&\\bar{x} \\ \\pm\\  t^{\\star}_{df} \\times \\frac{s}{\\sqrt{n}},\n\\end{align*}\\]\n\\(df = n - 1\\) computing one-sample \\(t\\)-interval.Example 17.6  Using summary statistics \nTable 17.1,\ncompute standard error average\nmercury content \\(n = 19\\) dolphins.plug \\(s\\) \\(n\\) formula:\n\\(SE(\\bar{x})  = s / \\sqrt{n}  = 2.3 / \\sqrt{19}  = 0.528\\).value \\(t^{\\star}_{df}\\) cutoff obtain based \nconfidence level \\(t\\)-distribution \\(df\\) degrees\nfreedom.\ncutoff found way normal\ndistribution: find \\(t^{\\star}_{df}\\) \nfraction \\(t\\)-distribution \\(df\\) degrees\nfreedom within distance \\(t^{\\star}_{df}\\)\n0 matches confidence level interest.Example 17.7  \\(n = 19\\), appropriate\ndegrees freedom?\nFind \\(t^{\\star}_{df}\\) degrees freedom\nconfidence level 95%.degrees freedom easy calculate:\n\\(df = n - 1 = 18\\).Using statistical software, find cutoff \nupper tail equal 2.5%:\n\\(t^{\\star}_{18} =\\) 2.10.\narea -2.10 also equal 2.5%.\n, 95% \\(t\\)-distribution \\(df = 18\\)\nlies within 2.10 units 0.Example 17.8  Compute interpret 95% confidence interval\naverage mercury content Risso’s dolphins.can construct confidence interval \n\\[\\begin{align*}\n  \\bar{x} \\ \\pm\\  t^{\\star}_{18} \\times SE(\\bar{x})\n    \\quad \\\\quad 4.4 \\ \\pm\\  2.10 \\times 0.528\n    \\quad \\\\quad (3.29, 5.51)\n  \\end{align*}\\]\n95% confident average mercury content muscles\npopulation Risso’s dolphins 3.29 5.51 \\(\\mu\\)g/wet gram,\nconsidered extremely high.Finding \\(t\\)-confidence interval population mean, \\(\\mu\\).Based sample \\(n\\) independent nearly normal\nobservations, confidence interval population\nmean \n\\[\\begin{align*}\n  &\\text{point estimate} \\ \\pm\\  t^{\\star}_{df} \\times SE(\\text{point estimate})\n  &&\\\n  &&\\bar{x} \\ \\pm\\  t^{\\star}_{df} \\times \\frac{s}{\\sqrt{n}}\n  \\end{align*}\\]\n\\(\\bar{x}\\) sample mean, \\(t^{\\star}_{df}\\)\ncorresponds confidence level degrees freedom\n\\(df\\), \\(SE\\) standard error estimated \nsample.FDA’s webpage provides data mercury content fish.\nBased sample 15 croaker white fish (Pacific), sample mean standard deviation computed 0.287 0.069 ppm (parts per million), respectively.\n15 observations ranged 0.18 0.41 ppm. assume observations independent.\nBased summary statistics data, objections normality condition individual observations?156Example 17.9  Calculate standard error \n\\(\\bar{x}\\) using data summaries previous Guided Practice. use \\(t\\)-distribution create \n90% confidence interval actual mean \nmercury content, identify degrees freedom\n\\(t^{\\star}_{df}\\).standard error: \\(SE(\\bar{x}) = \\frac{0.069}{\\sqrt{15}} = 0.0178\\).Degrees freedom: \\(df = n - 1 = 14\\).Since goal 90% confidence interval,\nchoose \\(t_{14}^{\\star}\\) two-tail area\n0.1:\n\\(t^{\\star}_{14} = 1.76\\).Using information results previous Guided Practice Example, compute 90% confidence interval average mercury content croaker white fish (Pacific).157The 90% confidence interval previous\nGuided Practice 0.256 ppm 0.318 ppm.\nCan say 90% croaker white fish (Pacific)\nmercury levels 0.256 0.318 ppm?158","code":"\n# use qt() to find the t-cutoff (with 95% in the middle)\nqt(0.025, df = 18)\n#> [1] -2.1\nqt(0.975, df = 18)\n#> [1] 2.1\n# use qt() to find the t-cutoff (with 90% in the middle)\nqt(0.05, df = 14)\n#> [1] -1.76\nqt(0.95, df = 14)\n#> [1] 1.76"},{"path":"inference-one-mean.html","id":"one-sample-t-tests","chapter":"17 Inference for a single mean","heading":"17.3.4 One sample \\(t\\)-tests","text":"Now ’ve used \\(t\\)-distribution making confidence\nintervals mean, let’s speed \nhypothesis tests mean.test statistic assessing single mean T.T score ratio sample mean differs hypothesized mean compared observations vary.\\[\\begin{align*}\nT = \\frac{\\bar{x} - \\mbox{null value}}{s/\\sqrt{n}}\n\\end{align*}\\]null hypothesis true conditions met, T \\(t\\)-distribution \\(df = n - 1\\).Conditions:independently observed datalarge enough sample satisfy normalityCompare T score — standardized sample mean — Z score — standardized sample proportion — presented Section 14.3. use “Z” standardizing proportions, “T” standardizing means?159Is typical US runner getting faster slower time? consider question context Cherry Blossom Race, 10-mile race Washington, DC spring.average time runners finished Cherry Blossom Race 2006 93.29 minutes (93 minutes 17 seconds). want determine using data 100 participants 2017 Cherry Blossom Race whether runners race getting faster slower, versus possibility change.appropriate hypotheses context?160The data come simple random sample participants, observations independent.\n\nhistogram race times given evaluate can move forward t-test. worried normality condition?161When completing hypothesis test one-sample mean,\nprocess nearly identical completing hypothesis\ntest single proportion.\nFirst, find Z score using observed value,\nnull value, standard error;\nhowever, call T score since use\n\\(t\\)-distribution calculating tail area.\nfind p-value using ideas used\npreviously: find area \\(t\\)-distribution extreme T score.Example 17.10  independence\nnormality conditions satisfied,\ncan proceed hypothesis test using\n\\(t\\)-distribution.\nsample mean sample standard deviation\nsample\n100 runners \n2017 Cherry Blossom Race\n97.32 16.98 minutes,\nrespectively.\nRecall sample size 100\naverage run time 2006 93.29 minutes.\nFind test statistic p-value.\nconclusion?hypotheses, found previous Guided Practice, :\\(H_0: \\mu = 93.29\\) minutes\\(H_A: \\mu \\neq 93.29\\) minutesTo find test statistic (T score),\nfirst must determine standard error:\n\\[\\begin{align*}\n  SE(\\bar{x})\n    = 16.98 / \\sqrt{100}\n    = 1.70\n  \\end{align*}\\]\nNow can compute T score\nusing sample mean (97.32),\nnull value (98.29), \\(SE\\):\n\\[\\begin{align*}\n  T\n    = \\frac{97.32 - 93.29}{1.70}\n    = 2.37\n  \\end{align*}\\]\n\\(df = 100 - 1 = 99\\),\ncan determine using statistical software\narea \\(t\\)-distribution 99 \\(df\\) \nobserved T score 2.37 0.01 (see ),\ndouble get p-value: 0.02.p-value small,\ndata provide strong evidence average\nrun time Cherry Blossom Run 2017 different\n2006 average.using \\(t\\)-distribution, use T score (similar Z score).help us remember use \\(t\\)-distribution,\nuse \\(T\\) represent test statistic,\noften call T score.\nZ score T score computed exact way\nconceptually identical:\nrepresents many standard errors observed value\nnull value.","code":"\n# using pt() to find the p-value\n1 - pt(2.37, df = 99)\n#> [1] 0.00986"},{"path":"inference-one-mean.html","id":"chp17-review","chapter":"17 Inference for a single mean","heading":"17.4 Chapter review","text":"","code":""},{"path":"inference-one-mean.html","id":"summary-12","chapter":"17 Inference for a single mean","heading":"Summary","text":"TODO","code":""},{"path":"inference-one-mean.html","id":"terms-13","chapter":"17 Inference for a single mean","heading":"Terms","text":"introduced following terms chapter. ’re sure terms mean, recommend go back text review definitions. purposefully presenting alphabetical order, instead order appearance, little challenging locate. However able easily spot bolded text.","code":""},{"path":"inference-one-mean.html","id":"key-ideas-12","chapter":"17 Inference for a single mean","heading":"Key ideas","text":"TODO","code":""},{"path":"inference-paired-means.html","id":"inference-paired-means","chapter":"18 Inference for comparing paired means","heading":"18 Inference for comparing paired means","text":"TODOPaired data represent particular type experimental structure analysis somewhat akin one-sample analysis (see Chapter 17) features resemble two-sample analysis (see Chapter 19). Quantitative measurements made two different levels explanatory variable, measurements paired — observational unit consists two measurements, two measurements subtracted difference retained. Table 18.1 presents examples studies paired designs implemented.\nTable 18.1: Examples studies paired design used measure difference measurement two conditions.\nPaired data.Two sets observations paired \nobservation one set special correspondence\nconnection exactly one observation \ndata set.inferential methods applied paired data, analysis virtually identical one-sample approach given Chapter 17.\nkey working paired data consider measurement interest difference measured values across pair observations.\nThinking differences single observation observational unit changes paired setting one-sample setting.comparison notation used Chapter 17 notation used chapter shown . subscript “d” stands “difference” since variable now paired difference.Instead \\(n\\) representing number observational units, paired data, \\(n\\) represents number pairs paired samples.\nSimilarly, \\(\\mu_d\\), \\(\\sigma_d\\), \\(\\bar{x}_d\\) \\(s_d\\) calculated\nusing differences measured values within pairs.","code":""},{"path":"inference-paired-means.html","id":"shifted-bootstrap-test-for-h_0-mu_d-0","chapter":"18 Inference for comparing paired means","heading":"18.1 Shifted bootstrap test for \\(H_0: \\mu_d = 0\\)","text":"Consider experiment done measure whether tire brand Smooth Turn tire brand Quick Spin longer tread wear. , 1,000 miles car, brand tires tread, average?","code":""},{"path":"inference-paired-means.html","id":"observed-data-7","chapter":"18 Inference for comparing paired means","heading":"18.1.1 Observed data","text":"observed data represent 25 tread measurements (inches) taken 25 Smooth Turn tires 25 Quick Spin tires.\nstudy used total 25 cars, car, one brand randomly assigned front driver’s side tire front passenger’s side tire.\nFigure 18.1 presents observed data.\nSmooth Turn manufacturer looks box plot says:clearly tread Smooth Turn tires higher, average, tread Quick Spin tires 1,000 miles driving.Quick Spin manufacturer skeptical retorts:25 cars, seems variability road conditions (sometimes one tire hits pothole, etc.) leads small difference average tread amount.’d like able systematically distinguish Smooth Turn manufacturer sees plot Quick Spin manufacturer sees plot. Fortunately us, excellent way simulate natural variability (road conditions, etc.) can lead tires worn different rates: bootstrapping.\nFigure 18.1: Boxplots tire tread remaining 1,000 miles brand tire original measurements came. Gray lines connect cars.\nSince paired data, interested differences tire tread two brands car. dotplot Figure 18.2 displays differences, summary statistics displayed .\nFigure 18.2: Difference tire tread (inches) remaining 1,000 miles two brands (Smooth Turn – Quick Spin).\n","code":"\nfavstats(differences)\n#>       min        Q1  median     Q3    max    mean      sd  n missing\n#>  -0.00506 -0.000972 0.00205 0.0042 0.0107 0.00196 0.00431 25       0"},{"path":"inference-paired-means.html","id":"variability-of-the-statistic-4","chapter":"18 Inference for comparing paired means","heading":"18.1.2 Variability of the statistic","text":"simulation-based test identify whether differences seen box plot plausibly happened just chance variability.\n, simulate variability sample statistics assumption null hypothesis true.\nstudy, null hypothesis average difference tire tread wear Smooth Turn Quick Spin tires zero. experiment conducted determine whether Smooth Turn Quick Spin longer tread wear. Taking order differences Smooth Turn \\(-\\) Quick Spin, express hypotheses follows.\\(H_0: \\mu_d = 0\\), true mean difference tire tread remaining 1,000 miles Smooth Turn Quick Spin (Smooth Turn \\(-\\) Quick Spin) tires equal zero.\\(H_A: \\mu_d \\neq 0\\), true mean difference tire tread remaining 1,000 miles Smooth Turn Quick Spin (Smooth Turn \\(-\\) Quick Spin) tires equal zero.simulate null distribution mean differences tread, implement method used Section 17.2 using shifted bootstrap distribution.Shifted bootstrap null distribution sample mean difference.simulate null distribution sample mean differences null hypothesis \\(H_0: \\mu_d = 0\\),Subtract \\(\\bar{x}_d\\) difference original sample:162\\[\nx_1 - \\bar{x}_d , \\hspace{2.5mm} x_2 - \\bar{x}_d, \\hspace{2.5mm}  x_3 - \\bar{x}_d, \\hspace{2.5mm}  \\ldots, \\hspace{2.5mm}  x_n - \\bar{x}_d.\n  \\]\nNote \\(\\bar{x}_d\\) negative number, adding distance \\(0\\) \\(\\bar{x}_d\\) value.Generate 1000s bootstrap resamples shifted distribution, plotting shifted bootstrap sample mean difference time.use bootstrapping generate null distribution sample mean differences tire tread, first shift data centered null value zero. shift data subtracting \\(\\bar{x}_d\\) = 0.00196 tire tread difference sample. process displayed Figure 18.3.\nFigure 18.3: Mean difference tire tread (inches) remaining 1,000 miles two brands (Smooth Turn – Quick Spin) (blue), shifted mean differences tire tread (red), found subtracting 0.00196 original difference.\n","code":""},{"path":"inference-paired-means.html","id":"observed-statistic-vs.-null-value","chapter":"18 Inference for comparing paired means","heading":"18.1.3 Observed statistic vs. null value","text":"repeatedly sampling 25 cars replacement shifted bootstrap null distribution, can create distribution sample mean difference tire tread, seen Figure 18.4.\nexpected (differences generated null hypothesis), histogram centered zero.\nline drawn observed mean difference, \\(\\bar{x}_d\\) = 0.00196, nowhere near differences simulated natural variability assume difference tire tread wear brands.\nobserved mean difference tire tread far away natural variability randomized mean differences tire tread, believe significant difference tire tread wear Smooth Turn Quick Spin brand tires, average.precise, proportion simulated \\(\\bar{x}_d\\)’s 0.00196 inches away zero 0.023. p-value gives us strong evidence favor alternative \\(H_A: \\mu_d \\neq 0\\).\nconclusion extra amount tire tread remaining Smooth Turn brand tires 1,000 miles, average, due just natural variability. Data experiment suggest , average, Smooth Turn tires differ tread wear compared Quick Spin tires.\nFigure 18.4: Histogram 1000 simulated mean differences tire tread, assuming two brands perform equally, average.\n","code":""},{"path":"inference-paired-means.html","id":"bootstrap-confidence-interval-for-mu_d","chapter":"18 Inference for comparing paired means","heading":"18.2 Bootstrap confidence interval for \\(\\mu_d\\)","text":"earlier edition textbook,\nfound Amazon prices , average,\nlower UCLA Bookstore UCLA courses\n2010.\n’s several years, many stores adapted\nonline market, wondered,\nUCLA Bookstore today?","code":""},{"path":"inference-paired-means.html","id":"observed-data-8","chapter":"18 Inference for comparing paired means","heading":"18.2.1 Observed data","text":"sampled 201 UCLA courses.\n, 68\nrequired books found Amazon.ucla_textbooks_f18 data can found openintro package.portion data set courses\nshown Table 18.2,\nprices US dollars. differences taken \n\\[\\begin{align*}\n\\text{UCLA Bookstore price} - \\text{Amazon price}\n\\end{align*}\\]important always subtract using\nconsistent order;\nAmazon prices always subtracted UCLA prices.\nfirst difference shown Table 18.2\ncomputed \\(47.97 - 47.45 = 0.52\\).\nSimilarly, second difference computed \n\\(14.26 - 13.55 = 0.71\\),\nthird \\(13.50 - 12.53 = 0.97\\).\nTable 18.2: Four cases ucla_textbooks_f18 dataset.\ndot plot data shown Figure 18.5, summary statistics displayed .\nFigure 18.5: Distribution differences new textbook price (UCLA Bookstore – Amazon) US dollars 68 required textbooks UCLA.\n\ntextbook two corresponding prices data set:\none UCLA Bookstore one Amazon.\nThus, two prices textbook paired,\nanalysis need focus differences\ntextbook price two suppliers.","code":"\nfavstats(ucla_textbooks_f18$price_diff)#>    min     Q1 median   Q3  max mean   sd  n missing\n#>  -12.2 -0.992  0.625 2.99 75.2 3.58 13.4 68       0"},{"path":"inference-paired-means.html","id":"variability-of-the-statistic-5","chapter":"18 Inference for comparing paired means","heading":"18.2.2 Variability of the statistic","text":"Following example bootstrapping single mean, observed mean differences can bootstrapped order understand variability average difference sample sample. can use bootstrap distribution mean differences calculate bootstrap percentile confidence intervals true mean difference population.Figure 18.6, 99% confidence interval mean difference cost new book UCLA Bookstore compared Amazon calculated.\nbootstrap percentile interval computing using 0.5th percentile 99.5th percentile bootstrapped mean differences found (-0.044, 8.138).\nSince confidence interval contains zero, support hypothesis UCLA Bookstore price , average, higher Amazon price. , since interval contains negative positive values, plausible prices UCLA textbooks lower, average, Amazon, also plausible prices UCLA textbooks higher, average, Amazon. interpret interval follows: 99% confident , average, new textbook prices UCLA Bookstore $0.04 lower $8.14 higher textbook Amazon.\nFigure 18.6: Bootstrap distribution average difference new book price UCLA Bookstore versus Amazon (UCLA – Amazon). bounds 99% bootstrap percentile confidence interval superimposed red, observed mean difference new book price superimposed blue.\n","code":""},{"path":"inference-paired-means.html","id":"paired-mean-math","chapter":"18 Inference for comparing paired means","heading":"18.3 Theory-based inferential methods for \\(\\mu_d\\)","text":"simulation-based inferential methods paired mean difference,\ntheory-based inferential methods scenario identical\ntheory-based inferential methods single mean—notation differs.","code":""},{"path":"inference-paired-means.html","id":"observed-data-9","chapter":"18 Inference for comparing paired means","heading":"18.3.1 Observed data","text":"Consider paired textbook price data previous section.\nhistogram differences new textbook price UCLA Bookstore Amazon shown \nFigure 18.7, summary statistics\ndisplayed Table 18.3.\nTable 18.3: Summary statistics 68 new textbook price differences (UCLA – Amazon).\n\nFigure 18.7: Histogram difference price book sampled.\n","code":""},{"path":"inference-paired-means.html","id":"variability-of-the-statistic-6","chapter":"18 Inference for comparing paired means","heading":"18.3.2 Variability of the statistic","text":"analyze paired data set,\nsimply analyze differences using one-sample \\(t\\)-distribution techniques\napplied \nSection 17.3.Set hypothesis test\ndetermine whether, average, UCLA Bookstore’s price \nnew textbook higher price \nbook Amazon.\nAlso, check conditions whether can move\nforward test using \\(t\\)-distribution.considering two scenarios:\\(H_0\\): \\(\\mu_{d} = 0\\).\ntrue mean difference new textbook prices (UCLA – Amazon)\nequal zero.\\(H_A\\): \\(\\mu_{d} > 0\\).\ntrue mean difference new textbook prices (UCLA – Amazon)\ngreater zero.Next, check independence normality conditions:observations based simple random sample,\nindependence reasonable.observations based simple random sample,\nindependence reasonable.outliers,\n\\(n = 68\\) none outliers\nparticularly extreme, normality\n\\(\\bar{x}\\) satisfied.outliers,\n\\(n = 68\\) none outliers\nparticularly extreme, normality\n\\(\\bar{x}\\) satisfied.conditions satisfied,\ncan move forward \\(t\\)-distribution.","code":""},{"path":"inference-paired-means.html","id":"observed-statistic-vs.-null-statistics-2","chapter":"18 Inference for comparing paired means","heading":"18.3.3 Observed statistic vs. null statistics","text":"mentioned previously, methods applied difference identical one-sample techniques. Therefore, full hypothesis test framework given example.Complete hypothesis test started\nprevious Example.start, compute standard error associated \n\\(\\bar{x}_{d}\\) using sample standard\ndeviation differences\n(\\(s_{d} = 13.42\\))\nnumber differences\n(\\(n = 68\\)):\n\\[\\begin{align*}\n  SE(\\bar{x}_{d})\n    = \\frac{s_{d}}{\\sqrt{n}}\n    = \\frac{13.42}{\\sqrt{68}} = 1.63\n  \\end{align*}\\]\ntest statistic T-score \n\\(\\bar{x}_{d}\\)\nnull condition actual mean\ndifference 0:\n\\[\\begin{align*}\n  T\n    = \\frac{\\bar{x}_{d} - 0}\n        { SE(\\bar{x}_{d})}\n    = \\frac{3.58 - 0}{1.63} = 2.20\n  \\end{align*}\\]\nvalue tells us sample mean difference price, $3.58,\n2.20 standard errors zero (null value).visualize p-value, approximate sampling distribution\n\\(\\bar{x}_{d}\\) drawn though\n\\(H_0\\) true,\np-value represented shaded upper tail Figure 18.8. area equivalent\narea 2.20 \\(t\\)-distribution \\(df = n - 1\\) = 68 \\(-\\) 1 = 67 degrees freedom.Using pt function R, find \nupper tail area 0.0156.conclusion, strong evidence \nAmazon prices , average, lower \nUCLA Bookstore prices UCLA courses.\nFigure 18.8: Distribution \\(\\bar{x}_{d}\\) null hypothesis difference. observed average difference 2.98 marked shaded areas extreme observed difference given p-value.\nCreate theory-based 95% confidence interval average price difference books UCLA Bookstore books Amazon.Conditions\nusing theory-based methods already verified standard error\ncomputed previous Example.find confidence interval,\nidentify \\(t^{\\star}_{67}\\) using R command: qt(0.975, 67) = 2.00,\nplug , point estimate,\nstandard error confidence\ninterval formula:\n\\[\\begin{align*}\n  \\bar{x}_d \\ \\pm\\ t^{\\star} \\times SE(\\bar{x}_d)\n      \\quad\\\\quad\n          3.58 \\ \\pm\\ 2.00 \\times 1.63\n      \\quad\\\\quad (0.32, 6.84)\n  \\end{align*}\\]\n95% confident Amazon , average,\n$0.32 $6.84 less expensive\nUCLA Bookstore UCLA course books.strong evidence Amazon ,\naverage, less expensive.\nconclusion affect UCLA student\nbuying habits?\nUCLA students always buy books\nAmazon?163\n","code":""},{"path":"inference-paired-means.html","id":"chp18-review","chapter":"18 Inference for comparing paired means","heading":"18.4 Chapter review","text":"","code":""},{"path":"inference-paired-means.html","id":"summary-13","chapter":"18 Inference for comparing paired means","heading":"Summary","text":"TODO","code":""},{"path":"inference-paired-means.html","id":"terms-14","chapter":"18 Inference for comparing paired means","heading":"Terms","text":"introduced following terms chapter. ’re sure terms mean, recommend go back text review definitions. purposefully presenting alphabetical order, instead order appearance, little challenging locate. However able easily spot bolded text.","code":""},{"path":"inference-paired-means.html","id":"key-ideas-13","chapter":"18 Inference for comparing paired means","heading":"Key ideas","text":"TODO","code":""},{"path":"inference-two-means.html","id":"inference-two-means","chapter":"19 Inference for comparing two independent means","heading":"19 Inference for comparing two independent means","text":"TODOBelow summarize notation used throughout chapter.Notation.\\(n_1\\), \\(n_2\\) = sample sizes two independent samples\\(\\bar{x}_1\\), \\(\\bar{x}_2\\) = sample means two independent samples\\(s_1\\), \\(s_2\\) = sample standard deviations two independent samples\\(\\mu_1\\), \\(\\mu_2\\) = population means two independent populations\\(\\sigma_1\\), \\(\\sigma_2\\) = population standard deviations two independent populationsIn section consider difference \ntwo population means, \\(\\mu_1 - \\mu_2\\), condition\ndata paired.\nJust single sample, identify conditions ensure\ncan use \\(t\\)-distribution point estimate\ndifference, \\(\\bar{x}_1 - \\bar{x}_2\\),\nnew standard error formula.details working inferential problems two independent means setting strikingly similar applied two independent proportions setting.\nfirst cover randomization test observations shuffled assumption null hypothesis true.\nbootstrap data (imposed null hypothesis) create confidence interval true difference population means, \\(\\mu_1 - \\mu_2\\).\nmathematical model, \\(t\\)-distribution, able describe randomization test boostrapping long conditions met.inferential tools applied three different data contexts: determining whether\nstem cells can improve heart function,\nexploring relationship pregnant women’s smoking\nhabits birth weights newborns,\nexploring whether statistically significant\nevidence one variation exam harder \nanother variation.\nsection motivated questions like\n“convincing evidence newborns mothers\nsmoke different average birth weight newborns\nmothers don’t smoke?”","code":""},{"path":"inference-two-means.html","id":"rand2mean","chapter":"19 Inference for comparing two independent means","heading":"19.1 Randomization test for \\(H_0: \\mu_1 - \\mu_2 = 0\\)","text":"instructor decided run two slight variations exam. Prior passing exams, shuffled exams together ensure student received random version. Summary statistics students performed two exams shown Table 19.1 plotted Figure 19.1. Anticipating complaints students took Version B, like evaluate whether difference observed groups large provides convincing evidence Version B difficult (average) Version .","code":""},{"path":"inference-two-means.html","id":"observed-data-10","chapter":"19 Inference for comparing two independent means","heading":"19.1.1 Observed data","text":"\nTable 19.1: Summary statistics scores exam version.\n\nFigure 19.1: Exam scores students given one three different exams.\nConstruct hypotheses evaluate whether observed\ndifference sample means, \\(\\bar{x}_A - \\bar{x}_B=3.1\\),\ndue chance. later evaluate hypotheses\ncomputing p-value test.164Before moving evaluate hypotheses previous Guided Practice, let’s think carefully dataset. observations across two groups independent? concerns outliers?165","code":""},{"path":"inference-two-means.html","id":"variability-of-the-statistic-7","chapter":"19 Inference for comparing two independent means","heading":"19.1.2 Variability of the statistic","text":"Sections 9.2 15.1, variability statistic (previously: \\(\\hat{p}_1 - \\hat{p}_2\\)) visualized shuffling observations across two treatment groups many times.\nshuffling process implements null hypothesis model (effect treatment).\nexam example, null hypothesis exam exam B equally difficult, average scores across two tests .\nexams equally difficult, due natural variability, sometimes expect students slightly better exam (\\(\\bar{x}_A > \\bar{x}_B\\)) sometimes expect students slightly better exam B (\\(\\bar{x}_B > \\bar{x}_A\\)).\nquestion hand : \\(\\bar{x}_A - \\bar{x}_B=3.1\\) indicate exam easier exam B?.Figure 19.2 shows process randomizing exam observed exam scores.\nnull hypothesis true, score exam represent true student ability material.\nshouldn’t matter whether given exam exam B.\nreallocating student got exam, able understand difference average exam scores changes due natural variability.\none iteration randomization process Figure 19.2, leading one simulated difference average scores.\nFigure 19.2: version test (B) randomly allocated test scores, null assumption tests equally difficult.\nBuilding Figure 19.2, Figure 19.3 shows values simulated statistics \\(\\bar{x}_{1, sim} - \\bar{x}_{2, sim}\\) 1000 random simulations.\nsee , just chance, difference scores can range anywhere -10 points +10 points.\nFigure 19.3: Histogram differences means, calculated 1000 different randomizations exam types.\n","code":""},{"path":"inference-two-means.html","id":"observed-statistic-vs.-null-value-1","chapter":"19 Inference for comparing two independent means","heading":"19.1.3 Observed statistic vs. null value","text":"goal randomization test assess observed data, statistic interest \\(\\bar{x}_A - \\bar{x}_B = 3.1\\).\nrandomization distribution allows us identify whether difference 3.1 points one expect natural variability.\nplotting value 3.1 Figure 19.4, can measure different similar 3.1 randomized differences generated null hypothesis.\nFigure 19.4: Histogram differences means, calculated 1000 different randomizations exam types. observed difference 3.1 points plotted vertical line, area extreme 3.1 shaded represent p-value.\nExample 19.1  Approximate p-value depicted Figure 19.4, provide conclusion context case study.Using software, find 231 1000 shuffled differences means away zero observed difference 3.1. , 23.1% shuffled statistics lie shaded blue area Figure 19.4. Thus, p-value 0.231.large p-value, data convincingly show one exam\nversion difficult , teacher\nconvinced add points \nVersion B exam scores.large p-value consistency \\(\\bar{x}_A - \\bar{x}_B=3.1\\) randomized differences leads us reject null hypothesis. Said differently, evidence think one tests easier .One might inclined conclude tests level difficulty, conclusion wrong. Indeed, best point estimate true average difference means two tests 3.1!\nhypothesis testing framework set reject null claim, set validate null claim.\nconcluded, data consistent exams B equally difficult, data also consistent exam 3.1 points “easier” exam B. fact, ’ll see next section, since 95% confidence interval \\(\\mu_A - \\mu_B\\) (-2.0, 8.3), data consistent difference exam 2.0 points “harder” exam 8.3 points “easier.”\ndata able adjudicate whether exams equally hard whether one slightly easier.Conclusions null hypothesis rejected often seem unsatisfactory.\nHowever, case, teacher class probably relieved evidence demonstrate one exams difficult .","code":""},{"path":"inference-two-means.html","id":"boot-ci-diff-means","chapter":"19 Inference for comparing two independent means","heading":"19.2 Bootstrap confidence interval for \\(\\mu_1 - \\mu_2\\)","text":"\nproviding full example working bootstrap analysis actual data, consider fictional situation like compare average price car one Awesome Auto franchise (Group 1) average price car different Awesome Auto franchise (Group 2). able randomly sample five cars Awesome Auto franchise, measure selling price car sample. process bootstrapping can applied Group separately, differences means recalculated time. Figure 19.5 visually describes bootstrap process interest statistic computed two separate samples. analysis proceeds one sample case, now (single) statistic interest difference sample means. , bootstrap resample done groups separately, results combined single bootstrapped difference means. Repetition produce 1000s bootstrapped differences means, histogram describe natural sampling variability associated difference means.\nFigure 19.5: two group comparison, 1000 bootstrap resamples taken separately group, difference sample means calculated pair bootstrap resamples. set 1000 differences analyzed distribution statistic interest, conclusions drawn parameter interest.\n","code":""},{"path":"inference-two-means.html","id":"observed-data-11","chapter":"19 Inference for comparing two independent means","heading":"19.2.1 Observed data","text":"treatment using embryonic stem cells (ESCs)\nhelp improve heart function following heart attack?\nTable 19.2 contains summary statistics\nexperiment test ESCs sheep heart attack.\nsheep randomly assigned ESC\ncontrol group, percent change hearts’ pumping\ncapacity measured study.\nFigure 19.6 provides\nhistograms two data sets.\npositive value corresponds increased pumping capacity,\ngenerally suggests stronger recovery.\ngoal identify 90% confidence interval\neffect ESCs change heart pumping\ncapacity relative control group.\nTable 19.2: Summary statistics embryonic stem cell study.\n\nFigure 19.6: Histograms embryonic stem cell control group.\npoint estimate true difference mean heart pumping variable\nstraightforward find: difference sample means.\n\\[\\begin{align*}\n\\bar{x}_{esc} - \\bar{x}_{control}\\\n  =\\ 3.50 - (-4.33)\\\n  =\\ 7.83\n\\end{align*}\\](-4.33) \n= 7.83\n\\end{align*}Identify roles two variables study — variable explanatory variable response? scope inference study?166","code":""},{"path":"inference-two-means.html","id":"variability-of-the-statistic-8","chapter":"19 Inference for comparing two independent means","heading":"19.2.2 Variability of the statistic","text":"saw Section 15.2, use bootstrapping estimate variability associated difference sample means taking repeated samples. method akin two proportions, separate sample taken replacement group (ESCs control), sample means calculated, difference taken. entire process repeated multiple times produce bootstrap distribution difference sample means (without null hypothesis assumption).Figure 19.7 displays variability differences sample means percentile bootstrap 90% confidence interval super imposed.\nFigure 19.7: Histogram differences means 1000 bootstrap resamples taken two groups. observed difference means original data plotted black vertical line 7.83. blue lines provide percentile bootstrap 90% confidence interval difference true population means.\nExample 19.2  bootstrap confidence interval true difference average change pumping capacity, \\(\\mu_{esc} - \\mu_{control}\\), show difference across two treatments?90% interval displayed contain zero (note zero never one bootstrapped differences 95% 99% intervals given conclusion!), conclude ESC treatment significantly better respect heart pumping capacity treatment.study randomized controlled experiment, can conclude treatment (ESC) causing change pumping capacity.","code":""},{"path":"inference-two-means.html","id":"math2samp","chapter":"19 Inference for comparing two independent means","heading":"19.3 Theory-based inferential methods for \\(\\mu_1 - \\mu_2\\)","text":"one-mean paired mean difference scenarios, difference sample means can modeled \\(t\\)-distribution certain conditions. conditions one-mean paired mean difference scenarios, now conditions need met sample. Similarly, compute test statistic theory-based confidence interval using standard error formula difference sample means.Using \\(t\\)-distribution difference means.\\(t\\)-distribution can used inference working\nstandardized difference two means ifIndependence (extended).\ndata independent within \ntwo groups, e.g., data come \nindependent random samples \nrandomized experiment.Normality.\ncheck outliers \ngroup separately.standard error may computed \n\\[\nSE(\\bar{x}_1 - \\bar{x}_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}},\n\\]official formula degrees freedom quite\ncomplex generally computed using software,\ninstead may use smaller \n\\(n_1 - 1\\) \\(n_2 - 1\\) degrees freedom\nsoftware isn’t readily available.","code":""},{"path":"inference-two-means.html","id":"t-test-for-mu_1---mu_2","chapter":"19 Inference for comparing two independent means","heading":"19.3.1 \\(t\\)-test for \\(\\mu_1 - \\mu_2\\)","text":"","code":""},{"path":"inference-two-means.html","id":"observed-data-12","chapter":"19 Inference for comparing two independent means","heading":"Observed data","text":"dataset called ncbirths represents random sample 150 cases mothers newborns North Carolina year. Four cases data set represented Table 19.3. particularly interested two variables: weight smoke. weight variable represents weights newborns smoke variable describes mothers smoked pregnancy. like know, convincing evidence newborns mothers smoke different average birth weight newborns mothers don’t smoke? use North Carolina sample try answer question. smoking group includes 50 cases nonsmoking group contains 100 cases.\nTable 19.3: Four cases ncbirths data set. value NA, shown first two entries first variable, indicates piece data missing.\nExample 19.3  Set appropriate hypotheses evaluate\nwhether relationship mother smoking\naverage birth weight.null hypothesis represents case difference\ngroups.\\(H_0\\): difference average birth weight \nnewborns mothers smoke.\nstatistical notation: \\(H_0: \\mu_{n} - \\mu_{s} = 0\\),\n\\(\\mu_{n}\\) represents true mean birth weight babies non-smoking mothers \n\\(\\mu_s\\) represents true mean birth weight babies mothers smoked.alternative hypothesis represents research question.\\(H_A\\): difference average newborn weights\nmothers smoke (\\(\\mu_{n} - \\mu_{s} \\neq 0\\)).","code":""},{"path":"inference-two-means.html","id":"variability-of-the-statistic-9","chapter":"19 Inference for comparing two independent means","heading":"Variability of the statistic","text":"check two conditions necessary model difference\nsample means using \\(t\\)-distribution: independence normality conditions sample.data come simple random sample,\nobservations independent,\nwithin samples.data sets 30 observations,\ninspect data \nFigure 19.8\nparticularly extreme outliers\nfind none.Since conditions satisfied, difference\nsample means may modeled using \\(t\\)-distribution.\nFigure 19.8: top panel represents birth weights infants whose mothers smoked. bottom panel represents birth weights infants whose mothers smoke.\nsummary statistics Table 19.4 may useful\nGuided Practice.167What point estimate population difference,\n\\(\\mu_{n} - \\mu_{s}\\)?Compute standard error point estimate \npart .\nTable 19.4: Summary statistics ncbirths data set.\n","code":""},{"path":"inference-two-means.html","id":"observed-statistic-vs.-null-value-2","chapter":"19 Inference for comparing two independent means","heading":"Observed statistic vs. null value","text":"test statistic comparing two means T.T score ratio groups differ compared observations within group vary.\\[\\begin{align*}\nT = \\frac{\\bar{x}_1 - \\bar{x}_2 - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\end{align*}\\]null hypothesis true conditions met, T \\(t\\)-distribution \\(df = min(n_1 - 1, n_2 -1)\\).Conditions:independent observations within across groupslarge samples extreme outliersExample 19.4  Complete hypothesis test started previous Example Guided Practice ncbirths dataset research question.\nreference, \\(\\bar{x}_{n} - \\bar{x}_{s} = 0.40\\),\n\\(SE(\\bar{x}_{n} - \\bar{x}_{s}) = 0.26\\), sample sizes \\(n_n = 100\\) \\(n_s = 50\\).can find test statistic test\nusing previous information:\n\\[\\begin{align*}\n  T = \\frac{\\ 0.40 - 0\\ }{0.26} = 1.54\n  \\end{align*}\\]\np-value represented two shaded tails\nFigure 19.9We find single tail area using software. (See R code .) ’ll use \nsmaller \\(n_n - 1 = 99\\) \\(n_s - 1 = 49\\) \ndegrees freedom: \\(df = 49\\).\none tail area 0.065;\ndoubling value gives two-tail area p-value,\n0.135.p-value 0.135 provides little evidence null hypothesis.\ninsufficient evidence say difference\naverage birth weight newborns North Carolina mothers\nsmoke pregnancy newborns North Carolina\nmothers smoke pregnancy.\nFigure 19.9: mathematical model T statistic null hypothesis true: \\(t\\)-distribution \\(min(100-1, 50-1) = 49\\) degrees freedom. expected, curve centered zero (null value). T score also plotted area extreme observed T score plotted indicate p-value.\n’ve seen much research suggesting smoking harmful\npregnancy, fail reject null\nhypothesis previous Example?168If made Type 2 Error difference,\ndone differently data collection\nlikely detect difference?169Public service announcement: used relatively\nsmall data set example, larger data sets show women\nsmoke tend smaller newborns.\nfact, tobacco industry actually audacity\ntout benefit smoking:’s true.\nbabies born women smoke smaller,\n’re just healthy babies born \nwomen smoke.\nwomen prefer smaller babies.\n- Joseph Cullman, Philip Morris’ Chairman Board CBS’ Face Nation, Jan 3, 1971Fact check: babies women smoke actually\nhealthy babies women \nsmoke.170","code":"\npt(1.54, df = 49, lower.tail = FALSE)\n#> [1] 0.065"},{"path":"inference-two-means.html","id":"t-confidence-interval-for-mu_1---mu_2","chapter":"19 Inference for comparing two independent means","heading":"19.3.2 \\(t\\) confidence interval for \\(\\mu_1 - \\mu_2\\)","text":"Finding \\(t\\)-confidence interval difference population means, \\(\\mu_1 - \\mu_2\\).Based two independent samples \\(n_1\\) \\(n_2\\) observational units, respectively, clear outliers, confidence interval difference population\nmeans \n\\[\\begin{align*}\n  \\text{point estimate} \\ &\\pm\\  t^{\\star}_{df} \\times SE(\\text{point estimate}) \\\\\n  &\\\\\\\n  \\bar{x}_1 - \\bar{x}_2 \\ &\\pm\\  t^{\\star}_{df} \\times \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n  \\end{align*}\\]\n\\(\\bar{x}_1\\) \\(\\bar{x}_2\\) two sample means, \\(t^{\\star}_{df}\\)\ncorresponds confidence level degrees freedom\n\\(df\\), \\(SE\\) standard error estimated \nsample.Example 19.5  Consider data Section 19.2 use embryonic stem cells (ESCs) improve heart function. Can \\(t\\)-distribution used make\ninference true difference average change heart pumping function using point estimate,\n\\(\\bar{x}_{esc} - \\bar{x}_{control} = 7.83\\)?First, check independence.\nsheep randomized \ngroups, independence within\ngroups satisfied.Figure 19.6\nreveal clear outliers\neither group.\n(ESC group bit variability,\nclear outliers.)conditions met, can use \n\\(t\\)-distribution model difference sample means.Generally, use statistical software find appropriate\ndegrees freedom using raw data, software isn’t available,\ncan use smaller\n\\(n_1 - 1\\) \\(n_2 - 1\\) degrees freedom.\ncase ESC example, means ’ll use \\(df = 8\\).Example 19.6  Calculate 95% confidence interval \ntrue difference mean change heart pumping capacity \nsheep ’ve suffered heart attack ESC treatment control treatment.First, compute point estimate standard error:\n\\[\\begin{align*}\n  \\bar{x}_{esc} - \\bar{x}_{control} &= 3.50 - (-4.33) = 7.83\\\\   \n  SE(\\bar{x}_{esc} - \\bar{x}_{control}) &= \\sqrt{\\frac{5.17^2}{9} + \\frac{2.76^2}{9}} = 1.95\n  \\end{align*}\\]\nUsing \\(df = 8\\), can identify \ncritical value \\(t^{\\star}_{8} = 2.31\\)\n95% confidence interval. (See R code .)\nFinally, can enter values confidence\ninterval formula:\n\\[\\begin{align*}\n   7.83 \\ \\pm\\ 2.31\\times 1.95\n    \\quad\\rightarrow\\quad (3.32, 12.34)\n  \\end{align*}\\]\n95% confident embryonic stem cells improve\nmean change heart’s pumping function sheep suffered\nheart attack 3.32% 12.34%.","code":"\nqt(0.975, df = 8)\n#> [1] 2.31"},{"path":"inference-two-means.html","id":"chp19-review","chapter":"19 Inference for comparing two independent means","heading":"19.4 Chapter review","text":"","code":""},{"path":"inference-two-means.html","id":"summary-14","chapter":"19 Inference for comparing two independent means","heading":"Summary","text":"TODO","code":""},{"path":"inference-two-means.html","id":"summary-of-t-procedures","chapter":"19 Inference for comparing two independent means","heading":"Summary of t-procedures","text":"past three chapters, seen \\(t\\)-distribution applied appropriate mathematical model three distinct settings. Although three data structures different, similarities differences worth pointing . provide Table 19.5 partly mechanism understanding \\(t\\)-procedures partly highlight extremely common usage \\(t\\)-distribution practice. often hear following three \\(t\\)-procedures referred one sample \\(t\\)-test (\\(t\\)-interval), paired \\(t\\)-test (\\(t\\)-interval), two sample \\(t\\)-test (\\(t\\)-interval).\nTable 19.5: Similarities \\(t\\)-methods across one sample, paired sample, two independent samples analysis numeric response variable.\nindependence, 2. normality large samples\nindependence, 2. normality large samples\nindependence, 2. normality large samples\nHypothesis tests. applying \\(t\\)-distribution hypothesis test involving means, proceed follows:Write appropriate hypotheses.Verify conditions using \\(t\\)-distribution.\nIndependence. Observational units must independent. typically true data came random sample (two random samples, one random sample randomly assigned two treatments).\nNormality. sample size less 30 clear outliers data, sample size least 30\nparticularly extreme outliers,\ncan apply \\(t\\)-distribution hypothesis tests means. difference means data paired, condition must met two samples.\nIndependence. Observational units must independent. typically true data came random sample (two random samples, one random sample randomly assigned two treatments).Normality. sample size less 30 clear outliers data, sample size least 30\nparticularly extreme outliers,\ncan apply \\(t\\)-distribution hypothesis tests means. difference means data paired, condition must met two samples.Compute statistic interest, standard error, degrees freedom. \\(df\\), use \\(n-1\\) one sample, two samples use either statistical software smaller \\(n_1 - 1\\) \\(n_2 - 1\\).Compute T-score using general formula:\n\\[\nT = \\frac{\\mbox{statistic} - \\mbox{null value}}{\\mbox{standard error statistic}} = \\frac{\\mbox{statistic} - \\mbox{null value}}{SE(\\mbox{statistic})}\n\\]Use statistical software find p-value using appropriate \\(t\\)-distribution:\nSign \\(H_A\\) \\(<\\): p-value = area T-score\nSign \\(H_A\\) \\(>\\): p-value = area T-score\nSign \\(H_A\\) \\(\\neq\\): p-value = 2 \\(\\times\\) area \\(-|\\mbox{T-score}|\\)\nSign \\(H_A\\) \\(<\\): p-value = area T-scoreSign \\(H_A\\) \\(>\\): p-value = area T-scoreSign \\(H_A\\) \\(\\neq\\): p-value = 2 \\(\\times\\) area \\(-|\\mbox{T-score}|\\)Make conclusion based p-value, write conclusion context, plain language, terms alternative hypothesis.Confidence intervals. Similarly, following generally computed confidence interval using \\(t\\)-distribution:Verify conditions using \\(t\\)-distribution. (See .)Compute point estimate interest, standard error, degrees freedom, \\(t^{\\star}_{df}\\). multiplier \\((1-\\alpha)\\times100\\)% confidence interval can found R : qt(1-(alpha/2), df). example, \\(t^{\\star}_{10}\\) 95% confidence qt(0.975, 10) = 2.228.Calculate confidence interval using general formula:\n\\[\n\\mbox{statistic} \\pm\\ t_{df}^{\\star} SE(\\mbox{statistic}).\n\\]Put conclusions context plain language even non-data scientists can understand results.","code":""},{"path":"inference-two-means.html","id":"terms-15","chapter":"19 Inference for comparing two independent means","heading":"Terms","text":"introduced following terms chapter. ’re sure terms mean, recommend go back text review definitions. purposefully presenting alphabetical order, instead order appearance, little challenging locate. However able easily spot bolded text.","code":""},{"path":"inference-two-means.html","id":"key-ideas-14","chapter":"19 Inference for comparing two independent means","heading":"Key ideas","text":"TODO","code":""},{"path":"inference-num-applications.html","id":"inference-num-applications","chapter":"20 Applications: Infer quantitative","heading":"20 Applications: Infer quantitative","text":"TODOOld content - revise needed","code":""},{"path":"inference-num-applications.html","id":"inference-for-quantitative-data-using-r-and-catstats","chapter":"20 Applications: Infer quantitative","heading":"20.1 Inference for quantitative data using R and catstats","text":"","code":""},{"path":"inference-num-applications.html","id":"using-the-t-distribution","chapter":"20 Applications: Infer quantitative","heading":"Using the \\(t\\)-distribution","text":"First, ’ll review obtain probabilities critical values \\(t\\)-distribution using R. \\(t\\)-statistic degrees freedom, can find probability \\(t\\)-distribution corresponding one- two-tailed hypothesis test using pt() (short “probability \\(t\\)-distribution”).tricky part making sure get correct area distribution. example, assume 12 degrees freedom. \\(t\\)-statistic positive, say \\(t = 1.33\\):\\(t\\)-statistic negative, say \\(t = -2.75\\):difference comes want area \\(t\\) units 0: \\(t\\) positive, “” greater \\(t\\) less negative \\(t\\), double area greater \\(t\\), since \\(t\\)-distribution symmetric. \\(t\\) negative, “” less \\(t\\) greater positive \\(t\\), double area less \\(t\\).find \\(t^*_{df}\\) using R, use qt() function (short “quantile \\(t\\)-distribution”). need degrees freedom confidence level confidence interval. Suppose 27 degrees freedom want 99% confidence interval. get middle 99% \\(t\\)-distribution, need cutoff 0.5% 99.5%:","code":"\n#Area less than observed:\npt(1.33, df = 12)\n#> [1] 0.896\n\n#Area greater than observed:\n1-pt(1.33, df = 12)\n#> [1] 0.104\n\n#Area at least as large as observed:\n2*(1-pt(1.33, df = 12))\n#> [1] 0.208\n#Area less than observed:\npt(-2.75, df = 12)\n#> [1] 0.0088\n\n#Area greater than observed:\n1-pt(-2.75, df = 12)\n#> [1] 0.991\n\n#Area at least as large as observed:\n2*pt(-2.75, df = 12)\n#> [1] 0.0176#> [1] -2.77\n#> [1] 2.77"},{"path":"inference-num-applications.html","id":"simulation-based-inference-for-paired-mean-difference","chapter":"20 Applications: Infer quantitative","heading":"Simulation-based inference for paired mean difference","text":"Simulation-based inference quantitative data use functions catstats package, categorical data.catstats functions paired data assume values two groups separate columns data frame. ’ll work example using tire wear data, currently stored “long format”, one variable brand another tread depth. First, ’ll convert “wide format”, column brand.\nformat, paired data functions catstats able handle data. First, can get look pairs observations:gives us idea distributions within groups differences within pairs. perform hypothesis test difference tread depth 1000 miles, use paired_test() function:Note data also vector differences. , can hypothesis testing generate confidence interval, won’t able use paired_observed_plot(). Now let’s take look output function:figure displays bootstrapped null distribution, mean standard deviation draws upper right corner. want see mean close null value (almost always zero). isn’t, check value shift input, /increase number_repetitions shift correct.red lines give cutoffs based observed statistic, values extreme colored red. one-sided test, one line. caption figure gives number proportion bootstrapped mean differences extreme observed statistic. case, 20 1000, p-value 0.02.Finally, want generate confidence interval true mean difference using paired_bootstrap_CI() function.bootstrap distribution, now bootstrap distribution mean difference , rather bootstrapped null distribution mean difference. ’ve requested 99% confidence interval, relevant percentiles bootstrap distribution highlighted, interval given caption. case, 99% confident true mean difference tire tread 0 0.004 inches greater Smooth Turn.","code":"\nlibrary(catstats)\ntiresWide <- tires %>% \n  select(brand, tread, car) %>%   #select only ID, group, and outcome vars\n  pivot_wider(names_from = brand,   #name of variable for group\n              values_from = tread)  #name of variable for outcome\ntiresWide <- as.data.frame(tiresWide)\npaired_observed_plot(tiresWide)\npaired_test(\n  data = tiresWide,  #data frame with observed values in groups\n  shift = -0.002,  #amount to shift differences to bootstrap null distribution\n  direction = \"two-sided\",  #Direction of hypothesis test\n  as_extreme_as = 0.002, #Observed statistic\n  number_repetitions = 1000,  #number of bootstrap draws for null distribution\n  which_first = 1  #Which column is first in order of subtraction: 1 or 2?\n)\nset.seed(1054)\npaired_test(\n  data = tiresWide,  #data frame with observed values in groups\n  shift = -0.002,  #amount to shift differences to bootstrap null distribution\n  direction = \"two-sided\",  #Direction of hypothesis test\n  as_extreme_as = 0.002, #Observed statistic\n  number_repetitions = 1000,  #number of bootstrap draws for null distribution\n  which_first = 1  #Which column is first in order of subtraction: 1 or 2?\n)\nset.seed(2374)\npaired_bootstrap_CI(\n  data = tiresWide,   #Wide-form data set or vector of differences\n  number_repetitions = 1000,   #number of draws for bootstrap distribution\n  confidence_level = 0.99,  #Confidence level as a proportion\n  which_first = 1  #Order of subtraction: 1st or 2nd set of values come first?\n)"},{"path":"inference-num-applications.html","id":"theory-based-inference-for-paired-mean-difference","chapter":"20 Applications: Infer quantitative","heading":"Theory-based inference for paired mean difference","text":"implement theory-based inference paired mean difference R, use t.test() function. example, ’ll use textbook cost data Chapter 18. two ways put paired data t-test using t.test(). First, prices two groups two separate variables (case, bookstore_new amazon_new):Important things note :must include paired = TRUE options, two-sample t-test.categorical data Chapters 14 15, one-sided alternative, need re-run t.test() two-sided alternative get correct confidence intervalNow let’s take look output call:output tells right top paired test - doesn’t, check paired = TRUE function call. next line gives t-statistic 2.20, degrees freedom df = 67, p-value 0.0156 (can look back Section 18.3 see values obtained example). point estimate mean difference final entry: average, new bookstore books cost $3.58 books new Amazon.confidence interval given one-sided confidence interval, since one-sided alternative. need re-run alternative = \"two.sided\" get correct interval true mean difference price $0.33 $6.83 greater cost buying UCLA Bookstore compared buying Amazon.might also single variable dataset contains differences within pairs: create textbook data variable called price_diff. format also usable t.test() function:requires two fewer arguments:y input, since differences contained single variableNo paired = TRUE, since already accounted pairing taking differences.output look almost identical two-variable version :Since input one variable, t.test() treats one-sample t-test, note works just fine: t-statistic, df, p-value, confidence interval, estimated mean put two groups separately indicated paired.","code":"\nt.test(x = ucla_textbooks_f18$bookstore_new, #Outcomes for one of each pair\n       y = ucla_textbooks_f18$amazon_new,  #Outcomes for other of each pair\n       paired = TRUE,  #Tell it to do a paired t-test!!\n       alternative = \"greater\",  #Direction of alternative \n       conf.level = 0.95  #confidence level for interval as a proportion\n)\nt.test(x = ucla_textbooks_f18$bookstore_new, #Outcomes for first in order of subtraction\n       y = ucla_textbooks_f18$amazon_new,  #Outcomes for second in order of subtraction\n       paired = TRUE,  #Tell it to do a paired t-test!!\n       alternative = \"greater\",  #Direction of alternative \n       conf.level = 0.95  #confidence level for interval as a proportion\n)\n#> \n#>  Paired t-test\n#> \n#> data:  ucla_textbooks_f18$bookstore_new and ucla_textbooks_f18$amazon_new\n#> t = 2, df = 67, p-value = 0.02\n#> alternative hypothesis: true mean difference is greater than 0\n#> 95 percent confidence interval:\n#>  0.868   Inf\n#> sample estimates:\n#> mean difference \n#>            3.58#> \n#>  Paired t-test\n#> \n#> data:  ucla_textbooks_f18$bookstore_new and ucla_textbooks_f18$amazon_new\n#> t = 2, df = 67, p-value = 0.03\n#> alternative hypothesis: true mean difference is not equal to 0\n#> 95 percent confidence interval:\n#>  0.334 6.832\n#> sample estimates:\n#> mean difference \n#>            3.58\nucla_textbooks_f18 %>% \n  mutate(price_diff = bookstore_new-amazon_new)\n\nt.test(x = ucla_textbooks_f18$price_diff,   #variable with differences\n       alternative = \"greater\",  #direction of alternative hypothesis\n       conf.level = 0.95)  #confidence level as a proportion\nucla_textbooks_f18 <- ucla_textbooks_f18 %>% \n  mutate(price_diff = bookstore_new-amazon_new)\n\nt.test(x = ucla_textbooks_f18$price_diff,   #variable with differences\n       alternative = \"greater\",  #direction of alternative hypothesis\n       conf.level = 0.95)  #confidence level as a proportion\n#> \n#>  One Sample t-test\n#> \n#> data:  ucla_textbooks_f18$price_diff\n#> t = 2, df = 67, p-value = 0.02\n#> alternative hypothesis: true mean is greater than 0\n#> 95 percent confidence interval:\n#>  0.868   Inf\n#> sample estimates:\n#> mean of x \n#>      3.58"},{"path":"inference-num-applications.html","id":"simulation-based-inference-for-the-difference-of-two-means","chapter":"20 Applications: Infer quantitative","heading":"Simulation-based inference for the difference of two means","text":"can perform simulation-based inference difference means using two_mean_test() two_mean_bootstrap_CI() functions catstats package. working example, let’s look embryonic stem cell data Section 19.1.perform simulation-based test difference mean change heart pumping capacity, use two_mean_test() function catstats package, similar use two_proportion_test() function Chapter 16:results give side--side boxplot observed data observed difference order subtraction top. Check right value observed difference! Next box plot, null distribution simulated differences means, observed statistic marked vertical red line, values extreme observed statistic colored red. figure caption gives approximate p-value: set 1000 simulations, 1/1000 = 0.001.couple things note using two_mean_test function:need identify variable response explanatory variable using formula argument.Specify order subtraction using first_in_subtraction putting EXACTLY category explanatory variable want first, quotes — must match capitalization, spaces, etc. text values!use bootstrapping find confidence interval true difference means two_mean_bootstrap_CI() function. arguments similar two_mean_test(), addition confidence level.function produces bootstrap distribution difference means, upper lower percentiles confidence range marked vertical lines. figure caption gives estimated confidence interval. case, 90% confidence ESCs increase change heart pumping capacity 4.72 11.09 percentage points average.","code":"\n#load data from openintro package\ndata (stem_cell)\n\n#Compute change in pumping capacity\nstem_cell <- stem_cell %>%\n  mutate(change = after - before)\nset.seed(4750)\ntwo_mean_test(\n  formula = change ~ trmt,  #Always use response ~ explanatory\n  data = stem_cell,  # name of data set\n  first_in_subtraction = \"esc\",  #value of group variable to be 1st in subtraction\n  direction = \"two-sided\",  #direction of alternative\n  as_extreme_as = 7.833,  #observed statistic\n  number_repetitions = 1000  #number of simulations\n)\nset.seed(450)\ntwo_mean_bootstrap_CI(\n  formula = change ~ trmt,  #Always use response ~ explanatory\n  data = stem_cell,  # name of data set\n  first_in_subtraction = \"esc\",  #value of group variable to be 1st in subtraction\n  confidence_level = 0.9, #confidence level as a proportion\n  number_repetitions = 1000  #number of bootstrap samples\n)"},{"path":"inference-num-applications.html","id":"theory-based-inference-for-the-difference-of-two-means","chapter":"20 Applications: Infer quantitative","heading":"Theory-based inference for the difference of two means","text":"demonstrate theory-based methods R difference means, continue use embryonic stem cell data. Something keep mind work example: theory-based methods appropriate ? results different results simulation-based methods?perform theory-based inference, use t.test() function R. Remember sometimes need change reference category explanatory variable correct order subtraction - case, default ctrl - esc, since ctrl first alphabetically. can get preferred order subtracation esc-ctrl way:results look familiar paired t-test . t-statistic, degrees freedom, p-values, confidence interval, group-specific means. degrees freedom may look little strange - remember correct formula complex! - obtain 12.225 df. computing results hand using pt() qt() saw earlier section, use 8 df, since \\(n_1 -1 = n_2 -1 = 8\\). end, conclude strong evidence null hypothesis difference mean change heart pumping capacity (p = 0.002).Remember one-sided alternative, need run t.test() two-sided alternative get correct confidence interval! reminder Inf confidence interval results, means R computing one-sided confidence interval. , two-sided alternative, can use CI reported: 90% confident true average improvement heart pumping capacity due ESCs 4.35 11.31 percentage points.","code":"\nstem_cell$trmt <- relevel(stem_cell$trmt, ref = \"esc\")\nt.test(stem_cell$change ~ stem_cell$trmt, #Always use response ~ explanatory\n       alternative = \"two.sided\", # Direction of alternative\n       conf.level = 0.9)  #confidence level as a proportion\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  stem_cell$change by stem_cell$trmt\n#> t = 4, df = 12, p-value = 0.002\n#> alternative hypothesis: true difference in means between group esc and group ctrl is not equal to 0\n#> 90 percent confidence interval:\n#>   4.35 11.31\n#> sample estimates:\n#>  mean in group esc mean in group ctrl \n#>               3.50              -4.33"},{"path":"inference-num-applications.html","id":"catstats-function-summary-1","chapter":"20 Applications: Infer quantitative","heading":"20.2 catstats function summary","text":"previous section, introduced four new R\nfunctions catstats library. provide summary \nfunctions, plus summary paired_observed_plot\nfunction can used plot paired data. can also access\nhelp files functions using ? command. \nexample, type ?paired_test R console bring \nhelp file paired_test function.\npaired_observed_plot: Produce plot observed matched pairs data. (Note: Input must observed value member pair, differences.)\ndata = two-column data frame, values group two columns\npaired_observed_plot: Produce plot observed matched pairs data. (Note: Input must observed value member pair, differences.)data = two-column data frame, values group two columnspaired_test: Simulation-based hypothesis test paired mean difference.\ndata = vector observed differences; two-column data frame, values group two columns\nwhich_first = name group first order subtraction (data two-column data frame)\nshift = amount shift differences bootstrapping null distribution\ndirection = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)\nas_extreme_as = value observed statistic\nnumber_repetitions = number simulated samples generate (least 1000!)\n\npaired_test: Simulation-based hypothesis test paired mean difference.data = vector observed differences; two-column data frame, values group two columnswhich_first = name group first order subtraction (data two-column data frame)shift = amount shift differences bootstrapping null distributiondirection = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)as_extreme_as = value observed statisticnumber_repetitions = number simulated samples generate (least 1000!)\npaired_bootstrap_CI: Bootstrap confidence interval paired mean difference.\ndata = vector observed differences; two-column data frame, values group two columns\nwhich_first = name group first order subtraction (data two-column data frame)\nconfidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)\nnumber_repetitions = number simulated samples generate (least 1000!)\n\npaired_bootstrap_CI: Bootstrap confidence interval paired mean difference.data = vector observed differences; two-column data frame, values group two columnswhich_first = name group first order subtraction (data two-column data frame)confidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)number_repetitions = number simulated samples generate (least 1000!)\ntwo_mean_test: Simulation-based hypothesis test difference two means.\nformula = y ~ x y name quantitative response variable data set x name binary explanatory variable\ndata = name data set\nfirst_in_subtraction = category explanatory variable first subtraction, written quotations\ndirection = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)\nas_extreme_as = value observed difference proportions\nnumber_repetitions = number simulated samples generate (least 1000!)\n\ntwo_mean_test: Simulation-based hypothesis test difference two means.formula = y ~ x y name quantitative response variable data set x name binary explanatory variabledata = name data setfirst_in_subtraction = category explanatory variable first subtraction, written quotationsdirection = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)as_extreme_as = value observed difference proportionsnumber_repetitions = number simulated samples generate (least 1000!)\ntwo_mean_bootstrap_CI: Bootstrap confidence interval difference two means.\nformula = y ~ x y name quantitative response variable data set x name binary explanatory variable\ndata = name data set\nfirst_in_subtraction = category explanatory variable first subtraction, written quotations\nconfidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)\nnumber_repetitions = number simulated samples generate (least 1000!)\ntwo_mean_bootstrap_CI: Bootstrap confidence interval difference two means.formula = y ~ x y name quantitative response variable data set x name binary explanatory variabledata = name data setfirst_in_subtraction = category explanatory variable first subtraction, written quotationsconfidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)number_repetitions = number simulated samples generate (least 1000!)","code":""},{"path":"inference-reg.html","id":"inference-reg","chapter":"21 Inference for correlation and slope","heading":"21 Inference for correlation and slope","text":"now bring together ideas inferential analyses descriptive models seen Chapter 6. setting now focused predicting quantitative response variable, \\(y\\), quantitative explanatory variable, \\(x\\). continue ask questions variability model sample sample. sampling variability inform conclusions population can drawn.Many inferential ideas remarkably similar covered previous chapters. technical conditions simple linear regression typically assessed graphically, although independence observations continues utmost importance.Old content - revise neededIn chapter, bring together inferential methods used make claims population information sample modeling ideas seen Chapter 6.\nparticular, conduct inference slope least squares regression line test whether relationship two quantitative variables.\nAdditionally, build confidence intervals quantify slope linear regression line.summarize notation used throughout chapter.Add list notation.","code":""},{"path":"inference-reg.html","id":"case-study-sandwich-store","chapter":"21 Inference for correlation and slope","heading":"21.1 Case study: Sandwich store","text":"","code":""},{"path":"inference-reg.html","id":"observed-data-13","chapter":"21 Inference for correlation and slope","heading":"21.1.1 Observed data","text":"start chapter hypothetical example describing linear relationship dollars spent advertising chain sandwich restaurant monthly revenue. hypothetical example serves purpose illustrating linear model varies sample sample. made example data (entire population), can take many many samples population visualize variability. Note real life, always exactly one sample (, one dataset), inference process, imagine might happened taken different sample. change sample sample leads understanding single observed dataset different population values, typically fundamental goal inference.Consider following hypothetical population sandwich stores particular chain seen Figure 21.1.\nmade-world, CEO actually relevant data, can plot .\nCEO omniscient can write population model describes true population relationship advertising dollars revenue.\nappears linear relationship advertising dollars revenue ($1000).\nFigure 21.1: Revenue linear model advertising dollars population sandwich stores, $1000.\nmay remember Chapter 6 population model : \\[y = \\beta_0 + \\beta_1 x + \\varepsilon.\\], omniscient CEO (full population information) can write true population model : \\[\\mbox{expected revenue} = 11.23 + 4.8 \\cdot \\mbox{advertising}.\\]","code":""},{"path":"inference-reg.html","id":"variability-of-the-statistic-10","chapter":"21 Inference for correlation and slope","heading":"21.1.2 Variability of the statistic","text":"Unfortunately, scenario, CEO willing part full set data, allow potential franchise buyers see small sample data order help potential buyer decide whether set new franchise.\nCEO willing give potential franchise buyer random sample data 20 stores.numerical characteristic describes subset population, estimated slope sample vary sample sample.\nConsider linear model describes revenue ($1000) based advertising dollars ($1000).least squares regression model uses data find sample linear fit: \\[\\hat{y} = b_0 + b_1 x,\\]\\(y\\) represents actual revenue, \\(\\hat{y}\\) represents predicted revenue model, \\(x\\) represents advertising dollars. random sample 20 stores shows different least square regression line depending observations selected.\nsubset size 20 stores shows similar positive trend advertising revenue (saw Figure 21.1, described population) despite fewer observations plot.\nFigure 21.2: random sample 20 stores entire population. positive linear trend advertising revenue continues observed.\nsecond sample size 20 also shows positive trend!\nFigure 21.3: different random sample 20 stores entire population. , positive linear trend advertising revenue observed.\nline slightly different!\nFigure 21.4: linear models two different random samples quite similar, line.\n, variability regression line sample sample. concept sampling variability something ’ve seen , lesson, focus variability line often measured variability single statistic: slope line.\nFigure 21.5: repeated samples size 20 taken entire population, linear model slightly different. red line provides linear fit entire population, shown Figure 21.1.\nmight notice Figure 21.5 \\(\\hat{y}\\) values given lines much consistent middle dataset ends. reason data anchors lines way line must pass center data cloud. effect fan-shaped lines predicted revenue advertising close $4,000 much precise revenue predictions made $1,000 $7,000 advertising.distribution slopes (samples size \\(n=20\\)) can seen histogram, Figure 21.6.\nFigure 21.6: Variability slope estimates taken many different samples stores, size 20.\nRecall, example described introduction hypothetical.\n, created entire population order demonstrate slope line vary sample sample.\ntools textbook designed evaluate one single sample data.\nactual studies, repeated samples, able use repeated samples visualize variability slopes.\nseen variability samples throughout text, come surprise different samples produce different linear models.\nHowever, nice visually consider linear models produced different slopes.\nAdditionally, measuring variability previous statistics (e.g., \\(\\bar{x}_1 - \\bar{x}_2\\) \\(\\hat{p}_1 - \\hat{p}_2\\)), histogram sample statistics can provide information related inferential considerations.following sections, distribution (.e., histogram) \\(b_1\\) (estimated slope coefficient) constructed three ways , now, may familiar .\nFirst (Section 21.2), distribution \\(b_1\\) \\(\\beta_1 = 0\\) constructed randomizing (permuting) response variable.\nNext (Section 21.3), can bootstrap data taking random samples size \\(n\\) original dataset.\nlast (Section 21.4), use mathematical tools describe variability using \\(t\\)-distribution first encountered Section 17.3.","code":""},{"path":"inference-reg.html","id":"randslope","chapter":"21 Inference for correlation and slope","heading":"21.2 Randomization test for \\(H_0: \\beta_1= 0\\)","text":"Consider data Global Crop Yields compiled World Data presented part TidyTuesday series seen Figure 21.7. scientific research interest hand determining linear relationship wheat yield (country-year) crop yields. dataset quite rich deserves exploring, example, focus annual crop yield United States.\nFigure 21.7: Yield (tonnes per hectare) six different crops US. color dot indicates year.\nseen previously, statistical inference typically relies setting null hypothesis hoped subsequently rejected. linear model setting, might hope linear relationship maize wheat settings maize production known wheat production needs predicted.relevant hypotheses linear model setting can written terms population slope parameter. population refers larger set years maize wheat grown US.\\(H_0: \\beta_1= 0\\), linear relationship wheat maize.\\(H_A: \\beta_1 \\ne 0\\), linear relationship wheat maize.Recall randomization test, shuffle one variable eliminate existing relationship variables. , set null hypothesis true, measure natural variability data due sampling due variables correlated. Figure 21.8 shows observed data scatterplot one permutation wheat variable. careful observer can see observed values wheat (maize) exist original data plot well permuted wheat plot, given wheat maize yields longer matched given year. , wheat yield randomly assigned new maize yield.\nFigure 21.8: Original (left) permuted (right) data. permutation removes linear relationship wheat maize. Repeated permutations allow quantifying variability slope condition linear relationship (.e., null hypothesis true).\nrepeatedly permuting response variable, pattern linear model observed due random chance (underlying relationship). randomization test compares slopes calculated permuted response variable observed slope. observed slope inconsistent slopes permuting, can conclude underlying relationship (slope merely due random chance).","code":""},{"path":"inference-reg.html","id":"observed-data-14","chapter":"21 Inference for correlation and slope","heading":"21.2.1 Observed data","text":"continue use crop data investigate linear relationship wheat maize. Note fitted least squares model (see Chapter 6) describing relationship given Table 21.1.\nTable 21.1: least squares estimates intercept slope given estimate column. observed slope 0.195.\n“estimate” column, can write least squares regression line \n\\[\n\\hat{y} = 1.033 + 0.195x,\n\\]\n\\(\\hat{y}\\) predicted wheat yield, \\(x\\) maize yield (tonnes per hectare).columns Table 21.1 described Section 21.4 introduce theory-based methods inference regression slope.","code":""},{"path":"inference-reg.html","id":"variability-of-the-statistic-11","chapter":"21 Inference for correlation and slope","heading":"21.2.2 Variability of the statistic","text":"permuting data, least squares estimate line can computed. Repeated permutations slope calculations describe variability line (.e., slope) due natural variability due relationship wheat maize. Figure 21.9 shows two different permutations wheat resulting linear models.\nFigure 21.9: Two different permutations wheat variable slightly different least squares regression lines.\ncan see, sometimes slope permuted data positive, sometimes negative. randomization happens condition underlying relationship (response variable completely mixed explanatory variable), expect see center randomized slope distribution zero.","code":""},{"path":"inference-reg.html","id":"observed-statistic-vs.-null-value-3","chapter":"21 Inference for correlation and slope","heading":"21.2.3 Observed statistic vs. null value","text":"\nFigure 21.10: Histogram slopes given different permutations wheat variable. vertical red line observed value slope, \\(b_1\\) = 0.195.\ncan see Figure 21.10, slope estimate extreme observed slope estimate (red line) never happened many repeated permutations wheat variable.\n, indeed linear relationship wheat maize, natural variability slopes produce estimates approximately -0.1 +0.1.\nTherefore, believe slope observed original data just due natural variability indeed, linear relationship wheat maize crop yield (tonnes per hectare) US.","code":""},{"path":"inference-reg.html","id":"bootbeta1","chapter":"21 Inference for correlation and slope","heading":"21.3 Bootstrap confidence interval for \\(\\beta_1\\)","text":"seen previous chapters, can use bootstrapping estimate sampling distribution statistic interest (, slope) without null assumption relationship (condition randomization test). interest now creating CI, null hypothesis, won’t reason permute either variables.","code":""},{"path":"inference-reg.html","id":"observed-data-15","chapter":"21 Inference for correlation and slope","heading":"21.3.1 Observed data","text":"Returning crop data, may want consider relationship yields peas wheat. peas good predictor wheat? , relationship? , slope models average wheat yield function peas yield (tonnes per hectare)?\nFigure 21.11: Original data: wheat yield linear model peas yield, tonnes per hectare. Notice relationship peas wheat strong relationship saw previously maize wheat.\n","code":""},{"path":"inference-reg.html","id":"variability-of-the-statistic-12","chapter":"21 Inference for correlation and slope","heading":"21.3.2 Variability of the statistic","text":"focused null distribution, sample replacement \\(n=58\\) \\((x,y)\\)-pairs original dataset. Recall bootstrapping, always resample number observations start order mimic process taking sample population. sampling linear model case, consider observation single dot scatterplot. dot resampled, wheat peas measurement observed. measurements linked dot (.e., year measurements taken).\nFigure 21.12: Original one bootstrap sample crop data. Note difficult differentiate two plots, (within single bootstrap sample) observations resampled twice plotted points top one another. orange circle represent points original data included bootstrap sample. blue circle represents point repeatedly resampled (therefore darker) bootstrap sample. green circle represents particular structure data observed original bootstrap samples.\nFigure 21.12 shows original data compared single bootstrap sample, resulting (slightly) different linear models.\norange circle represent points original data included bootstrap sample.\nblue circle represents point repeatedly resampled (therefore darker) bootstrap sample.\ngreen circle represents particular structure data observed original bootstrap samples.\nrepeatedly resampling, can see dozens bootstrapped slopes plot Figure 21.13.\nFigure 21.13: Repeated bootstrap resamples size 58 taken original data. bootstrapped linear model slightly different.\nRecall order create confidence interval slope, need find range values statistic (slope) takes different bootstrap samples.\nFigure 21.14 histogram relevant bootstrapped slopes.\ncan see 95% bootstrap percentile interval true population slope given (0.061, 0.52).\n95% confident model describing population crops peas wheat, one ton per hectare increase peas yield associated increase predicted average wheat yield 0.061 0.52 tonnes per hectare.\nFigure 21.14: original crop data wheat peas bootstrapped 1,000 times. histogram provides sense variability standard deviation linear model slope sample sample.\n","code":""},{"path":"inference-reg.html","id":"mathslope","chapter":"21 Inference for correlation and slope","heading":"21.4 Theory-based inferential methods for \\(\\beta_1\\)","text":"certain technical conditions apply, convenient use mathematical approximations test estimate slope parameter.\napproximations build \\(t\\)-distribution first encountered Chapter 17.\nmathematical model often correct usually easy implement computationally.\nvalidity technical conditions considered detail Section 21.5.section, discuss uncertainty estimates slope\n\\(y\\)-intercept regression line. Just identified standard\nerrors point estimates previous chapters, first discuss\nstandard errors new estimates.","code":""},{"path":"inference-reg.html","id":"case-study-midterm-elections-and-unemployment","chapter":"21 Inference for correlation and slope","heading":"21.4.1 Case study: Midterm elections and unemployment","text":"Elections members United States House Representatives\noccur every two years, coinciding every four years U.S.\nPresidential election. set House elections occurring \nmiddle Presidential term called midterm elections. America’s two-party\nsystem (vast majority House members history either Republicans Democrats), one political theory suggests higher unemployment rate,\nworse President’s party midterm elections. 2020, 232 Democrats, 198 Republicans, 1 Libertarian House.assess validity claim, can compile historical data \nlook connection. consider every midterm election 1898 \n2018, exception elections Great Depression.\nHouse Representatives made 435 voting members. Figure 21.15 shows data \nleast-squares regression line:\n\\[\\begin{aligned}\n&\\text{predicted % change House seats President's party}  \\\\\n&\\qquad\\qquad= -7.36 - 0.89 \\times \\text{(unemployment rate)}\\end{aligned}\\]\nconsider percent change number seats President’s\nparty (e.g., percent change number seats Republicans \n2018) unemployment rate.observational units data set?171Examining data, clear deviations linearity substantial outliers (see Section 21.5 discussion using residuals visualize well linear model fits data).\ndata collected sequentially, separate analysis used check apparent correlation successive observations time; correlation found.\nFigure 21.15: percent change House seats President’s party midterm election 1898 2010 plotted unemployment rate. two points Great Depression removed, least squares regression line fit data.\ndata Great Depression (1934 1938) removed \nunemployment rate 21% 18%, respectively. agree \nremoved investigation? ?172There negative slope line shown Figure 21.15. However, slope (\ny-intercept) estimates parameter values. might\nwonder, convincing evidence “true” linear model \nnegative slope? , data provide strong evidence \npolitical theory accurate, unemployment rate useful\npredictor midterm election? can frame investigation \nstatistical hypothesis test:\\(H_0\\): \\(\\beta_1 = 0\\). true linear model zero slope.\\(H_A\\): \\(\\beta_1 < 0\\). true linear model negative slope. percent change House seats President’s party negatively correlated percent unemployment.assess hypotheses, identify standard error estimate, compute appropriate test statistic, identify p-value.","code":""},{"path":"inference-reg.html","id":"understanding-regression-output-from-software","chapter":"21 Inference for correlation and slope","heading":"21.4.2 Understanding regression output from software","text":"Just like point estimates seen , can compute \nstandard error test statistic \\(b_1\\). generally label \ntest statistic using \\(T\\), since follows \\(t\\)-distribution.rely statistical software compute standard error \nleave explanation standard error determined \nsecond third statistics course.\nTable 21.2 shows software output least\nsquares regression line Figure 21.15. row labeled unemp includes relevant information slope estimate (.e., coefficient unemployment variable).\nTable 21.2: Output statistical software regression\nline modeling midterm election losses \nPresident’s party response unemployment.\nfirst second columns Table 21.2 represent?entries first column represent least squares estimates, \\(b_0\\) \\(b_1\\). Using estimates, write equation least\nsquare regression line \\[\\begin{aligned}\n  \\hat{y} = -7.36 - 0.89 x\n  \\end{aligned}\\] \\(\\hat{y}\\) case represents predicted\nchange number seats president’s party, \\(x\\)\nrepresents unemployment rate.values second column correspond standard errors \nestimate, \\(SE(b_0)\\) \\(SE(b_1)\\). use values computing test statistic confidence interval \\(\\beta_0\\) \\(\\beta_1\\).previously used \\(t\\)-test statistic hypothesis testing \ncontext numerical data. Regression similar. hypotheses\nconsider, null value slope 0, can compute \ntest statistic using T-score formula:\n\\[\\begin{aligned}\nT\n  = \\frac{\\text{estimate} - \\text{null value}}{\\text{SE(estimate)}}\n  = \\frac{-0.89 - 0}{0.835}\n  = -1.07\\end{aligned}\\] corresponds third column \nTable 21.2 .Use Table 21.2 determine p-value \nhypothesis test.last column table gives p-value \ntwo-sided hypothesis test coefficient unemployment rate:\n0.296. However, test one-sided — interested detecting true slope coefficient negative. Since estimated slope coefficient negative, one-sided p-value just half two-sided p-value: 0.148.173 p-value, data provide convincing evidence \nhigher unemployment rate negative correlation percent seats lost house. words, significant evidence political theory higher unemployment rate, worse President’s party midterm elections.","code":""},{"path":"inference-reg.html","id":"intuition-vs.-formal-inference","chapter":"21 Inference for correlation and slope","heading":"21.4.3 Intuition vs. formal inference","text":"final step mathematical hypothesis test slope, use information provided make conclusion whether data come population true slope zero (.e., \\(\\beta_1 = 0\\)). evaluating formal hypothesis claim, sometimes important check intuition. Based everything ’ve seen examples describing variability line sample sample, linear relationship given data come population slope truly zero.Elmhurst College Illinois released anonymized data family income financial support provided school Elmhurst’s first-year students 2011. Figure 21.16 shows least-squares regression line fit scatterplot sample data. sure slope \nstatistically significantly different zero? , think \nformal hypothesis test reject claim true slope \nline zero?relationship variables perfect, evident decreasing trend data.\nsuggests hypothesis test reject null claim slope zero.\nFigure 21.16: Gift aid family income random sample 50 first-year students Elmhurst College, shown regression line.\npoint tools section go beyond visual interpretation linear relationship toward formal mathematical claim statistical significance slope estimate.\nTable 21.3: Summary least squares fit Elmhurst College data, predicting gift aid university based family income students.\nTable 21.3 shows\nstatistical software output fitting least squares regression\nline shown Figure 21.16. Use output formally\nevaluate following hypotheses.\\(H_0\\): true coefficient family income zero.\\(H_A\\): true coefficient family income zero.174Inference regression.usually rely statistical software \nidentify point estimates, standard errors, test statistics, p-values\npractice. However, aware software generally check\nwhether method appropriate, meaning must still verify\nconditions met. See Section 21.5.","code":""},{"path":"inference-reg.html","id":"theory-based-confidence-interval-for-a-regression-coefficient","chapter":"21 Inference for correlation and slope","heading":"21.4.4 Theory-based confidence interval for a regression coefficient","text":"Similar can conduct hypothesis test model coefficient\nusing regression output, can also construct confidence interval \ncoefficient.Confidence intervals coefficientsConfidence intervals model\ncoefficients (e.g., \\(y\\)-intercept slope) can computed using \\(t\\)-distribution:\n\\[\\begin{aligned}\n  b_i \\ \\pm\\ t_{df}^{\\star} \\times SE(b_{})\n  \\end{aligned}\\] \\(t_{df}^{\\star}\\) appropriate \\(t\\)-value\ncorresponding confidence level model’s degrees \nfreedom. simple linear regression, model’s degrees freedom \\(n-1\\).Compute 95% confidence interval slope coefficient using \nregression output Table 21.3.point estimate \\(b_1 = -0.0431\\) standard error \\(SE(b_1) = 0.0108\\). degrees freedom distribution \\(df = n - 2 = 48\\), typically noted regression output, allowing us identify \\(t_{48}^{\\star} = 2.01\\) use confidence interval.can now construct confidence interval usual way:\n\\[\\begin{aligned}\n  \\text{point estimate} &\\pm t_{48}^{\\star} \\times SE(\\text{point estimate}) \\\\\n  &\\qquad\\\\qquad \\\\\n    -0.0431  &\\pm 2.01 \\times 0.0108 \\\\\n    &\\qquad\\\\qquad\\\\\n    (-0.&0648, -0.0214)\n  \\end{aligned}\\]\n95% confident , $1000 increase \nfamily income, university’s gift aid predicted decrease average $21.40 $64.80.topic intervals book, ’ve focused exclusively \nconfidence intervals model parameters. However, \ntypes intervals may interest, including prediction\nintervals response value also confidence intervals mean\nresponse value context regression. intervals typically covered second course statistics.","code":""},{"path":"inference-reg.html","id":"tech-cond-linmod","chapter":"21 Inference for correlation and slope","heading":"21.5 Checking model conditions","text":"previous sections, used randomization bootstrapping perform inference mathematical model valid due violations technical conditions. section, ’ll provide details mathematical model appropriate discussion technical conditions needed randomization bootstrapping procedures.","code":""},{"path":"inference-reg.html","id":"what-are-the-technical-conditions","chapter":"21 Inference for correlation and slope","heading":"What are the technical conditions?","text":"fitting least squares line, generally requireLinearity. data show linear trend. nonlinear trend\n(e.g., first panel Figure 21.17, advanced regression\nmethod another book later course applied.Linearity. data show linear trend. nonlinear trend\n(e.g., first panel Figure 21.17, advanced regression\nmethod another book later course applied.Independent observations. cautious applying regression data, sequential\nobservations time stock price day. data may\nunderlying structure considered model\nanalysis. example data set successive observations\nindependent shown fourth panel \nFigure 21.17. also \ninstances correlations within data important, paired data described Section 18.Independent observations. cautious applying regression data, sequential\nobservations time stock price day. data may\nunderlying structure considered model\nanalysis. example data set successive observations\nindependent shown fourth panel \nFigure 21.17. also \ninstances correlations within data important, paired data described Section 18.Nearly normal residuals. Generally, residuals must nearly normal. condition\nfound unreasonable, usually outliers \nconcerns influential points, discussed \nSection 6.3. example \nresidual potentially concern shown \nFigure 21.17, one observation \nclearly much regression line others.Nearly normal residuals. Generally, residuals must nearly normal. condition\nfound unreasonable, usually outliers \nconcerns influential points, discussed \nSection 6.3. example \nresidual potentially concern shown \nFigure 21.17, one observation \nclearly much regression line others.Constant equal variability. variability points around least squares line remains\nroughly constant. example non-constant variability shown \nthird panel \nFigure 21.17, represents \ncommon pattern observed condition fails: \nvariability \\(y\\) larger \\(x\\) larger.Constant equal variability. variability points around least squares line remains\nroughly constant. example non-constant variability shown \nthird panel \nFigure 21.17, represents \ncommon pattern observed condition fails: \nvariability \\(y\\) larger \\(x\\) larger.\nFigure 21.17: Four examples showing methods chapter insufficient apply data. top set graphs represents \\(x\\) \\(y\\) relationship. bottom set graphs residual plot. First panel: linearity fails. Second panel: outliers, especially one point far away line. Third panel: variability errors related value \\(x\\). Fourth panel: time series data set shown, successive observations highly correlated.\nconcerns applying least squares regression \nElmhurst data Figure 6.13?175The technical conditions often remembered using LINE mnemonic.\nlinearity, normality, equality variance conditions usually can assessed residual plots, seen Figure 21.17.\ncareful consideration experimental design undertaken confirm observed values indeed independent.L: linear modelI: independent observationsN: points normally distributed around lineE: equal variability around line values explanatory variable","code":""},{"path":"inference-reg.html","id":"why-do-we-need-technical-conditions","chapter":"21 Inference for correlation and slope","heading":"Why do we need technical conditions?","text":"inferential techniques covered text, technical conditions don’t hold, possible make concluding claims population.\n, without technical conditions, T-score (Z-score) assumed t-distribution (standard normal Z distribution).\nsaid, almost always impossible check conditions precisely, look large deviations conditions.\nlarge deviations, unable trust calculated p-value endpoints resulting confidence interval.","code":""},{"path":"inference-reg.html","id":"linearity","chapter":"21 Inference for correlation and slope","heading":"21.5.1 Linearity","text":"linearity condition among important goal understand linear model \\(x\\) \\(y\\).\nexample, value slope meaningful true relationship \\(x\\) \\(y\\) quadratic.\ncautious inference, model also accurate portrayal relationship variables. extended discussion different methods modeling functional forms linear outside scope text.","code":""},{"path":"inference-reg.html","id":"independence","chapter":"21 Inference for correlation and slope","heading":"21.5.2 Independence","text":"technical condition describing independence observations often crucial also difficult diagnose. also extremely difficult gather dataset true random sample population interest. (Note: true randomized experiment fixed set individuals much easier implement, indeed, randomized experiments done medical studies days.)Dependent observations can bias results ways produce fundamentally flawed analyses. , hang gym measuring height weight, linear model surely representation students university. best model describing students use gym (also willing talk , use gym times measuring, etc.).lieu trying answer whether observations true random sample, might instead focus whether believe observations representative populations.\nHumans notoriously bad implementing random procedures, wary process used human intuition balance data respect , example, demographics individuals sample.","code":""},{"path":"inference-reg.html","id":"normality","chapter":"21 Inference for correlation and slope","heading":"21.5.3 Normality","text":"normality condition requires points vary symmetrically around line, spreading bell-shaped fashion. consider “bell” normal distribution sitting top line (coming paper 3-D sense) indicate points dense close line disperse gradually get farther line.normality condition less important linearity independence reasons.\nFirst, linear model fit least squares still unbiased estimate true population model.\nHowever, standard errors associated variability line well estimated.\nFortunately Central Limit Theorem tells us inferential analyses (e.g., standard errors, p-values, confidence intervals) done using mathematical model still hold (even data normally distributed around line) long sample size large enough.One analysis method require normality, regardless sample size, creating intervals predict response individual outcomes given \\(x\\) value, using linear model, topic covered later courses.\nadditional reason worry slightly less normality neither randomization test bootstrapping procedures require data normal around line.","code":""},{"path":"inference-reg.html","id":"equal-variability","chapter":"21 Inference for correlation and slope","heading":"21.5.4 Equal variability","text":"normality, equal variability condition (points spread similar ways around line values \\(x\\)) cause problems estimate linear model, randomization test, bootstrap confidence interval.\nHowever, data exhibit non-equal variance across range \\(x\\)-values potential seriously mis-estimate variability slope consequences inference results (.e., hypothesis tests confidence intervals).equal variability condition violated theory-based analysis (e.g., p-value T-score) needed, existing methods can easily handle unequal variance (e.g., weighted least squares analysis), covered later course.","code":""},{"path":"inference-reg.html","id":"chp21-review","chapter":"21 Inference for correlation and slope","heading":"21.6 Chapter review","text":"","code":""},{"path":"inference-reg.html","id":"summary-15","chapter":"21 Inference for correlation and slope","heading":"Summary","text":"TODO","code":""},{"path":"inference-reg.html","id":"terms-16","chapter":"21 Inference for correlation and slope","heading":"Terms","text":"introduced following terms chapter. ’re sure terms mean, recommend go back text review definitions. purposefully presenting alphabetical order, instead order appearance, little challenging locate. However able easily spot bolded text.","code":""},{"path":"inference-reg.html","id":"key-ideas-15","chapter":"21 Inference for correlation and slope","heading":"Key ideas","text":"TODO","code":""},{"path":"inference-reg-applications.html","id":"inference-reg-applications","chapter":"22 Applications: Infer regression","heading":"22 Applications: Infer regression","text":"TODOOld content - revise needed","code":""},{"path":"inference-reg-applications.html","id":"inference-for-regression-using-r-and-catstats","chapter":"22 Applications: Infer regression","heading":"22.1 Inference for regression using R and catstats","text":"","code":""},{"path":"inference-reg-applications.html","id":"simulation-based-inference-for-the-regression-slope","chapter":"22 Applications: Infer regression","heading":"Simulation-based inference for the regression slope","text":"demonstration, apply simulation-based inference functions regression catstats package data change House seats President’s party midterm elections function national unemployment rate. need drop Great Depression years perform simulations:Now correct data, can perform randomization test slope simple linear regression.results give scatterplot observed data regression line superimposed, gives observed slope (match put as_extreme_as). Next scatterplot, null distribution slope coefficient, observed slope indicated vertical line values extreme highlighted red. caption gives number simulations resulting slope extreme observed: simulation 118/1000, approximate p-value 0.118.obtain confidence interval slope, use regression_bootstrap_CI(), core arguments regression_test().bootstrap distribution slope based observed data, upper lower bounds confidence interval highlighted red. confidence interval also given caption figure. , 95% confident true change number seats House Representatives additional percentage point unemployment decrease 2.6 percent seats increase 0.3 percent seats.Notice bootstrap distribution symmetric example! , bootstrap confidence interval different obtain theory-based methods: (-2.6, 0.8). LINE technical conditions satisfactorily met, though hard see scatterplot data.","code":"\n#load data\ndata(midterms_house)\n#Drop Great Depression years\nd <- midterms_house %>% \n  filter(!(year %in% c(1935, 1939)))\nlibrary(catstats)\nset.seed(621311)\nregression_test(\n  formula = house_change ~ unemp,  #Always use response ~ explanatory\n  data = d,  #name of data set\n  summary_measure = \"slope\", #Can also test correlation\n  direction = \"less\", #Direction of alternative hypothesis\n  as_extreme_as = -0.89, #Observed slope\n  number_repetitions = 1000  #Number of simulations\n)\nset.seed(31143518)\nregression_bootstrap_CI(\n  formula = house_change ~ unemp,  #Always use response ~ explanatory\n  data = d,  #name of data set\n  summary_measure = \"slope\", #Can also test correlation\n  confidence_level = 0.95, #confidence level as a proportion\n  number_repetitions = 1000  #Number of simulations\n)"},{"path":"inference-reg-applications.html","id":"theory-based-inference-for-the-regression-slope","chapter":"22 Applications: Infer regression","heading":"Theory-based inference for the regression slope","text":"demonstrate theory-based inference R, revisit gift aid income data. want know whether evidence suggest slope gift aid function family income non-zero. function linear regression R lm(). Unlike prob.test() t.test(), just running lm() doesn’t print information need. produce coefficient estimates.Instead, linear regression, want save regression results can get complete output using summary():produces lot output; focus Coefficients section. gives us estimated value slope, standard error estimate, t-statistic, p-value. want row labeled name explanatory variable, family_income. estimate slope -0.043 thousand dollars per additional thousand dollars family income, strong evidence null hypothesis slope 0.can compute confidence intervals hand using reported estimate, standard error, df. need compute t-value Chapters 17, 18, 19:can also use confint() function R compute confidence intervals regression coefficients.either case, 90% confident gift aid $24.90 $61.20 less per $1000 increase family income.","code":"#> \n#> Call:\n#> lm(formula = gift_aid ~ family_income, data = elmhurst)\n#> \n#> Coefficients:\n#>   (Intercept)  family_income  \n#>       24.3193        -0.0431\ngift_reg <- lm(gift_aid~family_income, #Always use reponse ~ explanatory\n               data = elmhurst)  #Name of data set\nsummary(gift_reg)  #Obtain full results for regression\n#> \n#> Call:\n#> lm(formula = gift_aid ~ family_income, data = elmhurst)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -10.113  -3.623  -0.216   3.159  11.571 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    24.3193     1.2915   18.83  < 2e-16 ***\n#> family_income  -0.0431     0.0108   -3.98  0.00023 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.78 on 48 degrees of freedom\n#> Multiple R-squared:  0.249,  Adjusted R-squared:  0.233 \n#> F-statistic: 15.9 on 1 and 48 DF,  p-value: 0.000229\n#Get t-star for 90% confidence interval\nqt(.95, df = 48)\n#> [1] 1.68\n#Lower confidence bound\n-0.04307 - 1.677224*0.01081\n#> [1] -0.0612\n\n#Upper confidence bound\n-0.04307 + 1.677224*0.01081\n#> [1] -0.0249\nconfint(gift_reg,   #name of regression results\n        level = 0.9)  #confidence level as a proportion\n#>                   5 %    95 %\n#> (Intercept)   22.1533 26.4854\n#> family_income -0.0612 -0.0249"},{"path":"inference-reg-applications.html","id":"catstats-function-summary-2","chapter":"22 Applications: Infer regression","heading":"22.2 catstats function summary","text":"previous section, introduced two new R\nfunctions catstats library. provide summary \nfunctions. can also access\nhelp files functions using ? command. \nexample, type ?regression_test R console bring \nhelp file regression_test function.\nregression_test: Simulation-based hypothesis test regression slope correlation two quantitative variables.\nformula = y ~ x y name quantitative response variable data set x name quantitative explanatory variable\ndata = data frame, columns variable\nsummary_measure = one \"slope\" \"correlation\" (quotations important !)\ndirection = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)\nas_extreme_as = value observed slope correlation\nnumber_repetitions = number simulated samples generate (least 1000!)\n\nregression_test: Simulation-based hypothesis test regression slope correlation two quantitative variables.formula = y ~ x y name quantitative response variable data set x name quantitative explanatory variabledata = data frame, columns variablesummary_measure = one \"slope\" \"correlation\" (quotations important !)direction = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)as_extreme_as = value observed slope correlationnumber_repetitions = number simulated samples generate (least 1000!)\nregression_bootstrap_CI: Bootstrap confidence interval regression slope correlation.\nformula = y ~ x y name quantitative response variable data set x name quantitative explanatory variable\ndata = data frame, columns variable\nsummary_measure = one \"slope\" \"correlation\" (quotations important !)\nconfidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)\nnumber_repetitions = number simulated samples generate (least 1000!)\n\nregression_bootstrap_CI: Bootstrap confidence interval regression slope correlation.formula = y ~ x y name quantitative response variable data set x name quantitative explanatory variabledata = data frame, columns variablesummary_measure = one \"slope\" \"correlation\" (quotations important !)confidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)number_repetitions = number simulated samples generate (least 1000!)\n","code":""},{"path":"probability.html","id":"probability","chapter":"23 Probability with tables","heading":"23 Probability with tables","text":"TODO","code":""},{"path":"probability.html","id":"defining-probability","chapter":"23 Probability with tables","heading":"23.1 Defining probability","text":"random process one outcome unpredictable. encounter random processes every day: rain today? many minutes pass receiving next text message? Seahawks win Super Bowl? Though outcome one particular random process unpredictable, observe process many many times, pattern outcomes, probability distribution, can often modeled mathematically. Though several philosophical definitions probability, use “frequentist” definition probability—long-run relative frequency.Probability.probability event long-run proportion times event occur random process repeated indefinitely (identical conditions).Consider simple example flipping fair coin . probability coin lands heads. physical properties, assume probability heads 0.5, let’s use simulation examine probability. Figure 23.1 shows long-run proportion times simulated coin flip lands heads y-axis, number tosses x-axis. Notice long-run proportion starts converging 0.5 number tosses increases.\nFigure 23.1: One simulation flipping fair coin, tracking long-run proportion times coin lands heads.\n","code":""},{"path":"probability.html","id":"finding-probabilities-with-tables","chapter":"23 Probability with tables","heading":"23.2 Finding probabilities with tables","text":"can solve many real-life probability problems without using equations creating hypothetical two-way table scenario. tool best demonstrated example.student Montana State University, suppose first class Mondays Wilson Hall 8:00am commute school. Bobcat parking permit. past experience, know 20% chance finding open parking spot Lot 6 Animal Bioscience. Otherwise, park Lot 18 graduate housing. find spot Lot 6, 5% chance late class. However, park Lot 18, 15% chance late class. probability late class Monday?two random variables scenario: whether park Lot 6 Lot 18, whether late class. Since know probability long-run relative frequency, let’s imagine 1000 hypothetical Mondays, fill contingency table frequencies ’d expect cell.Now can find probability late class reading table: 130/1000 = 0.13.create table last Example? Let’s work step--step.Identify unconditional probabilities given problem: 20% chance parking Lot 6, means 80% chance parking Lot 18. Take 20% 80% 1000 fill row totals:Identify conditional probabilities given problem: park Lot 6, probability late class 5%; park Log 18, probability late class 15%. Fill corresponding cells table taking 5% times parked Lot 6, 15% times parked Lot 18:Use subtraction fill remaining cells column “late class.” Use addition find column totals.Using hypothetical two-way table given last Example, find following probabilities:probability late class?probability park Lot 6 late class?Given late class, probability parked Lot 18?176Carefully read probabilities described Guided Practice—note subtle difference “probability late class, given parked Lot 18” (\\(120/800 = 0.15\\)) “probability parking Lot 18, given late class” (\\(120/130 = 0.923\\)). given extra information, called conditional probability, denominator probability calculation row total (e.g., 800) column total (e.g., 130) rather overall total hypothetical two-way table.previous Guided Practice, probabilities conditional probabilities? unconditional?177","code":""},{"path":"probability.html","id":"probability-notation","chapter":"23 Probability with tables","heading":"23.3 Probability notation","text":"ease translating probability problems calculations, let’s define notation. denote “events” (e.g., late class) upper case letters near beginning alphabet, e.g., \\(\\), \\(B\\), \\(C\\). probability event \\(\\) denoted \\(P()\\), \\(P()\\) number 0 1. event \\(\\) happen called complement \\(\\) denoted \\(P(^C)\\). Sometimes additional information like condition , denote conditional probability \\(\\) given \\(B\\) \\(P(| B)\\)—probability \\(\\) happens given \\(B\\) already happened.coin flip example, let \\(\\) event coin lands heads. can denote probability coin lands heads \\(P() = 0.5\\). flip coin twice let \\(H_1\\) event first flip lands heads, \\(H_2\\) event second flip lands heads. Since coin remember last flip, first flip lands heads, second flip still 50% chance landing heads. , \\(P(H_2 | H_1) = 0.5\\).","code":""},{"path":"probability.html","id":"diagnostic-testing","chapter":"23 Probability with tables","heading":"23.4 Diagnostic testing","text":"Medical diagnostic tests diseases spend years development. clinical trials, developers diagnostic test able determine two important properties test:sensitivity diagnostic test probability test yields positive result, given individual disease. words, proportion diseased population test positive?specificity diagnostic test probability test yields negative result, given individual disease. , proportion non-diseased population test negative?good diagnostic test high (near 100%) sensitivity specificity. However, even near-perfect test, probability disease given test positive still quite low. investigate counter-intuitive result, need another definition:call proportion population disease—probability contracting disease—prevalence (incidence) disease.Let \\(D\\) event individual disease \\(T\\) event individual tests positive. express following quantities using probability notation?sensitivityspecificityprevalence178Note sensitivity specificity conditional probabilities, prevalence unconditional probability. probabilities useful information, test positive diagnostic test, none quantities probability really want know: conditional probability disease, given tested positive, \\(P(D | T)\\).","code":""},{"path":"probability.html","id":"the-case-of-baby-jeff","chapter":"23 Probability with tables","heading":"23.4.1 The case of Baby Jeff","text":"following case study presented Slawson Shaughnessy (2002). poster hospital’s newborn nursery announced male newborns screened muscular dystrophy using heel stick blood test creatinine phosphokinase (CPK). test characteristics screening tests nearly perfect: sensitivity 100% specificity 99.98%. prevalence muscular dystrophy male newborns ranges 1 3,500 1 15,000. Baby Jeff abnormal CPK test. parents baby wanted know, “chance son muscular dystrophy?” Doctors informed parents though 100% likely, highly probable. First, take minute predict probability – think? 80% chance? 99% chance? Let’s investigate using two-way table hypothetical population 100,000 male newborns.calculations, let’s use prevalence 1 10,000. 100,000 hypothetical male newborns, expect 1 10,000 muscular dystrophy, 10: \\((1/10000)\\times 100000 = 10\\). sensitivity test perfect, 10 male newborns muscular dystrophy test positive. \\(100000-10 = 99,990\\) male newborns muscular dystrophy, 99.98% test negative: \\((0.9998)\\times 99990 = 99,970\\) infants. leaves \\(99990 - 99970 = 20\\) male newborns test positive even though muscular dystrophy. allows us fill counts hypothetical two-way table:Now can read desired probability table: 30 male newborns ’d expect test positive, 10 actually muscular dystrophy. means chance Baby Jeff muscular dystrophy 33%!probability change prevalence 1/3500? 1/15000? Try .179Why counter-intuitive result occur? high sensitivity specificity, test perform poorly? answer prevalence disease. rare disease, small proportion test positive large group people without disease overwhelm large proportion test positive small group people disease. number false positives can much higher number true positives.","code":""},{"path":"probability.html","id":"chp23-review","chapter":"23 Probability with tables","heading":"23.5 Chapter review","text":"","code":""},{"path":"probability.html","id":"summary-16","chapter":"23 Probability with tables","heading":"Summary","text":"TODO","code":""},{"path":"probability.html","id":"terms-17","chapter":"23 Probability with tables","heading":"Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"probability.html","id":"key-ideas-16","chapter":"23 Probability with tables","heading":"Key ideas","text":"Probabilities can either unconditional conditional. unconditional probability proportion proportion measured entire population; whereas conditional probability proportion proportion measured subgroup population. computing probabilities using contingency table, unconditional probabilities computed dividing cell total overall total; conditional probabilities computed dividing cell total row column total.Probabilities can either unconditional conditional. unconditional probability proportion proportion measured entire population; whereas conditional probability proportion proportion measured subgroup population. computing probabilities using contingency table, unconditional probabilities computed dividing cell total overall total; conditional probabilities computed dividing cell total row column total.Recall Chapter 4 two variables associated behavior one variable depends value variable. two categorical variables, occurs conditional probabilities category one variable change across probabilities variable.Recall Chapter 4 two variables associated behavior one variable depends value variable. two categorical variables, occurs conditional probabilities category one variable change across probabilities variable.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]

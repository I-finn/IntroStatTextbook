[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"hope readers take away three ideas book addition forming foundation statistical thinking methods.Statistics applied field wide range practical applications.don’t math guru learn interesting, real data.Data messy, statistical tools imperfect. However, understand strengths weaknesses tools, can use learn interesting things world.","code":""},{"path":"index.html","id":"textbook-overview","chapter":"Welcome","heading":"Textbook overview","text":"textbook accompanies curriculum STAT 216: Introduction Statistics Montana State University. syllabus course information can found\ncourse webpage. Detailed learning outcomes course can found .Introduction data. Data structures, variables, basic data collection techniques.Exploratory data analysis. Data visualization summarization one two variables, taste probability.Correlation regression. Visualizing, describing, quantifying relationships two quantitative variables.Multivariable models. Descriptive summaries quantifying relationship many variables.Inference categorical data. Inference one two proportions using simulation randomization techniques well normal distribution.Inference quantitative data. Inference one two means using simulation randomization techniques well \\(t\\)-distribution.Inference regression. Inference regression slope correlation using simulation randomization techniques well \\(t\\)-distribution.Case studies. series case studies assigned weekly course.Activities. Stat 216 Coursepack reading guides -class activities.","code":""},{"path":"index.html","id":"stat-computing","chapter":"Welcome","heading":"Statistical computing","text":"STAT 216 textbook use R RStudio statistical computing. R RStudio free open source. R programming language runs computations, RStudio interface engage R (called “integrated development environment”, IDE).Since R open source, users can contribute “packages” — collections R functions. 16,000 available packages! particular, use\ntidyverse collection packages designed data science.\nSTAT 216 also R package called catstats, contains functions\nrunning simulation-based inference course.","code":""},{"path":"index.html","id":"accessing-rstudio","chapter":"Welcome","heading":"Accessing RStudio","text":"MSU hosts web based version RStudio, can found : rstudio.math.montana.edu.username NetID, password password associated NetID.registered Stat 216 students access server. enrolled course, receive error “Incorrect invalide username/password” attempting log , please email Stat 216 Faculty Course Supervisor Dr. Stacey Hancock.Note work save server deleted, access removed semester ends. Thus, like save files, export computer prior end semester.","code":""},{"path":"index.html","id":"other-options-for-accessing-rstudio","chapter":"Welcome","heading":"Other options for accessing RStudio","text":"recommend using RStudio MSU RStudio server, options accessing free software:Use RStudio MSU virtual machine.Use RStudio MSU virtual machine.Use RStudio MSU -campus computer lab.Use RStudio MSU -campus computer lab.Use RStudio RStudio Cloud. resource allows use RStudio web browser. free use, limit certain number project hours per month.Use RStudio RStudio Cloud. resource allows use RStudio web browser. free use, limit certain number project hours per month.Download R RStudio laptop. (Note: R RStudio run iPad, notebooks, Chromebooks.)Download R RStudio laptop. (Note: R RStudio run iPad, notebooks, Chromebooks.)Download install R.Download install RStudio Desktop.Install catstats package.View tutorial video installing R RStudio \nlike additional installation instructions.","code":""},{"path":"index.html","id":"installing-catstats","chapter":"Welcome","heading":"Installing catstats","text":"need read section running RStudio laptop, case need install R packages used course.use R functions catstats package, need first install remotes package,\ninstall catstats Github.RStudio console, run following commands:installation, gives option update \nrecent versions packages, type 1 (choose install ),\ntype Yes asks want install.need run installation commands , need load\ncatstats package time restart RStudio using following command:Note catstats package install packages needed run code textbook,\nneed load packages (e.g., openintro) load catstats R session.","code":"\ninstall.packages(\"remotes\")\nremotes::install_github(\"greenwood-stat/catstats\")\nlibrary(catstats)"},{"path":"index.html","id":"about-the-authors","chapter":"Welcome","heading":"About the Authors","text":"","code":""},{"path":"index.html","id":"montana-state-university-authors","chapter":"Welcome","heading":"Montana State University Authors","text":"Nicole Carnegie \nAssociate Professor Statistics nicole.carnegie@montana.edu Stacey Hancock \nAssistant Professor Statistics stacey.hancock@montana.edu Elijah Meyer \nPhD Statistics Graduate Student elijah.meyer@montana.edu Jade Schmidt \nStudent Success Coordinator Statistics jade.schmidt2@montana.edu Melinda Yager \nAssistant Coordinator Statistics melinda.yager@montana.edu ","code":""},{"path":"index.html","id":"openintro-authors","chapter":"Welcome","heading":"OpenIntro Authors","text":"Mine Çetinkaya-Rundel mine@openintro.org \nUniversity Edinburgh, Duke University, RStudio Johanna Hardin jo@openintro.org \nPomona College ","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Welcome","heading":"Acknowledgements","text":"resource largely derivative 1st 2nd\neditions OpenIntro textbook\nIntroductory Statistics Randomization Simulation,\nwithout \neffort possible. authors \nalso like thank Montana State University Library,\ngenerously funded project.","code":""},{"path":"index.html","id":"licensing","chapter":"Welcome","heading":"Licensing","text":"resource released Creative Commons -NC-SA 4.0 license unless otherwise noted.Visit following link guidelines textbook’s source files modified /shared, additional copyright information:http://www.openintro.org/perm/stat2nd_v1.txtTo cite resource please use:Carnegie, N., Hancock, S., Meyer, E., Schmidt, J., Yager, M. (2021). Montana State Introductory Statistics R. Montana State University. https://mtstateintrostats.github.io/IntroStatTextbook/. Adapted Çetinkaya-Rundel, M. Hardin, J. (2021). Introduction Modern Statistics. OpenIntro. https://openintro-ims.netlify.app/.","code":""},{"path":"intro-to-data.html","id":"intro-to-data","chapter":"1 Introduction to data","heading":"1 Introduction to data","text":"Though calculating probabilities 16th century, first US Census directed Thomas Jefferson 17901, discipline statistics know came 1800s. 21st century, statistical investigation process looked something like (adapted Tintle et al. (2016)):Ask research question.Design study collect data.Summarize visualize data.Use statistical analysis methods draw inferences data.Communicate results answer research question.Revisit look forward.rise data science, however, may start research question,\ninstead start data set2.\ncase, statistical investigation process looks like data exploration cycle found Figure 1.1 taken Wickham Grolemund (2017).\nFigure 1.1: Wickham Grolemund’s data exploration cycle (2017).\neither case, ideas, concepts, methods presented book provide tools work statistical investigation process, whether starting research question starting data.","code":""},{"path":"intro-to-data.html","id":"basic-stents-strokes","chapter":"1 Introduction to data","heading":"1.1 Case study: using stents to prevent strokes","text":"section, introduce classic challenge statistics: evaluating efficacy medical treatment.\nTerms section, indeed much chapter, revisited later text.\nplan now simply get sense role statistics can play practice., consider experiment studies effectiveness stents treating patients risk stroke (Chimowitz et al. 2011).\nStents small mesh tubes placed inside narrow weak arteries assist patient recovery cardiac events reduce risk additional heart attack death.Many doctors hoped similar benefits patients risk stroke. start writing principal question researchers hope answer:use stents reduce risk stroke?researchers asked question conducted experiment 451 -risk patients. volunteer patient randomly assigned one two groups:Treatment group. Patients treatment group received stent medical management.\nmedical management included medications, management risk factors, help lifestyle modification.Control group. Patients control group received medical management treatment group, receive stents.Researchers randomly assigned 224 patients treatment group 227 control group.\nstudy, control group provides reference point can measure medical impact stents treatment group.Researchers studied effect stents two time points: 30 days enrollment 365 days enrollment.\ndata collected 5 patients summarized Table 1.1.\nPatient outcomes recorded stroke event, representing whether patient stroke time period.\nTable 1.1: Results five patients stent study.\nConsidering data 451 patients individually long, cumbersome path towards answering original research question.\nInstead, performing statistical data analysis allows us consider data .\nTable 1.2 summarizes raw data helpful way.\ntable, can quickly see happened entire study.\ninstance, identify number patients treatment group stroke within 30 days treatment, look leftmost column (30 days), intersection treatment stroke: 33.\nidentify number control patients stroke 365 days receiving treatment, look rightmost column (365 days), intersection control event: 199.\nTable 1.2: Descriptive statistics stent study.\ndata summarized table can also visualized barplot, seen Figure 1.2:\nFigure 1.2: Segmented barplot outcomes stent study group time.\ncan compute summary statistics table give us better idea impact stent treatment differed two groups.\nsummary statistic single number summarizing large amount data.\ninstance, primary results study 1 year described two summary statistics: proportion people stroke treatment control groups.Proportion stroke treatment (stent) group: \\(45/224 = 0.20 = 20\\%\\).Proportion stroke control group: \\(28/227 = 0.12 = 12\\%\\).two summary statistics useful looking differences groups, surprise: additional 8% patients treatment group stroke!\nimportant two reasons.\nFirst, contrary doctors expected, stents reduce rate strokes.\nSecond, leads statistical question: data show “real” difference groups?second question subtle, basis call statistical inference.\nSuppose flip coin 100 times. chance coin lands heads given coin flip 50%, probably won’t observe exactly 50 heads.\ntype fluctuation part almost type data generating process.\npossible 8% difference stent study due natural variation.\nHowever, larger difference observe (particular sample size), less believable difference due chance.\nreally asking following: difference large reject notion due chance?don’t yet statistical tools fully address question , can comprehend conclusions published analysis: compelling evidence harm stents study stroke patients.","code":""},{"path":"intro-to-data.html","id":"data-basics","chapter":"1 Introduction to data","heading":"1.2 Data basics","text":"Effective presentation description data first step analyses. section introduces one structure organizing data well terminology used throughout book.","code":""},{"path":"intro-to-data.html","id":"observations-variables-and-data-frames","chapter":"1 Introduction to data","heading":"1.2.1 Observations, variables, and data frames","text":", consider loans offered Lending Club, peer--peer lending company. data used explore characteristics people receiving loans platform, job titles, annual income, home ownership. Table 1.3 displays six rows data set 50 randomly sampled loans. observations referred loan50 data set.row table represents single loan.\nformal name row case observational unit. Since 50 observational units data set, sample size, denoted \\(n\\), 50 (\\(n = 50\\)).\ncolumns represent characteristics loan, column referred variable.example, first row represents loan $7,500 interest rate 7.34%, borrower based Maryland (MD) income $70,000.practice, especially important ask clarifying questions ensure important aspects data understood.\ninstance, always important sure know variable means units measurement.\nDescriptions variables loan50 data set given Table 1.4.\nTable 1.3: Six rows loan50 data set.\n\nTable 1.4: Variables descriptions loan50 data set.\ndata Table 1.3 represent data frame (data matrix), convenient common way organize data, especially collecting data spreadsheet.\nrow data frame corresponds unique case (observational unit), column corresponds variable.recording data, use data frame unless good reason use different structure.\nstructure allows new cases added rows new variables new columns.data described Guided Practice represent county data set, shown data frame Table 1.5.\nvariables well variables data set fit Table 1.5 described Table 1.6\nTable 1.5: Six observations six variables county data set.\n\nTable 1.6: Variables descriptions county data set.\n","code":""},{"path":"intro-to-data.html","id":"variable-types","chapter":"1 Introduction to data","heading":"1.2.2 Types of variables","text":"Examine unemployment_rate, pop2017, state, metro, median_edu variables county data set.\nvariables inherently different others, yet share certain characteristics.First consider unemployment_rate, said quantitative numerical variable since can take wide range numerical values, sensible add, subtract, take averages values.\nhand, classify variable reporting telephone area codes quantitative since average, sum, difference area codes doesn’t clear meaning.pop2017 variable also quantitative, although seems little different unemployment_rate.\nvariable population count can take whole non-negative numbers (0, 1, 2, …).\nreason, population variable said discrete since can take numerical values jumps.\nhand, unemployment rate variable said continuous.variable state can take 51 values accounting Washington, DC: AL, AK, …, WY.\nresponses categories, state called categorical variable, possible values called variable’s levels . variable metro also categorical, two levels (yes ). categorical variable two levels called binary variable. working generic binary variable, often call two possible levels “success” “failure.”Finally, consider median_edu variable, describes median education level county residents takes values below_hs, hs_diploma, some_college, bachelors county.\nvariable seems hybrid: categorical variable levels natural ordering.\nvariable properties called ordinal variable, regular categorical variable without type special ordering called nominal variable.\nsimplify analyses, ordinal variable book treated nominal (unordered) categorical variable.\nFigure 1.3: Breakdown variables respective types.\nExample 1.1  Data collected students statistics course.\nThree variables recorded student: number siblings, student height, whether student previously taken statistics course.\nClassify variables continuous quantitative, discrete quantitative, categorical.","code":""},{"path":"intro-to-data.html","id":"variable-relations","chapter":"1 Introduction to data","heading":"1.2.3 Relationships between variables","text":"Many analyses motivated researcher looking relationship two variables.\nsocial scientist may like answer following questions:higher average increase county population tend correspond counties higher lower median household incomes?homeownership lower national average one county, percent multi-unit structures county tend national average?useful predictor median education level median household income US counties?answer questions, data must collected, county data set shown Table 1.5.\nExamining summary statistics provide insights three questions counties.\nAdditionally, graphs can used visually explore data.Scatterplots one type graph used study relationship two quantitative variables.\nFigure 1.4 displays relationship variables homeownership multi_unit, percent units multi-unit structures (e.g., apartments, condos).\npoint plot represents single county (single observational unit).\ninstance, highlighted dot corresponds County 413 county data set: Chattahoochee County, Georgia, 39.4% units multi-unit structures homeownership rate 31.3%.\nscatterplot suggests relationship two variables: counties higher rate multi-units tend lower homeownership rates.\nmight brainstorm relationship exists investigate idea determine reasonable explanations.\nFigure 1.4: scatterplot homeownership versus percent units multi-unit structures US counties. highlighted dot represents Chattahoochee County, Georgia, multi-unit rate 39.4% homeownership rate 31.3%.\nmulti-unit homeownership rates said associated plot shows discernible pattern.\ntwo variables show connection one another, called associated variables.\nAssociated variables can also called dependent variables vice-versa.Example 1.2  example examines relationship change population 2010 2017 median household income counties, visualized scatterplot Figure 1.5.\nvariables associated?\nFigure 1.5: scatterplot showing pop_change median_hh_income. Owsley County Kentucky, highlighted, lost 3.63% population 2010 2017 median household income $22,736.\ndownward trend Figure 1.4—counties units multi-unit structures associated lower homeownership—variables said negatively associated.\npositive association shown relationship median_hh_income pop_change variables Figure 1.5, counties higher median household income tend higher rates population growth.two variables associated, said independent.\n, two variables independent evident relationship two.","code":""},{"path":"intro-to-data.html","id":"explanatory-and-response-variables","chapter":"1 Introduction to data","heading":"1.2.4 Explanatory and response variables","text":"ask questions relationship two variables, sometimes also want determine change one variable causes change .\nConsider following rephrasing earlier question county data set:increase median household income county, drive increase population?question, asking whether one variable affects another.\nunderlying belief, median household income explanatory variable variable population change response variable variable hypothesized relationship.9Explanatory response variables.suspect one variable might causally affect another,\nlabel first variable explanatory variable\nsecond response variable.Bear mind act labeling variables way nothing guarantee causal relationship exists.\nformal evaluation check whether one variable causes change another requires experiment.","code":""},{"path":"intro-to-data.html","id":"introducing-observational-studies-and-experiments","chapter":"1 Introduction to data","heading":"1.2.5 Introducing observational studies and experiments","text":"two primary types data collection: observational studies experiments. already encountered experiment case study Section 1.1, observational study Lending Club data section.Researchers perform observational study collect data way directly interfere data arise.\ninstance, researchers may collect information via surveys, review medical company records, follow cohort many similar individuals form hypotheses certain diseases might develop.\nsituations, researchers merely observe data arise.\ngeneral, observational studies can provide evidence naturally occurring association variables, show causal connection.researchers want investigate possibility causal connection, conduct experiment.\nUsually explanatory response variable.\ninstance, may suspect administering drug reduce mortality heart attack patients following year.\ncheck really causal connection explanatory variable response, researchers collect sample individuals split groups.\nindividuals group assigned treatment.\nindividuals randomly assigned group, experiment called randomized experiment.\nexample, heart attack patient drug trial randomly assigned, perhaps flipping coin, one two groups: first group receives placebo (fake treatment) second group receives drug.\nNote case study Section 1.1 use placebo.","code":""},{"path":"intro-to-data.html","id":"sampling-principles-strategies","chapter":"1 Introduction to data","heading":"1.3 Sampling principles and strategies","text":"\nfirst step conducting research identify topics questions investigated.\nclearly laid research question helpful identifying subjects cases studied variables important.\nalso important consider data collected reliable help achieve research goals.","code":""},{"path":"intro-to-data.html","id":"populations-and-samples","chapter":"1 Introduction to data","heading":"1.3.1 Populations and samples","text":"Consider following three research questions:average mercury content swordfish Atlantic Ocean?last 5 years, average time complete degree Duke undergrads?new drug reduce risk deaths patients severe heart disease?research question refers target population.\nfirst question, target population swordfish Atlantic ocean, fish represents case.\nOften times, expensive collect data every case population.\nInstead, sample taken.\nsample represents subset cases often small fraction population.\ninstance, 60 swordfish (number) population might selected, sample data may used provide estimate population average answer research question.","code":""},{"path":"intro-to-data.html","id":"anecdotal-evidence","chapter":"1 Introduction to data","heading":"1.3.2 Anecdotal evidence","text":"Consider following possible responses three research questions:man news got mercury poisoning eating swordfish, average mercury concentration swordfish must dangerously high.met two students took 7 years graduate Duke, must take longer graduate Duke many colleges.friend’s dad heart attack died gave new heart disease drug, drug must work.conclusion based data.\nHowever, two problems.\nFirst, data represent one two cases.\nSecond, importantly, unclear whether cases actually representative population. Data collected haphazard fashion called anecdotal evidence.\nFigure 1.6: February 2010, media pundits cited one large snow storm evidence global warming. comedian Jon Stewart pointed , “one storm, one region, one country.”\nAnecdotal evidence typically composed unusual cases recall based striking characteristics.\ninstance, likely remember two people met took 7 years graduate six others graduated four years.\nInstead looking unusual cases, examine sample many cases better represent population.","code":""},{"path":"intro-to-data.html","id":"sampling-from-a-population","chapter":"1 Introduction to data","heading":"1.3.3 Sampling from a population","text":"\nmight try estimate time graduation Duke undergraduates last 5 years collecting sample students.\ngraduates last 5 years represent population, graduates selected review collectively called sample.\ngeneral, always seek randomly select sample population.\nbasic type random selection equivalent raffles conducted–raffle ticket equal chance selected.\nexample, selecting graduates, write graduate’s name raffle ticket draw 100 tickets.\nselected names represent random sample 100 graduates.\npick samples randomly reduce chance introduce biases.\nFigure 1.7: graphic, five graduates randomly selected population (graduates last 5 years) included sample.\nExample 1.3  Suppose ask student happens majoring nutrition select several graduates study.\nkind students think might collect?\nthink sample representative graduates?\nFigure 1.8: Asked pick sample graduates, nutrition major might inadvertently pick disproportionate number graduates health-related majors.\nsomeone permitted pick choose exactly graduates included sample, entirely possible sample skewed person’s interests, may entirely unintentional.\nintroduces bias sampling method.three common types sampling bias discuss:Selection bias: method sample selected tends produce samples either -represent -represent certain portions population.Non-response bias: individuals selected sample unwilling respond.Response bias: individuals selected sample respond way accurately represent truth—due question wording, lack anonymity, issues.\ncommon downfall survey studies convenience sample, individuals easily accessible likely included sample.\ninstance, political survey done stopping people walking Bronx, represent New York City.\noften difficult discern sub-population convenience sample represents.Sampling randomly helps resolve selection bias.\nbasic random sample called simple random sample, equivalent using raffle select cases.\nmeans case population equal chance included implied connection cases sample.Even people picked random, however, caution must exercised non-response rate high, response bias present.\ninstance, 30% people randomly sampled survey actually respond, unclear whether results representative entire population.\nnon-response bias can produce results sample accurately reflect entire population.\nFigure 1.9: Due possibility non-response, survey studies may reach certain group within population. difficult, often times impossible, completely fix problem.\n\n\n\n\n","code":""},{"path":"intro-to-data.html","id":"samp-methods","chapter":"1 Introduction to data","heading":"1.3.4 Four sampling methods (special topic)","text":"Almost statistical methods based notion implied randomness.\nobservational data collected random framework population, statistical methods—estimates errors associated estimates—reliable.\nconsider four random sampling techniques: simple, stratified, cluster, multistage sampling. Figures 1.10 1.11 provide graphical representations techniques.\n\nFigure 1.10: Examples simple random stratified sampling. top panel, simple random sampling used randomly select 18 cases (denoted red). bottom panel, stratified sampling used: cases grouped strata, simple random sampling employed randomly select 3 cases within stratum.\nSimple random sampling probably intuitive form random sampling.\nConsider salaries Major League Baseball (MLB) players, player member one league’s 30 teams.\ntake simple random sample 120 baseball players salaries, write names season’s several hundreds players onto slips paper, drop slips bucket, shake bucket around sure names mixed , draw slips sample 120 players.\ngeneral, sample referred “simple random” case population equal chance included final sample knowing case included sample provide useful information cases included.Stratified sampling divide--conquer sampling strategy.\npopulation divided groups called strata.\nstrata chosen similar cases grouped together, second sampling method, usually simple random sampling, employed within stratum.\nbaseball salary example, 30 teams represent strata, since teams lot money (4 times much!).\nmight randomly sample 4 players team sample 120 players.Stratified sampling especially useful cases stratum similar respect outcome interest.\ndownside analyzing data stratified sample complex task analyzing data simple random sample.\nanalysis methods introduced book need extended analyze data collected using stratified sampling.Example 1.4  good cases within stratum similar?cluster sample, break population many groups, called clusters.\nsample fixed number clusters include observations clusters sample.\nmultistage sample like cluster sample, rather keeping observations cluster, collect random sample within selected cluster.\nFigure 1.11: Examples cluster multistage sampling. top panel, cluster sampling used: data binned nine clusters, three clusters sampled, observations within three cluster included sample. bottom panel, multistage sampling used, differs cluster sampling randomly select subset cluster included sample rather measuring every case sampled cluster.\nSometimes cluster multistage sampling can economical alternative sampling techniques.\nAlso, unlike stratified sampling, approaches helpful lot case--case variability within cluster clusters don’t look different one another.\nexample, neighborhoods represented clusters, cluster multistage sampling work best neighborhoods diverse.\ndownside methods advanced techniques typically required analyze data, though methods book can extended handle data.Example 1.5  Suppose interested estimating malaria rate densely tropical portion rural Indonesia.\nlearn 30 villages part Indonesian jungle, less similar next, distances villages substantial. goal test 150 individuals malaria.\nsampling method employed?","code":""},{"path":"intro-to-data.html","id":"observational-studies","chapter":"1 Introduction to data","heading":"1.4 Observational studies","text":"Data treatment explicitly applied (explicitly withheld) called observational data.\ninstance, loan data county data described Section 1.2 examples observational data.Observational studies generally sufficient show associations form hypotheses can later checked experiments. Making causal conclusions based experiments often reasonable. However, making causal conclusions based observational data can treacherous recommended. Indeed, making causal conclusions based observational data arguably common mistake news headlines social media posts!previous research tells us using sunscreen actually reduces skin cancer risk, maybe another variable can explain hypothetical association sunscreen usage skin cancer.\nOne important piece information absent sun exposure. someone sun day, likely use sunscreen likely get skin cancer. Exposure sun unaccounted simple investigation.Sun exposure called confounding variable14, variable associated explanatory response variables.\none method justify making causal conclusions observational studies exhaust search confounding variables, guarantee confounding variables can examined measured.confounding variable variable bothassociated explanatory variable, andassociated response variable.Observational studies come two forms: prospective retrospective studies.\nprospective study identifies individuals collects information events unfold.\ninstance, medical researchers may identify follow group patients many years assess possible influences behavior cancer risk.\nOne example study Nurses’ Health Study.\nStarted 1976 expanded 1989, Nurses’ Health Study collected data 275,000 nurses still enrolling participants.\nprospective study recruits registered nurses collects data using questionnaires.\nRetrospective studies collect data events taken place, e.g. researchers may review past events medical records.\ndata sets may contain prospectively- retrospectively-collected variables, medical studies gather information participants’ lives enter study subsequently collect data participants throughout study.","code":""},{"path":"intro-to-data.html","id":"experiments","chapter":"1 Introduction to data","heading":"1.5 Experiments","text":"Studies researchers assign treatments cases called experiments.\nassignment includes randomization, e.g., using coin flip decide treatment patient receives, called randomized experiment.\nRandomized experiments fundamentally important trying show causal connection two variables.","code":""},{"path":"intro-to-data.html","id":"principles-of-experimental-design","chapter":"1 Introduction to data","heading":"1.5.1 Principles of experimental design","text":"Randomized experiments generally built four principles:Controlling. Researchers assign treatments cases, best control differences groups17.\nexample, patients take drug pill form, patients take pill sip water others may entire glass water.\ncontrol effect water consumption, doctor may instruct every patient drink 12 ounce glass water pill.Controlling. Researchers assign treatments cases, best control differences groups17.\nexample, patients take drug pill form, patients take pill sip water others may entire glass water.\ncontrol effect water consumption, doctor may instruct every patient drink 12 ounce glass water pill.Randomization. Researchers randomize patients treatment groups account variables controlled.\nexample, patients may susceptible disease others due dietary habits.\nRandomizing patients treatment control group helps even differences, also prevents accidental bias entering study.Randomization. Researchers randomize patients treatment groups account variables controlled.\nexample, patients may susceptible disease others due dietary habits.\nRandomizing patients treatment control group helps even differences, also prevents accidental bias entering study.Replication. cases researchers observe, accurately can estimate effect explanatory variable response.\nsingle study, replicate collecting sufficiently large sample.\nAlternatively, group scientists may replicate entire study verify earlier finding.Replication. cases researchers observe, accurately can estimate effect explanatory variable response.\nsingle study, replicate collecting sufficiently large sample.\nAlternatively, group scientists may replicate entire study verify earlier finding.Blocking. Researchers sometimes know suspect variables, treatment, influence response.\ncircumstances, may first group individuals based variable blocks randomize cases within block treatment groups.\nstrategy often referred blocking.\ninstance, looking effect drug heart attacks, might first split patients study low-risk high-risk blocks, randomly assign half patients block control group half treatment group, shown Figure 1.12.\nstrategy ensures treatment group equal number low-risk high-risk patients.Blocking. Researchers sometimes know suspect variables, treatment, influence response.\ncircumstances, may first group individuals based variable blocks randomize cases within block treatment groups.\nstrategy often referred blocking.\ninstance, looking effect drug heart attacks, might first split patients study low-risk high-risk blocks, randomly assign half patients block control group half treatment group, shown Figure 1.12.\nstrategy ensures treatment group equal number low-risk high-risk patients.\nFigure 1.12: Blocking using variable depicting patient risk. Patients first divided low-risk high-risk blocks, block evenly separated treatment groups using randomization. strategy ensures equal representation patients treatment group low-risk high-risk categories.\nimportant incorporate first three experimental design principles study, book describes applicable methods analyzing data experiments.\nBlocking slightly advanced technique, statistical methods book may extended analyze data collected using blocking.","code":""},{"path":"intro-to-data.html","id":"reducing-bias-human-experiments","chapter":"1 Introduction to data","heading":"1.5.2 Reducing bias in human experiments","text":"Randomized experiments long considered gold standard data collection, ensure unbiased perspective cause effect relationship cases.\nHuman studies perfect examples bias can unintentionally arise.\nreconsider study new drug used treat heart attack patients.\nparticular, researchers wanted know drug reduced deaths patients.researchers designed randomized experiment wanted draw causal conclusions drug’s effect.\nStudy volunteers18 randomly placed two study groups.\nOne group, treatment group, received drug.\ngroup, called control group, receive drug treatment.Put place person study.\ntreatment group, given fancy new drug anticipate help .\nhand, person group doesn’t receive drug sits idly, hoping participation doesn’t increase risk death.\nperspectives suggest actually two effects study: one interest effectiveness drug, second emotional effect () taking drug, difficult quantify.Researchers aren’t usually interested emotional effect, might bias study.\ncircumvent problem, researchers want patients know group .\nresearchers keep patients uninformed treatment, study said blind.\none problem: patient doesn’t receive treatment, know ’re control group.\nsolution problem give fake treatments patients control group.\nfake treatment called placebo, effective placebo key making study truly blind.\nclassic example placebo sugar pill made look like actual treatment pill.\nOften times, placebo results slight real improvement patients.\neffect dubbed placebo effect.patients ones blinded: doctors researchers can accidentally bias study.\ndoctor knows patient given real treatment, might inadvertently give patient attention care patient know placebo.\nguard bias, found measurable effect instances, modern studies employ double-blind setup doctors researchers interact patients , just like patients, unaware receiving treatment.19You may many questions ethics sham surgeries create placebo.\nquestions may even arisen mind general experiment context, possibly helpful treatment withheld individuals control group; main difference sham surgery tends create additional risk, withholding treatment maintains person’s risk.always multiple viewpoints experiments placebos, rarely obvious ethically “correct”.\ninstance, ethical use sham surgery creates risk patient?\nHowever, don’t use sham surgeries, may promote use costly treatment real effect; happens, money resources diverted away treatments known helpful.\nUltimately, difficult situation perfectly protect patients volunteered study patients may benefit () treatment future.","code":""},{"path":"intro-to-data.html","id":"scope-of-inference","chapter":"1 Introduction to data","heading":"1.6 Scope of inference","text":"statisticians refer scope inference study,\nasking two questions:Generalizability: population can generalize results?Causation: results provide evidence causal relationship?answer first question determined sampling method—selected sample randomly, sources sampling bias, can reasonably generalize population sample taken. answer second question determined type study—study randomized experiment, can investigate whether changes explanatory variable caused changes response variable; observational study, one can investigate associations variables. summarize determine study’s scope inference Figure 1.13.\nFigure 1.13: Determining scope inference study.\n","code":""},{"path":"intro-to-data.html","id":"data-in-r","chapter":"1 Introduction to data","heading":"1.7 Data in R","text":"R powerful open source software tool working data.\nThroughout text, provide guidance use R within \ncontext statistical content covered.educators, see value teaching modern software \nempower students take optimal advantage concepts learning.\nHowever, understand limitations educational structures, \nknow every classroom able implement R alongside \nstatistical concepts. Generally, present R techniques end\nchapter. times text concepts \ndistinguishable software, cases, provided \nR code within main body chapter.start introduction R, focused data sets structured \nR user can work data object R.","code":""},{"path":"intro-to-data.html","id":"dataframes-in-r","chapter":"1 Introduction to data","heading":"1.7.1 Dataframes in R","text":"Throughout text, work many different data sets. data sets\npre-loaded R, get loaded R packages, data sets\ncreated student. Data sets can viewed RStudio\nenvironment, data can also investigated notebook features\nRMarkdown file.Consider  data described previously chapter.\ncan use glimpse() function see variables included data set\ndata type. , use head() function see first\nrows data set.Sometimes necessary extract column row data set.\nR, $ operator can used extract column data set.\nexample, data$variable extract variable column data dataframe.\nextracted, columns can thought vectors. vectors, desired pull specific entry, use square brackets ([ ]), index (number) entry wish extract brackets.\nexample, data$variable[2] extract second entry (row) variable column.dataframe can (roughly) thought set many different vectors, can extract rows columns dataframe using familiar matrix notation (e.g. [row, column].\nexample data[,j] extract \\((,j)^{th}\\) entry data, data[, ] extract \\(^{th}\\) row, data[ , j] extract \\(j^{th}\\) column. Notice, extracting entire row (column), need specify columns (rows) like, second entry contain number.\nTable 1.7: Data 47th row email data set.\n","code":"\ndata(email50)\nglimpse(email50)\n#> Rows: 50\n#> Columns: 21\n#> $ spam         <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,…\n#> $ to_multiple  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…\n#> $ from         <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n#> $ cc           <int> 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n#> $ sent_email   <dbl> 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,…\n#> $ time         <dttm> 2012-01-04 06:19:16, 2012-02-16 13:10:06, 2012-01-04 08…\n#> $ image        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#> $ attach       <dbl> 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0,…\n#> $ dollar       <dbl> 0, 0, 0, 0, 9, 0, 0, 0, 0, 23, 4, 0, 3, 2, 0, 0, 0, 0, 0…\n#> $ winner       <fct> no, no, no, no, no, no, no, no, no, no, no, no, yes, no,…\n#> $ inherit      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#> $ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#> $ password     <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 8,…\n#> $ num_char     <dbl> 21.705, 7.011, 0.631, 2.454, 41.623, 0.057, 0.809, 5.229…\n#> $ line_breaks  <int> 551, 183, 28, 61, 1088, 5, 17, 88, 242, 578, 1167, 198, …\n#> $ format       <dbl> 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,…\n#> $ re_subj      <dbl> 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,…\n#> $ exclaim_subj <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#> $ urgent_subj  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n#> $ exclaim_mess <dbl> 8, 1, 2, 1, 43, 0, 0, 2, 22, 3, 13, 1, 2, 2, 21, 10, 0, …\n#> $ number       <fct> small, big, none, small, small, small, small, small, sma…\nhead(email50) \n#> # A tibble: 6 x 21\n#>    spam to_multiple  from    cc sent_email time                image attach\n#>   <dbl>       <dbl> <dbl> <int>      <dbl> <dttm>              <dbl>  <dbl>\n#> 1     0           0     1     0          1 2012-01-04 06:19:16     0      0\n#> 2     0           0     1     0          0 2012-02-16 13:10:06     0      0\n#> 3     1           0     1     4          0 2012-01-04 08:36:23     0      2\n#> 4     0           0     1     0          0 2012-01-04 10:49:52     0      0\n#> 5     0           0     1     0          0 2012-01-27 02:34:45     0      0\n#> 6     0           0     1     0          0 2012-01-17 10:31:57     0      0\n#> # … with 13 more variables: dollar <dbl>, winner <fct>, inherit <dbl>,\n#> #   viagra <dbl>, password <dbl>, num_char <dbl>, line_breaks <int>,\n#> #   format <dbl>, re_subj <dbl>, exclaim_subj <dbl>, urgent_subj <dbl>,\n#> #   exclaim_mess <dbl>, number <fct>\nemail50$num_char\n#>  [1] 21.705  7.011  0.631  2.454 41.623  0.057  0.809  5.229  9.277 17.170\n#> [11] 64.401 10.368 42.793  0.451 29.233  9.794  2.139  0.130  4.945 11.533\n#> [21]  5.682  6.768  0.086  3.070 26.520 26.255  5.259  2.780  5.864  9.928\n#> [31] 25.209  6.563 24.599 25.757  0.409 11.223  3.778  1.493 10.613  0.493\n#> [41]  4.415 14.156  9.491 24.837  0.684 13.502  2.789  1.169  8.937 15.829\nemail50[47,3]\n#> # A tibble: 1 x 1\n#>    from\n#>   <dbl>\n#> 1     1"},{"path":"intro-to-data.html","id":"datastruc","chapter":"1 Introduction to data","heading":"1.7.2 Tidy structure of data","text":"plotting, analyses, model building, etc., data structured according certain principles.\nHadley Wickham provides thorough discussion advice cleaning data Wickham others (2014).Tidy data: rows (cases/observational units) columns (variables).\nkey every row case every column variable.\nexceptions.Creating tidy data often trivial.Within R (really within type computing language, Python, SQL, Java, etc.), important understand build data using patterns language.\nthings consider:object_name <- anything way assigning anything new object_name.object_name <- function_name(data_table, arguments) way using function create new object.object_name <- data_table %>% function_name(arguments) uses chaining syntax extension ideas functions.\nchaining, value left side %>% becomes first argument function right side.extended chaining. %>% never front line, always connecting one idea continuation idea next line.R, functions take arguments round parentheses (opposed subsetting observations variables data objects happen square parentheses). Additionally, spot left %>% always data table.R, functions take arguments round parentheses (opposed subsetting observations variables data objects happen square parentheses). Additionally, spot left %>% always data table.pipe syntax read , %>%.pipe syntax read , %>%.","code":"object_name <- data_table %>%\n                    function_name(arguments) %>% \n                    another_function_name(other_arguments)"},{"path":"intro-to-data.html","id":"using-the-pipe-to-chain","chapter":"1 Introduction to data","heading":"1.7.3 Using the pipe to chain","text":"pipe syntax (%>%) takes data frame (data table) sends argument function. mapping goes first available argument function.\nexample:x %>% f(y) f(x, y)y %>% f(x, ., z) f(x,y,z)Pipes used commonly functions dplyr package (see R examples Chapter 2) allow us sequentially build data wrangling operations.\n’ll start short pipes throughout course build longer pipes perform multiple operations.Consider  data, High School Beyond survey.\nTwo hundred observations randomly sampled High School Beyond survey, survey conducted high school seniors National Center Education Statistics.\ninterest proportion students two types school, public privaate.use table command tabulate many type school data set. Notice result produced $ command table chaining syntax done %>%.interested public schools?\nFirst, take note another piece R syntax: double equal sign.\nlogical test “equal ”.\nwords, first determine school type equal public observations data set filter true.can read : “take hsb2 data frame pipe filter function. Filter data cases school type equal public. , assign resulting data frame new object called hsb2 underscore public.”Suppose interested actual reading score students, instead whether reading score average average.\nFirst, need calculate average reading score mean function.\ngive us mean value, 52.23.\nHowever, order able refer back value later , might want store object can refer name.instead just printing result, let’s save new object called avg_read., quick tip: often ’ll want ; see value also store later use. approach used , running mean function twice, redundant.Instead, can simply wrap assignment code parentheses R assign average value reading test scores avg read, also print value.Next need determine whether student average. example, reading score 57 average, 68, 44 .\nObviously, going record like tedious error prone.Instead can create new variable mutate function dplyr package.start data frame, hsb2, pipe mutate, create new variable called read_cat (cat categorical).\nNote using new variable name order overwrite existing reading score variable.\nnew variable read_cat column existing data frame hsb2.\nindicate mutate function came dplyr package, use pacakge::function syntax.\nusually necessary provide package name (unless ambiguity function came ).decision criteria new variable based TRUE/FALSE question: reading score student average reading score, label “average”, otherwise, label “average”.can accomplished using ifelse function R.first argument function logical test.second argument result logical test TRUE, words, student’s score average score, last argument result FALSE.ifelse function can used complicated discretization rules well, nesting many ifelse statements within .\nnecessary example, come later course.","code":"\ndata(hsb2)\ntable(hsb2$schtyp)  \n#> \n#>  public private \n#>     168      32\nhsb2 %>% \n  select(schtyp) %>%\n  table()\n#> .\n#>  public private \n#>     168      32\n# Filter for public schools\nhsb2_public <- hsb2 %>%\n  filter(schtyp == \"public\")\n# Calculate average reading score and show the value\nmean(hsb2$read)\n#> [1] 52.2\n# Calculate average reading score and store as avg_read\navg_read <- mean(hsb2$read)\n# Do both\n(avg_read <- mean(hsb2$read))\n#> [1] 52.2\nhsb2 <- hsb2 %>% dplyr::mutate(read_cat = ifelse(read < avg_read, \n                                                 \"below average\", \"at or above average\"))"},{"path":"intro-to-data.html","id":"interactive-r-tutorials","chapter":"1 Introduction to data","heading":"1.7.4 Interactive R tutorials","text":"Navigate concepts ’ve learned chapter R using following self-paced tutorials.\nneed browser get started!Tutorial 1: Getting Started DataTutorial 1 - Lesson 1: Language dataTutorial 1 - Lesson 2: Types studiesTutorial 1 - Lesson 3: Sampling strategies Experimental designTutorial 1 - Lesson 4: Case studyYou can also access full list tutorials supporting book .","code":""},{"path":"intro-to-data.html","id":"r-labs","chapter":"1 Introduction to data","heading":"1.7.5 R labs","text":"apply concepts ’ve learned chapter R computational labs walk data analysis case study.Intro R - Birth ratesFull list labs supporting OpenIntro::Introduction Modern Statistics","code":""},{"path":"intro-to-data.html","id":"chp1-review","chapter":"1 Introduction to data","heading":"1.8 Chapter 1 review","text":"","code":""},{"path":"intro-to-data.html","id":"terms","chapter":"1 Introduction to data","heading":"1.8.1 Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"eda.html","id":"eda","chapter":"2 Exploratory data analysis","heading":"2 Exploratory data analysis","text":"","code":""},{"path":"eda.html","id":"categorical-data","chapter":"2 Exploratory data analysis","heading":"2.1 Exploring categorical data","text":"section, introduce tables basic tools organizing analyzing categorical data used throughout book. Table 2.1 displays first six rows email data set containing information 3,921 emails sent David Diez’s Gmail account (one authors OpenIntro textbooks). section examine whether presence numbers, small large, email provides useful value classifying email spam spam.\nDescriptions five email variables given Table 2.2.\nTable 2.1: Six rows email data set.\n\nTable 2.2: Variables descriptions email data set.\n","code":""},{"path":"eda.html","id":"contingency-tables-and-conditional-proportions","chapter":"2 Exploratory data analysis","heading":"2.1.1 Contingency tables and conditional proportions","text":"summary table single categorical variable reports number observations (frequency) category called frequency table. Table\n2.3 frequency table number variable.\nreplaced counts percentages proportions (relative frequencies),\ntable called relative frequency table.\nTable 2.3: Frequency table Number variable.\nTable 2.4 summarizes two variables:\ntype (spam spam) number. table summarizes data two categorical variables\nway called contingency table two-way table.\nvalue table represents number times, frequency\nparticular combination variable outcomes occurred.\nexample, value 149 corresponds number emails\ndata set spam number listed email.\nRow column totals also included.\nrow totals provide total counts across row\n(e.g., \\(149 + 168 + 50 = 367\\)), column totals total\ncounts column.textbook, generally take convention putting categories explanatory variable columns categories response variable rows (exists explanatory-response relationship two variables).\nTable 2.4: Contingency table number (cols) type (rows) variables.\nlike examine whether presence numbers, small large, email provides useful value classifying email spam spam—, association variables number type?proportion emails classified spam data set \\(3554/3921 = 0.906\\), 91%. Let’s compare unconditional proportion conditional proportions spam within number category: \\(400/549 = 73\\)% emails numbers spam; \\(2659/2827 = 94\\)% emails small numbers spam; \\(495/545 = 91\\)% emails big numbers spam. Since three conditional proportions differ, say variables number type associated data set. Note differ overall, unconditional, proportion spam emails data set—91%.Association two categorical variables.unconditional proportion proportion measured total sample size. conditional proportion proportion measured subgroup sample.","code":""},{"path":"eda.html","id":"row-and-column-proportions","chapter":"2 Exploratory data analysis","heading":"Row and column proportions","text":"Conditional proportions condition row category called row proportions; conditional proportions condition column category\ncalled column proportions.Table 2.5 shows row proportions Table 2.4. row proportions computed counts divided row totals. value 149 intersection type none replaced \\(149/367=0.406\\), .e., 149 divided row total, 367. 0.406 represent? corresponds conditional proportion spam emails sample numbers.\nTable 2.5: contingency table row proportions type number variables.\ncontingency table column proportions computed similar way, column proportion computed count divided corresponding column total. Table 2.6 shows table, value 0.271 indicates 27.1% emails numbers spam. rate spam much higher emails small numbers (5.9%) big numbers (9.2%). spam rates vary three levels number (none, small, big), provides evidence spam number variables associated data set.\nTable 2.6: contingency table column proportions type number variables.\n\nTable 2.7: contingency table type format.\nprevious Example points row column proportions equivalent. settling one form table, important consider ensure useful table constructed.","code":""},{"path":"eda.html","id":"sample-proportions-and-population-proportions","chapter":"2 Exploratory data analysis","heading":"Sample proportions and population proportions","text":"field statistics, summary measures summarize sample data called statistics. Numbers summarize entire population called parameters. can remember\ndistinction looking first letter term:Statistics summarize Samples.Parameters summarize Populations.Proportions calculated sample data denoted \\(\\hat{p}\\).\nexample, interested proportion spam emails data set, denote \\(\\hat{p} = 0.91\\). different groups want summarize proportion, can add subscripts: \\(\\hat{p}_{none} = 0.73\\), \\(\\hat{p}_{small} = 0.94\\), \\(\\hat{p}_{big} = 0.91\\). values statistic since computed sample data.3921 emails sample larger group emails—emails sent David Diez, either past future. larger group emails population. unknown value proportion emails population classified spam, denote \\(\\pi\\). Similarly, unknown values proportion emails numbers population classified spam, denoted \\(\\pi_{none}\\). unknown values called parameters.typically use Roman letters symbolize statistics (e.g., \\(\\bar{x}\\), \\(\\hat{p}\\)), Greek letters symbolize parameters (e.g., \\(\\mu\\), \\(\\pi\\)).\nSince rarely can measure entire population, thus rarely know\nactual parameter values, like say, “don’t know Greek,\ndon’t know parameters!”","code":""},{"path":"eda.html","id":"bar-plots-and-mosaic-plots","chapter":"2 Exploratory data analysis","heading":"2.1.2 Bar plots and mosaic plots","text":"bar plot common way display single categorical variable. left panel Figure 2.1 shows bar plot number variable.\nright panel, counts converted proportions (e.g., \\(549/3921=0.140\\) none).\nFigure 2.1: Two bar plots number. left panel shows counts, right panel shows proportions group.\nBar plots also used display relationship two categorical variables.\nbars stacked bar totals 100% segmented \nanother categorical variable, called segmented bar plot.segmented bar plot graphical display contingency table information. example, segmented bar plots representing Table 2.6 shown Figure 2.2, first created non-standardized segmented bar plot using number variable separated group levels type. standardized segmented bar plot using column proportions Table 2.6 helpful visualization fraction spam emails level number.\nFigure 2.2: () Segmented bar plot numbers found emails, counts broken type. (b) Segmented bar plot using column proportions type within number category.\nSince proportion spam changes across groups Figure 2.2 (seen plot (b)), can conclude variables dependent, something also able discern using column proportions Table 2.6. none big groups relatively observations compared small group, association difficult see plot () Figure 2.2.cases, segmented bar plot standardized useful communicating important information. settling particular segmented bar plot, create standardized non-standardized forms decide effective communicating features data.","code":""},{"path":"eda.html","id":"mosaic-plots","chapter":"2 Exploratory data analysis","heading":"Mosaic plots","text":"mosaic plot graphical display contingency table information similar bar plot one variable segmented bar plot using two variables. Figure 2.3 plot () shows mosaic plot number variable. column represents level number, column widths correspond proportion emails number type. instance, fewer emails numbers emails small numbers, number email column slimmer. general, mosaic plots use box areas represent number observations.\nFigure 2.3: () Mosaic plot numbers found emails. (b) Mosaic plot number counts broken type.\none-variable mosaic plot divided pieces Figure 2.3 plot (b) using type variable. column split proportionally according fraction emails spam number category. example, second column, representing emails small numbers, divided emails spam (lower) spam (upper).\nanother example, bottom third column represents spam emails big numbers, upper part third column represents regular emails big numbers. can use plot see type number variables associated since columns divided different vertical locations others, technique used checking association standardized version segmented bar plot.","code":""},{"path":"eda.html","id":"why-not-pie-charts","chapter":"2 Exploratory data analysis","heading":"2.1.3 Why not pie charts?","text":"pie charts well known, typically useful charts data analysis. pie chart shown Figure 2.4 alongside bar plot. generally difficult compare group sizes pie chart (comparing angles) bar plot (comparing heights), especially categories nearly identical counts proportions. case none big categories, difference slight may unable distinguish difference group sizes either plot!\nFigure 2.4: pie chart bar plot number email data set. pie chart see book!\nPie charts nearly useless trying compare two categorical variables, shown Figure 2.5.\nFigure 2.5: Try comparing distributions colors across pie charts , B, C—’s impossible!24\n’re still convinced shouldn’t use pie charts, read “Issue Pie Chart” “Data Viz” blog, “Worst Chart World” article Business Insider.","code":""},{"path":"eda.html","id":"simpson","chapter":"2 Exploratory data analysis","heading":"2.1.4 Simpson’s paradox","text":"","code":""},{"path":"eda.html","id":"race-and-capital-punishment","chapter":"2 Exploratory data analysis","heading":"Race and capital punishment","text":"1991 study Radelet Pierce examined whether race associated whether death penalty invoked homicide cases25. Table 2.8 Figure 2.6 summarize data 674 defendants indictments involving cases multiple murders Florida 1976 1987.\nTable 2.8: Contingency table homicide cases Florida 1976 1987.\n\nFigure 2.6: Segmented bar plot comparing proportion defendants received death penalty Caucasians African Americans.\nOverall, lower percentage African American defendants received death penalty Caucasian defendants (8% compared 11%). Given studies shown racial bias sentencing, may surprising. Let’s look data closely.subset data race victim, see different picture. Table 2.9 Figure 2.7 summarize data, separately Caucasian African American homicide victims.\nTable 2.9: Contingency table homicide cases Florida 1976 1987; sentences classified defendant’s race victim’s race.\n\nFigure 2.7: Segmented bar plots comparing proportion Caucasian African American defendants received death penalty; separate plots Caucasian victims African American victims.\ncompare Figures 2.6 2.7, see direction association race defendant sentence reversed subgroup race victim. Overall, larger proportion Caucasians sentenced death penalty African Americans. However, compare cases victim’s race, larger proportion African Americans sentenced death penalty Caucasians!happen? answer race victim confounding variable. Figure 2.8 shows two segmented barplots examining relationship race victim sentence (response variable), relationship race victim race defendant (explanatory variable). see race victim associated response explanatory variables: defendants likely involve victim race, cases African American victims less likely result death penalty.\nFigure 2.8: race victim associated sentence (death penalty death penalty) race defendant. Defendants likely involve victim race, cases African American victims less likely result death penalty.\nThus, extremely low chance homicide case resulting death penalty African Americans combined fact cases African American defendants also African American victim results overall lower rate death penalty sentences African American defendants Caucasian defendants. overall results Figure 2.6 results subgroup Figure 2.7 valid—result “bad statistics”—suggest opposite conclusions. Data , observed effect reverses examine variables within subgroups, exhibit Simpson’s Paradox.Simpson’s Paradox.","code":""},{"path":"eda.html","id":"probability-with-tables","chapter":"2 Exploratory data analysis","heading":"2.2 Probability with tables","text":"","code":""},{"path":"eda.html","id":"defining-probability","chapter":"2 Exploratory data analysis","heading":"2.2.1 Defining probability","text":"random process one outcome unpredictable. encounter random processes every day: rain today? many minutes pass receiving next text message? Seahawks win Super Bowl? Though outcome one particular random process unpredictable, observe process many many times, pattern outcomes, probability distribution, can often modeled mathematically. Though several philosophical definitions probability, use “frequentist” definition probability—long-run relative frequency.Probability.Consider simple example flipping fair coin . probability coin lands heads. physical properties, assume probability heads 0.5, let’s use simulation examine probability. Figure 2.9 shows long-run proportion times simulated coin flip lands heads y-axis, number tosses x-axis. Notice long-run proportion starts converging 0.5 number tosses increases.\nFigure 2.9: One simulation flipping fair coin, tracking long-run proportion times coin lands heads.\n","code":""},{"path":"eda.html","id":"finding-probabilities-with-tables","chapter":"2 Exploratory data analysis","heading":"2.2.2 Finding probabilities with tables","text":"can solve many real-life probability problems without using equations creating hypothetical two-way table scenario. tool best demonstrated example.Example 2.1  student Montana State University, suppose first class Mondays Wilson Hall 8:00am commute school. Bobcat parking permit. past experience, know 20% chance finding open parking spot Lot 6 Animal Bioscience. Otherwise, park Lot 18 graduate housing. find spot Lot 6, 5% chance late class. However, park Lot 18, 15% chance late class. probability late class Monday?two random variables scenario: whether park Lot 6 Lot 18, whether late class. Since know probability long-run relative frequency, let’s imagine 1000 hypothetical Mondays, fill contingency table frequencies ’d expect cell.create table last Example? Let’s work step--step.Identify unconditional probabilities given problem: 20% chance parking Lot 6, means 80% chance parking Lot 18. Take 20% 80% 1000 fill row totals:Identify conditional probabilities given problem: park Lot 6, probability late class 5%; park Log 18, probability late class 15%. Fill corresponding cells table taking 5% times parked Lot 6, 15% times parked Lot 18:Use subtraction fill remaining cells column “late class.” Use addition find column totals.Using hypothetical two-way table given last Example, find following probabilities:probability late class?probability park Lot 6 late class?Given late class, probability parked Lot 18?28\nCarefully read probabilities described Guided Practice—note subtle difference “probability late class, given parked Lot 18” (\\(120/800 = 0.15\\)) “probability parking Lot 18, given late class” (\\(120/130 = 0.923\\)). given extra information, called conditional probability, denominator probability calculation row total (e.g., 800) column total (e.g., 130) rather overall total hypothetical two-way table.","code":""},{"path":"eda.html","id":"probability-notation","chapter":"2 Exploratory data analysis","heading":"2.2.3 Probability notation","text":"ease translating probability problems calculations, let’s define notation. denote “events” (e.g., late class) upper case letters near beginning alphabet, e.g., \\(\\), \\(B\\), \\(C\\). probability event \\(\\) denoted \\(P()\\), \\(P()\\) number 0 1. event \\(\\) happen called complement \\(\\) denoted \\(P(^C)\\). Sometimes additional information like condition , denote conditional probability \\(\\) given \\(B\\) \\(P(| B)\\)—probability \\(\\) happens given \\(B\\) already happened.","code":""},{"path":"eda.html","id":"diagnostic-testing","chapter":"2 Exploratory data analysis","heading":"2.2.4 Diagnostic testing","text":"Medical diagnostic tests diseases spend years development. clinical trials, developers diagnostic test able determine two important properties test:sensitivity diagnostic test probability test yields positive result, given individual disease. words, proportion diseased population test positive?specificity diagnostic test probability test yields negative result, given individual disease. , proportion non-diseased population test negative?good diagnostic test high (near 100%) sensitivity specificity. However, even near-perfect test, probability disease given test positive still quite low. investigate counter-intuitive result, need another definition:call proportion population disease—probability contracting disease—prevalence (incidence) disease.Let \\(D\\) event individual disease \\(T\\) event individual tests positive. express following quantities using probability notation?sensitivityspecificityprevalence30\nNote sensitivity specificity conditional probabilities, prevalence unconditional probability. probabilities useful information, test positive diagnostic test, none quantities probability really want know: conditional probability disease, given tested positive, \\(P(D | T)\\).","code":""},{"path":"eda.html","id":"the-case-of-baby-jeff","chapter":"2 Exploratory data analysis","heading":"The case of Baby Jeff","text":"following case study presented Slawson Shaughnessy (2002). poster hospital’s newborn nursery announced male newborns screened muscular dystrophy using heel stick blood test creatinine phosphokinase (CPK). test characteristics screening tests nearly perfect: sensitivity 100% specificity 99.98%. prevalence muscular dystrophy male newborns ranges 1 3,500 1 15,000. Baby Jeff abnormal CPK test. parents baby wanted know, “chance son muscular dystrophy?” Doctors informed parents though 100% likely, highly probable. First, take minute predict probability – think? 80% chance? 99% chance? Let’s investigate using two-way table hypothetical population 100,000 male newborns.calculations, let’s use prevalence 1 10,000. 100,000 hypothetical male newborns, expect 1 10,000 muscular dystrophy, 10: \\((1/10000)\\times 100000 = 10\\). sensitivity test perfect, 10 male newborns muscular dystrophy test positive. \\(100000-10 = 99,990\\) male newborns muscular dystrophy, 99.98% test negative: \\((0.9998)\\times 99990 = 99,970\\) infants. leaves \\(99990 - 99970 = 20\\) male newborns test positive even though muscular dystrophy. allows us fill counts hypothetical two-way table:Now can read desired probability table: 30 male newborns ’d expect test positive, 10 actually muscular dystrophy. means chance Baby Jeff muscular dystrophy 33%!counter-intuitive result occur? high sensitivity specificity, test perform poorly? answer prevalence disease. rare disease, small proportion test positive large group people without disease overwhelm large proportion test positive small group people disease. number false positives can much higher number true positives.","code":""},{"path":"eda.html","id":"quantitative-data","chapter":"2 Exploratory data analysis","heading":"2.3 Exploring quantitative data","text":"section explore techniques summarizing quantitative variables.\nexample, consider loan_amount variable loan50 data set, represents loan size 50 loans data set.\nvariable quantitative since can sensibly discuss numerical difference size two loans.\nhand, area codes zip codes quantitative, rather categorical variables.Throughout section next, apply methods using loan50, county, email50 data sets, introduced Section 1.2.\n’d like review variables either data set, see Tables 1.4 1.6.","code":""},{"path":"eda.html","id":"scatterplots","chapter":"2 Exploratory data analysis","heading":"2.3.1 Scatterplots for paired data","text":"scatterplot provides case--case view data two quantitative variables.\nFigure 1.4, scatterplot used examine homeownership rate fraction housing units part multi-unit properties (e.g. apartments) county data set.\nAnother scatterplot shown Figure 2.10, comparing total income borrower total_income amount borrowed loan_amount loan50 data set.\nscatterplot, point represents single case.\nSince 50 cases loan50, 50 points Figure 2.10.examining scatterplots, describe four features:Form - trace trend points,\ntrend linear nonlinear?Direction - values x-axis increase, y-values\ntend increase (positive direction) decrease (negative direction)?Strength - closely points follow trend?Unusual observations outliers- unusual observations\nseem match overall pattern scatterplot?\nFigure 2.10: scatterplot loan_amount versus total_income loan50 data set.\nLooking Figure 2.10, see many\nborrowers income $100,000 left side graph, \nhandful borrowers income $250,000. loan amounts vary \n$10,000 around $40,000. data seem linear form, though\nrelationship two variables quite weak. direction \npositive—total income increases, loan amount also tends increase—may unusual observations higher income range,\nthough since relationship weak, hard tell.\nFigure 2.11: scatterplot median household income poverty rate county data set. Data 2017. statistical model also fit data shown dashed line.\nExample 2.3  Figure 2.11 shows plot median household income poverty rate 3,142 counties.\ncan said relationship variables?","code":""},{"path":"eda.html","id":"dotplots","chapter":"2 Exploratory data analysis","heading":"2.3.2 Dot plots and the mean","text":"Sometimes interested distribution single variable.\ncases, dot plot provides basic displays.\ndot plot one-variable scatterplot; example using interest rate 50 loans shown Figure 2.12.\nFigure 2.12: dot plot interest_rate loan50 data set. rates rounded nearest percent plot, distribution’s mean shown red triangle.\ndistribution variable description possible values\ntakes frequently value occurs. mean, often called average, common way measure center distribution data.\ncompute mean interest rate 50 loans , add interest rates divide number observations.sample mean often labeled \\(\\bar{x}\\).\nletter \\(x\\) used generic placeholder variable bar \\(x\\) communicates ’re looking average variable. example \\(x\\) represent interest rate, \\(\\bar{x}\\) = 11.57%.\nuseful think mean balancing point distribution34, ’s shown triangle Figure 2.12.Mean.sample mean can calculated sum observed values divided number observations:loan50 data set represents sample larger population loans made Lending Club.\ncompute mean population way sample mean.\nHowever, population mean special label: \\(\\mu\\).\nsymbol \\(\\mu\\) Greek letter mu represents average observations population.\nSometimes subscript, \\(_x\\), used represent variable population mean refers , e.g., \\(\\mu_x\\).\nOften times expensive time consuming measure population mean precisely, often estimate \\(\\mu\\) using sample mean, \\(\\bar{x}\\).Example 2.4  average interest rate across loans population can estimated using sample data. Based sample 50 loans, reasonable estimate \\(\\mu_x\\), mean interest rate loans full data set?mean useful making comparisons across different samples may different sample sizes allows us rescale standardize metric something easily interpretable comparable.Suppose like understand new drug effective treating asthma attacks standard drug.\ntrial 1500 adults set , 500 receive new drug, 1000 receive standard drug control group:Comparing raw counts 200 300 asthma attacks make appear new drug better, artifact imbalanced group sizes.\nInstead, look average number asthma attacks per patient group:New drug: \\(200 / 500 = 0.4\\) asthma attacks per patientStandard drug: \\(300 / 1000 = 0.3\\) asthma attacks per patientThe standard drug lower average number asthma attacks per patient average treatment group.Example 2.5  Emilio opened food truck last year sells burritos, business stabilized last 4 months.\n4 month period, made $11,000 working 625 hours.\nEmilio’s competition, Francis, made $13,000 last 4 months working 800 hours. Francis brags Emilio business profitable. Francis’ claim warranted?Emilio’s average hourly earnings provide useful statistic evaluating much venture , least financial perspective, worth:\\[ \\frac{\\$11000}{625\\text{ hours}} = \\$17.60\\text{ per hour} \\]knowing average hourly wage,\nEmilio now put earnings standard unit easier compare many jobs might consider.comparison, Francis’ average hourly wage \\[ \\frac{\\$13000}{800\\text{ hours}} = \\$16.25\\text{ per hour} \\]Example 2.6  Suppose want compute average income per person US. , might first think take mean per capita incomes across 3,142 counties  data set. better approach?county data set special county actually represents many individual people.\nsimply average across income variable, treating counties 5,000 5,000,000 residents equally calculations.\nInstead, compute total income county, add counties’ totals, divide number people counties.\ncompleted steps  data, find per capita income US $30,861.\ncomputed simple mean per capita income across counties, result just $26,093!","code":""},{"path":"eda.html","id":"histograms","chapter":"2 Exploratory data analysis","heading":"2.3.3 Histograms and shape","text":"Dot plots show exact value observation. useful small data sets, can become hard read larger samples. Rather showing value observation, prefer think value belonging bin. example, loan50 data set, created table counts number loans interest rates 5.0% 7.5%, number loans rates 7.5% 10.0%, . Observations fall boundary bin (e.g., 10.00%) allocated lower bin. tabulation shown Table 2.10. binned counts plotted bars Figure 2.13 called histogram, resembles heavily binned version stacked dot plot shown Figure 2.12.\nTable 2.10: Counts binned interest_rate data.\n\nFigure 2.13: histogram interest_rate. distribution strongly skewed right.\nHistograms provide view data density. Higher bars represent data relatively common, \n“dense.” instance, many loans rates 5% 10% loans rates 20% 25% data set. bars make easy see density data changes relative interest rate.Histograms especially convenient understanding shape data distribution. Figure 2.13 suggests loans rates 15%, handful loans rates 20%. data trail right way longer right tail, shape said right skewed37Data sets reverse characteristic—long, thinner tail left—said left skewed. also say distribution long left tail. Data sets show roughly equal trailing directions called symmetric.addition looking whether distribution skewed symmetric, histograms can used identify modes. mode represented prominent peak distribution. one prominent peak histogram interest_rate.definition mode sometimes taught math classes value occurrences data set. However, many real-world data sets, common observations value data set, making definition impractical data analysis.Figure 2.14 shows histograms one, two, three prominent peaks. distributions called unimodal, bimodal, multimodal, respectively. distribution two prominent peaks called multimodal. Notice one prominent peak unimodal distribution second less prominent peak counted since differs neighboring bins observations.\nFigure 2.14: Counting prominent peaks, distributions (left right) unimodal, bimodal, multimodal. Note left plot unimodal counting prominent peaks, just peak.\nLooking modes isn’t finding clear correct answer number modes distribution, prominent rigorously defined book. important part examination better understand data.Another type plot helpful exploring shape distribution smoothed histogram, called density plot. density plot scale \\(y\\)-axis total area density curve equal one. allows us get sense proportion data lie certain interval, rather frequency data interval. can change scale histogram plot proportions rather frequencies, overlay density curve rescaled histogram, seen Figure 2.15.\nFigure 2.15: density plot interest_rate overlayed histogram using density scale.\n","code":""},{"path":"eda.html","id":"variance-sd","chapter":"2 Exploratory data analysis","heading":"2.3.4 Variance and standard deviation","text":"mean introduced method describe center data set, variability data also important. , introduce two measures variability: variance standard deviation. useful data analysis, even though formulas bit tedious calculate hand. standard deviation easier two comprehend, roughly describes far away typical observation mean.call distance observation mean deviation. deviations \\(1^{st}\\), \\(2^{nd}\\), \\(3^{rd}\\), \\(50^{th}\\) observations interest_rate variable:\\[ x_1 - \\bar{x} = 10.9 - 11.57 = -0.67 \\] \\[ x_2 - \\bar{x} = 9.92 - 11.57 = -1.65 \\] \\[ x_3 - \\bar{x} = 26.3 - 11.57 = 14.73 \\] \\[ \\vdots \\] \\[ x_{50} - \\bar{x} = 6.08 - 11.57 = -5.49 \\]square deviations take average, result equal sample variance, denoted \\(s^2\\):\\[ s^2 = \\frac{(-0.67)^2 + (-1.65)^2 + (14.73)^2 + \\cdots + (-5.49)^2}{50 - 1} = \\frac{0.45 + 2.72 + \\cdots + 30.14}{49} = 25.52 \\]divide \\(n - 1\\), rather dividing \\(n\\), computing sample’s variance; ’s mathematical nuance , end result makes statistic slightly reliable useful.Notice squaring deviations two things. First, makes large values relatively much larger. Second, gets rid negative signs.standard deviation defined square root variance:\\[ s = \\sqrt{25.52} = 5.05 \\]often omitted, subscript \\(_x\\) may added variance standard deviation, .e., \\(s_x^2\\) \\(s_x\\), useful reminder variance standard deviation observations represented \\(x_1\\), \\(x_2\\), …, \\(x_n\\).Variance standard deviation.sample variance (near) average squared distance mean:\n\\[\n  s^2 = \\frac{((x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\cdots + (x_n - \\bar{x})^2)}{n-1}\n\\]\nsample standard deviation square root variance: \\(s = \\sqrt{s^2}\\).Like mean, population values variance standard deviation special symbols: \\(\\sigma^2\\) variance \\(\\sigma\\) standard deviation.\nFigure 2.16: interest_rate variable, 34 50 loans (68%) interest rates within 1 standard deviation mean, 48 50 loans (96%) rates within 2 standard deviations. Usually 70% data within 1 standard deviation mean 95% within 2 standard deviations, though far hard rule.\n\nFigure 2.17: Three different population distributions mean (0) standard deviation (1).\nExample 2.8  Describe distribution interest_rate variable using histogram Figure 2.13.\ndescription incorporate center, variability, shape distribution, also placed context.\nAlso note especially unusual cases.practice, variance standard deviation sometimes used means end, “end” able accurately estimate uncertainty associated sample statistic. example, Chapter 6 standard deviation used calculations help us understand much sample mean varies one sample next.","code":""},{"path":"eda.html","id":"box-plots-quartiles-and-the-median","chapter":"2 Exploratory data analysis","heading":"2.3.5 Box plots, quartiles, and the median","text":"box plot (box--whisker plot) summarizes data set using five statistics \nalso identifying unusual observations. five statistics—minimum, first quartile,\nmedian, third quartile, maximum—together called five number summary.\nFigure 2.18 provides dot plot alongside box plot interest_rate variable loan50 data set.\nFigure 2.18: Plot shows dot plot Plot B shows box plot distribution interest rates loan50 dataset.\ndark line inside box represents median, splits data half:\n50% data fall value 50% fall .\nSince loan50 dataset 50 observations (even number), median defined average two observations closest \\(50^{th}\\) percentile. Table @ref(tab:loan50_int_rate_sorted) shows interest rates, arranged ascending order.\ncan see \\(25^{th}\\) \\(26^{th}\\) values 9.93, corresponds dark line box plot Figure 2.18.\n(#tab:loan50_int_rate_sorted)Interest rates loan50 dataset, arranged ascending order.\nodd number observations, exactly one observation splits data two halves, case observation median (average needed).Median: number middle.data ordered smallest largest, median observation right middle.\neven number observations, two values middle, median taken average.Mathematically, denote sample size \\(n\\), thenif \\(n\\) odd, median \\([(n+1)/2]^{th}\\) smallest value data set, andif \\(n\\) even, median average \\((n/2)^{th}\\) \\((n/2+1)^{th}\\) smallest values data set.\nsecond step building box plot drawing rectangle represent middle 50% data.\nlength box called interquartile range, IQR short.\n, like standard deviation, measure variability data.\nvariable data, larger standard deviation IQR tend .\ntwo boundaries box called first quartile (\\(25^{th}\\) percentile, .e., 25% data fall value) third quartile (\\(75^{th}\\) percentile, .e., 75% data fall value)  , often labeled \\(Q_1\\) \\(Q_3\\), respectively42Interquartile range (IQR).Extending box, whiskers attempt capture data outside box.\nwhiskers box plot reach minimum maximum values data, unless points considered unusually high unusually low, identified potential outliers box plot.\nlabeled dot box plot.\npurpose labeling points—instead extending whiskers minimum maximum observed values—help identify observations appear unusually distant rest data.\nvariety formulas determining whether particular data point considered outlier, different statistical software use different formulas.\ncommonly used formula value beyond \\(1.5\\times IQR\\)[choice exactly 1.5 arbitrary, commonly used value box plots.] away box considered outlier.\nsense, box like body box plot whiskers like arms trying reach rest data, outliers.Figure 2.18,\nupper whisker extend last two points, 24.85% 26.3%, \n\\(Q_3 + 1.5\\times IQR\\), extends last point limit.\nlower whisker stops minimum value data set, 5.31%, since outliers lower end distribution.Outliers extreme.outlier observation appears extreme relative rest data.\nExamining data outliers serves many useful purposes, includingidentifying strong skew  distribution,identifying possible data collection data entry errors, andproviding insight interesting properties data.\n","code":""},{"path":"eda.html","id":"describing-and-comparing-quantitative-distributions","chapter":"2 Exploratory data analysis","heading":"2.3.6 Describing and comparing quantitative distributions","text":"review, describing scatterplot—association \ntwo quantitative variables, look four features:FormDirectionStrengthOutliersWhen asked describe compare univariate (single variable) quantitative distributions, look four features:CenterVariabilityShapeOutliersWe can compare quantitative distributions using side--side box plots,\nstacked histograms dot plots. Recall loan50 data set represents sample larger loan data set called loans.\nlarger data set contains information 10,000 loans made Lending Club. Figure 2.19 examines relationship homeownership, loans data can take value rent, mortgage (owns mortgage), , interest_rate. Note homeownership\ncategorical variable interest_rate quantitative variable.\nFigure 2.19: Side--side box plots loan interest rates homeownership category.\nsee immediately features easier discern box plots, others histograms. Shape shown clearly histograms, center (measured median) easy compare across groups side--side box plots.Example 2.9  Using Figure 2.19 write sentences comparing distributions loan amount\nacross different homeownership categories.median loan amount higher mortgage (around $16,000) rent (around $12,000-$13,000). However, variability loan amounts similar across homeownership categories, IQR around $15,000 loans ranging hundred dollars $40,000. see histograms distribution loan amounts skewed right three homeownership categories, means mean loan amount higher median loan amount. apparent outliers mortgage category, rent categories outliers $40,000.","code":""},{"path":"eda.html","id":"robust-statistics","chapter":"2 Exploratory data analysis","heading":"2.3.7 Robust statistics","text":"sample statistics  interest_rate data set affected observation, 26.3%?\nhappened loan instead 15%?\nhappen summary statistics  observation 26.3% even larger, say 35%?\nscenarios plotted alongside original data Figure 2.20, sample statistics computed scenario Table 2.11.\nFigure 2.20: Dot plots original interest rate data two modified data sets.\n\nTable 2.11: comparison median, IQR, mean, standard deviation change value extereme observation original interest data changes.\naffected extreme observations, mean median?standard deviation IQR affected extreme observations?45\nmedian IQR called robust statistics extreme observations little effect values—moving extreme value generally little influence statistics.\nhand, mean standard deviation heavily influenced changes extreme observations, can important situations. Additionally, mean tends get pulled direction distribution’s skewness, skewness little affect median.Example 2.10  median IQR change three scenarios Table 2.11.\nmight case?","code":""},{"path":"eda.html","id":"transforming-data-special-topic","chapter":"2 Exploratory data analysis","heading":"2.3.8 Transforming data (special topic)","text":"data strongly skewed, sometimes transform easier model. transformation rescaling data using function.\nFigure 2.21: Plot : histogram populations US counties. Plot B: histogram log\\(_{10}\\)-transformed county populations. plot, x-value corresponds power 10, e.g. 4 x-axis corresponds \\(10^4 =\\) 10,000. Data 2017.\nExample 2.11  Consider histogram county populations shown left Figure 2.21, shows extreme skew. useful plot?standard transformations may useful strongly right skewed data much data positive clustered near zero.\ninstance, plot logarithm (base 10) county populations results new histogram Figure 2.21.\ndata symmetric, potential outliers appear much less extreme original data set.\nreigning outliers extreme skew, transformations like often make easier build statistical models data.Transformations can also applied one variables scatterplot.\nscatterplot population change 2010 2017 population 2010 shown Figure 2.22.\nfirst scatterplot, ’s hard decipher interesting patterns population variable strongly skewed.\nHowever, apply log\\(_{10}\\) transformation population variable, shown \nFigure 2.22, positive association variables revealed.\nfact, may interested fitting trend line data explore methods around fitting regression lines Chapter 3.\nFigure 2.22: Plot : Scatterplot population change population change. Plot B: ~scatterplot data population size log-transformed.\nTransformations logarithm can useful, .\ninstance, square root (\\(\\sqrt{\\text{original observation}}\\)) inverse (\\(\\frac{1}{\\text{original observation}}\\)) commonly used data scientists.\nCommon goals transforming data see data structure differently, reduce skew, assist modeling, straighten nonlinear relationship scatterplot.","code":""},{"path":"eda.html","id":"mapping-data-special-topic","chapter":"2 Exploratory data analysis","heading":"2.3.9 Mapping data (special topic)","text":"county data set offers many numerical variables plot using dot plots, scatterplots, box plots, miss true nature data.\nRather, encounter geographic data, create intensity map, colors used show higher lower values variable.\nFigures 2.23 2.24 show intensity maps poverty rate percent (poverty), unemployment rate (unemployment_rate), homeownership rate percent (homeownership), median household income (median_hh_income).\ncolor key indicates colors correspond values.\nintensity maps generally helpful getting precise values given county, helpful seeing geographic trends generating interesting research questions hypotheses.Example 2.12  interesting features evident poverty unemployment rate intensity maps?\nFigure 2.23: Plot : Intensity map poverty rate (percent). Plot B: Intensity map unemployment rate (percent).\n\nFigure 2.24: Plot : Intensity map homeownership rate (percent). Plot B: Intensity map median household income ($1000s).\n","code":""},{"path":"eda.html","id":"r-exploratory-data-analysis","chapter":"2 Exploratory data analysis","heading":"2.4 R: Exploratory data analysis","text":"","code":""},{"path":"eda.html","id":"interactive-r-tutorials-1","chapter":"2 Exploratory data analysis","heading":"2.4.1 Interactive R tutorials","text":"Navigate concepts ’ve learned chapter R using following self-paced tutorials.\nneed browser get started!Tutorial 2: Summarizing visualizing dataTutorial 2 - Lesson 1: Visualizing categorical dataTutorial 2 - Lesson 2: Visualizing numerical dataTutorial 2 - Lesson 3: Summarizing statisticsTutorial 2 - Lesson 4: Case studyYou can also access full list tutorials supporting book .","code":""},{"path":"eda.html","id":"r-labs-1","chapter":"2 Exploratory data analysis","heading":"2.4.2 R labs","text":"apply concepts ’ve learned chapter R computational labs walk data analysis case study.Intro data - Flight delaysFull list labs supporting OpenIntro::Introduction Modern Statistics","code":""},{"path":"eda.html","id":"chp2-review","chapter":"2 Exploratory data analysis","heading":"2.5 Chapter 2 review","text":"’re encountered variety univariate (one variable) bivariate (two variable) summary statistics data visualization methods chapter.","code":""},{"path":"eda.html","id":"data-visualization-summary","chapter":"2 Exploratory data analysis","heading":"2.5.1 Data visualization summary","text":"Figure 2.25 presents decision tree deciding type plot appropriate given number types variables. next chapter, ’ll explore model association two quantitative variables, called regression. Chapter 4, ’ll look exploratory data analysis methods two variables.\nFigure 2.25: Decision tree determining appropriate plot given number variables types.\n","code":""},{"path":"eda.html","id":"summary-measures","chapter":"2 Exploratory data analysis","heading":"2.5.2 Summary measures","text":"Though summary measures covered later chapters, Table 2.5.2 provides comprehensive summary measures according type(s) variable(s) summarize.\nTable 2.12: Summary measures different types variables covered textbook Sections appear. binary variable categorical variable two categories.\n","code":""},{"path":"eda.html","id":"notation-summary","chapter":"2 Exploratory data analysis","heading":"2.5.3 Notation summary","text":"field statistics, summary measures summarize sample data called statistics. Numbers summarize entire population parameters. can remember\ndistinction looking first letter term:Statistics summarize Samples.Parameters summarize Populations.typically use Roman letters symbolize statistics (e.g., \\(\\bar{x}\\), \\(\\hat{p}\\)), Greek letters symbolize parameters (e.g., \\(\\mu\\), \\(\\pi\\)).","code":""},{"path":"eda.html","id":"terms-1","chapter":"2 Exploratory data analysis","heading":"2.5.4 Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"cor-reg.html","id":"cor-reg","chapter":"3 Correlation and regression","heading":"3 Correlation and regression","text":"Linear regression powerful statistical technique.\nMany people familiarity regression just reading news, straight lines overlaid scatterplots.\nLinear models can used prediction evaluate whether linear relationship two numerical variables.","code":""},{"path":"cor-reg.html","id":"fit-line-res-cor","chapter":"3 Correlation and regression","heading":"3.1 Fitting a line, residuals, and correlation","text":"’s helpful think deeply line fitting process. section, define form linear model, explore criteria makes good fit, introduce new statistic called correlation.","code":""},{"path":"cor-reg.html","id":"fitting-a-line-to-data","chapter":"3 Correlation and regression","heading":"3.1.1 Fitting a line to data","text":"Figure 3.1 shows two variables whose relationship can modeled perfectly straight line.\nequation line \\(y = 5 + 64.96 x\\).\nConsider perfect linear relationship means: know exact value \\(y\\) just knowing value \\(x\\).\nunrealistic almost natural process.\nexample, took family income (\\(x\\)), value provide useful information much financial support college may offer prospective student (\\(y\\)).\nHowever, prediction far perfect, since factors play role financial support beyond family’s finances.\nFigure 3.1: Requests twelve separate buyers simultaneously placed trading company purchase Target Corporation stock (ticker TGT, December 28th, 2018), total cost shares reported. cost computed using linear formula, linear fit perfect.\nLinear regression statistical method fitting line data relationship two variables, \\(x\\) \\(y\\), can modeled straight line error:\\[ y = \\beta_0 + \\beta_1x + \\varepsilon\\]values \\(\\beta_0\\) \\(\\beta_1\\) represent model’s parameters (\\(\\beta\\) Greek letter beta), error represented \\(\\varepsilon\\) (Greek letter epsilon).\nparameters estimated using data, write point estimates \\(b_0\\) \\(b_1\\).\nuse \\(x\\) predict \\(y\\), usually call \\(x\\) explanatory predictor variable, call \\(y\\) response. also often drop \\(\\epsilon\\) term writing model since main focus often prediction average outcome. \\(\\epsilon\\) term dropped, put “hat” \\(y\\) (\\(\\hat{y}\\)) signal model yields prediction \\(y\\), actual value.rare data fall perfectly straight line.\nInstead, ’s common data appear cloud points,\nexamples shown Figure 3.2.\ncase, data fall around straight line, even none observations fall exactly line.\nfirst plot shows relatively strong downward linear trend, remaining variability data around line minor relative strength relationship \\(x\\) \\(y\\).\nsecond plot shows upward trend , evident, strong first. last plot shows weak downward trend data, slight can hardly notice .\nexamples, uncertainty regarding estimates model parameters, \\(\\beta_0\\) \\(\\beta_1\\).\ninstance, might wonder, move line little, tilt less?\nmove forward chapter, learn criteria line-fitting, also learn uncertainty associated estimates model parameters.\nFigure 3.2: Three data sets linear model may useful even though data fall exactly line.\nalso cases fitting straight line data, even clear relationship variables, helpful.\nOne case shown Figure 3.3 clear relationship variables even though trend linear.\ndiscuss nonlinear trends chapter next, details fitting nonlinear models saved later course.\nFigure 3.3: best fitting line data flat, useful nonlinear case. data physics experiment.\n","code":""},{"path":"cor-reg.html","id":"using-linear-regression-to-predict-possum-head-lengths","chapter":"3 Correlation and regression","heading":"3.1.2 Using linear regression to predict possum head lengths","text":"Brushtail possums marsupial lives Australia, photo\none shown Figure 3.4.\nResearchers captured 104 animals took body measurements releasing animals back wild.\nconsider two measurements: total length possum, head tail, length possum’s head.\nFigure 3.4: common brushtail possum Australia. Photo Greg Schecter, flic.kr/p/9BAFbR, CC 2.0 license.\npossum data can found openintro package.Figure 3.5 shows scatterplot head length (mm) total length (cm) possums.\npoint represents single possum data.\nhead total length variables associated: possums average total length also tend average head lengths.\nrelationship perfectly linear, helpful partially explain connection variables straight line.\nFigure 3.5: scatterplot showing head length total length 104 brushtail possums. point representing possum head length 86.7 mm total length 84 cm highlighted.\nwant describe relationship head length total length variables possum data set using line.\nexample, use total length predictor variable, \\(x\\), predict possum’s head length, \\(y\\).\nfit linear relationship using technology (criteria discussed Section 3.2), Figure 3.6.\nFigure 3.6: reasonable linear model fit represent relationship head length total length.\nequation line \\[\\hat{y} = 43+0.57x.\\]“hat” \\(y\\) used signify estimate.\ncan use line discuss properties possums.\ninstance, equation predicts possum total length 80 cm head length \\[\\hat{y} = 43 + 0.57 \\times 80 = 88.6 \\text{ mm}.\\]estimate may viewed average: equation predicts possums total length 80 cm average head length 88.6 mm.\nAbsent information 80 cm possum, prediction head length uses average reasonable estimate.may variables help us predict head length possum besides length.\nPerhaps relationship little different male possums female possums, perhaps differ possums one region Australia versus another region.\nPlot Figure 3.7 shows relationship total length head length brushtail possums, taking consideration sex.\nMale possums (represented blue triangles) seem larger terms total length head length female possums (represented red circles).\nPlot B Figure 3.7 shows relationship, taking consideration age.\n’s harder tell age changes relationship total length head length possums.\nFigure 3.7: Relationship total length head lentgh brushtail possums, taking consideration sex (Plot ) age (Plot B).\nChapter 4, ’ll learn can include one predictor model.\nget , first need better understand best build simple linear model one predictor.","code":""},{"path":"cor-reg.html","id":"residuals","chapter":"3 Correlation and regression","heading":"3.1.3 Residuals","text":"Residuals leftover variation data accounting model fit:\\[\\text{Data} = \\text{Fit} + \\text{Residual}\\]observation residual, three residuals linear model fit data shown Figure 3.8.\nobservation regression line, residual, vertical distance observation line, positive.\nObservations line negative residuals.\nOne goal picking right linear model residuals small possible.Figure 3.8 almost replica Figure 3.6, three points data highlighted.\nobservation marked red circle small, negative residual -1; observation marked green diamond large residual +7; observation marked yellow triangle moderate residual -4.\nsize residual usually discussed terms absolute value.\nexample, residual observation marked yellow triangle larger observation marked red circle \\(|-4|\\) larger \\(|-1|\\).\nFigure 3.8: reasonable linear model fit represent relationship head length total length, three points residuals highlighted.\nResidual: Difference observed expected.residual \\(^{th}\\) observation \\((x_i, y_i)\\) difference observed response (\\(y_i\\)) response predict based model fit (\\(\\hat{y}_i\\)):\\[e_i = y_i - \\hat{y}_i\\]typically identify \\(\\hat{y}_i\\) plugging \\(x_i\\) model.Example 3.1  linear fit shown Figure 3.8 given \\(\\hat{y} = 43+0.57x\\).\nBased line, formally compute residual observation\n\\((76.0, 85.1)\\).\nobservation marked red circle Figure 3.8.\nCheck earlier visual estimate, \\(-1\\).first compute predicted value observation marked red circle based model:\\[\\hat{y} = 43+0.57x = 43+0.57\\times 76.0 = 86.3mm\\]Next, compute difference actual head length predicted head length:\\[e = y - \\hat{y} = 85.1 -  86.3 = -1.2 mm\\]model’s error \\(e = -1.2\\) mm, close \nvisual estimate \\(-1\\) mm. negative residual indicates linear model overpredicted head length particular possum.model underestimates observation, residual positive negative? overestimates observation?48Compute residuals observation marked green diamond, \\((85.0, 98.6)\\), observation marked yellow triangle, \\((95.5, 94.0)\\), figure using linear relationship \\(\\hat{y} = 43 + 0.57x\\).49Residuals helpful evaluating well linear model fits data set.\noften display residual plot one shown Figure 3.9 regression line Figure 3.8.\nresiduals plotted fitted values \\(x\\)-axis vertical coordinate residual.\ninstance, point \\((85.0, 98.6)\\) (marked green diamond) predicted value 91.45 mm residual 7.15 mm, residual plot placed \\((91.45, 7.15)\\).\nCreating residual plot sort like tipping scatterplot regression line horizontal.\nFigure 3.9: Residual plot model predicting head length total length brushtail possums.\nExample 3.2  One purpose residual plots identify characteristics patterns still apparent data fitting model.\nFigure 3.10 shows three scatterplots linear models first row residual plots second row. Can identify patterns remaining residuals?first data set (first column), residuals show obvious patterns.\nresiduals appear scattered randomly around dashed line represents 0.second data set shows pattern residuals.\ncurvature scatterplot, obvious residual plot.\nuse straight line model data. Instead, advanced technique used.last plot shows little upwards trend, residuals also show obvious patterns.\nreasonable try fit linear model data.\nHowever, unclear whether slope parameter statistically discernible zero.\npoint estimate slope parameter, labeled \\(b_1\\), zero, might wonder just due chance.\naddress sort scenario Chapter 7.\nFigure 3.10: Sample data best fitting lines (top row) corresponding residual plots (bottom row).\n","code":""},{"path":"cor-reg.html","id":"describing-linear-relationships-with-correlation","chapter":"3 Correlation and regression","heading":"3.1.4 Describing linear relationships with correlation","text":"’ve seen plots strong linear relationships others weak linear relationships.\nuseful quantify strength linear relationships statistic.Correlation: strength direction linear relationship.Correlation always takes values -1 1, summary statistic describes strength (magnitude) direction (sign) linear relationship two variables. denote correlation \\(R\\) \\(r\\).can compute correlation using formula, just sample mean standard deviation.\nformula rather complex50,\nlike statistics, generally perform calculations computer calculator.\nFigure 3.11 shows eight plots corresponding correlations. relationship perfectly linear correlation either -1  1.  relationship strong positive, correlation near +1.  strong negative, near -1.  apparent linear relationship variables, correlation near zero.\nFigure 3.11: Sample scatterplots correlations. first row shows variables positive relationshiop, represented trend right. second row shows variables negative trend, large value one variable associated low value .\ncorrelation intended quantify strength direction linear trend.\nNonlinear trends, even strong, sometimes produce correlations reflect strength relationship; see three examples \nFigure 3.12.\nFigure 3.12: Sample scatterplots correlations. case, strong relationship variables, However, relationship nonlinear, correlation relatively weak.\nstraight line good fit data sets represented Figure 3.12.\nTry drawing nonlinear curves plot.\ncreate curve , describe important  fit.51","code":""},{"path":"cor-reg.html","id":"least-squares-regression","chapter":"3 Correlation and regression","heading":"3.2 Least squares regression","text":"Fitting linear models eye open criticism since based individual’s preference. section, use least squares regression rigorous approach.","code":""},{"path":"cor-reg.html","id":"gift-aid-for-freshman-at-elmhurst-college","chapter":"3 Correlation and regression","heading":"3.2.1 Gift aid for freshman at Elmhurst College","text":"section considers family income gift aid data random sample fifty students freshman class Elmhurst College Illinois.\nGift aid financial aid need paid back, opposed loan.\nscatterplot data shown Figure 3.13 along two linear fits.\nlines follow negative trend data; students higher family incomes tended lower gift aid university.\nFigure 3.13: Gift aid family income random sample 50 freshman students Elmhurst College, shown least squares line (solid line) line fit minimizing sum residual magnitudes (dashed line).\ncorrelation positive negative Figure 3.13?52","code":""},{"path":"cor-reg.html","id":"an-objective-measure-for-finding-the-best-line","chapter":"3 Correlation and regression","heading":"3.2.2 An objective measure for finding the best line","text":"begin thinking mean “best”. Mathematically, want line small residuals.\nfirst option may come mind minimize sum residual magnitudes:\\[|e_1| + |e_2| + \\dots + |e_n|\\]accomplish computer program.\nresulting dashed line shown Figure 3.13 demonstrates fit can quite reasonable.\nHowever, common practice choose line minimizes sum squared residuals:\\[e_{1}^2 + e_{2}^2 + \\dots + e_{n}^2\\]line minimizes least squares criterion represented solid line Figure 3.13.\ncommonly called least squares line.\nfollowing three possible reasons choose option instead trying minimize sum residual magnitudes without squaring:commonly used method.Computing least squares line widely supported statistical software.many applications, residual twice large another residual twice bad. example, 4 usually twice bad 2. Squaring residuals accounts discrepancy.first two reasons largely tradition convenience; last reason explains least squares criterion typically helpful.53","code":""},{"path":"cor-reg.html","id":"finding-and-interpreting-the-least-squares-line","chapter":"3 Correlation and regression","heading":"3.2.3 Finding and interpreting the least squares line","text":"Elmhurst data, write equation linear regression model \n\\[aid = \\beta_0 + \\beta_{1}\\times \\textit{family_income} + \\epsilon.\\]\nmodel equation set predict gift aid based student’s family income, useful students considering Elmhurst.\ntwo unknown values \\(\\beta_0\\) \\(\\beta_1\\) parameters linear regression model.least squares regression line, computed based observed data, provides estimates parameters \\(\\beta_0\\) \\(\\beta_1\\):\n\\[\\widehat{aid} = b_0 + b_{1}\\times \\textit{family_income}.\\]\npractice, estimation done using computer way estimates, like sample mean, can estimated using computer calculator.dataset data stored called elmhurst.\nfirst 5 rows dataset given Table 3.1.\nTable 3.1: First five rows elmhurst dataset.\ncan see family income recorded variable called family_income gift aid university recorded variable called gift_aid.\nnow, won’t worry price_paid variable.\nalso note data 2011-2012 academic year, monetary amounts given $1,000s, .e., family income first student data shown Table 3.1 $92,900 received gift aid $21,700. (data source states numbers rounded nearest whole dollar.)Using data, can estimate linear regression line fitting linear model data lm() function R.model output tells us intercept approximately 24.319 slope approximately -0.043.values mean?\nInterpreting parameters regression model often one important steps analysis.Example 3.3  intercept slope estimates Elmhurst data \\(b_0\\) = 24.319 \\(b_1\\) = -0.043.\nnumbers really mean?Interpreting slope parameter helpful almost application.\nadditional $1,000 family income, expect student receive net difference 1,000 \\(\\times\\) (-0.0431) = -$43.10 aid average, .e., $43.10 less.\nNote higher family income corresponds less aid coefficient family income negative model.\nmust cautious interpretation: real association, interpret causal connection variables data observational.\n, increasing student’s family income may cause student’s aid drop. (reasonable contact college ask relationship causal, .e., Elmhurst College’s aid decisions partially based students’ family income.) appropriate interpretation : additional $1,000 family income associated estimated decrease $43.10 aid average.estimated intercept \\(b_0\\) = 24.319 describes average aid student’s family income.\nmeaning intercept relevant application since family income students Elmhurst  $0.\napplications, intercept may little practical value observations \\(x\\) near zero.Interpreting parameters estimated least squares.slope describes estimated difference \\(y\\) variable explanatory variable \\(x\\) case happened one unit larger.intercept describes average outcome \\(y\\) \\(x=0\\) linear model valid way \\(x=0\\), many applications case.Example 3.4  Suppose high school senior considering Elmhurst College.\nCan simply use linear equation estimated calculate financial aid university?may use estimate, though qualifiers approach important.\nFirst, data come one freshman class, way aid determined university may change year year.\nSecond, equation provide imperfect estimate.\nlinear equation good capturing trend data, individual student’s aid perfectly predicted.Statistical software usually used compute least squares line typical output generated result fitting regression models looks like one shown Table 3.2.\nnow focus first column output, lists \\({b}_0\\) \\({b}_1\\).\nChapter 7 dive deeper remaining columns give us information accurate precise values intercept slope calculated sample 50 students estimating population parameters intercept slope students.\nTable 3.2: Summary least squares fit Elmhurst data.\nlike learn using R fit linear models, see Section 3.4.1 interactive R tutorials.","code":"\nlm(gift_aid ~ family_income, data = elmhurst)\n#> \n#> Call:\n#> lm(formula = gift_aid ~ family_income, data = elmhurst)\n#> \n#> Coefficients:\n#>   (Intercept)  family_income  \n#>       24.3193        -0.0431"},{"path":"cor-reg.html","id":"calculating-the-least-squares-regression-line-using-summary-statistics-special-topic","chapter":"3 Correlation and regression","heading":"3.2.3.1 Calculating the least squares regression line using summary statistics (special topic)","text":"alternative way calculating values intercept slope least squares line manual calculations using formulas.\nmethod commonly used practicing statisticians data scientists, useful work first time ’re learning least squares line modeling general.\nCalculating values hand leverages two properties least squares line:slope least squares line can estimated \\[b_1 = \\frac{s_y}{s_x} R \\]\\(R\\) correlation two variables, \\(s_x\\) \\(s_y\\) sample standard deviations explanatory variable response, respectively.\\(\\bar{x}\\) sample mean explanatory variable \\(\\bar{y}\\) sample mean vertical variable, point \\((\\bar{x}, \\bar{y})\\) least squares line.Table 3.3 shows sample means family income gift aid $101,780 $19,940, respectively.\nplot point \\((102, 19.9)\\) Figure 3.13 verify falls least squares line (solid line).\nTable 3.3: Summary statistics family income gift aid.\nNext, formally find point estimates \\(b_0\\) \\(b_1\\) parameters \\(\\beta_0\\) \\(\\beta_1\\).Example 3.5  Using summary statistics Table 3.3, compute slope regression line gift aid family income.Compute slope using summary statistics Table 3.3:\\[b_1 = \\frac{s_y}{s_x} r = \\frac{5.46}{63.2}(-0.499) = -0.0431\\]might recall form line math class, can use find model fit, including estimate \\(b_0\\). Given slope line point line, \\((x_0, y_0)\\), equation line can written \\[y - y_0 = slope\\times (x - x_0)\\]Identifying least squares line summary statistics.identify least squares line summary statistics:Estimate slope parameter, \\(b_1 = (s_y / s_x) R\\).Noting point \\((\\bar{x}, \\bar{y})\\) least squares line, use \\(x_0 = \\bar{x}\\) \\(y_0 = \\bar{y}\\) point-slope equation: \\(y - \\bar{y} = b_1 (x - \\bar{x})\\).Simplify equation, reveal \\(b_0 = \\bar{y} - b_1 \\bar{x}\\).Example 3.6  Using point (102, 19.9) sample means slope estimate \\(b_1 = -0.0431\\), find least-squares line predicting aid based family income.Apply point-slope equation using \\((102, 19.9)\\) slope \\(b_1 = -0.0431\\):\\[\\begin{aligned}\ny - y_0  &= b_1 (x - x_0) \\\\\ny - 19.9 &= -0.0431(x - 102)\n\\end{aligned}\\]Expanding right side adding 19.9 side, equation simplifies:\\[\\begin{aligned}\n\\widehat{aid} = 24.3 - 0.0431 \\times \\textit{family_income}\n\\end{aligned}\\]replaced \\(y\\) \\(\\widehat{aid}\\) \\(x\\) family_income put equation context.\nfinal equation always include “hat” variable predicted, whether generic “\\(y\\)” named variable like “\\(aid\\)”.","code":""},{"path":"cor-reg.html","id":"extrapolation-is-treacherous","chapter":"3 Correlation and regression","heading":"3.2.4 Extrapolation is treacherous","text":"blizzards hit East Coast winter, proved satisfaction global warming fraud. snow freezing cold. alarming trend, temperatures spring risen. Consider : February \\(6^{th}\\) 10 degrees. Today hit almost 80. rate, August 220 degrees. clearly folks climate debate rages .54Stephen Colbert\nApril 6th, 2010Linear models can used approximate relationship two variables. However, models real limitations.\nLinear regression simply modeling framework.\ntruth almost always much complex simple line.\nexample, know data outside limited window behave.Example 3.7  Use model \\(\\widehat{aid} = 24.3 - 0.0431 \\times \\textit{family_income}\\) estimate aid another freshman student whose family income $1 million.want calculate aid family $1 million income.\nNote model, represented 1,000 since data $1,000s.\\[24.3 - 0.0431 \\times 1000 = -18.8 \\]model predicts student -$18,800 aid (!).\nHowever, Elmhurst College offer negative aid select students pay extra top tuition attend.Applying model estimate values outside realm original data called extrapolation.\nGenerally, linear model approximation real relationship two variables.\nextrapolate, making unreliable bet approximate linear relationship valid places analyzed.","code":""},{"path":"cor-reg.html","id":"describing-the-strength-of-a-fit","chapter":"3 Correlation and regression","heading":"3.2.5 Describing the strength of a fit","text":"evaluated strength linear relationship two variables earlier using correlation, \\(R\\). However, common explain strength linear fit using \\(R^2\\), called R-squared.\nprovided linear model, might like describe closely data cluster around linear fit.\\(R^2\\) linear model describes amount variation response explained least squares line.\nexample, consider Elmhurst data, shown Figure 3.13.\nvariance response variable, aid received, \\(s_{aid}^2 \\approx 29.8\\) million.\nHowever, apply least squares line, model reduces uncertainty predicting aid using student’s family income.\nvariability residuals describes much variation remains using model: \\(s_{_{RES}}^2 \\approx 22.4\\) million.\nshort, reduction \n\\[\\frac{s_{aid}^2 - s_{_{RES}}^2}{s_{aid}^2}\n  = \\frac{29800 - 22400}{29800}\n  = \\frac{7500}{29800}\n  \\approx 0.25\\]\n25% data’s variation using information family income predicting aid using linear model.\ncorresponds exactly R-squared value:\\[R = -0.499 \\rightarrow R^2 = 0.25\\]\nsquared correlation coefficient, \\(R^2\\), also called coefficient determination.Coefficient determination: proportion variability response explained model.Since \\(R\\) always -1 1, \\(R^2\\) always 0 1. statistic called coefficient determination measures proportion variation response variable, \\(y\\), can explained linear model predictor \\(x\\).Example 3.8  Examine scatterplot head length (mm) versus total length (cm) possums Figure 3.6.\ncorrelation two variables \\(R = 0.69\\).\nFind interpret coefficient determination.find \\(R^2\\), square correlation: \\(R^2 = (0.69)^2 = 0.48\\).\ntells us 48% variation possum head length can explained total length.\nvisualized Figure 3.14.\nFigure 3.14: 104 possums, range head lengths 103 \\(-\\) 83 = 20 mm. However, among possums total length (e.g., 85 cm), range head lengths reduced 10 mm, 50% reduction, matches \\(R^2 = 0.48\\), 48%.\nlinear model strong negative relationship correlation -0.97, much variation response explained explanatory variable?55More generally, \\(R^2\\) can calculated ratio measure variability around line divided measure total variability.Sums squares measure variability \\(y\\).can measure variability \\(y\\) values far tend fall mean, \\(\\bar{y}\\). define value total sum squares,\\[\nSST = (y_1 - \\bar{y})^2 + (y_2 - \\bar{y})^2 + \\cdots + (y_n - \\bar{y})^2.\n\\]Left-variability \\(y\\) values know \\(x\\) can measured sum squared errors, sum squared residuals56,\\[\nSSE = (y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\cdots + (y_n - \\hat{y}_n)^2 = e_{1}^2 + e_{2}^2 + \\dots + e_{n}^2\n\\]coefficient determination can calculated \\[\nR^2 = \\frac{SST - SSE}{SST} = 1 - \\frac{SSE}{SST}\n\\]Example 3.9  Among 104 possums, total variability head length (mm) \\(SST = 1315.2\\)57. sum squared residuals \\(SSE = 687.0\\). Find \\(R^2\\).Since know \\(SSE\\) \\(SST\\), can calculate \\(R^2\\) \\[\nR^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{687.0}{1315.2} = 0.48,\n\\]\nvalue found squared correlation: \\(R^2 = (0.69)^2 = 0.48\\).","code":""},{"path":"cor-reg.html","id":"categprical-predictor-two-levels","chapter":"3 Correlation and regression","heading":"3.2.6 Categorical predictors with two levels (special topic)","text":"Categorical variables also useful predicting outcomes.\nconsider categorical predictor two levels (recall level category).\n’ll consider Ebay auctions video game, Mario Kart Nintendo Wii, total price auction condition game recorded. want predict total price based game condition, takes values used new.mariokart data can found openintro package.plot auction data shown Figure 3.15.\nNote original dataset contains Mario Kart games sold prices $100 analysis limited focus 141 Mario Kart games sold $100.\nFigure 3.15: Total auction prices video game Mario Kart, divided used (\\(x = 0\\)) new (\\(x = 1\\)) condition games. least squares regression line also shown.\nincorporate game condition variable regression equation, must convert categories numerical form.\nusing indicator variable called condnew, takes value 1 game new 0 game used.\nUsing indicator variable, linear model may written \\[\\widehat{price} = \\beta_0 + \\beta_1 \\times condnew\\]parameter estimates given Table 3.4.\nTable 3.4: Least squares ression summary final auction price condition game.\nUsing values Table 3.4, model equation can summarized \\[\\widehat{price} = 42.871 + 10.90 \\times condnew\\]Example 3.10  Interpret two parameters estimated model price Mario Kart eBay auctions.\nintercept estimated price condnew takes value 0, .e. game used condition.\n, average selling price used version game $42.87.slope indicates , average, new games sell $10.90 used games.Interpreting model estimates categorical predictors.estimated intercept value response variable first category (.e. category corresponding indicator value  0).\nestimated slope average change response variable two categories.’ll elaborate topic Chapter 4, examine influence many predictor variables simultaneously using multiple regression.","code":""},{"path":"cor-reg.html","id":"outliers-in-regression","chapter":"3 Correlation and regression","heading":"3.3 Outliers in linear regression","text":"section, identify criteria determining outliers important influential.\nOutliers regression observations fall far cloud points.\npoints especially important can strong influence least squares line.","code":""},{"path":"cor-reg.html","id":"types-of-outliers","chapter":"3 Correlation and regression","heading":"3.3.1 Types of outliers","text":"Example 3.11  three plots shown Figure 3.16 along least squares line residual plots.\n scatterplot residual plot pair, identify outliers note influence least squares line.\nRecall outlier point doesn’t appear belong vast majority points.: one outlier far points, though appears slightly influence  line.: one outlier far points, though appears slightly influence  line.B: one outlier right, though quite close least squares line, suggests wasn’t influential.B: one outlier right, though quite close least squares line, suggests wasn’t influential.C: one point far away cloud, outlier appears pull least squares line right; examine line around primary cloud doesn’t appear fit  well.C: one point far away cloud, outlier appears pull least squares line right; examine line around primary cloud doesn’t appear fit  well.\nFigure 3.16: Three plots, least squares line residual plot. data sets least one outlier.\nExample 3.12  three plots shown Figure 3.17 along least squares line residual plots.\nprevious exercise,  scatterplot residual plot pair, identify outliers note influence least squares line.\nRecall outlier point doesn’t appear belong vast majority points.D: primary cloud small secondary cloud four outliers. secondary cloud appears influencing line somewhat strongly, making least square line fit poorly almost everywhere. might interesting explanation dual clouds, something investigated.D: primary cloud small secondary cloud four outliers. secondary cloud appears influencing line somewhat strongly, making least square line fit poorly almost everywhere. might interesting explanation dual clouds, something investigated.E: obvious trend main cloud points outlier right appears largely control slope least squares line.E: obvious trend main cloud points outlier right appears largely control slope least squares line.F: one outlier far cloud. However, falls quite close least squares line appear influential.F: one outlier far cloud. However, falls quite close least squares line appear influential.\nFigure 3.17: Three plots, least squares line residual plot. data sets least one outlier.\nExamine residual plots Figures 3.16 3.17.\nprobably find trend main clouds  Plots C, D, E.\ncases, outliers influenced slope least squares lines.\n Plot E, data clear trend assigned line large trend simply due one outlier (!).Leverage.Points fall horizontally away center cloud tend pull harder line, call points high leverage.Points fall horizontally far line points high leverage; points can strongly influence slope least squares line.\none high leverage points appear actually invoke influence slope line – Plots C, D, E Figures 3.16 3.17 – call influential point.Influential point.point influential , fitted line without , influential point unusually far least squares line.\nInfluential points tend pull slope line seen fit regression line without .tempting remove outliers. Don’t without good reason.\nModels ignore exceptional (interesting) cases often perform poorly.\ninstance, financial firm ignored largest market swings – “outliers” – soon go bankrupt making poorly thought-investments.","code":""},{"path":"cor-reg.html","id":"r-correlation-and-regression","chapter":"3 Correlation and regression","heading":"3.4 R: Correlation and regression","text":"","code":""},{"path":"cor-reg.html","id":"intro-linear-models-r-tutorial","chapter":"3 Correlation and regression","heading":"3.4.1 Interactive R tutorials","text":"Navigate concepts ’ve learned chapter R using following self-paced tutorials.\nneed browser get started!Tutorial 3: Introduction linear modelsTutorial 3 - Lesson 1: Visualizing two variablesTutorial 2 - Lesson 2: CorrelationTutorial 2 - Lesson 3: Simple linear regressionTutorial 2 - Lesson 4: Interpreting regression modelsTutorial 2 - Lesson 5: Model fitYou can also access full list tutorials supporting book .","code":""},{"path":"cor-reg.html","id":"r-labs-2","chapter":"3 Correlation and regression","heading":"3.4.2 R labs","text":"apply concepts ’ve learned chapter R computational labs walk data analysis case study.Introduction linear regression - Human Freedom IndexFull list labs supporting OpenIntro::Introduction Modern Statistics","code":""},{"path":"cor-reg.html","id":"chp3-review","chapter":"3 Correlation and regression","heading":"3.5 Chapter review","text":"","code":""},{"path":"cor-reg.html","id":"terms-2","chapter":"3 Correlation and regression","heading":"3.5.1 Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"mult-reg.html","id":"mult-reg","chapter":"4 Multivariable models","heading":"4 Multivariable models","text":"principles simple linear regression lay foundation sophisticated regression models used wide range challenging settings.\nchapter, explore idea “multivariable thinking” – investigating multiple variables interact response variable – examples.\nMultiple regression, introduces possibility one predictor linear model, logistic regression, technique predicting categorical outcomes two levels, presented special topics covered course.","code":""},{"path":"mult-reg.html","id":"gapminder-world","chapter":"4 Multivariable models","heading":"4.1 Gapminder world","text":"Gapminder “fact tank” uses publicly available world data produce data visualizations teaching resources global development. use excerpt data explore relationships among world health metrics across countries regions years 1952 2007.First, let’s look relationship Gross Domestic Product (GDP) per capita (measure wealth country) Life Expectancy (years) year 2007 Figure 4.1.\nFigure 4.1: Scatterplot displaying relationship Life Expectancy GDP per capita year 2007. Note GDP per capita plotted log scale. dot represent?58\n\nFigure 4.2: Scatterplot displaying relationship Life Expectancy GDP per capita region year 2007. Note GDP per capita plotted log scale. Regression lines continent added.\nExample 4.1  relationship GDP per capita life expectancy differ across regions world?Yes. Looking Figure 4.2, five regression lines differing slopes, telling us estimated change life expectancy given increase GDP per capita differs across countries. Americas Oceania, life expectancy seems rise faster GDP per capita three regions. case, say GDP per capita interacts continent relationship life expectancy.Interaction two explanatory variables.relationship explanatory variable \\(x\\) response variable \\(y\\) changes different levels another variable \\(z\\), say \\(x\\) \\(z\\) interact relationship \\(y\\).\\(x\\) \\(y\\) quantitative, \\(z\\) categorical, Figure 4.2 – \\(x\\) = GDP per capita, \\(y\\) = life expectancy, \\(z\\) = continent – different regression lines level \\(z\\) parallel slopes, say \\(x\\) \\(z\\) interact. slopes parallel, interaction exists \\(x\\) \\(z\\).far, ’ve explored relationships three variables, visualize relationships five variables?59Let’s add another variable plot – population. aesthetic visual property objects plot. variable mapped aesthetic. possible aesthetics whether used quantitative categorical variables listed Table 4.1.\nTable 4.1: Examples aesthetics types variables mapped aesthetics.\nFigure 4.3, quantitative variables GDP per capita, life expectancy, population mapped aesthetics: position \\(x\\)-axis, position \\(y\\)-axis, population, respectively. categorical variable Region mapped color. Explore individual countries hovering points.\nFigure 4.3: Scatterplot displaying relationship four variables year 2007: GDP per capita (x-axis), Life Expectancy (y-axis), Population (size), Region (color).]\npattern compare happening 1952 (see Figure 4.4)?\nFigure 4.4: Scatterplot displaying relationship four variables year 1952: GDP per capita (x-axis), Life Expectancy (y-axis), Population (size), Region (color).\ncan visualize relationships among four variables plots (three quantitative variables x- y-axes size, categorical variable color). even add fifth variable using another aesthetic, like using shape represent popular religion country. visualize happens across time? Hans Rosling answer dynamic visualization. Click image watch.","code":""},{"path":"mult-reg.html","id":"simpsons-paradox-revisited","chapter":"4 Multivariable models","heading":"4.2 Simpson’s Paradox, revisited","text":"Simpson’s Paradox introduced Section 2.1.4 example race capital punishment. example, three variables interest categorical. section, present another example paradox using three quantitative variables.1993, respected political essayist George , wrote following criticism spending public education United States.“10 states lowest per pupil spending included four – North Dakota, South Dakota, Tennessee, Utah – among 10 states top SAT scores. one 10 states highest per pupil expenditures – Wisconsin – among 10 states highest SAT scores. New Jersey highest per pupil expenditures, astonishing $10,561, teachers’ unions elsewhere try use negotiating benchmark. New Jersey’s rank regarding SAT scores? Thirty-ninth… fact quality schools… [fails correlate] education appropriations effect teacher unions’ insistence money crucial variable.”— George F. , September 12, 1993, “Meaningless Money Factor,” Washington Post, C7.George based claim state expenditures, average SAT scores, education-based variables. data data set SAT60, first six rows displayed Table 4.2. Variables data set described Table 4.3\nTable 4.2: Six rows SAT data set.\n\nTable 4.3: Variables descriptions SAT data set.\nMr. claims expenditure per pupil negative correlation average SAT scores across states. true? Indeed, correlation expend sat equal \\(r\\) = -0.381, relationship two variables shown Figure 4.5. Hover point view data particular State.\nFigure 4.5: Expenditure per pupil average daily attendance public elementary secondary schools ($1000) verses average SAT score 50 states plus District Columbia school year 1994-1995.\nmay seem surprising, remember – observational data. conclude, George , decreasing expenditures increase SAT scores. fact, one clear confounding variable data: percentage eligible students taking SAT.Example 4.2  confounding variables may present study? determine whether variable confounding relationship school expenditures SAT scores?states time data collected, common take ACT SAT. students states, wanted go state school, need take ACT. However, wanted attend college another state, might take SAT. Thus, percent students taking SAT state, frac, confounding variable.order frac confounding, needs associated explanatory variable, expend, well response variable, sat. One look scatterplots correlation frac expend, frac sat, determine frac confounding relationship expend sat.Scatterplots expend versus frac sat versus frac displayed Figure 4.6. correlation expend frac 0.593, correlation sat frac -0.887.\nFigure 4.6: Expenditure per pupil average daily attendance public elementary secondary schools ($1000) average SAT score plotted percent students taking SAT 50 states plus District Columbia school year 1994-1995.\nNow ’ve determined frac confounding variable, let’s examine modifies relationship expend sat. Since hard visualize three quantitative variables – 3-D scatterplots difficult visualize – let’s bin variable frac three groups. States fewer 15% eligible students taking SAT classified low percentage. States 15% - 55% eligible students taking SAT classified medium states 55% eligible students taking SAT called high. Next, fit separate regression lines group. model shown Figure 4.7.\nFigure 4.7: Average SAT score plotted school expenditures per pupil, categorized Low (\\(<\\) 15%), Medium (15-55%), High (\\(>\\) 55%) percent students taking SAT.\nFigure 4.7 demonstrates overall negative correlation SAT scores expenditures disappears, even turns slightly positive, examine relationship within states similar fractions students taking SAT.data exhibit Simpson’s Paradox?61","code":""},{"path":"mult-reg.html","id":"regression-multiple-predictors","chapter":"4 Multivariable models","heading":"4.3 Multiple regression (special topic)","text":"principles simple linear regression lay foundation sophisticated regression models used wide range challenging settings.\nsection, explore multiple regression, introduces possibility one predictor linear model.Multiple regression extends simple two-variable regression case still one response many predictors (denoted \\(x_1\\), \\(x_2\\), \\(x_3\\), ...). method motivated scenarios many variables may simultaneously connected output.consider data loans peer--peer lender, Lending Club, data set first encountered Chapter 1.\nloan data includes terms loan well information borrower.\noutcome variable like better understand interest rate assigned loan.\ninstance, characteristics held constant, matter much debt someone already ? matter income verified?\nMultiple regression help us answer questions.data set includes results 10,000 loans, ’ll looking subset available variables, new saw earlier chapters.\nfirst six observations data set shown Table 4.4, descriptions variable shown Table 4.5.\nNotice past bankruptcy variable (bankruptcy) indicator variable, takes value 1 borrower past bankruptcy record 0 .\nUsing indicator variable place category name allows variables \ndirectly used regression.\nTwo variables categorical (verified_income issue_month), can take one different non-numerical values; ’ll discuss handled model Section 4.3.1.data can found openintro package: loans_full_schema. Based data dataset created new variables: credit_util calculated total credit utilized divided total credit limit bankruptcy turns number bankruptcies indicator variable (0 bankrupties 1 least 1 bankruptcies). refer modified dataset loans.\nTable 4.4: First six rows loans_full_schema data set.\n\nTable 4.5: Variables descriptions loans data set.\n","code":""},{"path":"mult-reg.html","id":"ind-and-cat-predictors","chapter":"4 Multivariable models","heading":"4.3.1 Indicator and categorical predictors","text":"Let’s start fitting linear regression model interest rate single predictor indicating whether person bankruptcy record:\\[\\widehat{\\texttt{interest_rate}} = 12.33 + 0.74 \\times bankruptcy\\]Results model shown Table 4.6.\nTable 4.6: Summary linear model predicting interest rate based whether borrower bankruptcy record. Degrees freedom model 9998.\nExample 4.3  Interpret coefficient past bankruptcy variable model. coefficient significantly different 0?variable takes one two values: 1 borrower bankruptcy history 0 otherwise. slope 0.74 means model predicts 0.74%\nhigher interest rate borrowers bankruptcy \nrecord.\n(See Section 3.2.6 review interpretation two-level categorical predictor variables.)\nExamining regression output Table 4.6, can see p-value close zero, indicating strong evidence coefficient different zero using simple one-predictor model.Suppose fit model using 3-level categorical variable, verified_income.\noutput software shown Table 4.7.\nregression output provides multiple rows variable.\nrow represents relative difference level verified_income.\nHowever, missing one levels: Verified.\nmissing level called reference level represents default level levels measured .\nTable 4.7: Summary linear model predicting interest rate based whether borrower’s income source amount verified. predictor three levels, results 2 rows regression output.\nExample 4.4  write equation regression model?equation regression model may written model two predictors:\\[\\widehat{\\texttt{interest_rate}} = 11.10 + 1.42 \\times \\text{verified_income}_{\\text{Source Verified}} + 3.25 \\times \\text{verified_income}_{\\text{Verified}}\\]use notation \\(\\text{variable}_{\\text{level}}\\) represent indicator variables categorical variable takes particular value.\nexample, \\(\\text{verified_income}_{\\text{Source Verified}}\\) take value 1 \nloan, take value 0 otherwise.\nLikewise, \\(\\text{verified_income}_{\\text{Verified}}\\) take value 1 took \nvalue verified 0 took value.notation \\(\\text{variable}_{\\text{level}}\\) may feel bit confusing.\nLet’s figure use equation level verified_income variable.Example 4.5  Using model predicting interest rate income verification type, compute average interest rate borrowers whose income source amount unverified.verified_income takes value Verified, indicator functions equation linear model set 0:\\[\\widehat{\\texttt{interest_rate}} = 11.10 + 1.42 \\times 0 + 3.25 \\times 0 = 11.10\\]average interest rate borrowers 11.1%.\nlevel coefficient reference value, indicators levels variable drop .Example 4.6  Using model predicting interest rate income verification type, compute average interest rate borrowers whose income source amount unverified.verified_income takes value Source Verified, corresponding variable takes value 1 (\\(\\text{verified_income}_{\\text{Verified}}\\)) 0:\\[\\widehat{\\texttt{interest_rate}} = = 11.10 + 1.42 \\times 1 + 3.25 \\times 0 = 12.52\\]average interest rate borrowers 12.52%.Compute average interest rate borrowers whose income source amount verified.62Predictors several categories.fitting regression model categorical variable \\(k\\) levels \\(k > 2\\), software provide coefficient \\(k - 1\\) levels.\nlast level receive coefficient, , coefficients listed levels considered relative reference level.Interpret coefficients model.63The higher interest rate borrowers verified income source amount surprising.\nIntuitively, ’d think loan look less risky borrower’s income verified.\nHowever, note situation may complex, may confounding variables didn’t account .\nexample, perhaps lender require borrowers poor credit verify income.\n, verifying income data set might signal concerns borrower rather reassurance borrower pay back loan.\nreason, borrower deemed higher risk, resulting higher interest rate.\n(confounding variables might explain counter-intuitive relationship suggested model?)much larger interest rate expect borrower verified income source amount vs borrower whose income source verified?64","code":""},{"path":"mult-reg.html","id":"many-predictors-in-a-model","chapter":"4 Multivariable models","heading":"4.3.2 Many predictors in a model","text":"world complex, can helpful consider many factors statistical modeling.\nexample, might like use full context borrower predict interest rate receive rather using single variable.\nstrategy used multiple regression.\nremain cautious making causal interpretations using multiple regression observational data, models common first step gaining insights providing evidence causal connection.want construct model accounts past bankruptcy whether borrower income source amount verified, simultaneously accounts variables loans data set: verified_income, debt_to_income, credit_util, bankruptcy, term, issue_month, credit_checks.\\[\\begin{align*}\n\\widehat{\\texttt{interest_rate}}\n    &= \\beta_0 +\n        \\beta_1\\times \\texttt{verified_income}_{\\texttt{Source Verified}} +\n        \\beta_2\\times \\texttt{verified_income}_{\\texttt{Verified}} \\\\\n    &\\qquad\\  +\n        \\beta_3\\times \\texttt{debt_to_income} \\\\\n    &\\qquad\\  +\n        \\beta_4 \\times \\texttt{credit_util} \\\\\n    &\\qquad\\  +\n        \\beta_5 \\times \\texttt{bankruptcy} \\\\\n    &\\qquad\\  +\n        \\beta_6 \\times \\texttt{term} \\\\\n    &\\qquad\\  +\n        \\beta_7 \\times \\texttt{issue_month}_{\\texttt{Jan-2018}} +\n        \\beta_8 \\times \\texttt{issue_month}_{\\texttt{Mar-2018}} \\\\\n    &\\qquad\\  +\n        \\beta_9 \\times \\texttt{credit_checks}\n\\end{align*}\\]equation represents holistic approach modeling variables simultaneously.\nNotice two coefficients verified_income also two coefficients issue_month, since 3-level categorical variables.estimate parameters \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\cdots\\), \\(\\beta_9\\) way case single predictor.\nselect \\(b_0\\), \\(b_1\\), \\(b_2\\), \\(\\cdots\\), \\(b_9\\) minimize sum squared residuals:\\[SSE = e_1^2 + e_2^2 + \\dots + e_{10000}^2 = \\sum_{=1}^{10000} e_i^2 = \\sum_{=1}^{10000} \\left(y_i - \\hat{y}_i\\right)^2\\]\\(y_i\\) \\(\\hat{y}_i\\) represent observed interest rates estimated values according model, respectively.\n10,000 residuals calculated, one observation.\ntypically use computer minimize sum squares compute point estimates, shown sample output \nTable 4.8.\nUsing output, identify point estimates \\(b_i\\) \\(\\beta_i\\), just one-predictor case.\nTable 4.8: Output regression model, interest rate outcome variables listed predictors. Degrees freedom model 9990.\nMultiple regression model.multiple regression model linear model many predictors. general, write model \\[\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k\\]\\(k\\) predictors.\nalways estimate \\(\\beta_i\\) parameters using statistical software.Example 4.7  Write regression model using point estimates Table 4.8.\nmany predictors model?fitted model interest rate given :\\[\\begin{align*}\n\\widehat{\\texttt{interest_rate}}\n    &= 1.925 +\n        0.975 \\times \\texttt{verified_income}_{\\texttt{Source Verified}} \\\\\n    &\\qquad\\  +\n        2.537 \\times \\texttt{verified_income}_{\\texttt{Verified}} \\\\\n    &\\qquad\\  +\n        0.021 \\times \\texttt{debt_to_income} \\\\\n    &\\qquad\\  +\n        4.896 \\times \\texttt{credit_util} \\\\\n    &\\qquad\\  +\n        0.386 \\times \\texttt{bankruptcy} \\\\\n    &\\qquad\\  +\n        0.154 \\times \\texttt{term} \\\\\n    &\\qquad\\  +\n        0.028 \\times \\texttt{issue_month}_{\\texttt{Jan-2018}} \\\\\n    &\\qquad\\  -\n        0.040 \\times \\texttt{issue_month}_{\\texttt{Mar-2018}} \\\\\n    &\\qquad\\  +\n        0.228 \\times \\texttt{credit_checks}\n\\end{align*}\\]count number predictor coefficients, get effective number predictors model: \\(k = 9\\).\nNotice categorical predictor counts two, two levels shown model.\ngeneral, categorical predictor \\(p\\) different levels represented \\(p - 1\\) terms multiple regression model.\\(\\beta_4\\), coefficient variable , represent? point estimate  \\(\\beta_4\\)?65Compute residual first observation Table 4.4 page using full model.66Example 4.8  estimated coefficient Section 4.3.1 \\(b_4 = 0.74\\) standard error \\(SE_{b_1} = 0.15\\) using simple linear regression.\ndifference estimate estimated coefficient 0.39 multiple regression setting?examined data carefully, see predictors correlated.\ninstance, estimated connection outcome interest_rate predictor bankruptcy using simple linear regression, unable control variables like whether borrower income verified, borrower’s debt--income ratio, variables.\noriginal model constructed vacuum consider full context.\ninclude variables, underlying unintentional bias missed variables reduced eliminated.\ncourse, bias can still exist confounding variables.previous example describes common issue multiple regression: correlation among predictor variables.\nsay two predictor variables (pronounced co-linear) correlated, collinearity complicates model estimation.\nimpossible prevent collinearity arising observational data, experiments usually designed prevent predictors collinear.estimated value intercept 1.925, one might tempted make interpretation coefficient, , model’s predicted price variables take value zero: income source verified, borrower debt (debt--income credit utilization zero), .\nreasonable?\nvalue gained making interpretation?67","code":""},{"path":"mult-reg.html","id":"interactive-r-tutorials-2","chapter":"4 Multivariable models","heading":"4.3.3 Interactive R tutorials","text":"Explore additional topics multiple regression logistic regression R using following self-paced tutorials.\nneed browser get started!Tutorial 4: Multiple logistic regressionTutorial 4 - Lesson 1: Parallel slopesTutorial 4 - Lesson 2: Evaluating extending parallel slopes modelTutorial 4 - Lesson 3: Multiple regressionTutorial 4 - Lesson 4: Logistic regressionTutorial 4 - Lesson 5: Case study - Italian restaurants NYCYou can also access full list tutorials supporting book .","code":""},{"path":"mult-reg.html","id":"r-labs-3","chapter":"4 Multivariable models","heading":"4.3.4 R labs","text":"apply concepts ’ve learned chapter R computational labs walk data analysis case study.Multiple linear regression - Grading professorFull list labs supporting OpenIntro::Introduction Modern Statistics","code":""},{"path":"mult-reg.html","id":"chp4-review","chapter":"4 Multivariable models","heading":"4.4 Chapter 4 review","text":"","code":""},{"path":"mult-reg.html","id":"terms-3","chapter":"4 Multivariable models","heading":"4.4.1 Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"inference-cat.html","id":"inference-cat","chapter":"5 Inference for categorical data","heading":"5 Inference for categorical data","text":"Statistical inference primarily concerned understanding quantifying uncertainty parameter estimates—, variable sample statistic\nsample sample?\nequations details change depending setting, foundations inference throughout statistics. begin chapter discussion foundations inference, introduce two primary vehicles inference: hypothesis test confidence interval.rest chapter focuses statistical inference categorical data. two data structures detail :one binary variable, summarized using single proportion, andtwo binary variables, summarized using difference (ratio) two proportions.Throughout book far, worked data variety contexts.\nlearned summarize visualize data well visualize multiple variables time.\nSometimes data set hand represents entire research question.\noften , data collected answer research question larger group data (hopefully) representative subset.may agree almost always variability data (one data set identical second data set even collected population using methods).\nHowever, quantifying variability data neither obvious easy (different one data set another?).Example 5.1  Suppose professor splits students class two groups: students left students right. \\(\\hat{p}_{_L}\\) \\(\\hat{p}_{_R}\\) represent proportion students Apple product left right, respectively, surprised \\(\\hat{p}_{_L}\\) exactly equal \\(\\hat{p}_{_R}\\)?Studying randomness form key focus statistics.\nThroughout chapter, follow, provide two different approaches quantifying variability inherent data: simulation-based methods theory-based methods (mathematical models).\nUsing methods provided future chapters, able draw conclusions beyond data set hand research questions larger populations.","code":""},{"path":"inference-cat.html","id":"inf-foundations","chapter":"5 Inference for categorical data","heading":"5.1 Foundations of inference","text":"Given results seen sample, process determining can infer \npopulation based sample results called statistical inference. Statistical inferential methods enable us understand quantify uncertainty sample results. Statistical inference helps us answer two questions population:strong evidence effect?large effect?first question answered hypothesis test, second addressed confidence interval.Statistical inference practice making decisions conclusions data context uncertainty.\nErrors occur, just like rare events, data set hand might lead us wrong conclusion.\ngiven data set may always lead us correct conclusion, statistical inference gives us tools control evaluate often errors occur.","code":""},{"path":"inference-cat.html","id":"Martian","chapter":"5 Inference for categorical data","heading":"5.1.1 Motivating example: Martian alphabet","text":"well can humans distinguish one “Martian” letter another? Figure 5.1\ndisplays two Martian letters—one Kiki another Bumba. think\nKiki think Bumba?69\nFigure 5.1: Two Martian letters: Bumba Kiki. think letter Bumba left right?70\nimage question presented introductory statistics class \n38 students. class, 34 students correctly identified Bumba Martian letter left. Assuming can’t read Martian, result surprising?One two possibilities occurred:can’t read Martian, results just occurred chance.can read Martian, results reflect ability.decide two possibilities, calculate probability\nobserving results randomly selected sample 38 students, \nassumption students just guessing. probability low,\n’d reason reject first possibility favor second.\ncan calculate probability using one two methods:Simulation-based method: simulate lots samples (Classes) 38 students assumption \nstudents just guessing, calculate proportion \nsimulated samples saw 34 students guessing correctly, orTheory-based method: develop mathematical model sample proportion \nscenario use model calculate probability.situation—since “just guessing” means 50% chance guessing correctly—simulate sample 38 students’ guesses flipping coin 38 times counting number times lands heads. Using computer repeat process 1,000 times, create dot plot Figure 5.2.\nFigure 5.2: dot plot 1,000 sample proportions; calculated flipping coin 38 times calculating proportion times coin landed heads. None 1,000 simulations sample proportion least 89%, proportion observed study.\nNone simulated samples produce 34 38 correct guesses! , students just guessing, nearly impossible observe 34 correct guesses sample 38 students. Given low probability, plausible possibility 2. can read Martian, results reflect ability. ’ve just completed first hypothesis test!Now, obviously one can read Martian, realistic possibility humans tend choose Bumba left often right—greater 50% chance choosing Bumba letter left. Even though may think ’re guessing just chance, preference Bumba left. turns explanation preference called synesthesia, tendency humans correlate sharp sounding noises (e.g., Kiki) sharp looking images.72But wait—’re done! evidence humans tend prefer Bumba left, much? answer , need confidence interval—interval plausible values true probability humans select Bumba left letter. width interval determined variable sample proportions sample sample. turns , mathematical model variability explore later chapter. now, let’s take standard deviation simulated sample proportions estimate variability: 0.08. Since simulated distribution proportions bell-shaped, know 95% sample proportions fall within two standard deviations true proportion, can add subtract margin error sample proportion calculate approximate 95% confidence interval73:\n\\[\n\\frac{34}{38} \\pm 2\\times 0.08 = 0.89 \\pm 0.16 = (0.73, 1)\n\\]\nThus, based data, 95% confident probability human guesses Bumba left somewhere 73% 100%.","code":""},{"path":"inference-cat.html","id":"var-stat","chapter":"5 Inference for categorical data","heading":"5.1.2 Variability in a statistic","text":"two approaches modeling statistic, sample proportion, may vary sample sample.\nMartian alphabet example, used simulation-based approach model\nvariability, using standard deviation simulated distribution sample proportions quantitative measure sampling variability. Simulation-based methods include randomization tests bootstrapping methods use textbook. can also use theory-based approach—one makes use \nmathematical modeling—involves normal \\(t\\) probability distributions.theory-based methods discussed book work (certain conditions) important theorem Statistics called Central Limit Theorem.Central Limit Theorem.example perfect normal distribution shown Figure 5.3.\nmean (center) standard deviation (variability) may change different scenarios, general shape remains roughly intact.\nFigure 5.3: normal curve.\nRecall Chapter 2 distribution variable description possible values takes frequently value occurs. sampling distribution, “variable” sample statistic, sampling distribution description possible values sample statistic takes frequently value occurs looking across many many possible samples. quite amazing something like sample proportion, summarizing categorical variable, bell-shaped sampling distribution sample large enough samples!Theory-based methods also give us mathematical expressions \nstandard deviation sampling distribution. instance,\ntrue population proportion \\(\\pi\\), standard deviation\nsampling distribution sample proportions—far away expect sample proportion away population proportion—is74\n\\[\nSD(\\hat{p}) = \\sqrt{\\frac{\\pi(1-\\pi)}{n}}.\n\\]\nTypically, values parameters \\(\\pi\\) unknown, unable calculate standard deviations. case, substitute “best guess” \\(\\pi\\) formulas, either hypothesis point estimate.Standard error.standard deviation sampling distribution statistic, denoted \\(SD\\)(statistic), represents far away expect statistic land parameter.","code":""},{"path":"inference-cat.html","id":"HypothesisTesting","chapter":"5 Inference for categorical data","heading":"5.1.3 Hypothesis tests","text":"Martian alphabet example, utilized hypothesis test, formal technique evaluating two competing possibilities.\nhypothesis test involves null hypothesis, represents either skeptical perspective perspective difference effect, alternative hypothesis, represents new perspective possibility change treatment effect experiment. alternative hypothesis usually reason scientists set research first place.Null alternative hypotheses.observe effect sample, like determine\nobserved effect represents\nactual effect population, whether simply due \nchance.\nlabel two competing claims, \\(H_0\\) \\(H_A\\),\nspoken “H-naught” “H_A”.hypothesis testing framework general tool, often use without second thought.\nperson makes somewhat unbelievable claim, initially skeptical.\nHowever, sufficient evidence supports claim, set aside skepticism.\nhallmarks hypothesis testing also found US court system.","code":""},{"path":"inference-cat.html","id":"the-us-court-system","chapter":"5 Inference for categorical data","heading":"The US court system","text":"Example 5.2  US court considers two possible claims defendant: either innocent guilty. set claims hypothesis framework, null hypothesis alternative?Jurors examine evidence see whether convincingly shows defendant guilty.\nNotice jury finds defendant guilty, necessarily mean jury confident person’s innocence.\nsimply convinced alternative person guilty.also case hypothesis testing: even fail reject null hypothesis, typically accept null hypothesis truth.\nFailing find strong evidence alternative hypothesis equivalent providing evidence null hypothesis true.","code":""},{"path":"inference-cat.html","id":"p-value","chapter":"5 Inference for categorical data","heading":"p-value","text":"Martian alphabet example, performed simulation-based hypothesis test hypotheses:\\(H_0\\): chance human chooses Bumba left 50%.\\(H_0\\): chance human chooses Bumba left 50%.\\(H_A\\): Humans preference choosing Bumba left.\\(H_A\\): Humans preference choosing Bumba left.research question—can humans read Martian?—framed context hypotheses.null hypothesis (\\(H_0\\)) perspective effect (ability read Martian).\nstudent data provided point estimate 89.5% (\\(34/38 \\times 100\\)%) true probability choosing Bumba left.\ndetermined observing sample proportion chance alone (assuming \\(H_0\\)) rare—happen less 1 1000 samples. results\nlike inconsistent \\(H_0\\), reject \\(H_0\\) favor \\(H_A\\).\n, concluded humans preference choosing Bumba left.less 1--1000 chance call p-value, probability quantifying strength evidence null hypothesis favor alternative.p-value.interpreting p-value, remember definition p-value three components. (1) probability. probability ? probability (2) observed sample statistic one extreme. Assuming ? probability observed sample statistic one extreme, (3) assuming null hypothesis true:probabilitydata76null hypothesis\nExample 5.3  test statistic Martian alphabet example?Since p-value probability, value always 0 1. closer p-value 0, stronger evidence null hypothesis. ? small p-value means data unlikely occur, null hypothesis true. take mean null hypothesis isn’t plausible assumption, reject . process mimics scientific method—easier disprove theory prove . scientists want find evidence new drug reduces risk stroke, assume doesn’t reduce risk stroke show observed data unlikely occur plausible explanation drug works.\nabsence evidence evidence absence.\n\nFigure 5.4: Strength evidence null continuum p-values. p-value beyond around 0.10, data provide evidence null hypothesis.\nRegardless data structure analysis method, hypothesis testing framework always follows steps—details model randomness data change.General steps hypothesis test. Every hypothesis test follows general steps:Frame research question terms hypotheses.Collect summarize data using test statistic.Assume null hypothesis true, simulate mathematically model null distribution test statistic.Compare observed test statistic null distribution calculate p-value.Make conclusion based p-value, write conclusion context, plain language, terms alternative hypothesis.\n","code":""},{"path":"inference-cat.html","id":"decisions-and-statistical-significance","chapter":"5 Inference for categorical data","heading":"Decisions and statistical significance","text":"cases, decision hypothesis test needed, two possible decisions follows:Reject null hypothesisFail reject null hypothesisIn order decide two options, need previously set threshold p-value: p-value less previously set threshold, reject \\(H_0\\); otherwise, fail reject \\(H_0\\). threshold called significance level, p-value less significance level, say results statistically significant. means data provide strong evidence \\(H_0\\) reject null hypothesis favor alternative hypothesis.\nsignificance level, often represented \\(\\alpha\\) (Greek letter alpha), typically set \\(\\alpha = 0.05\\), can vary depending field application real-life consequences incorrect decision.\nUsing significance level \\(\\alpha = 0.05\\) Martian alphabet study, can say data provided statistically significant evidence null hypothesis.Statistical significance.’s special 0.05?Statistical significance hot topic news, related “reproducibility crisis” scientific fields. encourage read debate use p-values statistical significance. good place start Nature article, “Scientists rise statistical significance,” March 20, 2019.","code":""},{"path":"inference-cat.html","id":"ConfidenceIntervals","chapter":"5 Inference for categorical data","heading":"5.1.4 Confidence intervals","text":"point estimate provides single plausible value parameter.\nHowever, point estimate rarely perfect—usually error estimate.\naddition supplying point estimate parameter, next logical step provide plausible range values parameter.plausible range values population parameter called confidence interval.\nUsing single point estimate like fishing murky lake spear, using confidence interval like fishing net.\ncan throw spear saw fish, probably miss.\nhand, toss net area, good chance catching fish.report point estimate, probably hit exact population parameter.\nhand, report range plausible values—confidence interval—good shot capturing parameter.reasoning also explains can never prove null hypothesis. Sample statistics vary sample sample. can quantify uncertainty (e.g., 95% sure statistic within 0.15 parameter), can never certain parameter exact value. example, suppose want test whether coin fair coin, .e., \\(H_0: \\pi = 0.50\\) versus \\(H_0: \\pi \\neq 0.50\\), toss coin 10 times collect data. 10 tosses, 6 land heads 4 land tails, resulting p-value 0.75478. don’t enough evidence show coin biased, surely wouldn’t say just proved coin fair!explore simulation-based methods (bootstrapping) theory-based methods creating confidence intervals text. Though details change different scenarios, theory-based confidence intervals always take form:\n\\[\n\\mbox{statistic} \\pm (\\mbox{multiplier}) \\times (\\mbox{standard error statistic})\n\\]\nstatistic best guess value parameter, makes sense build confidence interval around value. standard error, measure uncertainty associated statistic, provides guide large make confidence interval. multiplier determined confident ’d like , tells us many standard errors need add subtract statistic. amount add subtract statistic called margin error.General form confidence interval.Section 5.3.3 discuss different percentages confidence interval (e.g., 90% confidence interval 99% confidence interval). Section 5.4.3 provides longer discussion “95% confidence” actually means.","code":""},{"path":"inference-cat.html","id":"normal","chapter":"5 Inference for categorical data","heading":"5.2 The normal distribution","text":"Among distributions see statistics, one overwhelmingly common.\nsymmetric, unimodal, bell curve ubiquitous throughout statistics.\ncommon people know variety names including normal curve, normal model, normal distribution.81\ncertain conditions, sample proportions, sample means, sample differences can modeled using normal distribution—basis theory-based inference methods.\nAdditionally, variables SAT scores heights US adult males closely follow normal distribution.Normal distribution facts.section, discuss normal distribution context data become familiar normal distribution techniques.","code":""},{"path":"inference-cat.html","id":"normal-distribution-model","chapter":"5 Inference for categorical data","heading":"5.2.1 Normal distribution model","text":"normal distribution always describes symmetric, unimodal, bell-shaped curve.\nHowever, normal curves can look different depending details model.\nSpecifically, normal model can adjusted using two parameters: mean standard deviation.\ncan probably guess, changing mean shifts bell curve left right, changing standard deviation stretches constricts curve.\nFigure 5.5 shows normal distribution mean \\(0\\) standard deviation \\(1\\) (commonly referred standard normal distribution) top.\nnormal distribution mean \\(19\\) standard deviation \\(4\\) shown bottom. Figure 5.6 shows two normal distributions axis.\nFigure 5.5: curves represent normal distribution, however, differ center spread. normal distribution mean 0 standard deviation 1 called standard normal distribution.\n\nFigure 5.6: two normal models shown now plotted together scale.\nnormal distribution mean \\(\\mu\\) standard deviation \\(\\sigma\\), may write distribution \\(N(\\mu, \\sigma)\\). two distributions Figure 5.6 can written \n\\[\\begin{align*}\nN(\\mu=0,\\sigma=1)\\quad\\text{}\\quad N(\\mu=19,\\sigma=4)\n\\end{align*}\\]\nmean standard deviation describe normal distribution exactly, called distribution’s parameters.","code":""},{"path":"inference-cat.html","id":"standardizing-with-z-scores","chapter":"5 Inference for categorical data","heading":"5.2.2 Standardizing with Z-scores","text":"\nTable 5.1: Mean standard deviation SAT ACT.\n\nFigure 5.7: Ann’s Tom’s scores shown distributions SAT ACT scores.\nsolution previous example relies standardization technique called Z-score, method commonly employed nearly normal observations (may used distribution). Z-score observation defined number standard deviations falls mean.\nobservation one standard deviation mean, Z-score 1. 1.5 standard deviations mean, Z-score -1.5.\n\\(x\\) observation distribution \\(N(\\mu, \\sigma)\\), define Z-score mathematically \\[\\begin{eqnarray*}\nZ = \\frac{x-\\mu}{\\sigma}\n\\end{eqnarray*}\\]\nUsing \\(\\mu_{SAT}=1500\\), \\(\\sigma_{SAT}=300\\), \\(x_{Ann}=1800\\), find Ann’s Z-score:\n\\[\\begin{eqnarray*}\nZ_{Ann} = \\frac{x_{Ann} - \\mu_{SAT}}{\\sigma_{SAT}} = \\frac{1800-1500}{300} = 1\n\\end{eqnarray*}\\]Z-score.Observations mean always positive Z-scores mean negative Z-scores.\nobservation equal mean (e.g., SAT score 1500), Z-score \\(0\\).can use Z-scores roughly identify observations unusual others.\nOne observation \\(x_1\\) said unusual another observation \\(x_2\\) absolute value Z-score larger absolute value observation’s Z-score: \\(|Z_1| > |Z_2|\\).\ntechnique especially insightful distribution symmetric.","code":""},{"path":"inference-cat.html","id":"normal-probability-calculations-in-r","chapter":"5 Inference for categorical data","heading":"5.2.3 Normal probability calculations in R","text":"Example 5.4  Ann SAT Guided Practice earned score 1800 SAT corresponding \\(Z=1\\). like know percentile falls among SAT test-takers.\nFigure 5.8: normal model SAT scores, shading area individuals scored Ann.\ncan use normal model find percentiles probabilities. R, function\ncalculate normal probabilities pnorm(). normTail() function available openintro R package draw associated curve helpful. code , find percentile \\(Z=0.43\\) 0.6664, \\(66.64^{th}\\) percentile.can also find Z-score associated percentile.\nexample, identify Z \\(80^{th}\\) percentile, use qnorm() identifies quantile given percentage. quantile represents cutoff value.88\ndetermine Z-score \\(80^{th}\\) percentile using qnorm(): 0.84.can use functions normal distributions standard normal distribution specifying mean argument m standard deviation argument s. determine proportion ACT test takers scored worse Tom ACT: 0.73.","code":"\npnorm(0.43, m = 0, s = 1)\n#> [1] 0.666\nopenintro::normTail(0.43, m = 0, s = 1)\nqnorm(0.80, m = 0, s = 1)\n#> [1] 0.842\nopenintro::normTail(0.80, m = 0, s = 1)\npnorm(24, m = 21, s = 5)\n#> [1] 0.726\nopenintro::normTail(24, m = 21, s = 5)"},{"path":"inference-cat.html","id":"normal-probability-examples","chapter":"5 Inference for categorical data","heading":"5.2.4 Normal probability examples","text":"Cumulative SAT scores approximated well normal model, \\(N(\\mu=1500, \\sigma=300)\\).Example 5.5  Shannon randomly selected SAT taker, nothing known Shannon’s SAT aptitude. probability Shannon scores least 1630 SATs?First, always draw label picture normal distribution. (Drawings need exact useful.) interested chance scores 1630, shade upper tail. See normal curve .picture shows mean values 2 standard deviations mean. simplest way find shaded area curve makes use Z-score cutoff value. \\(\\mu=1500\\), \\(\\sigma=300\\), cutoff value \\(x=1630\\), Z-score computed \n\\[\\begin{eqnarray*}\nZ = \\frac{x - \\mu}{\\sigma} = \\frac{1630 - 1500}{300} = \\frac{130}{300} = 0.43\n\\end{eqnarray*}\\]\nuse software find percentile \\(Z=0.43\\), yields 0.6664. However, percentile describes Z-score lower 0.43. find area \\(Z=0.43\\), compute one minus area lower tail, seen .Always draw picture first, find Z-score second.normal probability situation, always always always draw label normal curve shade area interest first.\npicture provide estimate probability.Example 5.6  Edward earned 1400 SAT. percentile?First, picture needed. Edward’s percentile proportion people get high 1400. scores left 1400.Areas right.Based sample 100 men,93 heights male adults ages 20 62 US nearly normal mean 70.0’’ standard deviation 3.3’’.last several problems focused finding probability percentile particular observation.\nlike know observation corresponding particular percentile?Example 5.7  Erik’s height \\(40^{th}\\) percentile. tall ?always, first draw picture (see ).case, lower tail probability known (0.40), can shaded diagram. want find observation corresponds value. first step direction, determine Z-score associated \\(40^{th}\\) percentile.percentile 50%, know \\(Z\\) negative. Looking negative part normal probability table, search probability inside table closest 0.4000. find 0.4000 falls row \\(-0.2\\) columns \\(0.05\\) \\(0.06\\). Since falls closer \\(0.05\\), take one: \\(Z=-0.25\\).Example 5.8  adult male height \\(82^{nd}\\) percentile?, draw figure first (see ).\\(95^{th}\\) percentile SAT scores?\\(97.5^{th}\\) percentile male heights? always normal probability problems, first draw picture.95\nprobability randomly selected male adult least 6’2’’ (74 inches)?probability male adult shorter 5’9’’ (69 inches)?96\nExample 5.9  probability random adult male 5’9’’ 6’2’’?heights correspond 69 inches 74 inches. First, draw figure. area interest longer upper lower tail.total area curve 1. find area two tails shaded (previous Guided Practice, areas \\(0.3821\\) \\(0.1131\\)), can find middle area:","code":"\nqnorm(0.4, m = 0, s = 1)\n#> [1] -0.253\nqnorm(0.82, m = 0, s = 1)\n#> [1] 0.915"},{"path":"inference-cat.html","id":"rule","chapter":"5 Inference for categorical data","heading":"5.2.5 68-95-99.7 rule","text":", present useful general rule probability falling within 1, 2, 3 standard deviations mean normal distribution. rule useful wide range practical settings, especially trying make quick estimate without calculator Z table.\nFigure 5.9: Probabilities falling within 1, 2, 3 standard deviations mean normal distribution.\npossible normal random variable fall 4, 5, even standard deviations mean. However, occurrences rare data nearly normal. probability 4 standard deviations mean 1--30,000. 5 6 standard deviations, 1--3.5 million 1--1 billion, respectively.","code":""},{"path":"inference-cat.html","id":"single-prop","chapter":"5 Inference for categorical data","heading":"5.3 One proportion","text":"Notation.\\(n\\) = sample size (number observational units data set)\\(\\hat{p}\\) = sample proportion (number “successes” divided sample size)\\(\\pi\\) = population proportion101\nsingle proportion used summarize data measured single categorical variable observational unit—single variable measured either success failure (e.g., “surgical complication” vs. “surgical complication”)102.","code":""},{"path":"inference-cat.html","id":"one-prop-null-boot","chapter":"5 Inference for categorical data","heading":"5.3.1 Simulation-based test for \\(H_0: \\pi = \\pi_0\\)","text":"\nSection 5.1.3, introduced general steps hypothesis test:General steps hypothesis test. Every hypothesis test follows general steps:Frame research question terms hypotheses.Collect summarize data using test statistic.Assume null hypothesis true, simulate mathematically model null distribution test statistic.Compare observed test statistic null distribution calculate p-value.Make conclusion based p-value, write conclusion context, plain language, terms alternative hypothesis.\nExample 5.10  People providing organ donation sometimes seek help special medical consultant.\nconsultants assist patient aspects surgery, goal reducing possibility complications medical procedure recovery.\nPatients might choose consultant based part historical complication rate consultant’s clients.One consultant tried attract patients noting average complication rate liver donor surgeries US 10%, clients 3 complications 62 liver donor surgeries facilitated.\nclaims strong evidence work meaningfully contributes reducing complications (therefore hired!).Using data, possible assess consultant’s claim work meaningfully contributes reducing complications?. claim causal connection, data observational, must lookout confounding variables.\nexample, maybe patients can afford medical consultant can afford better medical care, can also lead lower complication rate.","code":""},{"path":"inference-cat.html","id":"steps-1-and-2-hypotheses-and-test-statistic","chapter":"5 Inference for categorical data","heading":"Steps 1 and 2: Hypotheses and test statistic","text":"Regardless use simulation-based methods theory-based methods, first two steps hypothesis test start : setting hypotheses summarizing data test statistic.\nlet \\(\\pi\\) represent true complication rate liver donors working consultant. “true” complication probability called parameter interest103.\nsample proportion complication rate 3 complications divided 62 surgeries consultant worked : \\(\\hat{p} = 3/62 = 0.048\\). Since value estimated sample data, called statistic. statistic \\(\\hat{p}\\) also point estimate, “best guess,” \\(\\pi\\), use test statistic.Parameters statistics.parameter “true” value interest.\ntypically estimate parameter using statistic sample data. statistic used estimate parameter, called point estimate.example, estimate probability \\(\\pi\\) complication client medical consultant examining past complications rates clients:Summary measures summarize sample data, \\(\\hat{p}\\), called statistics. Numbers summarize entire population, \\(\\pi\\), called parameters. can remember\ndistinction looking first letter term:Statistics summarize Samples.Parameters summarize Populations.Write hypotheses plain statistical language test association consultant’s work true complication rate, \\(\\pi\\), consultant’s clients.words:\\(H_0\\): association consultant’s contributions clients’ complication rate.\\(H_A\\): Patients work consultant tend complication rate lower 10%.statistical language:","code":""},{"path":"inference-cat.html","id":"steps-3-and-4-null-distribution-and-p-value","chapter":"5 Inference for categorical data","heading":"Steps 3 and 4: Null distribution and p-value","text":"assess hypotheses, need evaluate possibility getting sample proportion far null value, \\(0.10\\), observed (\\(0.048\\)), null hypothesis true.Null value hypothesis test.deviation sample statistic null hypothesized parameter usually quantified p-value104. p-value computed based null distribution, distribution test statistic null hypothesis true. Supposing null hypothesis true, can compute p-value identifying chance observing test statistic favors alternative hypothesis least strongly observed test statistic.Null distribution.null distribution test statistic sampling distribution statistic assumption null hypothesis. describes statistic vary sample sample, null hypothesis true.want identify sampling distribution test statistic (\\(\\hat{p}\\)) null hypothesis true. words, want see sample proportion changes due chance alone. plan use information decide whether enough evidence reject null hypothesis.null hypothesis, 10% liver donors complications surgery. Suppose rate really different consultant’s clients (consultant’s clients, just 62 previously measured). case, simulate 62 clients get sample proportion complication rate null distribution.similar scenario one encountered Section 5.1.1, one important difference—null value 0.10, 0.50. Thus, flipping coin simulate whether client complications simulating correct null hypothesis.Assuming true complication rate consultant’s clients 10%, client can simulated using bag marbles 10% red marbles 90% white marbles.\nSampling marble bag (10% red marbles) one way simulating whether patient complication true complication rate 10% data. select 62 marbles compute proportion patients complications simulation, \\(\\hat{p}_{sim}\\), resulting sample proportion calculated exactly sample null distribution.undergraduate student paid $2 complete simulation. 5 simulated cases complication 57 simulated cases without complication, .e., \\(\\hat{p}_{sim} = 5/62 = 0.081\\).Example 5.11  one simulation enough determine whether reject null hypothesis?One simulation isn’t enough get sense null distribution; many simulation studies needed. Roughly 10,000 seems sufficient. However, paying someone simulate 10,000 studies hand waste time money. Instead, simulations typically programmed computer, much efficient.Figure 5.10 shows results 10,000 simulated studies. proportions equal less \\(\\hat{p}=0.048\\) shaded. shaded areas represent sample proportions null distribution provide least much evidence \\(\\hat{p}\\) favoring alternative hypothesis. 1222 simulated sample proportions \\(\\hat{p}_{sim} \\leq 0.048\\). use construct null distribution’s left-tail area find p-value:\n\\[\\begin{align}\n\\text{left tail area }\\label{estOfPValueBasedOnSimulatedNullForSingleProportion}\n    &= \\frac{\\text{Number observed simulations }\\hat{p}_{sim}\\leq\\text{ 0.048}}{10000}\n\\end{align}\\]\n10,000 simulated \\(\\hat{p}_{sim}\\), 1222 equal smaller \\(\\hat{p}\\). Since hypothesis test one-sided, estimated p-value equal tail area: 0.1222.\nFigure 5.10: null distribution \\(\\hat{p}\\), created 10,000 simulated studies. left tail, representing p-value hypothesis test, contains 12.22% simulations.\n","code":""},{"path":"inference-cat.html","id":"step-5-conclusion","chapter":"5 Inference for categorical data","heading":"Step 5: Conclusion","text":"","code":""},{"path":"inference-cat.html","id":"boot-ci-prop","chapter":"5 Inference for categorical data","heading":"5.3.2 Bootstrap confidence interval for \\(\\pi\\)","text":"confidence interval provides range \nplausible values parameter \\(\\pi\\).\ngoal produce range possible values population value, ideal world, sample data population recompute sample proportion.\n.\n.\ngood sense variability original estimate.\nideal world sampling data free extremely cheap almost never case, taking repeated samples population usually impossible.\n, instead using “resample population” approach, bootstrapping uses “resample sample” approach.Example 5.12  Let’s revisit medical consultant example Section 5.3.1. consultant tried attract patients noting average complication rate liver donor surgeries US 10%, clients 3 complications 62 liver donor surgeries facilitated. data, however, provide sufficient evidence consultant’s complication rate less 10%, since p-value approximately 0.122. mean can conclude consultant’s complication rate equal 10%?medical consultant case study, parameter \\(\\pi\\), true probability complication client medical consultant.\nreason believe \\(\\pi\\) exactly \\(\\hat{p} = 3/62\\), also reason believe \\(\\pi\\) particularly far \\(\\hat{p} = 3/62\\).\nsampling replacement data set (process called bootstrapping),108 variability possible \\(\\hat{p}\\) values can approximated, allow us generate range plausible values \\(\\pi\\), .e., confidence interval.inferential procedures covered text grounded quantifying one data set differ another taken population.\ndoesn’t make sense take repeated samples population means take samples, larger sample size benefit exact sample twice.\nInstead, measure samples behave estimate population. Figure 5.11 shows unknown original population red white marbles can estimated using multiple copies sample seven marbles.\nFigure 5.11: unknown population red white marbles. estimated population right many copies observed sample.\ntaking repeated samples estimated population, variability sample sample can observed. Figure 5.12 repeated bootstrap samples obviously different , original sample, original population.\nRecall bootstrap samples taken (estimated) population, differences due entirely natural variability sampling procedure.\nFigure 5.12: Selecting \\(k\\) random samples estimated population created copies observed sample.\nsummarizing bootstrap samples (, using sample proportion), see, directly, variability sample proportion red marbles, \\(\\hat{p}\\), sample sample.\ndistribution bootstrapped \\(\\hat{p}\\)’s example scenario shown Figure 5.13, bootstrap distribution medical consultant data shown Figure 5.14.\nFigure 5.13: Calculate sample proportion red marbles bootstrap resample, plot simulated sample proportions dot plot. dot plot sample proportion provides us sense sample proportions vary sample sample take many samples original population.\nturns practice, difficult computers work infinite population (proportional breakdown sample).\nHowever, physical computational model produces equivalent bootstrap distribution sample proportion computationally efficient manner.\nConsider observed data bag marbles 3 red 4 white. drawing marbles bag replacement, depict sampling process done infinitely large estimated population.\nNote sampling original observations replacement, particular marble may end new sample one time, multiple times, .Bootstrapping one sample.Take random sample size \\(n\\) original sample, replacement. called bootstrapped resample.Record sample proportion (statistic interest) boostrapped resample. called bootstrapped statistic.Repeat steps (1) (2) 1000s times create distribution bootstrapped statistics.\napply bootstrap sampling process medical consultant example, consider client one marbles bag.\n59 white marbles (complication) 3 red marbles (complication).\n62 choose marbles bag (one time), replacing chosen marble color recorded, compute proportion simulated patients complications, \\(\\hat{p}_{bs}\\), “bootstrap” proportion represents single simulated proportion “resample sample” approach.One simulated bootstrap resample isn’t enough get sense variability one bootstrap proportion another bootstrap proportion, repeated simulation 10,000 times using computer.\nFigure 5.14 shows distribution 10,000 bootstrap simulations.\nbootstrapped proportions vary zero 0.15. taking range middle 95% distribution, can construct 95% bootstrapped confidence interval \\(\\pi\\). 2.5th percentile 0, 97.5th percentile 0.113, middle 95% distribution range (0, 0.113).\nvariability bootstrapped proportions leads us believe true risk complication (parameter, \\(\\pi\\)) somewhere 0 11.3%.\nFigure 5.14: original medical consultant data bootstrapped 10,000 times. simulation creates sample original data probability complication \\(\\hat{p} = 3/62\\). bootstrap 2.5 percentile proportion 0 97.5 percentile 0.113. result : confident , population, true probability complication 0% 11.3%.\n95% Bootstrap confidence interval population proportion \\(\\pi\\).can find confidence intervals difference confidence levels changing percent distribution take, e.g., locate middle 90% bootstrapped statistics 90% confidence interval.Example 5.13  original claim consultant’s true rate complication national rate 10%. interval estimate 0 11.3% true probability complication indicate surgical consultant lower rate complications national average?\nExplain.","code":""},{"path":"inference-cat.html","id":"theory-prop","chapter":"5 Inference for categorical data","heading":"5.3.3 Theory-based inferential methods for \\(\\pi\\)","text":"Section 5.1.2, introduced normal distribution showed can used mathematical model describe variability sample mean sample proportion result Central Limit Theorem. explored normal distribution\nSection 5.2. Theory-based hypothesis tests confidence intervals proportions use normal distribution calculate p-value determine width confidence interval.Central Limit Theorem sample proportion.","code":""},{"path":"inference-cat.html","id":"evaluating-the-two-conditions-required-for-modeling-hatp-using-theory-based-methods","chapter":"5 Inference for categorical data","heading":"Evaluating the two conditions required for modeling \\(\\hat{p}\\) using theory-based methods","text":"two conditions required apply \nCentral Limit Theorem\nsample proportion \\(\\hat{p}\\).\nsample observations\nindependent sample size sufficiently\nlarge, normal model describe variability sample proportions quite well; observations violate conditions, normal model can inaccurate.Conditions sampling distribution \n\\(\\hat{p}\\) approximately normal.sampling distribution \\(\\hat{p}\\) based \nsample size \\(n\\) population true\nproportion \\(\\pi\\) can modeled\nusing normal distribution :Independence. sample observations independent, .e., outcome one observation influence outcome another. condition met data come simple random sample target population.Independence. sample observations independent, .e., outcome one observation influence outcome another. condition met data come simple random sample target population.Success-failure condition. expected see least 10 successes \n10 failures sample, .e., \\(n\\pi\\geq10\\) \n\\(n(1-\\pi)\\geq10\\). condition met least 10 successes 10 failures observed data.Success-failure condition. expected see least 10 successes \n10 failures sample, .e., \\(n\\pi\\geq10\\) \n\\(n(1-\\pi)\\geq10\\). condition met least 10 successes 10 failures observed data.\nTypically don’t know true proportion \\(\\pi\\),\nsubstitute value check success-failure condition\nestimate standard deviation sampling distribution \\(\\hat{p}\\).\nindependence condition nuanced requirement.\nisn’t met, important understand \nisn’t met.\nexample, exist statistical methods available truly correct inherent biases data convenience sample.\nhand, took cluster random sample\n(see Section 1.3.4), observations wouldn’t independent, suitable statistical methods available analyzing data (beyond scope even second third courses\nstatistics)111.Example 5.14  examples based large sample theory, modeled \\(\\hat{p}\\) using normal distribution. appropriate study medical consultant?Since theory-based methods used medical consultant example, ’ll turn another example demonstrate methods, conditions approximating distribution \\(\\hat{p}\\) normal distribution met.","code":""},{"path":"inference-cat.html","id":"hypothesis-test-for-h_0-pi-pi_0","chapter":"5 Inference for categorical data","heading":"Hypothesis test for \\(H_0: \\pi = \\pi_0\\)","text":"One possible regulation payday lenders \nrequired credit check evaluate debt\npayments borrower’s finances.\nlike know: borrowers support form\nregulation?Example 5.15  Set hypotheses evaluate whether borrowers\nmajority support \ntype regulation. take “majority” mean\ngreater 50% population.words,\\(H_0\\): majority support regulation\\(H_A\\): majority borrowers support regulationIn statistical notation,\\(H_0\\): \\(\\pi = 0.50\\)\\(H_A\\): \\(\\pi > 0.50\\),Note null hypothesis stated \\(H_0: \\pi = 0.50\\), even though saying “majority support” imply \\(\\pi \\leq 0.50\\). Indeed, textbooks \nwrite \\(H_0: \\pi \\leq 0.50\\) case, incorrect statement. However,\ncalculating p-value, need assume particular value \\(\\pi\\) \nnull hypothesis, textbook, null hypothesis always form:apply normal distribution model null distribution, independence\nsuccess-failure conditions must satisfied.\nhypothesis test, success-failure condition \nchecked using null proportion:\nverify \\(n\\pi_0\\) \\(n(1-\\pi_0)\\) least 10,\n\\(\\pi_0\\) null value.Example 5.16  Continuing previous Example,\nevaluate whether poll lending regulations provides convincing evidence\nmajority payday loan borrowers support\nnew regulation \nrequire lenders pull credit reports\nevaluate debt payments.hypotheses already set conditions checked,\ncan move onto calculations.\nnull standard error context one proportion\nhypothesis test computed using null value, \\(\\pi_0\\):\n\\[\\begin{align*}\n  SE_0(\\hat{p}) = \\sqrt{\\frac{\\pi_0 (1 - \\pi_0)}{n}}\n      = \\sqrt{\\frac{0.5 (1 - 0.5)}{826}}\n      = 0.017\n  \\end{align*}\\]\npicture normal model null distribution sample proportions\nscenario shown Figure 5.15,\np-value represented shaded region.\nNote null distribution centered 0.50, null value,\nstandard deviation 0.017.\\(H_0\\), probability observing \\(\\hat{p} = 0.51\\) higher\n0.278, area \n0.51 null distribution.p-value 0.278, poll provide convincing evidence \nmajority payday loan borrowers support regulations around credit checks evaluation \ndebt payments.\nFigure 5.15: Approximate sampling distribution \\(\\hat{p}\\) across possible samples assuming \\(\\pi = 0.50\\). shaded area represents p-value corresponding observed sample proportion 0.51.\nOften, theory-based methods, use standardized statistic rather \noriginal statistic test statistic. standardized statistic computed subtracting mean null distribution original statistic, dividing standard error:\n\\[\n\\mbox{standardized statistic} = \\frac{\\mbox{observed statistic} - \\mbox{null value}}{\\mbox{null standard error}}\n\\]\nnull standard error (\\(SE_0(\\text{statistic})\\)) observed statistic estimated standard deviation assuming null hypothesis true. can interpret standardized statistic number standard errors observed statistic (positive) (negative) null value. modeling null distribution normal\ndistribution, standardized statistic called \\(Z\\), since Z-score sample proportion.Standardized sample proportion.standardized statistic test statistic, can find\np-value area standard normal distribution extreme\nobserved \\(Z\\) value.Example 5.17  payday loan borrowers support regulation require lenders pull credit report evaluate debt payments? random sample 826 borrowers, 51% said support regulation. set hypotheses checked conditions previously. Now calculate interpret standardized statistic, use standard normal distribution calculate approximate p-value.sample proportion \\(\\hat{p} = 0.51\\). Since null value \\(\\pi_0 = 0.50\\),\nnull standard error \n\\[\\begin{align*}\n  SE_0(\\hat{p}) = \\sqrt{\\frac{\\pi_0 (1 - \\pi_0)}{n}}\n      = \\sqrt{\\frac{0.5 (1 - 0.5)}{826}}\n      = 0.017\n  \\end{align*}\\]standardized statistic \n\\[\\begin{align*}\nZ = \\frac{0.51 - 0.50}{0.017} = 0.57\n\\end{align*}\\]Interpreting value, can say sample proportion 0.51 0.57 standard errors null value 0.50.\nFigure 5.16: Approximate sampling distribution \\(Z\\) across possible samples assuming \\(\\pi = 0.50\\). shaded area represents p-value corresponding observed standardized statistic 0.57. Compare Figure 5.15.\nTheory-based hypothesis test proportion: one-sample \\(Z\\)-test.Frame research question terms hypotheses.Using null value, \\(\\pi_0\\), verify conditions using normal distribution approximate null distribution.Calculate test statistic:\n\\[\n Z = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}} = \\frac{\\hat{p} - \\pi_0}{SE_0(\\hat{p})}\n \\]Use test statistic standard normal distribution calculate p-value.Make conclusion based p-value, write conclusion context, plain language, terms alternative hypothesis.\nRegardless statistical method chosen, p-value always derived analyzing null distribution test statistic. normal model poorly approximates null distribution \\(\\hat{p}\\) success-failure condition satisfied. substitute, can generate null distribution using simulated sample proportions use distribution compute tail area, .e., p-value.\nNeither p-value approximated normal distribution simulated p-value exact, normal distribution simulated null distribution exact, close approximation.\nexact p-value can generated using binomial distribution, method covered text.","code":""},{"path":"inference-cat.html","id":"confidence-interval-for-pi","chapter":"5 Inference for categorical data","heading":"Confidence interval for \\(\\pi\\)","text":"confidence interval provides range \nplausible values parameter \\(\\pi\\).\npoint estimate best guess value parameter, makes sense build confidence interval around value. standard error, measure uncertainty associated point estimate, provides guide large make confidence interval. \\(\\hat{p}\\) can modeled using \nnormal distribution, 68-95-99.7 rule tells us , general, 95% observations within 2 standard errors mean. , use value 1.96 slightly precise. confidence interval\n\\(\\pi\\) takes form\n\\[\\begin{align*}\n\\hat{p} \\pm z^{\\star} \\times SE(\\hat{p}).\n\\end{align*}\\]seen \\(\\hat{p}\\) sample proportion. value \\(z^{\\star}\\) comes standard normal distribution determined chosen confidence level. value standard error \\(\\hat{p}\\), \\(SE(\\hat{p})\\), approximates far expect sample proportion fall \\(\\pi\\), depends heavily sample size.Standard error one proportion, \\(\\hat{p}\\).conditions met distribution \\(\\hat{p}\\) nearly normal, variability single proportion, \\(\\hat{p}\\) well described standard deviation:\\[SD(\\hat{p}) = \\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\]Note almost never know true value \\(\\pi\\), can substitute best guess \\(\\pi\\) obtain approximate standard deviation, called standard error \\(\\hat{p}\\):\\[SD(\\hat{p}) \\approx \\hspace{3mm} SE(\\hat{p}) = \\sqrt{\\frac{(\\mbox{best guess }\\pi)(1 - \\mbox{best guess }\\pi)}{n}}\\]Consider taking many polls registered voters (.e., random samples) size 300 asking support legalized marijuana.\nsuspected 2/3 voters support legalized marijuana.\nunderstand sample proportion (\\(\\hat{p}\\)) vary across samples, calculate standard error \\(\\hat{p}\\).113A simple random sample 826\npayday loan borrowers surveyed better\nunderstand interests around regulation costs.\n51% responses supported new\nregulations payday lenders.reasonable model variability \\(\\hat{p}\\) sample sample\nusing normal distribution?reasonable model variability \\(\\hat{p}\\) sample sample\nusing normal distribution?Calculate standard error \\(\\hat{p}\\).Calculate standard error \\(\\hat{p}\\).Construct 95% confidence interval \\(\\pi\\),\nproportion payday borrowers support increased\nregulation payday lenders.Construct 95% confidence interval \\(\\pi\\),\nproportion payday borrowers support increased\nregulation payday lenders.data random sample, observations \nindependent representative population \ninterest.\nalso must check success-failure condition,\nusing \\(\\hat{p}\\) place\n\\(\\pi\\) computing confidence interval:\n\\[\\begin{align*}\n\\text{Support: }\n  n \\hat{p} &\n      = 826 \\times 0.51\n  \\approx 421\n&\\text{: }\n  n (1 - \\hat{p}) &\n    = 826 \\times (1 - 0.51)\n  \\approx 405\n\\end{align*}\\]\nSince values least 10, can use normal\ndistribution model sampling distribution \\(\\hat{p}\\).data random sample, observations \nindependent representative population \ninterest.also must check success-failure condition,\nusing \\(\\hat{p}\\) place\n\\(\\pi\\) computing confidence interval:\n\\[\\begin{align*}\n\\text{Support: }\n  n \\hat{p} &\n      = 826 \\times 0.51\n  \\approx 421\n&\\text{: }\n  n (1 - \\hat{p}) &\n    = 826 \\times (1 - 0.51)\n  \\approx 405\n\\end{align*}\\]\nSince values least 10, can use normal\ndistribution model sampling distribution \\(\\hat{p}\\).\\(\\pi\\) unknown standard error \nconfidence interval, use \\(\\hat{p}\\) best guess \\(\\pi\\)\nformula.\n\\(SE(\\hat{p}) = \\sqrt{\\frac{0.51 (1 - 0.51)}  {826}} = 0.017\\).\\(\\pi\\) unknown standard error \nconfidence interval, use \\(\\hat{p}\\) best guess \\(\\pi\\)\nformula.\\(SE(\\hat{p}) = \\sqrt{\\frac{0.51 (1 - 0.51)}  {826}} = 0.017\\).Using\npoint estimate \\(0.51\\),\n\\(z^{\\star} = 1.96\\) 95% confidence interval,\n\nstandard error \\(SE = 0.017\\) previous\nGuided Practice,\nconfidence interval \n\\[\\begin{eqnarray*}\n  \\text{point estimate} \\ \\pm\\ z^{\\star} \\times SE\n   \\quad\\\\quad\n   0.51 \\ \\pm\\ 1.96 \\times 0.017\n   \\quad\\\\quad\n   (0.477, 0.543)\n  \\end{eqnarray*}\\]\n95% confident true proportion \npayday borrowers supported regulation time\npoll 0.477 \n0.543.\nUsing\npoint estimate \\(0.51\\),\n\\(z^{\\star} = 1.96\\) 95% confidence interval,\n\nstandard error \\(SE = 0.017\\) previous\nGuided Practice,\nconfidence interval \n\\[\\begin{eqnarray*}\n  \\text{point estimate} \\ \\pm\\ z^{\\star} \\times SE\n   \\quad\\\\quad\n   0.51 \\ \\pm\\ 1.96 \\times 0.017\n   \\quad\\\\quad\n   (0.477, 0.543)\n  \\end{eqnarray*}\\]\n95% confident true proportion \npayday borrowers supported regulation time\npoll 0.477 \n0.543.Constructing confidence interval single proportion.four steps constructing confidence\ninterval \\(p\\).Check independence success-failure condition\nusing \\(\\hat{p}\\).\nconditions met, sampling distribution\n\\(\\hat{p}\\) may well-approximated normal model.Construct standard error:\n\\[\n SE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n  \\]Use statistical software find multiplier \\(z^{\\star}\\) corresponding confidence level.Apply general confidence interval formula \\(\\mbox{statistic} \\pm (\\mbox{multiplier}) \\times SE\\):\n\\[\n \\hat{p} \\pm z^{\\star}\\times \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n  \\]\n","code":""},{"path":"inference-cat.html","id":"zstar-and-the-confidence-level","chapter":"5 Inference for categorical data","heading":"\\(z^{\\star}\\) and the confidence level","text":"Suppose want consider confidence intervals confidence level somewhat higher 95%: perhaps like confidence level 99%. Think back analogy trying catch fish: want sure catch fish, use wider net. create 99% confidence level, must also widen 95% interval. hand, want interval lower confidence, 90%, make original 95% interval slightly slimmer.95% confidence interval structure provides guidance make intervals new confidence levels. general 95% confidence interval parameter whose point estimate nearly normal distribution:\n\\[\\begin{eqnarray}\n\\text{point estimate}\\ \\pm\\ 1.96\\times SE\n\\end{eqnarray}\\]\nthree components interval: point estimate, “1.96”, standard error. choice \\(1.96\\times SE\\) based capturing 95% sampling distribution statistics since point estimate within 1.96 standard errors true parameter 95% time. choice 1.96 corresponds 95% confidence level.\nFigure 5.17: area -\\(z^{\\star}\\) \\(z^{\\star}\\) increases \\(|z^{\\star}|\\) becomes larger. confidence level 99%, choose \\(z^{\\star}\\) 99% normal curve -\\(z^{\\star}\\) \\(z^{\\star}\\), corresponds 0.5% lower tail 0.5% upper tail: \\(z^{\\star}=2.58\\).\ncreate 99% confidence interval, change 1.96 95% confidence interval formula 2.58. previous Guided Practice highlights 99% time normal random variable within 2.58 standard deviations mean. approach—using Z-scores normal model compute confidence levels—appropriate point estimate associated normal distribution can properly compute standard error. Thus, formula 99% confidence interval :\\[\\begin{eqnarray*}\n\\text{point estimate}\\ \\pm\\ 2.58\\times SE\n\\end{eqnarray*}\\]normal approximation crucial precision \\(z^\\star\\) confidence intervals. normal model good fit, use alternative distributions better characterize sampling distribution use bootstrapping procedures.Theory-based \\((1-\\alpha)\\times 100\\)% confidence interval.","code":""},{"path":"inference-cat.html","id":"using-r-to-find-zstar","chapter":"5 Inference for categorical data","heading":"Using R to find \\(z^{\\star}\\)","text":"Figure 5.17 provides picture identify \\(z^{\\star}\\) based confidence level. select \\(z^{\\star}\\) area -\\(z^{\\star}\\) \\(z^{\\star}\\) normal model corresponds confidence level. R, can find \\(z^{\\star}\\) using qnorm() function:","code":"\n# z* for 90% --> alpha = 0.15 --> need 5% on each size:\nqnorm(.90 + .05)\n#> [1] 1.645\n\n# z* for 95% --> alpha = 0.05 --> need 2.5% on each size:\nqnorm(.95 + .025)\n#> [1] 1.96\n\n# z* for 99% --> alpha = 0.01 --> need .5% on each size:\nqnorm(.99 + .005)\n#> [1] 2.576"},{"path":"inference-cat.html","id":"violating-conditions","chapter":"5 Inference for categorical data","heading":"Violating conditions","text":"’ve spent lot time discussing conditions \n\\(\\hat{p}\\) can reasonably modeled normal distribution.\nhappens success-failure condition fails?\nindependence condition fails?\neither case, general ideas confidence intervals\nhypothesis tests remain , strategy\ntechnique used generate interval p-value\nchange.success-failure condition isn’t met\nhypothesis test, can simulate null distribution\n\\(\\hat{p}\\) using null value, \\(\\pi_0\\), seen Section 5.3.1. Unfortunately, methods dealing observations \nindependent outside scope book.","code":""},{"path":"inference-cat.html","id":"diff-two-prop","chapter":"5 Inference for categorical data","heading":"5.4 Difference of two proportions","text":"Notation.\\(n_1\\), \\(n_2\\) = sample sizes two independent samples\\(\\hat{p}_1\\), \\(\\hat{p}_2\\) = sample proportions two independent samples\\(\\pi_1\\), \\(\\pi_2\\) = population proportions two independent samples\nnow extend methods Section 5.3 apply confidence intervals hypothesis tests differences population proportions come two groups: \\(\\pi_1 - \\pi_2\\).investigations, ’ll identify reasonable\npoint estimate \\(\\pi_1 - \\pi_2\\) based sample,\nmay already guessed form:\n\\(\\hat{p}_1 - \\hat{p}_2\\).\n\n’ll look statistical inference difference proportions\ntwo ways: simulation-based methods randomization test\nbootstrap confidence interval, theory-based methods \ntwo sample \\(z\\)-test \\(z\\)-interval.","code":""},{"path":"inference-cat.html","id":"two-prop-errors","chapter":"5 Inference for categorical data","heading":"5.4.1 Randomization test for \\(H_0: \\pi_1 - \\pi_2 = 0\\)","text":"learned Chapter 1, randomized experiment done assess whether one variable (explanatory variable) causes changes second variable (response variable).\nEvery data set variability , decide whether variability data due (1) causal mechanism (randomized explanatory variable experiment) instead (2) natural variability inherent data, set sham randomized experiment comparison.\n, assume observational unit gotten exact response value regardless treatment level.\nreassigning treatments many many times, can compare actual experiment sham experiment. actual experiment extreme results sham experiments, led believe explanatory variable causing result inherent data variability.\nUsing different studies, let’s look carefully idea randomization test.","code":""},{"path":"inference-cat.html","id":"caseStudyGenderDiscrimination","chapter":"5 Inference for categorical data","heading":"5.4.1.1 Case study: Gender discrimination","text":"consider study investigating gender discrimination 1970s, set context personnel decisions within bank.117 research question hope answer , “females discriminated promotion decisions made male managers?”","code":""},{"path":"inference-cat.html","id":"observed-data","chapter":"5 Inference for categorical data","heading":"Observed data","text":"participants study 48 male bank supervisors attending management institute University North Carolina 1972.\nasked assume role personnel director bank given personnel file judge whether person promoted branch manager position.\nfiles given participants identical, except half indicated candidate male half indicated candidate female.\nfiles randomly assigned subjects.supervisor recorded gender associated assigned file promotion decision.\nUsing results study summarized Table 5.2, like evaluate females unfairly discriminated promotion decisions.\nstudy, smaller proportion females promoted males (0.583 versus 0.875), unclear whether difference provides convincing evidence females unfairly discriminated .\nTable 5.2: Summary results gender discrimination study.\ndata visualized Figure 5.18. Note promoted decision colored red (promoted) white(promoted). Additionally, observations broken male female groups.\nFigure 5.18: gender descrimination study can thought 48 red black cards.\nExample 5.18  Statisticians sometimes called upon evaluate strength evidence.\nlooking rates promotion males females study, might tempted immediately conclude females discriminated ?large difference promotion rates (58.3% females versus 87.5% males) suggest might discrimination women promotion decisions.\nHowever, yet sure observed difference represents discrimination just random chance.\nGenerally little bit fluctuation sample data, wouldn’t expect sample proportions exactly equal, even truth promotion decisions independent gender.previous example reminder observed outcomes sample may perfectly reflect true relationships variables underlying population.\nTable 5.2 shows 7 fewer promotions female group male group, difference promotion rates 29.2%:\n\\[\n\\hat{p}_M - \\hat{p}_F = \\frac{21}{24} - \\frac{14}{24} = 0.292. \n\\]\npoint estimate true difference large, sample size study small, making unclear observed difference represents discrimination whether simply due chance.\ntwo competing claims null alternative hypotheses:\\(H_0\\): Null hypothesis. variables gender decision independent. relationship, observed difference proportion males females promoted, 29.2%, due chance.\\(H_0\\): Null hypothesis. variables gender decision independent. relationship, observed difference proportion males females promoted, 29.2%, due chance.\\(H_A\\): Alternative hypothesis. variables gender decision independent. difference promotion rates 29.2% due chance, equally qualified females less likely promoted males.\\(H_A\\): Alternative hypothesis. variables gender decision independent. difference promotion rates 29.2% due chance, equally qualified females less likely promoted males.statistical notation:\\(H_0: \\pi_M - \\pi_F = 0\\)\\(H_0: \\pi_M - \\pi_F = 0\\)\\(H_A: \\pi_M - \\pi_F > 0\\)\\(H_A: \\pi_M - \\pi_F > 0\\)mean null hypothesis, says variables gender decision unrelated, true?\nmean banker decide whether promote candidate without regard gender indicated file.\n, difference promotion percentages due way files randomly divided bankers, randomization just happened give rise relatively large difference 29.2%.Consider alternative hypothesis: bankers influenced gender listed personnel file.\ntrue, especially influence substantial, expect see difference promotion rates male female candidates.\ngender bias females, expect smaller fraction promotion recommendations female personnel files relative male files.choose two competing claims assessing data conflict much \\(H_0\\) null hypothesis deemed reasonable.\ncase, data support \\(H_A\\), reject notion independence conclude data provide strong evidence discrimination.","code":""},{"path":"inference-cat.html","id":"variability-of-the-statistic","chapter":"5 Inference for categorical data","heading":"Variability of the statistic","text":"Table 5.2 shows 35 bank supervisors recommended promotion 13 .\nNow, suppose bankers’ decisions independent gender.\n, conducted experiment different random assignment gender files, differences promotion rates based random fluctuation.\ncan actually perform randomization, simulates happened bankers’ decisions independent gender distributed file genders differently.119In simulation, thoroughly shuffle 48 personnel files, 35 labeled promoted 13 labeled promoted, deal files two stacks.\nNote keeping 35 promoted 13 promoted, assuming 35 bank managers promoted individual whose content contained file (independent gender).\ndeal 24 files first stack, represent 24 “female” files.\nsecond stack also 24 files, represent 24 “male” files.\nFigure 5.19 highlights shuffle reallocation sham gender groups.\nFigure 5.19: gender descrimination data shuffled reallocated gender groups.\n, original data, tabulate results determine fraction male female promoted.Since randomization files simulation independent promotion decisions, difference two promotion rates entirely due chance.\nTable 5.3 show results one simulation.\nTable 5.3: Simulation results, difference promotion rates male female purely due chance.\nFigure 5.20 shows difference promotion rates much larger original data simulated groups (0.292 >>> 0.042).\nquantity interest throughout case study difference promotion rates.\nsummary value statistic interest (often test statistic).\nFigure 5.20: summarize randomized data produce one estimate difference proportions given gender discrimination.\n","code":""},{"path":"inference-cat.html","id":"observed-statistic-vs.-null-value","chapter":"5 Inference for categorical data","heading":"Observed statistic vs. null value","text":"computed one possible sample difference proportions null hypothesis Guided Practice , represents one difference due chance.\nfirst simulation, physically dealt files, much efficient perform simulation using computer.\nRepeating simulation computer, get another difference due chance: -0.042.\nanother: 0.208.\nrepeat simulation enough times good idea represents distribution differences sample proportions chance alone.\nFigure 5.21 shows plot differences found 100 simulations, dot represents simulated difference proportions male female files recommended promotion.\nFigure 5.21: dot plot differences 100 simulations produced null hypothesis, \\(H_0\\), gender_simulated decision independent. Two 100 simulations difference least 29.2%, difference observed study, shown solid red dots.\nNote distribution simulated differences proportions centered around 0.\nsimulated differences way made distinction men women, makes sense: expect differences chance alone fall around zero random fluctuation simulation.Example 5.19  often observe difference least 29.2% (0.292) according Figure 5.21?\nOften, sometimes, rarely, never?difference 29.2% rare event really impact listing gender candidates’ files, provides us two possible interpretations study results:\\(H_0\\): Null hypothesis. Gender effect promotion decision, observed difference large happen rarely.\\(H_0\\): Null hypothesis. Gender effect promotion decision, observed difference large happen rarely.\\(H_A\\): Alternative hypothesis. Gender effect promotion decision, observed actually due equally qualified women discriminated promotion decisions, explains large difference 29.2%.\\(H_A\\): Alternative hypothesis. Gender effect promotion decision, observed actually due equally qualified women discriminated promotion decisions, explains large difference 29.2%.conduct formal studies, reject null position (idea data result chance ) data strongly conflict null position.121\nanalysis, determined \\(\\approx\\) 2% probability obtaining sample \\(\\geq\\) 29.2% males females get promoted chance alone, conclude data provide strong evidence gender discrimination women supervisors.","code":""},{"path":"inference-cat.html","id":"scope-of-inference-1","chapter":"5 Inference for categorical data","heading":"Scope of inference","text":"Since study randomized experiment, can conclude effect due gender discrimination—gender application caused lower rate promotion. However, since study convenience sample, can generalize result individuals similar study. Thus, evidence gender discrimination, among male bank supervisors attending management institute University North Carolina 1972 similar study.","code":""},{"path":"inference-cat.html","id":"caseStudyOpportunityCost","chapter":"5 Inference for categorical data","heading":"5.4.1.2 Case study: Opportunity cost","text":"rational consistent behavior typical American college student?\nsection, ’ll explore whether college student consumers always consider following: money spent now can spent later.particular, interested whether reminding students well-known fact money causes little thriftier.\nskeptic might think reminder impact.\ncan summarize two different perspectives using null alternative hypothesis framework.\\(H_0\\): Null hypothesis. Reminding students can save money later purchases impact students’ spending decisions.\\(H_0\\): Null hypothesis. Reminding students can save money later purchases impact students’ spending decisions.\\(H_A\\): Alternative hypothesis. Reminding students can save money later purchases reduce chance continue purchase.\\(H_A\\): Alternative hypothesis. Reminding students can save money later purchases reduce chance continue purchase.statistical notation, can define parameters \\(\\pi_{ctrl}\\) = probability student control condition (reminding can save money later purchases) refrains making purchase, \\(\\pi_{trmt}\\) = probability student treatment condition (reminding can save money later purchases) refrains makes purchase. hypotheses \\(H_0: \\pi_{trmt} - \\pi_{ctrl} = 0\\)\\(H_0: \\pi_{trmt} - \\pi_{ctrl} = 0\\)\\(H_A: \\pi_{trmt} - \\pi_{ctrl} > 0\\)\\(H_A: \\pi_{trmt} - \\pi_{ctrl} > 0\\)section, ’ll explore experiment conducted researchers investigates question students university southwestern United States.124","code":""},{"path":"inference-cat.html","id":"observed-data-1","chapter":"5 Inference for categorical data","heading":"Observed data","text":"One-hundred fifty students recruited study, given following statement:Imagine saving extra money side make purchases, recent visit video store come across special sale new video. video one favorite actor actress, favorite type movie (comedy, drama, thriller, etc.). particular video considering one thinking buying long time. available special sale price $14.99.situation? Please circle one options .Half 150 students randomized control group given following two options:Buy entertaining video.buy entertaining video.remaining 75 students placed treatment group, saw slightly modified option (B):Buy entertaining video.buy entertaining video. Keep $14.99 purchases.extra statement reminding students obvious fact impact purchasing decision?\nTable 5.4 summarizes study results.\nTable 5.4: Summary student choices opportunity cost study.\nmight little easier review results using row proportions, specifically considering proportion participants group said buy buy DVD.\nsummaries given Table 5.5, segmented bar plot provided Figure 5.22.\nTable 5.5: data now summarized using row proportions. Row proportions particularly useful since can view proportion buy buy decisions group.\n\nFigure 5.22: Segmented bar plot comparing proportion bought buy DVD control treatment groups.\ndefine success study student chooses buy DVD.125\n, value interest change DVD purchase rates results reminding students spending money now means can spend money later.can construct point estimate difference \n\\[\\begin{align*}\n\\hat{p}_{trmt} - \\hat{p}_{ctrl}\n = \\frac{34}{75} - \\frac{19}{75}\n = 0.453 - 0.253\n = 0.200\n\\end{align*}\\]\nproportion students chose buy DVD 20% higher treatment group control group.\nHowever, result statistically significant? words, 20% difference two groups prominent unlikely occurred chance alone?","code":""},{"path":"inference-cat.html","id":"variability-of-the-statistic-1","chapter":"5 Inference for categorical data","heading":"Variability of the statistic","text":"primary goal data analysis understand sort differences might see null hypothesis true, .e., treatment effect students.\n, ’ll use procedure applied Section 5.4.1.1: randomization.Let’s think data context hypotheses.\nnull hypothesis (\\(H_0\\)) true treatment impact student decisions, observed difference two groups 20% attributed entirely chance.\n, hand, alternative hypothesis (\\(H_A\\)) true, difference indicates reminding students saving later purchases actually impacts buying decisions.","code":""},{"path":"inference-cat.html","id":"observed-statistic-vs.-null-value-1","chapter":"5 Inference for categorical data","heading":"Observed statistic vs. null value","text":"Just like gender discrimination study, can perform statistical analysis.\nUsing randomization technique last section, let’s see happens simulate experiment scenario effect treatment.reality simulation computer, might useful think go carrying simulation without computer.\nstart 150 index cards label card indicate distribution response variable: decision.\n, 53 cards labeled “buy DVD” represent 53 students opted buy, 97 labeled “buy DVD” 97 students.\nshuffle cards thoroughly divide two stacks size 75, representing simulated treatment control groups.\nobserved difference proportions “buy DVD” cards (earlier defined success) can attributed entirely chance.Example 5.20  randomly assigning cards simulated treatment control groups, many “buy DVD” cards expect end simulated group?\nexpected difference proportions “buy DVD” cards group?results single randomization chance alone shown Table 5.6.\ntable, can compute difference occurred chance alone:\n\\[\\begin{align*}\n\\hat{p}_{trmt, simulated} - \\hat{p}_{ctrl, simulated}\n = \\frac{24}{75} - \\frac{29}{75}\n = 0.32 - 0.387\n = - 0.067\n\\end{align*}\\]\n\nTable 5.6: Summary student choices simulated groups. group assignment connection student decisions, difference two groups due chance.\nJust one simulation enough get sense sorts differences happen chance alone.\n’ll simulate another set simulated groups compute new difference: 0.013.\n: 0.067.\n: -0.173.\n’ll 1,000 times. results summarized dot plot Figure 5.23, point represents simulation.\nSince many points, convenient summarize results histogram one Figure 5.24, height histogram bar represents fraction observations group.\nFigure 5.23: stacked dot plot 1,000 chance differences produced null hypothesis, \\(H_0\\). Six 1,000 simulations difference least 20% , difference observed study.\n\nFigure 5.24: histogram 1,000 chance differences produced null hypothesis, \\(H_0\\). Histograms like one convenient representation data results large number observations.\ntreatment effect, ’d observe difference least +20% 0.6% time, 1--150 times.\nreally rare!\nInstead, conclude data provide strong evidence treatment effect: reminding students purchase instead spend money later something else lowers chance continue purchase.\nNotice able make causal statement study since study experiment.","code":""},{"path":"inference-cat.html","id":"scope-of-inference-2","chapter":"5 Inference for categorical data","heading":"Scope of inference","text":"Since study randomized experiment, can conclude effect due reminder saving money purchases—reminder caused lower rate purchase. However, since study used volunteer sample (students “recruited”), can generalize result individuals similar study. Thus, evidence reminding students can save money later purchases reduce chance continue purchase, among students similar study.","code":""},{"path":"inference-cat.html","id":"caseStudyMalaria","chapter":"5 Inference for categorical data","heading":"5.4.1.3 Case study: Malaria vaccine","text":"","code":""},{"path":"inference-cat.html","id":"observed-data-2","chapter":"5 Inference for categorical data","heading":"Observed data","text":"consider study new malaria vaccine\ncalled PfSPZ.\nstudy, volunteer patients randomized\none two experiment groups:\n14 patients received experimental vaccine\n6 patients received placebo vaccine.\nNineteen weeks later, 20 patients exposed\ndrug-sensitive malaria virus strain;\nmotivation using drug-sensitive strain\nvirus ethical considerations,\nallowing infections treated effectively.\nresults summarized \nTable 5.7,\n9 14 treatment patients remained free\nsigns infection 6 patients\ncontrol group patients showed baseline\nsigns infection.\nTable 5.7: Summary results malaria vaccine experiment.\nstudy, smaller proportion patients\nreceived vaccine showed signs infection\n(35.7% versus 100%).\nHowever, sample small,\nunclear whether difference provides\nconvincing evidence vaccine \neffective. determine , need perform\nstatistical inference.Instead using difference proportion infected summary measure,\nlet’s use relative risk infection case study. Thus,\nparameter interest \\(\\pi_{Vac} / \\pi_{Pla}\\), point estimate\nparameter \\[\n\\frac{\\hat{p}_{Vac}}{\\hat{p}_{Pla}} = \\frac{5/14}{6/6} = 0.357.\n\\]Converting percent decrease127, \nsee patients vaccine group 64.3% reduced risk infection\ncompared placebo group.128In terms relative risk, null alternative hypotheses areIndependence model \\(H_0: \\dfrac{\\pi_{Vac}}{\\pi_{Pla}} = 1\\)\nAlternative model \\(H_A: \\dfrac{\\pi_{Vac}}{\\pi_{Pla}} < 1\\)mean independence model,\nsays vaccine influence \nrate infection, true?\nmean 11 patients going \ndevelop infection matter group\nrandomized ,\n9 patients develop infection\nmatter group randomized\n.\n, vaccine affect rate\ninfection, difference infection rates\ndue chance alone patients \nrandomized.Now consider alternative model:\ninfection rates influenced whether patient\nreceived vaccine .\ntrue, especially influence\nsubstantial, expect see difference\ninfection rates patients groups.choose two competing claims\nassessing data conflict much \n\\(H_0\\) independence model deemed\nreasonable.\ncase, data support \\(H_A\\),\nreject notion independence\nconclude vaccine effective.","code":""},{"path":"inference-cat.html","id":"variability-of-the-statistic-2","chapter":"5 Inference for categorical data","heading":"Variability of the statistic","text":"’re going implement simulation,\npretend know malaria\nvaccine tested  work.\nUltimately, want understand large\ndifference observed common \nsimulations.\ncommon, maybe difference\nobserved purely due chance.\nuncommon, possibility\nvaccine helpful seems plausible.can randomize responses (infection infection) treatment conditions null hypothesis independence, time, ’ll compute sample relative risks simulated sample.Figure 5.25 shows histogram\nrelative risks found 1,000 randomization simulations,\ndot represents simulated relative risk infection (treatment rate divided control rate).\nFigure 5.25: histogram relative risks infection 1,000 simulations produced independence model \\(H_0\\), simulations infections unaffected vaccine. Seventeen 1,000 simulations (shaded red) relative risk 0.357, relative risk observed study.\n","code":""},{"path":"inference-cat.html","id":"observed-statistic-vs-null-value","chapter":"5 Inference for categorical data","heading":"Observed statistic vs null value","text":"Note distribution simulated differences\ncentered around 1.\nsimulated relative risks assuming independence\nmodel true, condition,\nexpect difference near one random\nfluctuation, near pretty generous \ncase since sample sizes small study.Example 5.21  often observe sample relative risk\n0.357 (least 64.3% reduction risk vaccine)\naccording Figure 5.25?\nOften, sometimes, rarely, never?Based simulations, two options:conclude study results provide\nstrong evidence independence model.\n, sufficiently strong evidence\nconclude vaccine effect \nclinical setting.conclude study results provide\nstrong evidence independence model.\n, sufficiently strong evidence\nconclude vaccine effect \nclinical setting.conclude evidence sufficiently strong\nreject \\(H_0\\) assert vaccine useful.\nconduct formal studies, usually reject \nnotion just happened observe rare\nevent.130We conclude evidence sufficiently strong\nreject \\(H_0\\) assert vaccine useful.\nconduct formal studies, usually reject \nnotion just happened observe rare\nevent.130In case, reject independence model favor\nalternative.\n, concluding data provide strong evidence\nvaccine provides protection malaria\nclinical setting.Statistical inference built\nevaluating whether differences due chance.\nstatistical inference, data scientists evaluate \nmodel reasonable given data.\nErrors occur, just like rare events, might choose\nwrong model.\nalways choose correctly, statistical\ninference gives us tools control evaluate \noften errors occur.","code":""},{"path":"inference-cat.html","id":"two-sided-tests","chapter":"5 Inference for categorical data","heading":"5.4.2 Two-sided hypotheses","text":"gender discrimination opportunity cost studies, explored whether women discriminated whether simple trick make students little thriftier.\ntwo case studies, ’ve actually ignored possibilities:men actually discriminated ?money trick actually makes students spend ?original hypotheses ’ve seen called one-sided hypothesis tests explored one direction possibilities.\nhypotheses appropriate exclusively interested single direction, usually want consider possibilities.\n, let’s learn two-sided hypothesis tests context new study examines impact using blood thinners patients undergone CPR.","code":""},{"path":"inference-cat.html","id":"case-study-cpr-and-blood-thinner-cpr","chapter":"5 Inference for categorical data","heading":"Case study: CPR and blood thinner {#cpr}","text":"Cardiopulmonary resuscitation (CPR) procedure used individuals suffering heart attack emergency resources unavailable.\nprocedure helpful providing blood circulation keep person alive, CPR chest compressions can also cause internal injuries. Internal bleeding injuries can result CPR complicate additional treatment efforts.\ninstance, blood thinners may used help release clot causing heart attack patient arrives hospital. However, blood thinners negatively affect internal injuries.consider experiment patients underwent CPR heart attack subsequently admitted hospital.131\npatient randomly assigned either receive blood thinner (treatment group) receive blood thinner (control group).\noutcome variable interest whether patient survived least 24 hours.Example 5.22  Form hypotheses study plain statistical language.\nLet \\(\\pi_c\\) represent true survival rate people receive blood thinner (corresponding control group) \\(\\pi_t\\) represent true survival rate people receiving blood thinner (corresponding treatment group).want understand whether blood thinners helpful harmful.\n’ll consider possibilities using two-sided hypothesis test.\\(H_0\\): Blood thinners overall survival effect, .e., \\(\\pi_t - \\pi_c = 0\\).\\(H_0\\): Blood thinners overall survival effect, .e., \\(\\pi_t - \\pi_c = 0\\).\\(H_A\\): Blood thinners impact survival, either positive negative, zero, .e., \\(\\pi_t - \\pi_c \\neq 0\\).\\(H_A\\): Blood thinners impact survival, either positive negative, zero, .e., \\(\\pi_t - \\pi_c \\neq 0\\).Note done one-sided hypothesis test, resulting hypotheses :\\(H_0\\): Blood thinners positive overall survival effect, .e., \\(\\pi_t - \\pi_c = 0\\).\\(H_0\\): Blood thinners positive overall survival effect, .e., \\(\\pi_t - \\pi_c = 0\\).\\(H_A\\): Blood thinners positive impact survival, .e., \\(\\pi_t - \\pi_c > 0\\).\\(H_A\\): Blood thinners positive impact survival, .e., \\(\\pi_t - \\pi_c > 0\\).50 patients experiment receive blood thinner 40 patients .\nstudy results shown Table 5.8.\nTable 5.8: Results CPR study. Patients treatment group given blood thinner, patients control group .\nAccording point estimate, patients undergone CPR outside hospital, additional 13% patients survive treated blood thinners. Interpreting relative risk, patients sample undergone CPR outside hospital 59% higher survival rate treated blood thinners.\nHowever, wonder difference easily explainable chance.past studies chapter, simulate type differences might see chance alone null hypothesis.\nrandomly assigning “simulated treatment” “simulated control” stickers patients’ files, get new grouping.\nrepeat simulation 10,000 times, can build null distribution differences sample proportions shown Figure 5.26.\nFigure 5.26: Null distribution point estimate difference proportions, \\(\\hat{p}_t - \\hat{p}_c\\). shaded right tail shows observations least large observed difference, 0.13.\nright tail area 0.131.133\nHowever, contrary calculated p-value previous studies, p-value test 0.131!p-value defined chance observe result least favorable alternative hypothesis result (.e., difference) observe.\ncase, differences less equal $-$0.13 also provide equally strong evidence favoring alternative hypothesis difference $+$0.13 .\ndifference $-$0.13 correspond survival rate control group 0.13 higher treatment group.134\nFigure 5.27 ’ve also shaded differences left tail distribution.\ntwo shaded tails provide visual representation p-value two-sided test.\nFigure 5.27: Null distribution point estimate difference proportions, \\(\\hat{p}_t - \\hat{p}_c\\). values least extreme +0.13 either direction away 0 shaded.\ntwo-sided test, since null distribution symmetric, take single tail (case, 0.131) double get p-value: 0.262.\nlarge p-value, find statistically significant evidence blood thinner influence survival patients undergo CPR prior arriving hospital.Default two-sided test.Computing p-value two-sided test.Example 5.23  Consider situation medical consultant.\nNow know one-sided two-sided tests, type test think appropriate?Generally, find two-sided p-value double single tail area, remains reasonable approach even sampling distribution asymmetric.\nHowever, approach can result p-values larger 1 point estimate near mean null distribution; cases, write p-value 1.\nAlso, large p-values computed way (e.g., 0.85), may also slightly inflated.\nTypically, worry much precision large p-values lead analysis conclusion, even value slightly .","code":""},{"path":"inference-cat.html","id":"two-prop-boot-ci","chapter":"5 Inference for categorical data","heading":"5.4.3 Bootstrap confidence interval for \\(\\pi_1 - \\pi_2\\)","text":"Section 5.4.1, worked randomization distribution understand distribution \\(\\hat{p}_1 - \\hat{p}_2\\) null hypothesis \\(H_0: \\pi_1 - \\pi_2 = 0\\) true.\nNow, bootstrapping, study variability \\(\\hat{p}_1 - \\hat{p}_2\\) without null assumption.","code":""},{"path":"inference-cat.html","id":"observed-data-3","chapter":"5 Inference for categorical data","heading":"Observed data","text":"Reconsider CPR data Section 5.4.1 provided Table 5.8. experiment consisted two treatments patients underwent CPR heart attack subsequently admitted hospital. patient randomly assigned either receive blood thinner (treatment group) receive blood thinner (control group).\noutcome variable interest whether patient survived least 24 hours., use difference sample proportions observed statistic interest. , value statistic : \\(\\hat{p}_t - \\hat{p}_c = 0.35 - 0.22 = 0.13\\).","code":""},{"path":"inference-cat.html","id":"variability-of-the-statistic-3","chapter":"5 Inference for categorical data","heading":"Variability of the statistic","text":"bootstrap method applied two samples extension method described Section 5.3.2. Now, two samples, sample estimates population came. CPR setting, treatment sample estimates population individuals gotten (get) treatment; control sample estimate population individuals get treatment controls. Figure 5.28 extends Figure 5.11 show bootstrapping process two samples simultaneously.\nFigure 5.28: Creating two estimated populations two different samples different populations.\n, population estimated, can randomly resample observations create bootstrap samples, seen Figure 5.29. Computationally, bootstrap resample\ncreated randomly sampling replacement original sample.\nFigure 5.29: Bootstrapped resamples two separate estimated populations.\nvariability statistic (difference sample proportions) can calculated taking one treatment bootstrap sample one control bootstrap sample calculating difference bootstrap survival proportions. Figure @(boot2samp2) displays one bootstrap resample estimated populations, difference sample proportions calculated treatment bootstrap sample control bootstrap sample.\nFigure 5.30: bootstrap resample left first estimated population; one right second. case, value simulated bootstrap statistic \\(\\hat{p}_1 - \\hat{p}_2 = \\frac{2}{7}-\\frac{1}{7}\\).\nalways, variability difference proportions can estimated repeated simulations, case, repeated bootstrap samples. Figure 5.31 shows multiple bootstrap differences calculated repeated bootstrap samples.\nFigure 5.31: graph, kind connection two sides\nRepeated bootstrap simulations lead bootstrap sampling distribution statistic interest, difference sample proportions.\nFigure 5.32 visualizes process toy example, Figure 5.33 shows 1000 bootstrap differences proportions CPR data.\nFigure 5.32: process repeatedly resampling estimated population (sampling replacement original sample), computing difference sample proportions pair samples, plotting distribution.\n\nFigure 5.33: histogram differences proportions (treatment \\(-\\) control) 1000 bootstrap simulations using CPR data.\n","code":""},{"path":"inference-cat.html","id":"percentile-vs.-se-bootstrap-confidence-intervals","chapter":"5 Inference for categorical data","heading":"Percentile vs. SE bootstrap confidence intervals","text":"Figure 5.33 provides estimate variability difference survival proportions sample sample, values histogram can used two different ways create confidence interval parameter interest: \\(\\pi_1 - \\pi_2\\).","code":""},{"path":"inference-cat.html","id":"percentile-bootstrap-interval","chapter":"5 Inference for categorical data","heading":"Percentile bootstrap interval","text":"Section 5.3.2, bootstrap confidence interval can calculated directly bootstrapped differences Figure 5.33. interval created percentiles distribution called percentile interval.\nNote calculate 90% confidence interval finding 5th 95th percentile values bootstrapped differences.\nbootstrap 5 percentile proportion -0.155 95 percentile 0.167.\nresult : 90% confident , population, true difference probability survival (treatment \\(-\\) control) -0.155 0.167.\nclearly, 90% confident probability survival heart attack patients underwent CPR blood thinners 0.155 less 0.167 patients given blood thinners. interval shows much definitive evidence affect blood thinners, one way another.\nFigure 5.34: CPR data bootstrapped 1000 times. simulation creates sample original data probability survival treatment group \\(\\hat{p}_{t} = 14/40\\) probability survival control group \\(\\hat{p}_{c} = 11/50\\).\n","code":""},{"path":"inference-cat.html","id":"se-bootstrap-interval","chapter":"5 Inference for categorical data","heading":"SE bootstrap interval","text":"Alternatively, can use variability bootstrapped differences calculate standard error difference.\nresulting interval called SE interval.\nSection 5.4.4 details mathematical model standard error difference sample proportions, bootstrap distribution typically excellent job estimating variability.\\[SE(\\hat{p}_t - \\hat{p}_c) \\approx SD(\\hat{p}_{bs,t} - \\hat{p}_{bs,c}) = 0.0975\\]variability bootstrapped difference proportions calculated R using sd() function, statistical software calculate standard deviation differences, , exact quantity hope approximate.Note know know true distribution \\(\\hat{p}_t - \\hat{p}_c\\), use rough approximation find confidence interval \\(\\pi_t - \\pi_c\\). seen bootstrap histograms, shape distribution roughly symmetric bell-shaped. rough approximation, apply 68-95-99.7 rule tells us 95% observed differences roughly farther 2 SE true parameter difference. approximate 95% confidence interval \\(\\pi_t - \\pi_c\\) given :\\[\\begin{align*}\n\\hat{p}_t - \\hat{p}_c \\pm 2 \\cdot SE \\ \\ \\ \\rightarrow \\ \\ \\ 14/40 - 11/50 \\pm 2 \\cdot 0.0975 \\ \\ \\  \\rightarrow \\ \\ \\  (-0.065, 0.325)\n\\end{align*}\\]95% confident true value \\(\\pi_t - \\pi_c\\) -0.065 0.325. , wide confidence interval overlaps zero indicates study provides little evidence effectiveness blood thinners.","code":""},{"path":"inference-cat.html","id":"what-does-95-mean","chapter":"5 Inference for categorical data","heading":"What does 95% mean?","text":"Recall goal confidence interval find plausible range values parameter interest.\nestimated statistic value interest, typically best guess unknown parameter.\nconfidence level (often 95%) number takes get used . Surprisingly, percentage doesn’t describe data set hand, describes many possible data sets.\nOne way understand confidence interval think confidence intervals ever made ever make scientist, confidence level describes intervals.Figure 5.35 demonstrates hypothetical situation 25 different studies performed exact population (goal estimating true parameter value \\(\\pi_1 - \\pi_2 = 0.47\\)).\nstudy hand represents one point estimate (dot) corresponding interval.\npossible know whether interval hand right unknown true parameter value (black line) left line.\nalso impossible know whether interval captures true parameter (blue) doesn’t (red).\nmaking 95% intervals, 5% intervals create lifetime capture parameter interest (e.g., red Figure 5.35 ).\nknow lifetimes scientists, 95% intervals created reported capture parameter value interest: thus language “95% confident.”\nFigure 5.35: One hypothetical population, parameter value : \\(\\pi_1 - \\pi_2 = 0.47\\). Twenty-five different studies led different point estimate, SE, confidence interval. study hand one horizontal lines (hopefully blue line!).\nchoice 95% 90% even 99% confidence level admittedly somewhat arbitrary; however, related logic used deciding p-value declared significant lower 0.05 (0.10 0.01, respectively).\nIndeed, one can show mathematically, 95% confidence interval two-sided hypothesis test cutoff 0.05 provide conclusion data mathematical tools applied analysis.\nfull derivation explicit connection confidence intervals hypothesis tests beyond scope text.","code":""},{"path":"inference-cat.html","id":"math-2prop","chapter":"5 Inference for categorical data","heading":"5.4.4 Theory-based inferential methods for \\(\\pi_1 - \\pi_2\\)","text":"Like \\(\\hat{p}\\), difference two sample\nproportions \\(\\hat{p}_1 - \\hat{p}_2\\) can modeled\nusing normal distribution certain conditions\nmet.","code":""},{"path":"inference-cat.html","id":"evaluating-the-two-conditions-required-for-modeling-pi_1---pi_2-using-theory-based-methods","chapter":"5 Inference for categorical data","heading":"Evaluating the two conditions required for modeling \\(\\pi_1 - \\pi_2\\) using theory-based methods","text":"First, require broader independence condition,\nsecondly,\nsuccess-failure condition must met groups.Conditions sampling distribution \\(\\hat{p}_1 -\\hat{p}_2\\) normal.difference \\(\\hat{p}_1 - \\hat{p}_2\\) can modeled\nusing normal distribution whenIndependence (extended). data independent within \ntwo groups. Generally satisfied data come two independent random samples data come randomized experiment.Success-failure condition.\nsuccess-failure condition holds \ngroups, check successes failures\ngroup separately. condition met least 10 successes 10 failures sample. data displayed two-way table, equivalent checking cells table least 10 observations.conditions satisfied,\nsampling distribution \\(\\hat{p}_1 - \\hat{p}_2\\) \napproximately normal mean \\(\\pi_1 - \\pi_2\\) standard deviationAs case one proportion, typically don’t know true proportions \\(\\pi_1\\) \\(\\pi_2\\),\nsubstitute value check success-failure condition\nestimate standard deviation sampling distribution \\(\\hat{p}_1 - \\hat{p}_2\\).","code":""},{"path":"inference-cat.html","id":"confidence-interval-for-pi_1---pi_2","chapter":"5 Inference for categorical data","heading":"Confidence interval for \\(\\pi_1 - \\pi_2\\)","text":"Standard error difference two proportions, \\(\\hat{p}_1 -\\hat{p}_2\\): confidence intervals.computing theory-based confidence interval \\(\\pi_1 - \\pi_2\\), substitute \\(\\hat{p}_1\\) \\(\\pi_1\\) \\(\\hat{p}_2\\) \\(\\pi_2\\) expression standard deviation statistic, resulting standard error:\\[\\begin{eqnarray*}\n  SE(\\hat{p}_1 -\\hat{p}_2) = \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\n  \\end{eqnarray*}\\]conditions sampling distribution \\(\\hat{p}_1 - \\hat{p}_2\\) normal met, can apply generic confidence interval formula\ndifference two proportions,\nuse \\(\\hat{p}_1 - \\hat{p}_2\\) point\nestimate substitute \\(SE\\) formula :\n\\[\\begin{align*}\n&\\text{point estimate} \\ \\pm\\  z^{\\star} \\times SE\n&&\\\n&&\\hat{p}_1 - \\hat{p}_2 \\ \\pm\\ \n    z^{\\star} \\times\n   \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\n\\end{align*}\\]Example 5.24  reconsider experiment patients\nunderwent cardiopulmonary resuscitation (CPR)\nheart attack \nsubsequently admitted \nhospital.\npatients randomly divided treatment\ngroup received blood thinner control\ngroup receive blood thinner.\noutcome variable interest whether \npatients survived least 24 hours.\nresults shown \nTable 5.8.\nCheck whether can model difference \nsample proportions using normal distribution.first check independence:\nsince randomized experiment,\ncondition satisfied.Next, check success-failure condition \ngroup.\nleast 10 successes 10 failures \nexperiment arm (11, 14, 39, 26),\ncondition also satisfied.Example 5.25  Create interpret 90% confidence interval \ndifference survival rates CPR study.’ll use \\(\\pi_t\\) true survival\nrate treatment group \\(\\pi_c\\) control\ngroup. point estimate \\(\\pi_t - \\pi_c\\) :\n\\[\\begin{align*}\n  \\hat{p}_{t} - \\hat{p}_{c}\n    = \\frac{14}{40} - \\frac{11}{50}\n    = 0.35 - 0.22\n    = 0.13\n  \\end{align*}\\]\nuse standard error formula previously provided.\none-sample proportion case,\nuse sample estimates proportion\nformula confidence interval context:\n\\[\\begin{align*}\n  SE \\approx \\sqrt{\\frac{0.35 (1 - 0.35)}{40} +\n      \\frac{0.22 (1 - 0.22)}{50}}\n    = 0.095\n  \\end{align*}\\]\n90% confidence interval, use \\(z^{\\star} = 1.65\\):\n\\[\\begin{align*}\n  \\text{point estimate} \\ \\pm\\ z^{\\star} \\times SE\n    \\quad \\\\quad 0.13 \\ \\pm\\ 1.65 \\times  0.095\n    \\quad \\\\quad (-0.027, 0.287)\n  \\end{align*}\\]\n90% confident survival probability \npatients given blood thinners 0.027 lower 0.287 higher\npatients given blood thinners, among patients like\nstudy.\n0% contained interval,\nenough information say\nwhether blood thinners help harm\nheart attack patients admitted \nundergone CPR.5-year experiment\nconducted evaluate effectiveness\nfish oils reducing cardiovascular events,\nsubject randomized one two\ntreatment groups.\nconsider heart attack outcomes patients listed Table 5.9.\nTable 5.9: Results study n-3 fatty acid supplement related health benefits.\n","code":""},{"path":"inference-cat.html","id":"hypothesis-test-for-h_0-pi_1---pi_2-0","chapter":"5 Inference for categorical data","heading":"Hypothesis test for \\(H_0: \\pi_1 - \\pi_2 = 0\\)","text":"\nmammogram X-ray procedure used check \nbreast cancer.\nWhether mammograms used part \ncontroversial discussion, ’s topic \nnext example learn two proportion\nhypothesis tests \\(H_0\\) \\(\\pi_1 - \\pi_2 = 0\\)\n(equivalently, \\(\\pi_1 = \\pi_2\\)).30-year study conducted nearly 90,000 female participants. 5-year screening period, woman randomized one two groups: first group, women received regular mammograms screen breast cancer, second group, women received regular non-mammogram breast cancer exams. intervention made following 25 years study, ’ll consider death resulting breast cancer full 30-year period. Results study summarized Figure 5.10.mammograms much effective non-mammogram breast cancer exams, expect see additional deaths breast cancer control group. hand, mammograms effective regular breast cancer exams, expect see increase breast cancer deaths mammogram group.\nTable 5.10: Summary results breast cancer study.\nresearch question describing mammograms set address specific hypotheses (contrast confidence interval parameter). order fully take advantage hypothesis testing structure, assess randomness condition null hypothesis true (always hypothesis testing).\nUsing data Table 5.10,\ncheck conditions using normal distribution \nanalyze results study using hypothesis test.\ndetails checking conditions similar confidence intervals.\nHowever, null hypothesis \\(\\pi_1 - \\pi_2 = 0\\),\nuse special proportion called \npooled proportion check success-failure condition computing standard error:\n\\[\\begin{align*}\n\\hat{p}_{\\textit{pool}}\n    &= \\frac\n        {\\text{# patients died breast cancer entire study}}\n        {\\text{# patients entire study}} \\\\\n        &\\\\\n    &= \\frac{500 + 505}{500 + \\text{44,425} + 505 + \\text{44,405}} \\\\\n    &\\\\\n    &= 0.0112\n\\end{align*}\\]\nproportion estimate breast cancer death rate\nacross entire study, ’s best estimate \ndeath rates \\(\\pi_{mgm}\\) \\(\\pi_{ctrl}\\)\nnull hypothesis true \\(\\pi_{mgm} = \\pi_{ctrl}\\).Use pooled proportion \\(H_0\\) \\(\\pi_1 - \\pi_2 = 0\\).Example 5.26  reasonable model difference\nproportions using normal distribution \nstudy?used pooled proportion check success-failure\ncondition139. next use standard error calculation.Standard error difference two proportions, \\(\\hat{p}_1 -\\hat{p}_2\\): hypothesis tests.Since assume \\(\\pi_1 = \\pi_2\\) conduct theory-based hypothesis test \\(H_0: \\pi_1 - \\pi_2 = 0\\), substitute pooled sample proportion, \\(\\hat{p}_{pool}\\) \\(\\pi_1\\) \\(\\pi_2\\) expression standard deviation statistic, resulting null standard error:\\[\\begin{eqnarray*}\n  SE_0(\\hat{p}_1 -\\hat{p}_2) = \\sqrt{\\frac{\\hat{p}_{pool}(1-\\hat{p}_{pool})}{n_1} + \\frac{\\hat{p}_{pool}(1-\\hat{p}_{pool})}{n_2}} = \\sqrt{\\hat{p}_{pool}(1-\\hat{p}_{pool})\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}\n  \\end{eqnarray*}\\]Example 5.27  Compute point estimate difference\nbreast cancer death rates two groups,\nuse pooled proportion\n\\(\\hat{p}_{\\textit{pool}} = 0.0112\\) calculate\nstandard error.point estimate difference breast cancer death\nrates \n\\[\\begin{align*}\n  \\hat{p}_{mgm} - \\hat{p}_{ctrl}\n    &= \\frac{500}{500 + 44,425} - \\frac{505}{505 + 44,405} \\\\\n  & \\\\\n    &= 0.01113 - 0.01125 \\\\\n  & \\\\\n    &= -0.00012\n  \\end{align*}\\]\nbreast cancer death rate mammogram group\n0.00012 less control group.Example 5.28  Using point estimate \\(\\hat{p}_{mgm} - \\hat{p}_{ctrl} = -0.00012\\) standard error \\(SE = 0.00070\\), calculate p-value hypothesis test write conclusion.Just like past tests, first compute test statistic draw picture:\n\\[\\begin{align*}\nZ = \\frac{\\text{point estimate} - \\text{null value}}{\\mbox{Null }SE}\n    = \\frac{-0.00012 - 0}{0.00070}\n    = -0.17\n\\end{align*}\\]\nFigure 5.36: Standard normal distribution p-value shaded. shaded area represents probability observing difference sample proportions -0.17 away zero, true proportions equal.\nCan conclude mammograms benefits harm?\nconsiderations keep mind reviewing\nmammogram study well medical study:accept null hypothesis. can say\ndon’t sufficient evidence conclude \nmammograms reduce breast cancer deaths, \ndon’t sufficient evidence conclude \nmammograms increase breast cancer deaths.mammograms helpful harmful, data\nsuggest effect isn’t large.mammograms less expensive \nnon-mammogram breast exam?\none option much expensive \ndoesn’t offer clear benefits,\nlean towards less expensive\noption.study’s authors also found mammograms\nled -diagnosis breast cancer,\nmeans breast cancers found\n(thought found) cancers\ncause symptoms patients’ lifetimes.\n, something else kill patient\nbreast cancer symptoms appeared.\nmeans patients may treated\nbreast cancer unnecessarily, \ntreatment another cost consider.\nalso important recognize \n-diagnosis can cause unnecessary physical\nemotional harm patients.considerations highlight complexity around medical care treatment recommendations. Experts medical boards study medical treatments use considerations like provide best recommendation based current evidence.\n","code":""},{"path":"inference-cat.html","id":"power","chapter":"5 Inference for categorical data","heading":"5.5 Errors, Power, and Practical Importance","text":"","code":""},{"path":"inference-cat.html","id":"types-of-errors","chapter":"5 Inference for categorical data","heading":"5.5.1 Decision errors","text":"Hypothesis tests flawless. Just think court system: innocent people sometimes wrongly convicted guilty sometimes walk free.\nSimilarly, data can point wrong conclusion.\nHowever, distinguishes statistical hypothesis tests court system framework allows us quantify control often data lead us incorrect conclusion.hypothesis test, two competing hypotheses: null alternative.\nmake statement one might true, might choose incorrectly. four possible scenarios hypothesis test, summarized Table 5.11.\nTable 5.11: Four different scenarios hypothesis tests.\nType 1 Error rejecting null hypothesis \\(H_0\\) actually true.\nSince rejected null hypothesis gender discrimination, opportunity cost, malaria studies, possible made Type 1 Error one, two, three studies.Type 2 Error failing reject null hypothesis alternative actually true. Since failed reject null hypothesis medical consultant study, possible made Type 2 Error study.Example 5.29  US court, defendant either innocent (\\(H_0\\)) guilty (\\(H_A\\)).\nType 1 Error represent context?\nType 2 Error represent?\nTable 5.11 may useful.Example 5.30  reduce Type 1 Error rate US courts?\ninfluence Type 2 Error rate?example guided practice provide important lesson: reduce often make one type error, generally make type.","code":""},{"path":"inference-cat.html","id":"controlling-the-type-i-error-rate","chapter":"5 Inference for categorical data","heading":"5.5.2 Controlling the Type I error rate","text":"\nChoosing significance level test important many contexts, traditional level 0.05.\nHowever, sometimes helpful adjust significance level based application.\nmay select level smaller larger 0.05 depending consequences conclusions reached test.Significance level = probability making Type 1 error.making Type 1 Error dangerous especially costly, choose small significance level (e.g., 0.01 0.001).\nwant cautious rejecting null hypothesis, demand strong evidence favoring alternative \\(H_A\\) reject \\(H_0\\).Type 2 Error relatively dangerous much costly Type 1 Error, choose higher significance level (e.g., 0.10).\nwant cautious failing reject \\(H_0\\) null actually false.Significance levels reflect consequences errors.result increased error rates, never okay change two-sided tests one-sided tests observing data. two dangers ignore possibilities disagree data conflict world view:Framing alternative hypothesis simply match direction data point generally inflate Type 1 Error rate. work ’ve done (continue ) rigorously control error rates hypothesis tests, careless construction alternative hypotheses can disrupt hard work.Framing alternative hypothesis simply match direction data point generally inflate Type 1 Error rate. work ’ve done (continue ) rigorously control error rates hypothesis tests, careless construction alternative hypotheses can disrupt hard work.use alternative hypotheses agree worldview, ’re going subjecting confirmation bias, means looking data supports ideas. ’s scientific, can better!use alternative hypotheses agree worldview, ’re going subjecting confirmation bias, means looking data supports ideas. ’s scientific, can better!explore consequences ignoring advice next example.Example 5.31  Using \\(\\alpha=0.05\\), show freely switching two-sided tests one-sided tests lead us make twice many Type 1 Errors intended.Suppose interested finding difference 0.\n’ve created smooth-looking null distribution representing differences due chance Figure 5.37.Suppose sample difference larger 0.\ncan flip one-sided test, use \\(H_A\\): difference \\(> 0\\).\nNow obtain observation upper 5% distribution, reject \\(H_0\\) since p-value just single tail.\nThus, null hypothesis true, incorrectly reject null hypothesis 5% time sample mean null value, shown Figure 5.37.Suppose sample difference smaller 0.\nchange one-sided test, use \\(H_A\\): difference \\(< 0\\).\nobserved difference falls lower 5% figure, reject \\(H_0\\).\n, null hypothesis true, observe situation 5% time.\nFigure 5.37: shaded regions represent areas reject \\(H_0\\) bad practices considered \\(\\alpha = 0.05\\).\nHypothesis tests set seeing data.","code":""},{"path":"inference-cat.html","id":"power-1","chapter":"5 Inference for categorical data","heading":"5.5.3 Power","text":"null hypothesis true, probability Type 1 error chosen significance level, \\(\\alpha\\), means probability correct decision \\(1 - \\alpha\\).alternative hypothesis true, probability Type 2 error, denote \\(\\beta\\), depends several components:significance levelsample sizewhether alternative hypothesis one-sided two-sidedstandard deviation statistichow far alternative parameter value null valueOnly first three components within control researcher.probability correct decision alternative hypothesis true, \\(1 - \\beta\\), called power test. Higher power means likely detect effect actually exists.Power.Example 5.32  Suppose like test whether less 65% large population approves new law: \\(H_0: \\pi = 0.65\\) versus \\(H_A: \\pi < 0.65\\). collect random sample \\(n = 200\\) individuals population. values sample proportion, \\(\\hat{p}\\), reject \\(H_0\\) using significance level \\(\\alpha = 0.05\\)?assumption null hypothesis, standard deviation \\(\\hat{p}\\) \\(\\sqrt{0.65(1-0.65)/200} = 0.0337\\). Thus, Central Limit Theorem, sample proportions vary according approximate normal distribution mean 0.65 standard deviation 0.0337. reject null hypothesis true proportion 0.65 sample proportion low probability less 0.05, shown Figure 5.38.precise, reject \\(H_0\\) \\(\\hat{p}\\) less 5th percentile null distribution: qnorm(0.05, 0.65, 0.0337) = 0.59.\nFigure 5.38: Shaded area null distribution reject null hypothesis. area equal significance level.\ncalculate power, need know true value parameter. previous example, alternative \\(H_0: \\pi < 0.65\\), just say alternative hypothesis true, still know value \\(\\pi\\). Thus, power calculations done specific value parameter, power changes value parameter changes.Example 5.33  Consider test whether less 65% large population approves new law: \\(H_0: \\pi = 0.65\\) versus \\(H_A: \\pi < 0.65\\). Suppose population approval rate actually \\(\\pi = 0.58\\). probability detect effect?example asks us calculate power – probability test provide evidence \\(\\pi < 0.65\\) true value \\(\\pi\\) 0.58. Recall previous example reject null \\(\\hat{p} < 0.59\\). Thus, power probability \\(\\hat{p}\\) less 0.59 true proportion 0.58: pnorm(0.59, 0.58, 0.0337) = 0.62. 62% chance data collect provide strong enough evidence conclude \\(\\pi < 0.65\\). probability represented red area Figure 5.39.\nFigure 5.39: blue distribution distribution sample proportions null hypothesis true, \\(\\pi = 0.65\\) – blue shaded area represents probability reject true null hypothesis. red distribution distribution sample proportions particular alternative hypothesis, \\(\\pi = 0.58\\) – red shaded area represents power.\nIncreasing power.power test increase :significance level increasesthe sample size increaseswe change two-sided one-sided testthe standard deviation statistic decreaseshow far alternative parameter value null value increases","code":""},{"path":"inference-cat.html","id":"statistical-significance-versus-practical-importance","chapter":"5 Inference for categorical data","heading":"5.5.4 Statistical Significance versus Practical Importance","text":"Example 5.34  Austrian study heights 507,125 military recruits reported men born spring statistically significantly taller men born fall (p-value < 0.0001). confidence interval true difference mean height men born spring men born fall (0.598, 0.602) cm. result practically important?, results don’t mean much context – difference average height around 0.6 cm even noticeable human eye! Just result statistically significant mean necessarily practically important – meaningful context problem.previous example, saw two groups men differed average height, difference statistically significant – , observed difference sample means 0.6 cm unlikely occur true difference average height zero. , difference 0.6 cm height meaningful – practically important.happen? Recall variability sample statistics decreases sample size increases. example, unknown , suppose slight majority population, say 50.5%, support new ballot measure. want test \\(H_0: \\pi = 0.50\\) versus \\(H_0: \\pi > 0.50\\) population. Since true proportion exactly 0.50, can make p-value smaller given significance level long choose large enough sample size!Figure 5.40 displays scenario. distribution possible sample proportions support new ballot measure samples size \\(n = 100,000\\) 50.5% population supports measure represented black normal curve. dotted red normal curve null distribution sample proportions \\(H_0: \\pi = 0.5\\). little overlap two distributions due large sample size. shaded blue area represents power test \\(H_0: \\pi = 0.5\\) versus \\(H_A: \\pi > 0.5\\) \\(\\alpha = 0.05\\) – 0.885! , 88.5% chance p-value less 0.05, even though true proportion 0.05 0.5!\nFigure 5.40: Black curve: sampling distribution sample proportions samples size 100,000 true proportion 0.505. Red curve: null distribution sample proportions null value 0.50.\nConsider opposite scenario – small sample sizes meaningful difference. Suppose like determine majority population support new ballot measure. However, time money survey 20 people community. Unknown , 65% population support measure.Examine Figure 5.41. distribution possible sample proportions support new ballot measure samples size \\(n = 20\\) 65% population supports measure represented black normal curve. dotted red normal curve null distribution sample proportions \\(H_0: \\pi = 0.5\\). Even though 0.65 quite bit higher 0.50, still lot overlap two distributions due small sample size. shaded blue area represents power test \\(H_0: \\pi = 0.5\\) versus \\(H_A: \\pi > 0.5\\) \\(\\alpha = 0.05\\) – 0.29! , even though 65% population supports measure (much higher 50%), 29% chance detecting difference small sample size.\nFigure 5.41: Black curve: approximate sampling distribution sample proportions samples size 20 true proportion 0.65. Red curve: approximate null distribution sample proportions null value 0.50.\nStatistical significance versus practical importance.large sample sizes, results may statistically significant, practically important. Since sample statistics vary little among samples large sample sizes, easy hypothesis test result small p-value, even observed effect practically meaningless.","code":""},{"path":"inference-cat.html","id":"summary-of-z-procedures","chapter":"5 Inference for categorical data","heading":"5.6 Summary of Z-procedures","text":"far chapter, seen normal distribution applied appropriate mathematical model two distinct settings. Although two data structures different, similarities differences worth pointing . provide Table 5.12 partly mechanism understanding \\(z\\)-procedures partly highlight extremely common usage normal distribution practice.\noften hear following two \\(z\\)-procedures referred one sample \\(z\\)-test (\\(z\\)-interval) two sample \\(z\\)-test (\\(z\\)-interval).\nTable 5.12: Similarities \\(z\\)-methods across one sample two independent samples analysis categorical response variable.\nindependence, 2. large samples (least 10 successes 10 failures)\nindependence, 2. large samples (least 10 successes 10 failures sample)\nHypothesis tests. applying normal distribution hypothesis test, proceed follows:Write appropriate hypotheses.Verify conditions using normal distribution.\nOne-sample: observations must independent, must least 10 successes 10 failures.\ndifference proportions: sample must separately satisfy one-sample conditions normal distribution, data groups must also independent.\nOne-sample: observations must independent, must least 10 successes 10 failures.difference proportions: sample must separately satisfy one-sample conditions normal distribution, data groups must also independent.Compute statistic interest, null standard error, degrees freedom. \\(df\\), use \\(n-1\\) one sample, two samples use either statistical software smaller \\(n_1 - 1\\) \\(n_2 - 1\\).Compute Z-score using general formula:\n\\[\n Z = \\frac{\\mbox{statistic} - \\mbox{null value}}{\\mbox{null standard error statistic}} = \\frac{\\mbox{statistic} - \\mbox{null value}}{SE_0(\\mbox{statistic})}\n \\]Use statistical software find p-value using standard normal distribution:\nSign \\(H_A\\) \\(<\\): p-value = area Z-score\nSign \\(H_A\\) \\(>\\): p-value = area Z-score\nSign \\(H_A\\) \\(\\neq\\): p-value = 2 \\(\\times\\) area \\(-|\\mbox{Z-score}|\\)\nSign \\(H_A\\) \\(<\\): p-value = area Z-scoreSign \\(H_A\\) \\(>\\): p-value = area Z-scoreSign \\(H_A\\) \\(\\neq\\): p-value = 2 \\(\\times\\) area \\(-|\\mbox{Z-score}|\\)Make conclusion based p-value, write conclusion context, plain language, terms alternative hypothesis.Confidence intervals. Similarly, following generally compute confidence interval using normal distribution:Verify conditions using normal distribution. (See .)Compute statistic interest, standard error, \\(z^{\\star}\\).Calculate confidence interval using general formula:\n\\[\n \\mbox{statistic} \\pm\\ z^{\\star} SE(\\mbox{statistic}).\n \\]Put conclusions context plain language even non-data scientists can understand results.","code":""},{"path":"inference-cat.html","id":"r-inference-for-categorical-data","chapter":"5 Inference for categorical data","heading":"5.7 R: Inference for categorical data","text":"","code":""},{"path":"inference-cat.html","id":"inference-using-r-and-catstats","chapter":"5 Inference for categorical data","heading":"5.7.1 Inference using R and catstats","text":"","code":""},{"path":"inference-cat.html","id":"making-tables-from-raw-data","chapter":"5 Inference for categorical data","heading":"Making tables from raw data","text":"collecting data, analysis starts raw data frame, one observational unit per row one variable per column. case, first need find counts (frequencies) observations category prior inference. can use table() function R raw data create contingency table counts categorical variable.one-proportion case, suppose data frame called loans variable regulate contains Yes/response 826 payday loan borrowers Section 5.3.3 regarding support regulation require lenders pull credit report evaluate debt payments. can obtain counts borrowers response using table() function R:R code , loans name data set R, regulate name variable. $ tells R select regulate variable loans data set. also used pipe command generate table:can also use table() compute proportions group:comparisons two proportions, get two-way table. Consider case study effect blood thinners survival receiving CPR Section 5.4.2. Data study stored data frame called cpr, variables survival – giving patient’s outcome decision – group – indicating whether treatment (blood thinner) control (blood thinner) group. glimpse() summary() functions can help us see variables dataset values take :obtain two-way table choices group, use table() function R. key thing remember put response variable (outcome) first argument explanatory variable (grouping) second. example, survival response variable, group explanatory variable.set table useful making segmented bar plots using column percentages compute test statistics. order either things, need store table R object can manipulate :get column percentages, use prop.table() function:","code":"\ntable(loans$regulate)#> \n#>  No Yes \n#> 404 422\nloans %>% select(regulate) %>% table()#> .\n#>  No Yes \n#> 404 422\n#If we know the number of observations:\ntable(loans$regulate)/826#> \n#>        No       Yes \n#> 0.4891041 0.5108959\n#If we don't know the number of observations:\ntable(loans$regulate)/length(loans$regulate)#> \n#>        No       Yes \n#> 0.4891041 0.5108959\nglimpse(cpr)#> Rows: 90\n#> Columns: 2\n#> $ survival <fct> Survived, Survived, Survived, Survived, Survived, Survived, …\n#> $ group    <fct> control, control, control, control, control, control, contro…\nsummary(cpr)#>      survival        group   \n#>  Died    :65   control  :50  \n#>  Survived:25   treatment:40\ntable(cpr$survival, cpr$group)#>           \n#>            control treatment\n#>   Died          39        26\n#>   Survived      11        14\ndata_tbl <- table(cpr$survival, cpr$group)\nprop.table(data_tbl,  #Feed in your two-way table\n           margin = 2)  #Tell it to compute percentages for columns#>           \n#>            control treatment\n#>   Died        0.78      0.65\n#>   Survived    0.22      0.35"},{"path":"inference-cat.html","id":"simulation-based-inference-for-one-proportion","chapter":"5 Inference for categorical data","heading":"Simulation-based inference for one proportion","text":"simulation-based inference, use functions included catstats package, created MSU Statistics courses. See Statistical computing section beginning textbook instructions install catstats haven’t already. package installed, can load R session make functions available using library() function:loaded package, able use functions simulation-based inference. one-proportion inference, one_proportion_test() function one_proportion_bootstrap_CI() function. Returning payday loan regulation example, can obtain simulation distribution p-value using following function call:resulting graph, see null distribution simulated proportions, greater observed value 0.51 highlighted blue. figure caption, see 308 1000 simulations resulted proportion successes least large observed value, yielding approximate p-value 0.308.find confidence interval true proportion payday loan borrowers support regulation, use one_proportion_bootstrap_CI() function:produces plot bootstrapped proportions upper lower bounds confidence interval marked, gives interval figure caption: case, 95% confident true proportion payday loan borrowers support proposed regulation 0.479 0.546.","code":"\nlibrary(catstats)\none_proportion_test(\n  probability_success = 0.5, #null hypothesis probability of success\n  sample_size = 826,  #number of observations\n  number_repetitions = 1000,  #number of simulations to create\n  as_extreme_as = 0.51,  #observed statistic\n  direction = \"greater\",  #alternative hypothesis direction\n  report_value = \"proportion\"  #Report number or proportion of successes?\n)\none_proportion_bootstrap_CI(\n  sample_size = 826,  #Number of observations\n  number_successes = 422,  #Number of observed successes \n  number_repetitions = 1000,  #Number of bootstrap draws\n  confidence_level = 0.95  #Confidence level, as a proportion\n)"},{"path":"inference-cat.html","id":"simulation-based-inference-for-difference-in-two-proportions","chapter":"5 Inference for categorical data","heading":"Simulation-based inference for difference in two proportions","text":"inference difference two proportions, use two_proportion_test() two_proportion_bootstrap_CI() functions. functions assume data frame group outcome included variables. Using CPR blood thinner study, call two_proportion_test() look like :results give segmented bar plot data — can check formula correct making sure explanatory variable \\(x\\)-axis response variable \\(y\\)-axis. Look top right bar plot check correct order subtraction. Next bar plot, null distribution simulated differences proportions, observed statistic marked vertical line values extreme observed statistic colored red. figure caption gives approximate p-value: case 181/1000 = 0.181.couple things note using two_proportion_test function:need identify variable outcome group using formula argument.Specify order subtraction using first_in_subtraction putting EXACTLY category explanatory variable want first, quotes — must match capitalization, spaces, etc. text values!Specify success using response_value_numerator putting EXACTLY category response consider success, quotes. , capitalization, spaces, etc. matter !\nproduce confidence interval true difference proportion patients survive receiving CPR, use two_proportion_bootstrap_CI(), uses arguments two_proportion_test() function:produces distribution bootstrapped statistics bounds confidence interval marked, value included caption. , 99% confident true proportion patients survive receiving CPR 0.1 lower 0.35 higher patients given blood thinners compared .","code":"\ntwo_proportion_test(\n  formula = survival ~ group,  #Always do response ~ explanatory\n  data = cpr,   #Name of the data set\n  first_in_subtraction = \"treatment\", #Value of the explanatory variable that is first in order of subtraction\n  response_value_numerator = \"Survived\",  #Value of response that is a \"success\"\n  number_repetitions = 1000,\n  as_extreme_as = 0.13,  #Observed statistic\n  direction = \"two-sided\"  #Direction of the alternative hypothesis\n)\ntwo_proportion_bootstrap_CI(\n  formula = survival ~ group,  #Always do response ~ explanatory\n  data = cpr,   #Name of the data set\n  first_in_subtraction = \"treatment\", #Value of the explanatory variable that is first in order of subtraction\n  response_value_numerator = \"Survived\",  #Value of response that is a \"success\"\n  number_repetitions = 1000, #Number of bootstrap samples\n  confidence_level = 0.99  #Confidence level, as a proportion\n)"},{"path":"inference-cat.html","id":"theory-based-inference-for-one-proportion","chapter":"5 Inference for categorical data","heading":"Theory-based inference for one proportion","text":"theory-based inference, can use built-R function prop.test().143 one-proportion test, need tell number successes, number trials (sample size), null value. Using payday loan regulation example, call look like :output, can get observed statistic \\(\\hat{p} = 0.51\\) last line output (sample estimates: p), p-value 0.2656 second line output. always check null value alternative hypothesis output matches problem.prop.test() function also produce theory-based confidence interval, get correct confidence interval, need run function two-sided alternative:145From output, obtain 95% confidence interval true proportion payday loan borrowers support new regulation (0.477, 0.545).","code":"\nprop.test(x = 422,  #Number of successes\n          n = 826, #Number of trials\n          p = .5, #Null hypothesis value\n          alternative = \"greater\", #Direction of alternative,\n          conf.level = 0.95) #Confidence level as a proportion#> \n#>  1-sample proportions test with continuity correction\n#> \n#> data:  422 out of 826\n#> X-squared = 0.34988, df = 1, p-value = 0.2771\n#> alternative hypothesis: true p is greater than 0.5\n#> 95 percent confidence interval:\n#>  0.4816939 1.0000000\n#> sample estimates:\n#>         p \n#> 0.5108959\nprop.test(x = 422,  #Number of successes\n          n = 826, #Number of trials\n          p = .5, #Null hypothesis value\n          alternative = \"two.sided\", #Direction of alternative,\n          conf.level = 0.95, #Confidence level as a proportion\n          correct = FALSE) #We will not use  a continuity correction#> \n#>  1-sample proportions test without continuity correction\n#> \n#> data:  422 out of 826\n#> X-squared = 0.39225, df = 1, p-value = 0.5311\n#> alternative hypothesis: true p is not equal to 0.5\n#> 95 percent confidence interval:\n#>  0.4768346 0.5448563\n#> sample estimates:\n#>         p \n#> 0.5108959"},{"path":"inference-cat.html","id":"theory-based-inference-for-a-difference-in-two-proportions","chapter":"5 Inference for categorical data","heading":"Theory-based inference for a difference in two proportions","text":"comparing two proportions, use function, prop.test, inputs now “vectors” rather “scalars”. example, use CPR blood thinner study Section 5.4.2.First, use table() function determine number successes sample size category explanatory variable:results (example Section 5.4.2), see Treatment group (call group 1) 14 survive (successes) 40 patients (sample size); Control group (call group 2) 11 survive 50 patients. counts input prop.test function follows:x = vector number successes = c(num successes group 1, num successes group 2)n = vector sample sizes = c(sample size group 1, sample size group 2)R creates vector using c() (“combine”) function. R take order subtraction group 1 \\(-\\) group 2.observed proportions group given sample estimates: prop 1 prop 2. R always take (prop 1 - prop 2) order subtraction. observed proportions don’t match calculations proportion successes wrong order subtraction, go back check inputs x n arguments., obtain p-value 0.1712 — little evidence null hypothesis difference two groups.Since used two-sided alternative prop.testc, call also produces correct confidence interval. 99% confidence interval true difference proportion patients survive \\((-0.116, 0.376)\\). 99% confident true proportion patients survive treatment 0.116 lower 0.376 higher true proportion patients survive control condition. fact confidence interval contains zero also shows us little evidence difference probability survival two groups.function prop.test also take table created table function. First, need create table counts raw data, becomes primary input prop.test(). one important step take creating table: R always put categories categorical variable alphabetical order building tables, unless told otherwise.However, prop.test() assume top row “success” order subtraction (column 1 - column 2). Without re-arranging table, get wrong order subtraction /wrong proportion successes group. fix , need relevel() inputs tell R put order want:re-arranging table, can use data_tbl first argument prop.test() function:","code":"\ntable(cpr$survival, cpr$group)#>           \n#>            control treatment\n#>   Died          39        26\n#>   Survived      11        14\nprop.test(x = c(14, 11), #Number successes in group 1 and group 2\n          n = c(40, 50), #Sample size of group 1 and group 2\n          alternative = \"two.sided\", #Match sign of alternative\n                                     #for order of subtraction \n                                     #group 1 - group 2\n          conf.level = 0.99, #Confidence level as a proportion\n          correct = FALSE)  #No continuity correction#> \n#>  2-sample test for equality of proportions without continuity\n#>  correction\n#> \n#> data:  c out of c14 out of 4011 out of 50\n#> X-squared = 1.872, df = 1, p-value = 0.1712\n#> alternative hypothesis: two.sided\n#> 99 percent confidence interval:\n#>  -0.1159816  0.3759816\n#> sample estimates:\n#> prop 1 prop 2 \n#>   0.35   0.22\ndata_tbl <- table(cpr$survival, cpr$group)\ndata_tbl#>           \n#>            control treatment\n#>   Died          39        26\n#>   Survived      11        14\n#Switch order of subtraction:\ncpr$group <- relevel(cpr$group, ref = \"treatment\")\ntable(cpr$survival, cpr$group)#>           \n#>            treatment control\n#>   Died            26      39\n#>   Survived        14      11\n#Switch \"success\":\ncpr$survival <- relevel(cpr$survival, ref = \"Survived\")\ntable(cpr$survival, cpr$group)#>           \n#>            treatment control\n#>   Survived        14      11\n#>   Died            26      39\ndata_tbl <- table(cpr$survival, cpr$group)\n\nstats::prop.test(x = data_tbl,\n          alternative = \"two.sided\",\n          conf.level = 0.99, #Confidence level as a proportion\n          correct = FALSE)  #No continuity correction#> \n#>  2-sample test for equality of proportions without continuity\n#>  correction\n#> \n#> data:  data_tbl\n#> X-squared = 1.872, df = 1, p-value = 0.1712\n#> alternative hypothesis: two.sided\n#> 99 percent confidence interval:\n#>  -0.1398193  0.4598193\n#> sample estimates:\n#> prop 1 prop 2 \n#>   0.56   0.40"},{"path":"inference-cat.html","id":"catstats-function-summary","chapter":"5 Inference for categorical data","heading":"5.7.2 catstats function summary","text":"previous section, introduced four new R functions catstats library. provide summary functions. can also access help files functions using ? command. example, type ?one_proportion_test R console bring help file one_proportion_test function.\none_proportion_test: Simulation-based hypothesis test single proportion.\nprobability_success = null value\nsample_size = sample size (\\(n\\))\nreport_value = one \"number\" \"proportion\" (quotations important !) simulate either sample counts sample proportions (needs match form observed statistic)\ndirection = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)\nas_extreme_as = value observed statistic\nnumber_repetitions = number simulated samples generate (least 1000!)\n\none_proportion_test: Simulation-based hypothesis test single proportion.probability_success = null valuesample_size = sample size (\\(n\\))report_value = one \"number\" \"proportion\" (quotations important !) simulate either sample counts sample proportions (needs match form observed statistic)direction = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)as_extreme_as = value observed statisticnumber_repetitions = number simulated samples generate (least 1000!)\none_proportion_bootstrap_CI: Bootstrap confidence interval one proportion.\nsample_size = sample size (\\(n\\))\nnumber_successes = number successes (note \\(\\hat{p}\\) = number_successes/sample_size)\nconfidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)\nnumber_repetitions = number simulated samples generate (least 1000!)\n\none_proportion_bootstrap_CI: Bootstrap confidence interval one proportion.sample_size = sample size (\\(n\\))number_successes = number successes (note \\(\\hat{p}\\) = number_successes/sample_size)confidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)number_repetitions = number simulated samples generate (least 1000!)\ntwo_proportion_test: Simulation-based hypothesis test single proportion.\nformula = y ~ x y name response variable data set x name explanatory variable\ndata = name data set\nfirst_in_subtraction = category explanatory variable first subtraction, written quotations\nresponse_value_numerator = category response variable counting “success”, written quotations\ndirection = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)\nas_extreme_as = value observed difference proportions\nnumber_repetitions = number simulated samples generate (least 1000!)\n\ntwo_proportion_test: Simulation-based hypothesis test single proportion.formula = y ~ x y name response variable data set x name explanatory variabledata = name data setfirst_in_subtraction = category explanatory variable first subtraction, written quotationsresponse_value_numerator = category response variable counting “success”, written quotationsdirection = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)as_extreme_as = value observed difference proportionsnumber_repetitions = number simulated samples generate (least 1000!)\ntwo_proportion_bootstrap_CI: Bootstrap confidence interval one proportion.\nformula = y ~ x y name response variable data set x name explanatory variable\ndata = name data set\nfirst_in_subtraction = category explanatory variable first subtraction, written quotations\nresponse_value_numerator = category response variable counting “success”, written quotations\nconfidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)\nnumber_repetitions = number simulated samples generate (least 1000!)\ntwo_proportion_bootstrap_CI: Bootstrap confidence interval one proportion.formula = y ~ x y name response variable data set x name explanatory variabledata = name data setfirst_in_subtraction = category explanatory variable first subtraction, written quotationsresponse_value_numerator = category response variable counting “success”, written quotationsconfidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)number_repetitions = number simulated samples generate (least 1000!)","code":""},{"path":"inference-cat.html","id":"interactive-r-tutorials-3","chapter":"5 Inference for categorical data","heading":"5.7.3 Interactive R tutorials","text":"Navigate concepts ’ve learned chapter R using following self-paced tutorials.\nneed browser get started!Tutorial 5: Introduction statistical inferenceTutorial 5 - Lesson 1: Sampling variabilityTutorial 5 - Lesson 2: Randomization testTutorial 5 - Lesson 3: Errors hypothesis testingTutorial 5 - Lesson 4: Parameters confidence intervalsTutorial 6: Inference categorical responsesTutorial 6 - Lesson 1: Inference single proportionTutorial 6 - Lesson 2: Hypothesis tests compare proportionsYou can also access full list tutorials supporting book .","code":""},{"path":"inference-cat.html","id":"r-labs-4","chapter":"5 Inference for categorical data","heading":"5.7.4 R labs","text":"apply concepts ’ve learned chapter R computational labs walk data analysis case study.Sampling distributions - science benefit ?Confidence intervals - Climate changeInference categorical responses - Texting drivingFull list labs supporting OpenIntro::Introduction Modern Statistics","code":""},{"path":"inference-cat.html","id":"chp5-review","chapter":"5 Inference for categorical data","heading":"5.8 Chapter 5 review","text":"","code":""},{"path":"inference-cat.html","id":"terms-4","chapter":"5 Inference for categorical data","heading":"5.8.1 Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"inference-num.html","id":"inference-num","chapter":"6 Inference for quantitative data","heading":"6 Inference for quantitative data","text":"Focusing now statistical inference quantitative data, revisit expand upon foundational aspects hypothesis testing Section 5.1.important data structure chapter quantitative response variable (, outcome numerical).\nthree data structures detail :one quantitative response variable, summarized single mean,one quantitative response variable difference across pair observations, summarized paired mean difference, anda quantitative response variable broken binary explanatory variable, summarized difference means.appropriate, data structures analyzed using two methods introduced Section 5.1: simulation-based theory-based.summarize quantitative response variable, focus sample mean (instead , example, sample median range observations) well-studied mathematical model describes behavior sample mean.\nsample mean calculated one group, two paired groups, two independent groups. cover mathematical models describe statistics, bootstrap randomization techniques described immediately extendable function observed data.\ntechniques described setting vary slightly, well served find structural similarities across different settings.","code":""},{"path":"inference-num.html","id":"one-mean","chapter":"6 Inference for quantitative data","heading":"6.1 One mean","text":"Notation.\\(n\\) = sample size\\(\\bar{x}\\) = sample mean\\(s\\) = sample standard deviation\\(\\mu\\) = population mean\\(\\sigma\\) = population standard deviation\nsingle mean used summarize data measured single quantitative variable observational unit, e.g., GPA, age, salary. Aside slight differences notation, inferential methods presented section identical paired mean difference, see Section 6.2.","code":""},{"path":"inference-num.html","id":"bootstrap-confidence-interval-for-mu","chapter":"6 Inference for quantitative data","heading":"6.1.1 Bootstrap confidence interval for \\(\\mu\\)","text":"section, use bootstrapping, first introduced Section 5.3.2, construct confidence interval population mean. Recall bootstrapping best suited modeling studies data generated random sampling population. bootstrapped distribution sample means mimic process randomly sampling population give us sense sample means vary sample sample.","code":""},{"path":"inference-num.html","id":"observed-data-4","chapter":"6 Inference for quantitative data","heading":"Observed data","text":"employer subsidizes housing employees, need know average monthly rental price three bedroom flat Edinburgh.\norder walk example clearly, let’s say able randomly sample five Edinburgh flats (real example, surely able take much larger sample size, possibly even able measure entire population!).Figure 6.1 presents details random sample observations monthly rent five flats recorded.\nFigure 6.1: Five randomly sampled flats Edinburgh.\nsample average monthly rent £1648 first guess price three bedroom flats. However, student statistics, understand one sample mean based sample five observations necessarily equal true population average rent three bedroom flats Edinburgh.\nIndeed, can see observed rent prices vary standard deviation £340.232, surely average monthly rent different different sample size five taken population.\nFortunately, can use bootstrapping approximate variability sample mean sample sample.","code":""},{"path":"inference-num.html","id":"variability-of-the-statistic-4","chapter":"6 Inference for quantitative data","heading":"Variability of the statistic","text":"inferential ideas covered Chapter 5, inferential analysis methods chapter grounded quantifying one data set differs another taken population.Figure 6.2 shows unknown original population three bedroom flats Edinburgh can estimated using many duplicates sample. estimated population—consisting infinitely many copies original sample—can used generate bootstrapped resamples.\nFigure 6.2: Using original sample five Edinburgh flats generate estimated population, used generate bootstrapped resamples. process generating bootstrapped sample equivalent sampling five flats original sample, replacement.\nFigure 6.2, repeated bootstrap resamples obviously different original sample.\nSince bootstrap resamples taken (estimated) population, differences due entirely natural variability sampling procedure.\nsummarizing bootstrap resamples (, using sample mean), see, directly, variability sample mean sample sample.\ndistribution \\(\\bar{x}_{boot}\\), bootstrapped sample means, Edinburgh flats shown Figure 6.3.\nFigure 6.3: Distribution bootstrapped means 1,000 simulated bootstrapped samples generated sampling replacement original sample five Edinburgh flats. histogram provides sense variability average rent values sample sample samples size 5.\nbootstrapped average rent prices vary £1250 £1995 (small observed sample size 5, bootstrap resample can sometimes, although rarely, include repeated measurements observation).Bootstrapping one sample.Take random sample size \\(n\\) original sample, replacement. called bootstrapped resample.Record sample mean (statistic interest) bootstrapped resample. called bootstrapped statistic.Repeat steps (1) (2) 1000s times.\nDue theory beyond text, know bootstrap means \\(\\bar{x}_{boot}\\) vary around original sample mean, \\(\\bar{x}\\), similar way different sample (.e., different data sets produce different \\(\\bar{x}\\) values) means vary around true parameter \\(\\mu\\).\nTherefore, interval estimate \\(\\mu\\) can produced using \\(\\bar{x}_{boot}\\) values . 95% bootstrap confidence interval \\(\\mu\\), population mean rent price three bedroom flats Edinburgh, found locating middle 95% bootstrapped sample means Figure 6.3.95% Bootstrap confidence interval population mean \\(\\mu\\).can find confidence intervals difference confidence levels changing percent distribution take, e.g., locate middle 90% bootstrapped statistics 90% confidence interval.Example 6.1  Using Figure 6.3, find 90% 95% confidence intervals true mean monthly rental price three bedroom flat Edinburgh.90% confidence interval given (£1429, £1876). conclusion 90% confident true average rental price three bedroom flats Edinburgh lies somewhere £1429 £1876.","code":""},{"path":"inference-num.html","id":"bootstrap-percentile-confidence-interval-for-sigma-special-topic","chapter":"6 Inference for quantitative data","heading":"Bootstrap percentile confidence interval for \\(\\sigma\\) (special topic)","text":"Suppose research question hand seeks understand variable rental price three bedroom flats Edinburgh.\n, interest longer average rental price flats standard deviation rental prices three bedroom flats Edinburgh, \\(\\sigma\\).\nmay already realized sample standard deviation, \\(s\\), work good point estimate parameter interest: population standard deviation, \\(\\sigma\\).\npoint estimate five observations calculated \\(s =\\) £340.23.\n\\(s =\\) £340.23 might good guess \\(\\sigma\\), prefer interval.\nAlthough mathematical model describes \\(s\\) varies sample sample, mathematical model presented text.\neven without mathematical model, bootstrapping can used find confidence interval parameter \\(\\sigma\\).Example 6.2  Describe bootstrap distribution standard deviation shown Figure 6.4.\nFigure 6.4: original Edinburgh data bootstrapped 1,000 times. histogram provides sense variability standard deviation rent values sample sample.\n","code":""},{"path":"inference-num.html","id":"bootstrapping-is-not-a-solution-to-small-sample-sizes","chapter":"6 Inference for quantitative data","heading":"Bootstrapping is not a solution to small sample sizes!","text":"example presented done sample five observations.\nanalysis techniques build mathematical models, bootstrapping works best large random sample taken population.\nBootstrapping method capturing variability statistic mathematical model unknown — method navigating small samples.\nmight guess, larger random sample, accurately sample represent target population.","code":""},{"path":"inference-num.html","id":"one-mean-null-boot","chapter":"6 Inference for quantitative data","heading":"6.1.2 Shifted bootstrap test for \\(H_0: \\mu = \\mu_0\\)","text":"can also use bootstrapping conduct simulation-based test null hypothesis population mean equal specified value, \\(\\mu_0\\), called null value. case, first shift value data set sample distribution centered \\(\\mu_0\\). , bootstrap shifted data order generate null distribution sample means. Consider following example.1851, Carl Wunderlich, German physician, measured body temperatures around 25,000 adults found average body temperature 98.6\\(^{\\circ}\\)F, ’ve believed ever since. However, recent study conducted Stanford University suggests average body temperature may actually lower 98.6\\(^{\\circ}\\)F.147","code":""},{"path":"inference-num.html","id":"observed-data-5","chapter":"6 Inference for quantitative data","heading":"Observed data","text":"Curious average body temperature decreased since 1851, decided collect data random sample twenty Montana State University students. mean body temperature sample \\(\\bar{x}\\) = 97.47\\(^{\\circ}\\)F, standard deviation \\(s\\) = 0.35\\(^{\\circ}\\)F. dot plot data shown Figure 6.5, summary statistics displayed .\nFigure 6.5: Distribution body temperatures random sample twenty Montana State University students.\n","code":"\nfavstats(temperatures)#>   min   Q1 median   Q3  max mean    sd  n missing\n#>  96.7 97.3   97.5 97.7 98.1 97.5 0.353 20       0"},{"path":"inference-num.html","id":"shifted-bootstrapped-null-distribution","chapter":"6 Inference for quantitative data","heading":"Shifted bootstrapped null distribution","text":"like test set hypotheses \\(H_0: \\mu = 98.6\\) versus \\(H_A: \\mu < 98.6\\), \\(\\mu\\) true mean body temperature among adults (degrees F). simulate sample mean body temperatures \\(H_0\\), expect null distribution centered \\(\\mu_0\\) = 98.6\\(^\\circ\\)F. However, bootstrap sample means observed sample, bootstrap distribution centered sample mean body temperature \\(\\bar{x}\\) = 97.5\\(^\\circ\\)F.use bootstrapping generate null distribution sample means, first shift data centered null value. adding \\(\\mu_0 - \\bar{x} = 98.6 - 97.5 = 1.1^\\circ\\)F body temperature sample. process displayed Figure 6.6.\nFigure 6.6: Distribution body temperatures random sample twenty Montana State University students (blue) shifted body temperatures (red), found adding 1.1 degree F original body temperature.\nbootstrapped null distribution generated sampling 20 shifted temperatures, replacement, shifted data 1,000 times shown Figure 6.7.\nFigure 6.7: Bootstrapped null distribution sample mean temperatures assuming true mean temperature 98.6 degrees F.\nShifted bootstrap null distribution sample mean.simulate null distribution sample means null hypothesis \\(H_0: \\mu = \\mu_0\\):Add \\(\\mu_0 - \\bar{x}\\) value original sample:\n\\[\n x_1 + \\mu_0 - \\bar{x}, \\hspace{2.5mm} x_2 + \\mu_0 - \\bar{x}, \\hspace{2.5mm}  x_3 + \\mu_0 - \\bar{x}, \\hspace{2.5mm}  \\ldots, \\hspace{2.5mm}  x_n + \\mu_0 - \\bar{x}.\n  \\]\nNote \\(\\bar{x}\\) larger \\(\\mu\\), quantity \\(\\mu_0 - \\bar{x}\\) negative, subtracting distance \\(\\mu\\) \\(\\bar{x}\\) value.Generate 1000s bootstrap resamples shifted distribution, plotting shifted bootstrap sample mean time.\ncalculate p-value, since \\(H_A: \\mu < 98.6\\), find proportion simulated sample means less equal original sample mean, \\(\\bar{x}\\) = 97.47. shown Figure 6.7, none simulated sample means 97.5\\(^\\circ\\)F lower, giving us strong evidence true mean body temperature among Montana State University students less commonly accepted 98.6\\(^\\circ\\)F average temperature.","code":""},{"path":"inference-num.html","id":"one-mean-math","chapter":"6 Inference for quantitative data","heading":"6.1.3 Theory-based inferential methods for \\(\\mu\\)","text":"sample proportion, variability sample mean well described mathematical theory given Central Limit Theorem. Similar can model behavior sample proportion \\(\\hat{p}\\) using normal distribution, sample mean \\(\\bar{x}\\) can also modeled using\nnormal distribution certain conditions met.\nHowever, missing information inherent variability population, \\(t\\)-distribution used place standard normal performing hypothesis test confidence interval analyses.diving confidence intervals hypothesis\ntests using \\(\\bar{x}\\), first need cover two topics:modeled \\(\\hat{p}\\) using normal distribution,\ncertain conditions satisfied.\nconditions working \\(\\bar{x}\\)\nlittle complex, , discuss\ncheck conditions inference using mathematical model.standard deviation sample mean dependent population\nstandard deviation, \\(\\sigma\\).\nHowever, rarely know \\(\\sigma\\), instead\nmust estimate .\nestimation imperfect,\nuse new distribution called \n\\(t\\)-distribution\nfix problem.","code":""},{"path":"inference-num.html","id":"evaluating-the-two-conditions-required-for-modeling-barx-using-theory-based-methods","chapter":"6 Inference for quantitative data","heading":"Evaluating the two conditions required for modeling \\(\\bar{x}\\) using theory-based methods","text":"two conditions required apply \nCentral Limit Theorem\nsample mean \\(\\bar{x}\\).\nsample observations\nindependent sample size sufficiently\nlarge, normal model describe variability sample means quite well; observations violate conditions, normal model can inaccurate.Conditions modeling\n\\(\\bar{x}\\) using theory-based methods.sampling distribution \\(\\bar{x}\\) based \nsample size \\(n\\) population true\nmean \\(\\mu\\) true standard deviation \\(\\sigma\\) can modeled\nusing normal distribution :Independence. sample observations must independent,\ncommon way satisfy condition \nsample simple random sample \npopulation.\ndata come random process,\nanalogous rolling die,\nalso satisfy independence condition.Independence. sample observations must independent,\ncommon way satisfy condition \nsample simple random sample \npopulation.\ndata come random process,\nanalogous rolling die,\nalso satisfy independence condition.Normality. sample small,\nalso require sample observations\ncome normally distributed population.\ncan relax condition \nlarger larger sample sizes.\ncondition obviously vague,\nmaking difficult evaluate,\nnext introduce couple rules thumb\nmake checking condition easier.\nconditions satisfied, sampling\ndistribution \\(\\bar{x}\\) approximately normal mean\n\\(\\mu\\) standard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\).\nNormality. sample small,\nalso require sample observations\ncome normally distributed population.\ncan relax condition \nlarger larger sample sizes.\ncondition obviously vague,\nmaking difficult evaluate,\nnext introduce couple rules thumb\nmake checking condition easier.conditions satisfied, sampling\ndistribution \\(\\bar{x}\\) approximately normal mean\n\\(\\mu\\) standard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\).General rule: perform normality check.perfect way check normality condition,\ninstead use two general rules:\\(\\mathbf{n < 30}\\): sample size \\(n\\) less 30\nclear outliers data,\ntypically assume data come \nnearly normal distribution satisfy \ncondition.\\(\\mathbf{n \\geq 30}\\): sample size \\(n\\) least 30\nparticularly extreme outliers,\ntypically assume sampling distribution\n\\(\\bar{x}\\) nearly normal, even underlying\ndistribution individual observations .\nfirst course statistics, aren’t expected\ndevelop perfect judgement normality condition.\nHowever, expected able handle\nclear cut cases based rules thumb.148Example 6.3  Consider following two plots\ncome simple random samples \ndifferent populations.\nsample sizes \\(n_1 = 15\\) \\(n_2 = 50\\).independence normality conditions met\ncase?sample simple random sample \nrespective population, independence condition\nsatisfied.\nLet’s next check normality condition \nusing rule thumb.first sample fewer 30 observations,\nwatching clear outliers.\nNone present; small gap \nhistogram right, gap small \n20% observations small sample\nrepresented far right bar histogram,\ncan hardly call clear outliers.\nclear outliers, normality condition\nreasonably met.second sample sample size greater 30 \nincludes outlier appears roughly 5 times\ncenter distribution \nnext furthest observation.\nexample particularly extreme outlier,\nnormality condition satisfied.practice, ’s typical also mental check evaluate\nwhether reason believe underlying population\nmoderate skew (\\(n < 30\\))\nparticularly extreme outliers \\((n \\geq 30)\\)\nbeyond observe data.\nexample, consider number followers\nindividual account Twitter,\nimagine distribution.\nlarge majority accounts built \ncouple thousand followers fewer,\nrelatively tiny fraction amassed\ntens millions followers,\nmeaning distribution extremely skewed.\nknow data come extremely\nskewed distribution,\ntakes effort understand sample\nsize large enough normality condition\nsatisfied.","code":""},{"path":"inference-num.html","id":"introducing-the-t-distribution","chapter":"6 Inference for quantitative data","heading":"Introducing the \\(t\\)-distribution","text":"\npractice, directly calculate standard deviation\n\\(\\bar{x}\\) since know population standard\ndeviation, \\(\\sigma\\).\nencountered similar issue computing standard\nerror sample proportion, relied population\nproportion, \\(\\pi\\).\nsolution proportion context use sample\nvalue place\npopulation value calculate standard error.\n’ll employ similar strategy compute standard\nerror \\(\\bar{x}\\), using sample\nstandard deviation \\(s\\) place \\(\\sigma\\):\n\\[\\begin{align*}\nSE(\\bar{x}) = \\frac{s}{\\sqrt{n}} \\approx SD(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}.\n\\end{align*}\\]\nstandard error \\(\\bar{x}\\) provides estimate standard deviation \\(\\bar{x}\\). strategy tends work well \nlot data can estimate \\(\\sigma\\) using \\(s\\) accurately.\nHowever, estimate less precise smaller samples,\nleads problems using normal\ndistribution model \\(\\bar{x}\\) know \\(\\sigma\\).’ll find useful use new distribution \ninference calculations called \\(t\\)-distribution.\n\\(t\\)-distribution, shown solid line \nFigure 6.8, bell shape.\nHowever, tails thicker normal distribution’s,\nmeaning observations likely fall beyond two\nstandard deviations mean normal\ndistribution.extra thick tails \\(t\\)-distribution exactly\ncorrection needed resolve problem (due extra variability test statistic) using \\(s\\)\nplace \\(\\sigma\\) \\(SE(\\bar{x})\\) calculation.\nFigure 6.8: Comparison \\(t\\)-distribution normal distribution.\n\\(t\\)-distribution always centered zero \nsingle parameter: degrees freedom (\\(df\\)).\ndegrees freedom\ndescribes precise form bell-shaped \\(t\\)-distribution.\nSeveral \\(t\\)-distributions shown \nFigure 6.9\ncomparison normal distribution.inference single mean, ’ll use \\(t\\)-distribution\n\\(df = n - 1\\) model sample mean\nsample size \\(n\\).\n, observations,\ndegrees freedom larger \n\\(t\\)-distribution look like \nstandard normal distribution;\ndegrees freedom 30 ,\n\\(t\\)-distribution nearly indistinguishable\nnormal distribution.\nFigure 6.9: larger degrees freedom, closely \\(t\\)-distribution resembles standard normal distribution.\nDegrees freedom: \\(df\\).degrees freedom describes shape \n\\(t\\)-distribution.\nlarger degrees freedom, closely\ndistribution approximates normal model.modeling \\(\\bar{x}\\) using \\(t\\)-distribution,\nuse \\(df = n - 1\\).\\(t\\)-distribution allows us greater flexibility \nnormal distribution analyzing numerical data.\npractice, ’s common use statistical software,\nR, Python, SAS analyses.\nR, function used calculating probabilities \\(t\\)-distribution pt() (seem similar previous R function pnorm()).\nDon’t forget \\(t\\)-distribution, degrees freedom must always specified!examples guided practices , use R find answers. recommend trying problems get sense \\(t\\)-distribution can vary width depending degrees freedom, confirm working\nunderstanding \\(t\\)-distribution.Example 6.4  proportion \\(t\\)-distribution\n18 degrees freedom falls -2.10?Just like normal probability problem, first draw\npicture Figure 6.10\nshade area -2.10.Using statistical software, can obtain precise\nvalue: 0.0250.\nFigure 6.10: \\(t\\)-distribution 18 degrees freedom. area -2.10 shaded.\nExample 6.5  \\(t\\)-distribution 20 degrees freedom\nshown top panel \nFigure 6.11.\nEstimate proportion distribution falling\n1.65.Note 20 degrees freedom, \\(t\\)-distribution relatively close normal distribution.\nnormal distribution, correspond \n0.05, expect \\(t\\)-distribution\ngive us value neighborhood.\nUsing statistical software: 0.0573.\nFigure 6.11: Top: \\(t\\)-distribution 20 degrees freedom, area 1.65 shaded. Bottom: \\(t\\)-distribution 2 degrees freedom, area 3 units 0 shaded.\nExample 6.6  \\(t\\)-distribution 2 degrees freedom\nshown bottom panel \nFigure 6.11.\nEstimate proportion distribution falling \n3 units mean ().degrees freedom, \\(t\\)-distribution \ngive notably different value normal\ndistribution.\nnormal distribution, area \n0.003 using 68-95-99.7 rule.\n\\(t\\)-distribution \\(df = 2\\), area \ntails beyond 3 units totals 0.0955.\narea dramatically different \nobtain normal distribution.proportion \\(t\\)-distribution 19 degrees\nfreedom falls -1.79 units?149\n","code":"\n# using pt() to find probability under the $t$-distribution\npt(-2.10, df = 18)\n#> [1] 0.025\n# using pt() to find probability under the $t$-distribution\npt(1.65, df = 20, lower.tail=FALSE)\n#> [1] 0.0573\n# or\n1 - pt(1.65, df = 20)\n#> [1] 0.0573\n# using pt() to find probability under the $t$-distribution\n2 * pt(-3, df = 2)\n#> [1] 0.0955"},{"path":"inference-num.html","id":"one-sample-t-confidence-intervals","chapter":"6 Inference for quantitative data","heading":"One sample \\(t\\)-confidence intervals","text":"Let’s get first taste applying \\(t\\)-distribution\ncontext example mercury content\ndolphin muscle.\nElevated mercury concentrations important problem\ndolphins\nanimals, like humans, occasionally eat .\nFigure 6.12: Risso’s dolphin. Photo Mike Baird, www.bairdphotos.com.\n","code":""},{"path":"inference-num.html","id":"observed-data-6","chapter":"6 Inference for quantitative data","heading":"Observed data","text":"identify confidence interval average mercury content dolphin muscle using sample 19 Risso’s dolphins Taiji area Japan. data summarized Table 6.1. minimum maximum observed values can used evaluate whether clear outliers.\nTable 6.1: Summary mercury content muscle 19 Risso’s dolphins Taiji area. Measurements micrograms mercury per wet gram\nmuscle (\\(\\mu\\)g/wet g).\nExample 6.7  independence \nnormality conditions satisfied data set?observations simple random sample,\ntherefore independence reasonable.\nsummary statistics \nTable 6.1\nsuggest clear outliers, \nobservations within 2.5 standard deviations\nmean.\nBased evidence, normality condition\nseems reasonable.normal model, used \\(z^{\\star}\\) standard error determine width confidence interval. revise confidence interval formula slightly using \\(t\\)-distribution:\n\\[\\begin{align*}\n&\\text{point estimate} \\ \\pm\\  t^{\\star}_{df} \\times SE(\\text{point estimate})\n&&\\\n&&\\bar{x} \\ \\pm\\  t^{\\star}_{df} \\times \\frac{s}{\\sqrt{n}},\n\\end{align*}\\]\n\\(df = n - 1\\) computing one-sample \\(t\\)-interval.Example 6.8  Using summary statistics \nTable 6.1,\ncompute standard error average\nmercury content \\(n = 19\\) dolphins.plug \\(s\\) \\(n\\) formula:\n\\(SE(\\bar{x})  = s / \\sqrt{n}  = 2.3 / \\sqrt{19}  = 0.528\\).value \\(t^{\\star}_{df}\\) cutoff obtain based \nconfidence level \\(t\\)-distribution \\(df\\) degrees\nfreedom.\ncutoff found way normal\ndistribution: find \\(t^{\\star}_{df}\\) \nfraction \\(t\\)-distribution \\(df\\) degrees\nfreedom within distance \\(t^{\\star}_{df}\\)\n0 matches confidence level interest.Example 6.9  \\(n = 19\\), appropriate\ndegrees freedom?\nFind \\(t^{\\star}_{df}\\) degrees freedom\nconfidence level 95%.degrees freedom easy calculate:\n\\(df = n - 1 = 18\\).Using statistical software, find cutoff \nupper tail equal 2.5%:\n\\(t^{\\star}_{18} =\\) 2.10.\narea -2.10 also equal 2.5%.\n, 95% \\(t\\)-distribution \\(df = 18\\)\nlies within 2.10 units 0.Example 6.10  Compute interpret 95% confidence interval\naverage mercury content Risso’s dolphins.can construct confidence interval \n\\[\\begin{align*}\n  \\bar{x} \\ \\pm\\  t^{\\star}_{18} \\times SE(\\bar{x})\n    \\quad \\\\quad 4.4 \\ \\pm\\  2.10 \\times 0.528\n    \\quad \\\\quad (3.29, 5.51)\n  \\end{align*}\\]\n95% confident average mercury content muscles\npopulation Risso’s dolphins 3.29 5.51 \\(\\mu\\)g/wet gram,\nconsidered extremely high.Finding \\(t\\)-confidence interval population mean, \\(\\mu\\).Based sample \\(n\\) independent nearly normal\nobservations, confidence interval population\nmean \n\\[\\begin{align*}\n  &\\text{point estimate} \\ \\pm\\  t^{\\star}_{df} \\times SE(\\text{point estimate})\n  &&\\\n  &&\\bar{x} \\ \\pm\\  t^{\\star}_{df} \\times \\frac{s}{\\sqrt{n}}\n  \\end{align*}\\]\n\\(\\bar{x}\\) sample mean, \\(t^{\\star}_{df}\\)\ncorresponds confidence level degrees freedom\n\\(df\\), \\(SE\\) standard error estimated \nsample.FDA’s webpage provides data mercury content fish.\nBased sample 15 croaker white fish (Pacific), sample mean standard deviation computed 0.287 0.069 ppm (parts per million), respectively.\n15 observations ranged 0.18 0.41 ppm. assume observations independent.\nBased summary statistics data, objections normality condition individual observations?150Example 6.11  Calculate standard error \n\\(\\bar{x}\\) using data summaries previous Guided Practice. use \\(t\\)-distribution create \n90% confidence interval actual mean \nmercury content, identify degrees freedom\n\\(t^{\\star}_{df}\\).standard error: \\(SE(\\bar{x}) = \\frac{0.069}{\\sqrt{15}} = 0.0178\\).Degrees freedom: \\(df = n - 1 = 14\\).Since goal 90% confidence interval,\nchoose \\(t_{14}^{\\star}\\) two-tail area\n0.1:\n\\(t^{\\star}_{14} = 1.76\\).Using information results previous Guided Practice Example, compute 90% confidence interval average mercury content croaker white fish (Pacific).151The 90% confidence interval previous\nGuided Practice 0.256 ppm 0.318 ppm.\nCan say 90% croaker white fish (Pacific)\nmercury levels 0.256 0.318 ppm?152","code":"\n# use qt() to find the t-cutoff (with 95% in the middle)\nqt(0.025, df = 18)\n#> [1] -2.1\nqt(0.975, df = 18)\n#> [1] 2.1\n# use qt() to find the t-cutoff (with 90% in the middle)\nqt(0.05, df = 14)\n#> [1] -1.76\nqt(0.95, df = 14)\n#> [1] 1.76"},{"path":"inference-num.html","id":"one-sample-t-tests","chapter":"6 Inference for quantitative data","heading":"One sample \\(t\\)-tests","text":"Now ’ve used \\(t\\)-distribution making confidence\nintervals mean, let’s speed \nhypothesis tests mean.test statistic assessing single mean T.T score ratio sample mean differs hypothesized mean compared observations vary.\\[\\begin{align*}\nT = \\frac{\\bar{x} - \\mbox{null value}}{s/\\sqrt{n}}\n\\end{align*}\\]null hypothesis true conditions met, T \\(t\\)-distribution \\(df = n - 1\\).Conditions:independently observed datalarge samples extreme outliersCompare T score — standardized sample mean — Z score — standardized sample proportion — presented Section 5.3.3. use “Z” standardizing proportions, “T” standardizing means?153Is typical US runner getting faster slower time? consider question context Cherry Blossom Race, 10-mile race Washington, DC spring.average time runners finished Cherry Blossom Race 2006 93.29 minutes (93 minutes 17 seconds). want determine using data 100 participants 2017 Cherry Blossom Race whether runners race getting faster slower, versus possibility change.appropriate hypotheses context?154The data come simple random sample participants, observations independent.\n\nhistogram race times given evaluate can move forward t-test. worried normality condition?155When completing hypothesis test one-sample mean,\nprocess nearly identical completing hypothesis\ntest single proportion.\nFirst, find Z score using observed value,\nnull value, standard error;\nhowever, call T score since use\n\\(t\\)-distribution calculating tail area.\nfind p-value using ideas used\npreviously: find area \\(t\\)-distribution extreme T score.Example 6.12  independence\nnormality conditions satisfied,\ncan proceed hypothesis test using\n\\(t\\)-distribution.\nsample mean sample standard deviation\nsample\n100 runners \n2017 Cherry Blossom Race\n97.32 16.98 minutes,\nrespectively.\nRecall sample size 100\naverage run time 2006 93.29 minutes.\nFind test statistic p-value.\nconclusion?hypotheses, found previous Guided Practice, :\\(H_0: \\mu = 93.29\\) minutes\\(H_A: \\mu \\neq 93.29\\) minutesTo find test statistic (T score),\nfirst must determine standard error:\n\\[\\begin{align*}\n  SE(\\bar{x})\n    = 16.98 / \\sqrt{100}\n    = 1.70\n  \\end{align*}\\]\nNow can compute T score\nusing sample mean (97.32),\nnull value (98.29), \\(SE\\):\n\\[\\begin{align*}\n  T\n    = \\frac{97.32 - 93.29}{1.70}\n    = 2.37\n  \\end{align*}\\]\n\\(df = 100 - 1 = 99\\),\ncan determine using statistical software\narea \\(t\\)-distribution 99 \\(df\\) \nobserved T score 2.37 0.01 (see ),\ndouble get p-value: 0.02.p-value small,\ndata provide strong evidence average\nrun time Cherry Blossom Run 2017 different\n2006 average.using \\(t\\)-distribution, use T score (similar Z score).help us remember use \\(t\\)-distribution,\nuse \\(T\\) represent test statistic,\noften call T score.\nZ score T score computed exact way\nconceptually identical:\nrepresents many standard errors observed value\nnull value.","code":"\n# using pt() to find the p-value\n1 - pt(2.37, df = 99)\n#> [1] 0.00986"},{"path":"inference-num.html","id":"paired-data","chapter":"6 Inference for quantitative data","heading":"6.2 Paired mean difference","text":"Notation.\\(n\\) = number pairs paired samples\\(\\bar{x}_{d}\\) = sample mean differences paired samples\\(s_{d}\\) = sample standard deviation differences paired samples\\(\\mu_{d}\\) = population mean differences paired samples\\(\\sigma_{d}\\) = population standard deviation differences paired samples\nPaired data represent particular type experimental structure analysis somewhat akin one-sample analysis (see Section 6.1) features resemble two-sample analysis (see Section 6.3). Quantitative measurements made two different levels explanatory variable, measurements paired — observational unit consists two measurements, two measurements subtracted difference retained. Table 6.2 presents examples studies paired designs implemented.\nTable 6.2: Examples studies paired design used measure difference measurement two conditions.\nPaired data.inferential methods applied paired data, analysis virtually identical one-sample approach given Section 6.1.\nkey working paired data consider measurement interest difference measured values across pair observations.\nThinking differences single observation observational unit changes paired setting one-sample setting.","code":""},{"path":"inference-num.html","id":"shifted-bootstrap-test-for-h_0-mu_d-0","chapter":"6 Inference for quantitative data","heading":"6.2.1 Shifted bootstrap test for \\(H_0: \\mu_d = 0\\)","text":"Consider experiment done measure whether tire brand Smooth Turn tire brand Quick Spin longer tread wear. , 1,000 miles car, brand tires tread, average?","code":""},{"path":"inference-num.html","id":"observed-data-7","chapter":"6 Inference for quantitative data","heading":"Observed data","text":"observed data represent 25 tread measurements (inches) taken 25 Smooth Turn tires 25 Quick Spin tires.\nstudy used total 25 cars, car, one brand randomly assigned front driver’s side tire front passenger’s side tire.\nFigure 6.13 presents observed data.\nSmooth Turn manufacturer looks box plot says:clearly tread Smooth Turn tires higher, average, tread Quick Spin tires 1,000 miles driving.Quick Spin manufacturer skeptical retorts:25 cars, seems variability road conditions (sometimes one tire hits pothole, etc.) leads small difference average tread amount.’d like able systematically distinguish Smooth Turn manufacturer sees plot Quick Spin manufacturer sees plot. Fortunately us, excellent way simulate natural variability (road conditions, etc.) can lead tires worn different rates: bootstrapping.\nFigure 6.13: Boxplots tire tread remaining 1,000 miles brand tire original measurements came. Gray lines connect cars.\nSince paired data, interested differences tire tread two brands car. dotplot Figure 6.14 displays differences, summary statistics displayed .\nFigure 6.14: Difference tire tread (inches) remaining 1,000 miles two brands (Smooth Turn – Quick Spin).\n","code":"\nfavstats(differences)\n#>       min        Q1  median     Q3    max    mean      sd  n missing\n#>  -0.00506 -0.000972 0.00205 0.0042 0.0107 0.00196 0.00431 25       0"},{"path":"inference-num.html","id":"variability-of-the-statistic-5","chapter":"6 Inference for quantitative data","heading":"Variability of the statistic","text":"simulation-based test identify whether differences seen box plot plausibly happened just chance variability.\n, simulate variability sample statistics assumption null hypothesis true.\nstudy, null hypothesis average difference tire tread wear Smooth Turn Quick Spin tires zero. experiment conducted determine whether Smooth Turn Quick Spin longer tread wear. Taking order differences Smooth Turn \\(-\\) Quick Spin, express hypotheses follows.\\(H_0: \\mu_d = 0\\), true mean difference tire tread remaining 1,000 miles Smooth Turn Quick Spin (Smooth Turn \\(-\\) Quick Spin) tires equal zero.\\(H_A: \\mu_d \\neq 0\\), true mean difference tire tread remaining 1,000 miles Smooth Turn Quick Spin (Smooth Turn \\(-\\) Quick Spin) tires equal zero.simulate null distribution mean differences tread, implement method used Section 6.1.2 using shifted bootstrap distribution.Shifted bootstrap null distribution sample mean difference.simulate null distribution sample mean differences null hypothesis \\(H_0: \\mu_d = 0\\),Subtract \\(\\bar{x}_d\\) difference original sample:156\\[\n x_1 - \\bar{x}_d , \\hspace{2.5mm} x_2 - \\bar{x}_d, \\hspace{2.5mm}  x_3 - \\bar{x}_d, \\hspace{2.5mm}  \\ldots, \\hspace{2.5mm}  x_n - \\bar{x}_d.\n  \\]\nNote \\(\\bar{x}_d\\) negative number, adding distance \\(0\\) \\(\\bar{x}_d\\) value.Generate 1000s bootstrap resamples shifted distribution, plotting shifted bootstrap sample mean difference time.\nuse bootstrapping generate null distribution sample mean differences tire tread, first shift data centered null value zero. shift data subtracting \\(\\bar{x}_d\\) = 0.00196 tire tread difference sample. process displayed Figure 6.15.\nFigure 6.15: Mean difference tire tread (inches) remaining 1,000 miles two brands (Smooth Turn – Quick Spin) (blue), shifted mean differences tire tread (red), found subtracting 0.00196 original difference.\n","code":""},{"path":"inference-num.html","id":"observed-statistic-vs.-null-value-2","chapter":"6 Inference for quantitative data","heading":"Observed statistic vs. null value","text":"repeatedly sampling 25 cars replacement shifted bootstrap null distribution, can create distribution sample mean difference tire tread, seen Figure 6.16.\nexpected (differences generated null hypothesis), histogram centered zero.\nline drawn observed mean difference, \\(\\bar{x}_d\\) = 0.00196, nowhere near differences simulated natural variability assume difference tire tread wear brands.\nobserved mean difference tire tread far away natural variability randomized mean differences tire tread, believe significant difference tire tread wear Smooth Turn Quick Spin brand tires, average.precise, proportion simulated \\(\\bar{x}_d\\)’s 0.00196 inches away zero 0.023. p-value gives us strong evidence favor alternative \\(H_A: \\mu_d \\neq 0\\).\nconclusion extra amount tire tread remaining Smooth Turn brand tires 1,000 miles, average, due just natural variability. Data experiment suggest , average, Smooth Turn tires differ tread wear compared Quick Spin tires.\nFigure 6.16: Histogram 1000 simulated mean differences tire tread, assuming two brands perform equally, average.\n","code":""},{"path":"inference-num.html","id":"bootstrap-confidence-interval-for-mu_d","chapter":"6 Inference for quantitative data","heading":"6.2.2 Bootstrap confidence interval for \\(\\mu_d\\)","text":"earlier edition textbook,\nfound Amazon prices , average,\nlower UCLA Bookstore UCLA courses\n2010.\n’s several years, many stores adapted\nonline market, wondered,\nUCLA Bookstore today?","code":""},{"path":"inference-num.html","id":"observed-data-8","chapter":"6 Inference for quantitative data","heading":"Observed data","text":"sampled 201 UCLA courses.\n, 68\nrequired books found Amazon.portion data set courses\nshown Table 6.3,\nprices US dollars. differences taken \n\\[\\begin{align*}\n\\text{UCLA Bookstore price} - \\text{Amazon price}\n\\end{align*}\\]important always subtract using\nconsistent order;\nAmazon prices always subtracted UCLA prices.\nfirst difference shown Table 6.3\ncomputed \\(47.97 - 47.45 = 0.52\\).\nSimilarly, second difference computed \n\\(14.26 - 13.55 = 0.71\\),\nthird \\(13.50 - 12.53 = 0.97\\).\nTable 6.3: Four cases ucla_textbooks_f18 dataset.\ndot plot data shown Figure 6.17, summary statistics displayed .\nFigure 6.17: Distribution differences new textbook price (UCLA Bookstore – Amazon) US dollars 68 required textbooks UCLA.\n\ntextbook two corresponding prices data set:\none UCLA Bookstore one Amazon.\nThus, two prices textbook paired,\nanalysis need focus differences\ntextbook price two suppliers.","code":"\nfavstats(ucla_textbooks_f18$price_diff)#>    min     Q1 median   Q3  max mean   sd  n missing\n#>  -12.2 -0.992  0.625 2.99 75.2 3.58 13.4 68       0"},{"path":"inference-num.html","id":"variability-of-the-statistic-6","chapter":"6 Inference for quantitative data","heading":"Variability of the statistic","text":"Following example bootstrapping single mean, observed mean differences can bootstrapped order understand variability average difference sample sample. can use bootstrap distribution mean differences calculate bootstrap percentile confidence intervals true mean difference population.Figure 6.18, 99% confidence interval mean difference cost new book UCLA Bookstore compared Amazon calculated.\nbootstrap percentile interval computing using 0.5th percentile 99.5th percentile bootstrapped mean differences found (-0.044, 8.138).\nSince confidence interval contains zero, support hypothesis UCLA Bookstore price , average, higher Amazon price. , since interval contains negative positive values, plausible prices UCLA textbooks lower, average, Amazon, also plausible prices UCLA textbooks higher, average, Amazon. interpret interval follows: 99% confident , average, new textbook prices UCLA Bookstore $0.04 lower $8.14 higher textbook Amazon.\nFigure 6.18: Bootstrap distribution average difference new book price UCLA Bookstore versus Amazon (UCLA – Amazon). bounds 99% bootstrap percentile confidence interval superimposed red, observed mean difference new book price superimposed blue.\n","code":""},{"path":"inference-num.html","id":"paired-mean-math","chapter":"6 Inference for quantitative data","heading":"6.2.3 Theory-based inferential methods for \\(\\mu_d\\)","text":"Thinking paired differences single observation observational unit, theory-based inferential methods paired mean difference identical theory-based methods single mean. Theory-based methods one sample mean case covered Section 6.1.3. difference methods Section 6.1 methods described section notation, shown . subscript “d” stands “difference” since variable paired difference.","code":""},{"path":"inference-num.html","id":"observed-data-9","chapter":"6 Inference for quantitative data","heading":"Observed data","text":"Consider paired textbook price data previous section.\nhistogram differences new textbook price UCLA Bookstore Amazon shown \nFigure 6.19, summary statistics\ndisplayed Table 6.4.\nTable 6.4: Summary statistics 68 new textbook price differences (UCLA – Amazon).\n\nFigure 6.19: Histogram difference price book sampled.\n","code":""},{"path":"inference-num.html","id":"variability-of-the-statistic-7","chapter":"6 Inference for quantitative data","heading":"Variability of the statistic","text":"analyze paired data set,\nsimply analyze differences using one sample \\(t\\)-distribution techniques\napplied \nSection 6.1.3.Example 6.13  Set hypothesis test\ndetermine whether, average, UCLA Bookstore’s price \nnew textbook higher price \nbook Amazon.\nAlso, check conditions whether can move\nforward test using \\(t\\)-distribution.considering two scenarios:\\(H_0\\): \\(\\mu_{d} = 0\\).\ntrue mean difference new textbook prices (UCLA – Amazon)\nequal zero.\\(H_A\\): \\(\\mu_{d} > 0\\).\ntrue mean difference new textbook prices (UCLA – Amazon)\ngreater zero.Next, check independence normality conditions:observations based simple random sample,\nindependence reasonable.observations based simple random sample,\nindependence reasonable.outliers,\n\\(n = 68\\) none outliers\nparticularly extreme, normality\n\\(\\bar{x}\\) satisfied.outliers,\n\\(n = 68\\) none outliers\nparticularly extreme, normality\n\\(\\bar{x}\\) satisfied.","code":""},{"path":"inference-num.html","id":"observed-statistic-vs.-null-statistics","chapter":"6 Inference for quantitative data","heading":"Observed statistic vs. null statistics","text":"mentioned previously, methods applied difference identical one-sample techniques. Therefore, full hypothesis test framework given example.Example 6.14  Complete hypothesis test started\nprevious Example.start, compute standard error associated \n\\(\\bar{x}_{d}\\) using sample standard\ndeviation differences\n(\\(s_{d} = 13.42\\))\nnumber differences\n(\\(n = 68\\)):\n\\[\\begin{align*}\n  SE(\\bar{x}_{d})\n    = \\frac{s_{d}}{\\sqrt{n}}\n    = \\frac{13.42}{\\sqrt{68}} = 1.63\n  \\end{align*}\\]\ntest statistic T-score \n\\(\\bar{x}_{d}\\)\nnull condition actual mean\ndifference 0:\n\\[\\begin{align*}\n  T\n    = \\frac{\\bar{x}_{d} - 0}\n        { SE(\\bar{x}_{d})}\n    = \\frac{3.58 - 0}{1.63} = 2.20\n  \\end{align*}\\]\nvalue tells us sample mean difference price, $3.58,\n2.20 standard errors zero (null value).visualize p-value, approximate sampling distribution\n\\(\\bar{x}_{d}\\) drawn though\n\\(H_0\\) true,\np-value represented shaded upper tail Figure 6.20. area equivalent\narea 2.20 \\(t\\)-distribution \\(df = n - 1\\) = 68 \\(-\\) 1 = 67 degrees freedom.Using pt function R, find \nupper tail area 0.0156.\nFigure 6.20: Distribution \\(\\bar{x}_{d}\\) null hypothesis difference. observed average difference 2.98 marked shaded areas extreme observed difference given p-value.\nExample 6.15  Create theory-based 95% confidence interval average price difference books UCLA Bookstore books Amazon.Conditions\nusing theory-based methods already verified standard error\ncomputed previous Example.\n","code":""},{"path":"inference-num.html","id":"differenceOfTwoMeans","chapter":"6 Inference for quantitative data","heading":"6.3 Difference of two means","text":"Notation.\\(n_1\\), \\(n_2\\) = sample sizes two independent samples\\(\\bar{x}_1\\), \\(\\bar{x}_2\\) = sample means two independent samples\\(s_1\\), \\(s_2\\) = sample standard deviations two independent samples\\(\\mu_1\\), \\(\\mu_2\\) = population means two independent populations\\(\\sigma_1\\), \\(\\sigma_2\\) = population standard deviations two independent populations\nsection consider difference \ntwo population means, \\(\\mu_1 - \\mu_2\\), condition\ndata paired.\nJust single sample, identify conditions ensure\ncan use \\(t\\)-distribution point estimate\ndifference, \\(\\bar{x}_1 - \\bar{x}_2\\),\nnew standard error formula.details working inferential problems two independent means setting strikingly similar applied two independent proportions setting.\nfirst cover randomization test observations shuffled assumption null hypothesis true.\nbootstrap data (imposed null hypothesis) create confidence interval true difference population means, \\(\\mu_1 - \\mu_2\\).\nmathematical model, \\(t\\)-distribution, able describe randomization test boostrapping long conditions met.inferential tools applied three different data contexts: determining whether\nstem cells can improve heart function,\nexploring relationship pregnant women’s smoking\nhabits birth weights newborns,\nexploring whether statistically significant\nevidence one variation exam harder \nanother variation.\nsection motivated questions like\n“convincing evidence newborns mothers\nsmoke different average birth weight newborns\nmothers don’t smoke?”","code":""},{"path":"inference-num.html","id":"rand2mean","chapter":"6 Inference for quantitative data","heading":"6.3.1 Randomization test for \\(H_0: \\mu_1 - \\mu_2 = 0\\)","text":"instructor decided run two slight variations exam. Prior passing exams, shuffled exams together ensure student received random version. Summary statistics students performed two exams shown Table 6.5 plotted Figure 6.21. Anticipating complaints students took Version B, like evaluate whether difference observed groups large provides convincing evidence Version B difficult (average) Version .","code":""},{"path":"inference-num.html","id":"observed-data-10","chapter":"6 Inference for quantitative data","heading":"Observed data","text":"\nTable 6.5: Summary statistics scores exam version.\n\nFigure 6.21: Exam scores students given one three different exams.\nConstruct hypotheses evaluate whether observed\ndifference sample means, \\(\\bar{x}_A - \\bar{x}_B=3.1\\),\ndue chance. later evaluate hypotheses\ncomputing p-value test.158Before moving evaluate hypotheses previous Guided Practice, let’s think carefully dataset. observations across two groups independent? concerns outliers?159","code":""},{"path":"inference-num.html","id":"variability-of-the-statistic-8","chapter":"6 Inference for quantitative data","heading":"Variability of the statistic","text":"Section 5.4, variability statistic (previously: \\(\\hat{p}_1 - \\hat{p}_2\\)) visualized shuffling observations across two treatment groups many times.\nshuffling process implements null hypothesis model (effect treatment).\nexam example, null hypothesis exam exam B equally difficult, average scores across two tests .\nexams equally difficult, due natural variability, sometimes expect students slightly better exam (\\(\\bar{x}_A > \\bar{x}_B\\)) sometimes expect students slightly better exam B (\\(\\bar{x}_B > \\bar{x}_A\\)).\nquestion hand : \\(\\bar{x}_A - \\bar{x}_B=3.1\\) indicate exam easier exam B?.Figure 6.22 shows process randomizing exam observed exam scores.\nnull hypothesis true, score exam represent true student ability material.\nshouldn’t matter whether given exam exam B.\nreallocating student got exam, able understand difference average exam scores changes due natural variability.\none iteration randomization process Figure 6.22, leading one simulated difference average scores.\nFigure 6.22: version test (B) randomly allocated test scores, null assumption tests equally difficult.\nBuilding Figure 6.22, Figure 6.23 shows values simulated statistics \\(\\bar{x}_{1, sim} - \\bar{x}_{2, sim}\\) 1000 random simulations.\nsee , just chance, difference scores can range anywhere -10 points +10 points.\nFigure 6.23: Histogram differences means, calculated 1000 different randomizations exam types.\n","code":""},{"path":"inference-num.html","id":"observed-statistic-vs.-null-value-3","chapter":"6 Inference for quantitative data","heading":"Observed statistic vs. null value","text":"goal randomization test assess observed data, statistic interest \\(\\bar{x}_A - \\bar{x}_B = 3.1\\).\nrandomization distribution allows us identify whether difference 3.1 points one expect natural variability.\nplotting value 3.1 Figure 6.24, can measure different similar 3.1 randomized differences generated null hypothesis.\nFigure 6.24: Histogram differences means, calculated 1000 different randomizations exam types. observed difference 3.1 points plotted vertical line, area extreme 3.1 shaded represent p-value.\nExample 6.16  Approximate p-value depicted Figure 6.24, provide conclusion context case study.Using software, find 231 1000 shuffled differences means away zero observed difference 3.1. , 23.1% shuffled statistics lie shaded blue area Figure 6.24. Thus, p-value 0.231.large p-value, data convincingly show one exam\nversion difficult , teacher\nconvinced add points \nVersion B exam scores.large p-value consistency \\(\\bar{x}_A - \\bar{x}_B=3.1\\) randomized differences leads us reject null hypothesis. Said differently, evidence think one tests easier .One might inclined conclude tests level difficulty, conclusion wrong. Indeed, best point estimate true average difference means two tests 3.1!\nhypothesis testing framework set reject null claim, set validate null claim.\nconcluded, data consistent exams B equally difficult, data also consistent exam 3.1 points “easier” exam B. fact, ’ll see next section, since 95% confidence interval \\(\\mu_A - \\mu_B\\) (-2.0, 8.3), data consistent difference exam 2.0 points “harder” exam 8.3 points “easier.”\ndata able adjudicate whether exams equally hard whether one slightly easier.Conclusions null hypothesis rejected often seem unsatisfactory.\nHowever, case, teacher class probably relieved evidence demonstrate one exams difficult .","code":""},{"path":"inference-num.html","id":"boot-ci-diff-means","chapter":"6 Inference for quantitative data","heading":"6.3.2 Bootstrap confidence interval for \\(\\mu_1 - \\mu_2\\)","text":"\nproviding full example working bootstrap analysis actual data, consider fictional situation like compare average price car one Awesome Auto franchise (Group 1) average price car different Awesome Auto franchise (Group 2). able randomly sample five cars Awesome Auto franchise, measure selling price car sample. process bootstrapping can applied Group separately, differences means recalculated time. Figure 6.25 visually describes bootstrap process interest statistic computed two separate samples. analysis proceeds one sample case, now (single) statistic interest difference sample means. , bootstrap resample done groups separately, results combined single bootstrapped difference means. Repetition produce 1000s bootstrapped differences means, histogram describe natural sampling variability associated difference means.\nFigure 6.25: two group comparison, 1000 bootstrap resamples taken separately group, difference sample means calculated pair bootstrap resamples. set 1000 differences analyzed distribution statistic interest, conclusions drawn parameter interest.\n","code":""},{"path":"inference-num.html","id":"observed-data-11","chapter":"6 Inference for quantitative data","heading":"Observed data","text":"treatment using embryonic stem cells (ESCs)\nhelp improve heart function following heart attack?\nTable 6.6 contains summary statistics\nexperiment test ESCs sheep heart attack.\nsheep randomly assigned ESC\ncontrol group, percent change hearts’ pumping\ncapacity measured study.\nFigure 6.26 provides\nhistograms two data sets.\npositive value corresponds increased pumping capacity,\ngenerally suggests stronger recovery.\ngoal identify 90% confidence interval\neffect ESCs change heart pumping\ncapacity relative control group.\nTable 6.6: Summary statistics embryonic stem cell study.\n\nFigure 6.26: Histograms embryonic stem cell control group.\npoint estimate true difference mean heart pumping variable\nstraightforward find: difference sample means.\n\\[\\begin{align*}\n\\bar{x}_{esc} - \\bar{x}_{control}\\ \n  =\\ 3.50 - (-4.33)\\ \n  =\\ 7.83\n\\end{align*}\\]Identify roles two variables study — variable explanatory variable response? scope inference study?160","code":""},{"path":"inference-num.html","id":"variability-of-the-statistic-9","chapter":"6 Inference for quantitative data","heading":"Variability of the statistic","text":"saw Section 5.4.3, use bootstrapping estimate variability associated difference sample means taking repeated samples. method akin two proportions, separate sample taken replacement group (ESCs control), sample means calculated, difference taken. entire process repeated multiple times produce bootstrap distribution difference sample means (without null hypothesis assumption).Figure 6.27 displays variability differences sample means percentile bootstrap 90% confidence interval super imposed.\nFigure 6.27: Histogram differences means 1000 bootstrap resamples taken two groups. observed difference means original data plotted black vertical line 7.83. blue lines provide percentile bootstrap 90% confidence interval difference true population means.\nExample 6.17  bootstrap confidence interval true difference average change pumping capacity, \\(\\mu_{esc} - \\mu_{control}\\), show difference across two treatments?90% interval displayed contain zero (note zero never one bootstrapped differences 95% 99% intervals given conclusion!), conclude ESC treatment significantly better respect heart pumping capacity treatment.study randomized controlled experiment, can conclude treatment (ESC) causing change pumping capacity.","code":""},{"path":"inference-num.html","id":"math2samp","chapter":"6 Inference for quantitative data","heading":"6.3.3 Theory-based inferential methods for \\(\\mu_1 - \\mu_2\\)","text":"one-mean paired mean difference scenarios, difference sample means can modeled \\(t\\)-distribution certain conditions. conditions one-mean paired mean difference scenarios, now conditions need met sample. Similarly, compute test statistic theory-based confidence interval using standard error formula difference sample means.Using \\(t\\)-distribution difference means.\\(t\\)-distribution can used inference working\nstandardized difference two means ifIndependence (extended).\ndata independent within \ntwo groups, e.g., data come \nindependent random samples \nrandomized experiment.Normality.\ncheck outliers \ngroup separately.standard error may computed \n\\[\nSE(\\bar{x}_1 - \\bar{x}_2) = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}},\n\\]official formula degrees freedom quite\ncomplex generally computed using software,\ninstead may use smaller \n\\(n_1 - 1\\) \\(n_2 - 1\\) degrees freedom\nsoftware isn’t readily available.","code":""},{"path":"inference-num.html","id":"t-test-for-mu_1---mu_2","chapter":"6 Inference for quantitative data","heading":"\\(t\\)-test for \\(\\mu_1 - \\mu_2\\)","text":"","code":""},{"path":"inference-num.html","id":"observed-data-12","chapter":"6 Inference for quantitative data","heading":"Observed data","text":"dataset called ncbirths represents random sample 150 cases mothers newborns North Carolina year. Four cases data set represented Table 6.7. particularly interested two variables: weight smoke. weight variable represents weights newborns smoke variable describes mothers smoked pregnancy. like know, convincing evidence newborns mothers smoke different average birth weight newborns mothers don’t smoke? use North Carolina sample try answer question. smoking group includes 50 cases nonsmoking group contains 100 cases.\nTable 6.7: Four cases ncbirths data set. value NA, shown first two entries first variable, indicates piece data missing.\nExample 6.18  Set appropriate hypotheses evaluate\nwhether relationship mother smoking\naverage birth weight.null hypothesis represents case difference\ngroups.\\(H_0\\): difference average birth weight \nnewborns mothers smoke.\nstatistical notation: \\(H_0: \\mu_{n} - \\mu_{s} = 0\\),\n\\(\\mu_{n}\\) represents true mean birth weight babies non-smoking mothers \n\\(\\mu_s\\) represents true mean birth weight babies mothers smoked.alternative hypothesis represents research question.\\(H_A\\): difference average newborn weights\nmothers smoke (\\(\\mu_{n} - \\mu_{s} \\neq 0\\)).","code":""},{"path":"inference-num.html","id":"variability-of-the-statistic-10","chapter":"6 Inference for quantitative data","heading":"Variability of the statistic","text":"check two conditions necessary model difference\nsample means using \\(t\\)-distribution: independence normality conditions sample.data come simple random sample,\nobservations independent,\nwithin samples.data sets 30 observations,\ninspect data \nFigure 6.28\nparticularly extreme outliers\nfind none.Since conditions satisfied, difference\nsample means may modeled using \\(t\\)-distribution.\nFigure 6.28: top panel represents birth weights infants whose mothers smoked. bottom panel represents birth weights infants whose mothers smoke.\nsummary statistics Table 6.8 may useful\nGuided Practice.161What point estimate population difference,\n\\(\\mu_{n} - \\mu_{s}\\)?Compute standard error point estimate \npart .\nTable 6.8: Summary statistics ncbirths data set.\n","code":""},{"path":"inference-num.html","id":"observed-statistic-vs.-null-value-4","chapter":"6 Inference for quantitative data","heading":"Observed statistic vs. null value","text":"test statistic comparing two means T.T score ratio groups differ compared observations within group vary.\\[\\begin{align*}\nT = \\frac{\\bar{x}_1 - \\bar{x}_2 - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\end{align*}\\]null hypothesis true conditions met, T \\(t\\)-distribution \\(df = min(n_1 - 1, n_2 -1)\\).Conditions:independent observations within across groupslarge samples extreme outliersExample 6.19  Complete hypothesis test started previous Example Guided Practice ncbirths dataset research question.\nreference, \\(\\bar{x}_{n} - \\bar{x}_{s} = 0.40\\),\n\\(SE(\\bar{x}_{n} - \\bar{x}_{s}) = 0.26\\), sample sizes \\(n_n = 100\\) \\(n_s = 50\\).can find test statistic test\nusing previous information:\n\\[\\begin{align*}\n  T = \\frac{\\ 0.40 - 0\\ }{0.26} = 1.54\n  \\end{align*}\\]\np-value represented two shaded tails\nFigure 6.29We find single tail area using software. (See R code .) ’ll use \nsmaller \\(n_n - 1 = 99\\) \\(n_s - 1 = 49\\) \ndegrees freedom: \\(df = 49\\).\none tail area 0.065;\ndoubling value gives two-tail area p-value,\n0.135.p-value 0.135 provides little evidence null hypothesis.\ninsufficient evidence say difference\naverage birth weight newborns North Carolina mothers\nsmoke pregnancy newborns North Carolina\nmothers smoke pregnancy.\nFigure 6.29: mathematical model T statistic null hypothesis true: \\(t\\)-distribution \\(min(100-1, 50-1) = 49\\) degrees freedom. expected, curve centered zero (null value). T score also plotted area extreme observed T score plotted indicate p-value.\n’ve seen much research suggesting smoking harmful\npregnancy, fail reject null\nhypothesis previous Example?162If made Type 2 Error difference,\ndone differently data collection\nlikely detect difference?163Public service announcement: used relatively\nsmall data set example, larger data sets show women\nsmoke tend smaller newborns.\nfact, tobacco industry actually audacity\ntout benefit smoking:’s true.\nbabies born women smoke smaller,\n’re just healthy babies born \nwomen smoke.\nwomen prefer smaller babies.\n- Joseph Cullman, Philip Morris’ Chairman Board CBS’ Face Nation, Jan 3, 1971Fact check: babies women smoke actually\nhealthy babies women \nsmoke.164","code":"\npt(1.54, df = 49, lower.tail = FALSE)\n#> [1] 0.065"},{"path":"inference-num.html","id":"t-confidence-interval-for-mu_1---mu_2","chapter":"6 Inference for quantitative data","heading":"\\(t\\) confidence interval for \\(\\mu_1 - \\mu_2\\)","text":"Finding \\(t\\)-confidence interval difference population means, \\(\\mu_1 - \\mu_2\\).Based two independent samples \\(n_1\\) \\(n_2\\) observational units, respectively, clear outliers, confidence interval difference population\nmeans \n\\[\\begin{align*}\n  \\text{point estimate} \\ &\\pm\\  t^{\\star}_{df} \\times SE(\\text{point estimate}) \\\\\n  &\\\\\\\n  \\bar{x}_1 - \\bar{x}_2 \\ &\\pm\\  t^{\\star}_{df} \\times \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_1}}\n  \\end{align*}\\]\n\\(\\bar{x}_1\\) \\(\\bar{x}_2\\) two sample means, \\(t^{\\star}_{df}\\)\ncorresponds confidence level degrees freedom\n\\(df\\), \\(SE\\) standard error estimated \nsample.Example 6.20  Consider data Section 6.3.2 use embryonic stem cells (ESCs) improve heart function. Can \\(t\\)-distribution used make\ninference true difference average change heart pumping function using point estimate,\n\\(\\bar{x}_{esc} - \\bar{x}_{control} = 7.83\\)?First, check independence.\nsheep randomized \ngroups, independence within\ngroups satisfied.Figure 6.26\nreveal clear outliers\neither group.\n(ESC group bit variability,\nclear outliers.)conditions met, can use \n\\(t\\)-distribution model difference sample means.Generally, use statistical software find appropriate\ndegrees freedom using raw data, software isn’t available,\ncan use smaller\n\\(n_1 - 1\\) \\(n_2 - 1\\) degrees freedom.\ncase ESC example, means ’ll use \\(df = 8\\).Example 6.21  Calculate 95% confidence interval \ntrue difference mean change heart pumping capacity \nsheep ’ve suffered heart attack ESC treatment control treatment.First, compute point estimate standard error:\n\\[\\begin{align*}\n  \\bar{x}_{esc} - \\bar{x}_{control} &= 3.50 - (-4.33) = 7.83\\\\   \n  SE(\\bar{x}_{esc} - \\bar{x}_{control}) &= \\sqrt{\\frac{5.17^2}{9} + \\frac{2.76^2}{9}} = 1.95\n  \\end{align*}\\]\nUsing \\(df = 8\\), can identify \ncritical value \\(t^{\\star}_{8} = 2.31\\)\n95% confidence interval. (See R code .)\nFinally, can enter values confidence\ninterval formula:\n\\[\\begin{align*}\n   7.83 \\ \\pm\\ 2.31\\times 1.95\n    \\quad\\rightarrow\\quad (3.32, 12.34)\n  \\end{align*}\\]\n95% confident embryonic stem cells improve\nmean change heart’s pumping function sheep suffered\nheart attack 3.32% 12.34%.","code":"\nqt(0.975, df = 8)\n#> [1] 2.31"},{"path":"inference-num.html","id":"summary-of-t-procedures","chapter":"6 Inference for quantitative data","heading":"6.4 Summary of t-procedures","text":"far chapter, seen \\(t\\)-distribution applied appropriate mathematical model three distinct settings. Although three data structures different, similarities differences worth pointing . provide Table 6.9 partly mechanism understanding \\(t\\)-procedures partly highlight extremely common usage \\(t\\)-distribution practice. often hear following three \\(t\\)-procedures referred one sample \\(t\\)-test (\\(t\\)-interval), paired \\(t\\)-test (\\(t\\)-interval), two sample \\(t\\)-test (\\(t\\)-interval).\nTable 6.9: Similarities \\(t\\)-methods across one sample, paired sample, two independent samples analysis numeric response variable.\nindependence, 2. normality large samples\nindependence, 2. normality large samples\nindependence, 2. normality large samples\nHypothesis tests. applying \\(t\\)-distribution hypothesis test involving means, proceed follows:Write appropriate hypotheses.Verify conditions using \\(t\\)-distribution.\nIndependence. Observational units must independent. typically true data came random sample (two random samples, one random sample randomly assigned two treatments).\nNormality. sample size less 30 clear outliers data, sample size least 30\nparticularly extreme outliers,\ncan apply \\(t\\)-distribution hypothesis tests means. difference means data paired, condition must met two samples.\nIndependence. Observational units must independent. typically true data came random sample (two random samples, one random sample randomly assigned two treatments).Normality. sample size less 30 clear outliers data, sample size least 30\nparticularly extreme outliers,\ncan apply \\(t\\)-distribution hypothesis tests means. difference means data paired, condition must met two samples.Compute statistic interest, standard error, degrees freedom. \\(df\\), use \\(n-1\\) one sample, two samples use either statistical software smaller \\(n_1 - 1\\) \\(n_2 - 1\\).Compute T-score using general formula:\n\\[\n T = \\frac{\\mbox{statistic} - \\mbox{null value}}{\\mbox{standard error statistic}} = \\frac{\\mbox{statistic} - \\mbox{null value}}{SE(\\mbox{statistic})}\n \\]Use statistical software find p-value using appropriate \\(t\\)-distribution:\nSign \\(H_A\\) \\(<\\): p-value = area T-score\nSign \\(H_A\\) \\(>\\): p-value = area T-score\nSign \\(H_A\\) \\(\\neq\\): p-value = 2 \\(\\times\\) area \\(-|\\mbox{T-score}|\\)\nSign \\(H_A\\) \\(<\\): p-value = area T-scoreSign \\(H_A\\) \\(>\\): p-value = area T-scoreSign \\(H_A\\) \\(\\neq\\): p-value = 2 \\(\\times\\) area \\(-|\\mbox{T-score}|\\)Make conclusion based p-value, write conclusion context, plain language, terms alternative hypothesis.Confidence intervals. Similarly, following generally computed confidence interval using \\(t\\)-distribution:Verify conditions using \\(t\\)-distribution. (See .)Compute point estimate interest, standard error, degrees freedom, \\(t^{\\star}_{df}\\). multiplier \\((1-\\alpha)\\times100\\)% confidence interval can found R : qt(1-(alpha/2), df). example, \\(t^{\\star}_{10}\\) 95% confidence qt(0.975, 10) = 2.228.Calculate confidence interval using general formula:\n\\[\n \\mbox{statistic} \\pm\\ t_{df}^{\\star} SE(\\mbox{statistic}).\n \\]Put conclusions context plain language even non-data scientists can understand results.","code":""},{"path":"inference-num.html","id":"r-inference-for-quantitative-data","chapter":"6 Inference for quantitative data","heading":"6.5 R: Inference for quantitative data","text":"","code":""},{"path":"inference-num.html","id":"inference-using-r-and-catstats-1","chapter":"6 Inference for quantitative data","heading":"6.5.1 Inference using R and catstats","text":"","code":""},{"path":"inference-num.html","id":"using-the-t-distribution","chapter":"6 Inference for quantitative data","heading":"Using the \\(t\\)-distribution","text":"First, ’ll review obtain probabilities critical values \\(t\\)-distribution using R. \\(t\\)-statistic degrees freedom, can find probability \\(t\\)-distribution corresponding one- two-tailed hypothesis test using pt() (short “probability \\(t\\)-distribution”).tricky part making sure get correct area distribution. example, assume 12 degrees freedom. \\(t\\)-statistic positive, say \\(t = 1.33\\):\\(t\\)-statistic negative, say \\(t = -2.75\\):difference comes want area \\(t\\) units 0: \\(t\\) positive, “” greater \\(t\\) less negative \\(t\\), double area greater \\(t\\), since \\(t\\)-distribution symmetric. \\(t\\) negative, “” less \\(t\\) greater positive \\(t\\), double area less \\(t\\).find \\(t^*_{df}\\) using R, use qt() function (short “quantile \\(t\\)-distribution”). need degrees freedom confidence level confidence interval. Suppose 27 degrees freedom want 99% confidence interval. get middle 99% \\(t\\)-distribution, need cutoff 0.5% 99.5%:","code":"\n#Area less than observed:\npt(1.33, df = 12)\n#> [1] 0.896\n\n#Area greater than observed:\n1-pt(1.33, df = 12)\n#> [1] 0.104\n\n#Area at least as large as observed:\n2*(1-pt(1.33, df = 12))\n#> [1] 0.208\n#Area less than observed:\npt(-2.75, df = 12)\n#> [1] 0.0088\n\n#Area greater than observed:\n1-pt(-2.75, df = 12)\n#> [1] 0.991\n\n#Area at least as large as observed:\n2*pt(-2.75, df = 12)\n#> [1] 0.0176#> [1] -2.77\n#> [1] 2.77"},{"path":"inference-num.html","id":"simulation-based-inference-for-paired-mean-difference","chapter":"6 Inference for quantitative data","heading":"Simulation-based inference for paired mean difference","text":"Simulation-based inference quantitative data use functions catstats package, categorical data.catstats functions paired data assume values two groups separate columns data frame. ’ll work example using tire wear data, currently stored “long format”, one variable brand another tread depth. First, ’ll convert “wide format”, column brand.format, paired data functions catstats able handle data. First, can get look pairs observations:gives us idea distributions within groups differences within pairs. perform hypothesis test difference tread depth 1000 miles, use paired_test() function:Note data also vector differences. , can hypothesis testing generate confidence interval, won’t able use paired_observed_plot(). Now let’s take look output function:figure displays bootstrapped null distribution, mean standard deviation draws upper right corner. want see mean close null value (almost always zero). isn’t, check value shift input, /increase number_repetitions shift correct.red lines give cutoffs based observed statistic, values extreme colored red. one-sided test, one line. caption figure gives number proportion bootstrapped mean differences extreme observed statistic. case, 20 1000, p-value 0.02.Finally, want generate confidence interval true mean difference using paired_bootstrap_CI() function.bootstrap distribution, now bootstrap distribution mean difference , rather bootstrapped null distribution mean difference. ’ve requested 99% confidence interval, relevant percentiles bootstrap distribution highlighted, interval given caption. case, 99% confident true mean difference tire tread 0 0.004 inches greater Smooth Turn.","code":"\nlibrary(catstats)\ntiresWide <- tires %>% \n  select(brand, tread, car) %>%   #select only ID, group, and outcome vars\n  pivot_wider(names_from = brand,   #name of variable for group\n              values_from = tread)  #name of variable for outcome\ntiresWide <- as.data.frame(tiresWide)\npaired_observed_plot(tiresWide)\npaired_test(\n  data = tiresWide,  #data frame with observed values in groups\n  shift = -0.002,  #amount to shift differences to bootstrap null distribution\n  direction = \"two-sided\",  #Direction of hypothesis test\n  as_extreme_as = 0.002, #Observed statistic\n  number_repetitions = 1000,  #number of bootstrap draws for null distribution\n  which_first = 1  #Which column is first in order of subtraction: 1 or 2?\n)\nset.seed(1054)\npaired_test(\n  data = tiresWide,  #data frame with observed values in groups\n  shift = -0.002,  #amount to shift differences to bootstrap null distribution\n  direction = \"two-sided\",  #Direction of hypothesis test\n  as_extreme_as = 0.002, #Observed statistic\n  number_repetitions = 1000,  #number of bootstrap draws for null distribution\n  which_first = 1  #Which column is first in order of subtraction: 1 or 2?\n)\nset.seed(2374)\npaired_bootstrap_CI(\n  data = tiresWide,   #Wide-form data set or vector of differences\n  number_repetitions = 1000,   #number of draws for bootstrap distribution\n  confidence_level = 0.99,  #Confidence level as a proportion\n  which_first = 1  #Order of subtraction: 1st or 2nd set of values come first?\n)"},{"path":"inference-num.html","id":"theory-based-inference-for-paired-mean-difference","chapter":"6 Inference for quantitative data","heading":"Theory-based inference for paired mean difference","text":"implement theory-based inference paired mean difference R, use t.test() function. example, ’ll use textbook cost data Section 6.2. two ways put paired data t-test using t.test(). First, prices two groups two separate variables (case, bookstore_new amazon_new):Important things note :must include paired = TRUE options, two-sample t-test.categorical data Chapter 5, one-sided alternative, need re-run t.test() two-sided alternative get correct confidence intervalNow let’s take look output call:output tells right top paired test - doesn’t, check paired = TRUE function call. next line gives t-statistic 2.20, degrees freedom df = 67, p-value 0.0156 (can look back Section 6.2.3 see values obtained example). point estimate mean difference final entry: average, new bookstore books cost $3.58 books new Amazon.confidence interval given one-sided confidence interval, since one-sided alternative. need re-run alternative = \"two.sided\" get correct interval true mean difference price $0.33 $6.83 greater cost buying UCLA Bookstore compared buying Amazon.might also single variable dataset contains differences within pairs: create textbook data variable called price_diff. format also usable t.test() function:requires two fewer arguments:y input, since differences contained single variableNo paired = TRUE, since already accounted pairing taking differences.output look almost identical two-variable version :Since input one variable, t.test() treats one-sample t-test, note works just fine: t-statistic, df, p-value, confidence interval, estimated mean put two groups separately indicated paired.","code":"\nt.test(x = ucla_textbooks_f18$bookstore_new, #Outcomes for one of each pair\n       y = ucla_textbooks_f18$amazon_new,  #Outcomes for other of each pair\n       paired = TRUE,  #Tell it to do a paired t-test!!\n       alternative = \"greater\",  #Direction of alternative \n       conf.level = 0.95  #confidence level for interval as a proportion\n)\nt.test(x = ucla_textbooks_f18$bookstore_new, #Outcomes for first in order of subtraction\n       y = ucla_textbooks_f18$amazon_new,  #Outcomes for second in order of subtraction\n       paired = TRUE,  #Tell it to do a paired t-test!!\n       alternative = \"greater\",  #Direction of alternative \n       conf.level = 0.95  #confidence level for interval as a proportion\n)\n#> \n#>  Paired t-test\n#> \n#> data:  ucla_textbooks_f18$bookstore_new and ucla_textbooks_f18$amazon_new\n#> t = 2, df = 67, p-value = 0.02\n#> alternative hypothesis: true difference in means is greater than 0\n#> 95 percent confidence interval:\n#>  0.868   Inf\n#> sample estimates:\n#> mean of the differences \n#>                    3.58#> \n#>  Paired t-test\n#> \n#> data:  ucla_textbooks_f18$bookstore_new and ucla_textbooks_f18$amazon_new\n#> t = 2, df = 67, p-value = 0.03\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  0.334 6.832\n#> sample estimates:\n#> mean of the differences \n#>                    3.58\nucla_textbooks_f18 %>% \n  mutate(price_diff = bookstore_new-amazon_new)\n\nt.test(x = ucla_textbooks_f18$price_diff,   #variable with differences\n       alternative = \"greater\",  #direction of alternative hypothesis\n       conf.level = 0.95)  #confidence level as a proportion\nucla_textbooks_f18 <- ucla_textbooks_f18 %>% \n  mutate(price_diff = bookstore_new-amazon_new)\n\nt.test(x = ucla_textbooks_f18$price_diff,   #variable with differences\n       alternative = \"greater\",  #direction of alternative hypothesis\n       conf.level = 0.95)  #confidence level as a proportion\n#> \n#>  One Sample t-test\n#> \n#> data:  ucla_textbooks_f18$price_diff\n#> t = 2, df = 67, p-value = 0.02\n#> alternative hypothesis: true mean is greater than 0\n#> 95 percent confidence interval:\n#>  0.868   Inf\n#> sample estimates:\n#> mean of x \n#>      3.58"},{"path":"inference-num.html","id":"simulation-based-inference-for-the-difference-of-two-means","chapter":"6 Inference for quantitative data","heading":"Simulation-based inference for the difference of two means","text":"can perform simulation-based inference difference means using two_mean_test() two_mean_bootstrap_CI() functions catstats package. working example, let’s look embryonic stem cell data Section 6.3.1.perform simulation-based test difference mean change heart pumping capacity, use two_mean_test() function catstats package, similar use two_proportion_test() function Chapter 5:results give side--side boxplot observed data observed difference order subtraction top. Check right value observed difference! Next box plot, null distribution simulated differences means, observed statistic marked vertical red line, values extreme observed statistic colored red. figure caption gives approximate p-value: set 1000 simulations, 1/1000 = 0.001.couple things note using two_mean_test function:need identify variable response explanatory variable using formula argument.Specify order subtraction using first_in_subtraction putting EXACTLY category explanatory variable want first, quotes — must match capitalization, spaces, etc. text values!\nuse bootstrapping find confidence interval true difference means two_mean_bootstrap_CI() function. arguments similar two_mean_test(), addition confidence level.function produces bootstrap distribution difference means, upper lower percentiles confidence range marked vertical lines. figure caption gives estimated confidence interval. case, 90% confidence ESCs increase change heart pumping capacity 4.72 11.09 percentage points average.","code":"\n#load data from openintro package\ndata (stem_cell)\n\n#Compute change in pumping capacity\nstem_cell <- stem_cell %>%\n  mutate(change = after - before)\nset.seed(4750)\ntwo_mean_test(\n  formula = change ~ trmt,  #Always use response ~ explanatory\n  data = stem_cell,  # name of data set\n  first_in_subtraction = \"esc\",  #value of group variable to be 1st in subtraction\n  direction = \"two-sided\",  #direction of alternative\n  as_extreme_as = 7.833,  #observed statistic\n  number_repetitions = 1000  #number of simulations\n)\nset.seed(450)\ntwo_mean_bootstrap_CI(\n  formula = change ~ trmt,  #Always use response ~ explanatory\n  data = stem_cell,  # name of data set\n  first_in_subtraction = \"esc\",  #value of group variable to be 1st in subtraction\n  confidence_level = 0.9, #confidence level as a proportion\n  number_repetitions = 1000  #number of bootstrap samples\n)"},{"path":"inference-num.html","id":"theory-based-inference-for-the-difference-of-two-means","chapter":"6 Inference for quantitative data","heading":"Theory-based inference for the difference of two means","text":"demonstrate theory-based methods R difference means, continue use embryonic stem cell data. Something keep mind work example: theory-based methods appropriate ? results different results simulation-based methods?perform theory-based inference, use t.test() function R. Remember sometimes need change reference category explanatory variable correct order subtraction - case, default ctrl - esc, since ctrl first alphabetically. can get preferred order subtracation esc-ctrl way:results look familiar paired t-test . t-statistic, degrees freedom, p-values, confidence interval, group-specific means. degrees freedom may look little strange - remember correct formula complex! - obtain 12.225 df. computing results hand using pt() qt() saw earlier section, use 8 df, since \\(n_1 -1 = n_2 -1 = 8\\). end, conclude strong evidence null hypothesis difference mean change heart pumping capacity (p = 0.002).Remember one-sided alternative, need run t.test() two-sided alternative get correct confidence interval! reminder Inf confidence interval results, means R computing one-sided confidence interval. , two-sided alternative, can use CI reported: 90% confident true average improvement heart pumping capacity due ESCs 4.35 11.31 percentage points.","code":"\nstem_cell$trmt <- relevel(stem_cell$trmt, ref = \"esc\")\nt.test(stem_cell$change ~ stem_cell$trmt, #Always use response ~ explanatory\n       alternative = \"two.sided\", # Direction of alternative\n       conf.level = 0.9)  #confidence level as a proportion\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  stem_cell$change by stem_cell$trmt\n#> t = 4, df = 12, p-value = 0.002\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 90 percent confidence interval:\n#>   4.35 11.31\n#> sample estimates:\n#>  mean in group esc mean in group ctrl \n#>               3.50              -4.33"},{"path":"inference-num.html","id":"catstats-function-summary-1","chapter":"6 Inference for quantitative data","heading":"6.5.2 catstats function summary","text":"previous section, introduced four new R\nfunctions catstats library. provide summary \nfunctions, plus summary paired_observed_plot\nfunction can used plot paired data. can also access\nhelp files functions using ? command. \nexample, type ?paired_test R console bring \nhelp file paired_test function.\npaired_observed_plot: Produce plot observed matched pairs data. (Note: Input must observed value member pair, differences.)\ndata = two-column data frame, values group two columns\npaired_observed_plot: Produce plot observed matched pairs data. (Note: Input must observed value member pair, differences.)data = two-column data frame, values group two columnspaired_test: Simulation-based hypothesis test paired mean difference.\ndata = vector observed differences; two-column data frame, values group two columns\nwhich_first = name group first order subtraction (data two-column data frame)\nshift = amount shift differences bootstrapping null distribution\ndirection = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)\nas_extreme_as = value observed statistic\nnumber_repetitions = number simulated samples generate (least 1000!)\n\npaired_test: Simulation-based hypothesis test paired mean difference.data = vector observed differences; two-column data frame, values group two columnswhich_first = name group first order subtraction (data two-column data frame)shift = amount shift differences bootstrapping null distributiondirection = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)as_extreme_as = value observed statisticnumber_repetitions = number simulated samples generate (least 1000!)\npaired_bootstrap_CI: Bootstrap confidence interval paired mean difference.\ndata = vector observed differences; two-column data frame, values group two columns\nwhich_first = name group first order subtraction (data two-column data frame)\nconfidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)\nnumber_repetitions = number simulated samples generate (least 1000!)\n\npaired_bootstrap_CI: Bootstrap confidence interval paired mean difference.data = vector observed differences; two-column data frame, values group two columnswhich_first = name group first order subtraction (data two-column data frame)confidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)number_repetitions = number simulated samples generate (least 1000!)\ntwo_mean_test: Simulation-based hypothesis test difference two means.\nformula = y ~ x y name quantitative response variable data set x name binary explanatory variable\ndata = name data set\nfirst_in_subtraction = category explanatory variable first subtraction, written quotations\ndirection = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)\nas_extreme_as = value observed difference proportions\nnumber_repetitions = number simulated samples generate (least 1000!)\n\ntwo_mean_test: Simulation-based hypothesis test difference two means.formula = y ~ x y name quantitative response variable data set x name binary explanatory variabledata = name data setfirst_in_subtraction = category explanatory variable first subtraction, written quotationsdirection = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)as_extreme_as = value observed difference proportionsnumber_repetitions = number simulated samples generate (least 1000!)\ntwo_mean_bootstrap_CI: Bootstrap confidence interval difference two means.\nformula = y ~ x y name quantitative response variable data set x name binary explanatory variable\ndata = name data set\nfirst_in_subtraction = category explanatory variable first subtraction, written quotations\nconfidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)\nnumber_repetitions = number simulated samples generate (least 1000!)\ntwo_mean_bootstrap_CI: Bootstrap confidence interval difference two means.formula = y ~ x y name quantitative response variable data set x name binary explanatory variabledata = name data setfirst_in_subtraction = category explanatory variable first subtraction, written quotationsconfidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)number_repetitions = number simulated samples generate (least 1000!)","code":""},{"path":"inference-num.html","id":"interactive-r-tutorials-4","chapter":"6 Inference for quantitative data","heading":"6.5.3 Interactive R tutorials","text":"Navigate concepts ’ve learned chapter R using following self-paced tutorials.\nneed browser get started!Tutorial 7: Inference categorical responsesTutorial 7 - Lesson 1: Bootstrapping estimating parameterTutorial 7 - Lesson 2: Introducing t-distributionTutorial 7 - Lesson 3: Inference difference two meansTutorial 7 - Lesson 4: Comparing many meansYou can also access full list tutorials supporting book .","code":""},{"path":"inference-num.html","id":"r-labs-5","chapter":"6 Inference for quantitative data","heading":"6.5.4 R labs","text":"apply concepts ’ve learned chapter R computational labs walk data analysis case study.Inference numerical responses - Youth Risk Behavior Surveillance SystemFull list labs supporting OpenIntro::Introduction Modern Statistics","code":""},{"path":"inference-num.html","id":"chp6-review","chapter":"6 Inference for quantitative data","heading":"6.6 Chapter 6 review","text":"","code":""},{"path":"inference-num.html","id":"terms-5","chapter":"6 Inference for quantitative data","heading":"6.6.1 Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"inference-reg.html","id":"inference-reg","chapter":"7 Inference for regression","heading":"7 Inference for regression","text":"now bring together ideas inferential analyses Chapters 5 6 descriptive models seen Chapter 3. setting now focused predicting quantitative response variable, \\(y\\), quantitative explanatory variable, \\(x\\). continue ask questions variability model sample sample. sampling variability inform conclusions population can drawn.Many inferential ideas remarkably similar covered previous chapters. technical conditions simple linear regression typically assessed graphically, although independence observations continues utmost importance.","code":""},{"path":"inference-reg.html","id":"inferenceForLinearRegression","chapter":"7 Inference for regression","heading":"7.1 Inference for linear regression","text":"chapter, bring together inferential ideas (see Chapters 5 6) used make claims population information sample modeling ideas seen Chapter 3.\nparticular, conduct inference slope least squares regression line test whether relationship two quantitative variables.\nAdditionally, build confidence intervals quantify slope linear regression line.","code":""},{"path":"inference-reg.html","id":"observed-data-13","chapter":"7 Inference for regression","heading":"Observed data","text":"start chapter hypothetical example describing linear relationship dollars spent advertising chain sandwich restaurant monthly revenue. hypothetical example serves purpose illustrating linear model varies sample sample. made example data (entire population), can take many many samples population visualize variability. Note real life, always exactly one sample (, one dataset), inference process, imagine might happened taken different sample. change sample sample leads understanding single observed dataset different population values, typically fundamental goal inference.Consider following hypothetical population sandwich stores particular chain seen Figure 7.1.\nmade-world, CEO actually relevant data, can plot .\nCEO omniscient can write population model describes true population relationship advertising dollars revenue.\nappears linear relationship advertising dollars revenue ($1000).\nFigure 7.1: Revenue linear model advertising dollars population sandwich stores, $1000.\nmay remember Chapter 3 population model : \\[y = \\beta_0 + \\beta_1 x + \\varepsilon.\\], omniscient CEO (full population information) can write true population model : \\[\\mbox{expected revenue} = 11.23 + 4.8 \\cdot \\mbox{advertising}.\\]","code":""},{"path":"inference-reg.html","id":"variability-of-the-statistic-11","chapter":"7 Inference for regression","heading":"Variability of the statistic","text":"Unfortunately, scenario, CEO willing part full set data, allow potential franchise buyers see small sample data order help potential buyer decide whether set new franchise.\nCEO willing give potential franchise buyer random sample data 20 stores.numerical characteristic describes subset population, estimated slope sample vary sample sample.\nConsider linear model describes revenue ($1000) based advertising dollars ($1000).least squares regression model uses data find sample linear fit: \\[\\hat{y} = b_0 + b_1 x,\\]\\(y\\) represents actual revenue, \\(\\hat{y}\\) represents predicted revenue model, \\(x\\) represents advertising dollars. random sample 20 stores shows different least square regression line depending observations selected.\nsubset size 20 stores shows similar positive trend advertising revenue (saw Figure 7.1, described population) despite fewer observations plot.\nFigure 7.2: random sample 20 stores entire population. positive linear trend advertising revenue continues observed.\nsecond sample size 20 also shows positive trend!\nFigure 7.3: different random sample 20 stores entire population. , positive linear trend advertising revenue observed.\nline slightly different!\nFigure 7.4: linear models two different random samples quite similar, line.\n, variability regression line sample sample. concept sampling variability something ’ve seen , lesson, focus variability line often measured variability single statistic: slope line.\nFigure 7.5: repeated samples size 20 taken entire population, linear model slightly different. red line provides linear fit entire population, shown Figure ??.\nmight notice Figure 7.5 \\(\\hat{y}\\) values given lines much consistent middle dataset ends. reason data anchors lines way line must pass center data cloud. effect fan-shaped lines predicted revenue advertising close $4,000 much precise revenue predictions made $1,000 $7,000 advertising.distribution slopes (samples size \\(n=20\\)) can seen histogram, Figure 7.6.\nFigure 7.6: Variability slope estimates taken many different samples stores, size 20.\nRecall, example described introduction hypothetical.\n, created entire population order demonstrate slope line vary sample sample.\ntools textbook designed evaluate one single sample data.\nactual studies, repeated samples, able use repeated samples visualize variability slopes.\nseen variability samples throughout text, come surprise different samples produce different linear models.\nHowever, nice visually consider linear models produced different slopes.\nAdditionally, measuring variability previous statistics (e.g., \\(\\bar{x}_1 - \\bar{x}_2\\) \\(\\hat{p}_1 - \\hat{p}_2\\)), histogram sample statistics can provide information related inferential considerations.following sections, distribution (.e., histogram) \\(b_1\\) (estimated slope coefficient) constructed three ways , now, may familiar .\nFirst (Section 7.1.1), distribution \\(b_1\\) \\(\\beta_1 = 0\\) constructed randomizing (permuting) response variable.\nNext (Section 7.1.2), can bootstrap data taking random samples size \\(n\\) original dataset.\nlast (Section 7.1.3), use mathematical tools describe variability using \\(t\\)-distribution first encountered Section 6.1.3.","code":""},{"path":"inference-reg.html","id":"randslope","chapter":"7 Inference for regression","heading":"7.1.1 Randomization test for \\(H_0: \\beta_1= 0\\)","text":"Consider data Global Crop Yields compiled World Data presented part TidyTuesday series seen Figure 7.7. scientific research interest hand determining linear relationship wheat yield (country-year) crop yields. dataset quite rich deserves exploring, example, focus annual crop yield United States.\nFigure 7.7: Yield (tonnes per hectare) six different crops US. color dot indicates year.\nseen previously, statistical inference typically relies setting null hypothesis hoped subsequently rejected. linear model setting, might hope linear relationship maize wheat settings maize production known wheat production needs predicted.relevant hypotheses linear model setting can written terms population slope parameter. population refers larger set years maize wheat grown US.\\(H_0: \\beta_1= 0\\), linear relationship wheat maize.\\(H_A: \\beta_1 \\ne 0\\), linear relationship wheat maize.Recall randomization test, shuffle one variable eliminate existing relationship variables. , set null hypothesis true, measure natural variability data due sampling due variables correlated. Figure 7.8 shows observed data scatterplot one permutation wheat variable. careful observer can see observed values wheat (maize) exist original data plot well permuted wheat plot, given wheat maize yields longer matched given year. , wheat yield randomly assigned new maize yield.\nFigure 7.8: Original (left) permuted (right) data. permutation removes linear relationship wheat maize. Repeated permutations allow quantifying variability slope condition linear relationship (.e., null hypothesis true).\nrepeatedly permuting response variable, pattern linear model observed due random chance (underlying relationship). randomization test compares slopes calculated permuted response variable observed slope. observed slope inconsistent slopes permuting, can conclude underlying relationship (slope merely due random chance).","code":""},{"path":"inference-reg.html","id":"observed-data-14","chapter":"7 Inference for regression","heading":"Observed data","text":"continue use crop data investigate linear relationship wheat maize. Note fitted least squares model (see Chapter 3) describing relationship given Table 7.1.\nTable 7.1: least squares estimates intercept slope given estimate column. observed slope 0.195.\n“estimate” column, can write least squares regression line \n\\[\n\\hat{y} = 1.033 + 0.195x,\n\\]\n\\(\\hat{y}\\) predicted wheat yield, \\(x\\) maize yield (tonnes per hectare).columns Table 7.1 described Section 7.1.3 introduce theory-based methods inference regression slope.","code":""},{"path":"inference-reg.html","id":"variability-of-the-statistic-12","chapter":"7 Inference for regression","heading":"Variability of the statistic","text":"permuting data, least squares estimate line can computed. Repeated permutations slope calculations describe variability line (.e., slope) due natural variability due relationship wheat maize. Figure 7.9 shows two different permutations wheat resulting linear models.\nFigure 7.9: Two different permutations wheat variable slightly different least squares regression lines.\ncan see, sometimes slope permuted data positive, sometimes negative. randomization happens condition underlying relationship (response variable completely mixed explanatory variable), expect see center randomized slope distribution zero.","code":""},{"path":"inference-reg.html","id":"observed-statistic-vs.-null-value-5","chapter":"7 Inference for regression","heading":"Observed statistic vs. null value","text":"\nFigure 7.10: Histogram slopes given different permutations wheat variable. vertical red line observed value slope, \\(b_1\\) = 0.195.\ncan see Figure 7.10, slope estimate extreme observed slope estimate (red line) never happened many repeated permutations wheat variable.\n, indeed linear relationship wheat maize, natural variability slopes produce estimates approximately -0.1 +0.1.\nTherefore, believe slope observed original data just due natural variability indeed, linear relationship wheat maize crop yield (tonnes per hectare) US.","code":""},{"path":"inference-reg.html","id":"bootbeta1","chapter":"7 Inference for regression","heading":"7.1.2 Bootstrap confidence interval for \\(\\beta_1\\)","text":"seen previous chapters, can use bootstrapping estimate sampling distribution statistic interest (, slope) without null assumption relationship (condition randomization test). interest now creating CI, null hypothesis, won’t reason permute either variables.","code":""},{"path":"inference-reg.html","id":"observed-data-15","chapter":"7 Inference for regression","heading":"Observed data","text":"Returning crop data, may want consider relationship yields peas wheat. peas good predictor wheat? , relationship? , slope models average wheat yield function peas yield (tonnes per hectare)?\nFigure 7.11: Original data: wheat yield linear model peas yield, tonnes per hectare. Notice relationship peas wheat strong relationship saw previously maize wheat.\n","code":""},{"path":"inference-reg.html","id":"variability-of-the-statistic-13","chapter":"7 Inference for regression","heading":"Variability of the statistic","text":"focused null distribution, sample replacement \\(n=58\\) \\((x,y)\\)-pairs original dataset. Recall bootstrapping, always resample number observations start order mimic process taking sample population. sampling linear model case, consider observation single dot scatterplot. dot resampled, wheat peas measurement observed. measurements linked dot (.e., year measurements taken).\nFigure 7.12: Original one bootstrap sample crop data. Note difficult differentiate two plots, (within single bootstrap sample) observations resampled twice plotted points top one another. orange circle represent points original data included bootstrap sample. blue circle represents point repeatedly resampled (therefore darker) bootstrap sample. green circle represents particular structure data observed original bootstrap samples.\nFigure 7.12 shows original data compared single bootstrap sample, resulting (slightly) different linear models.\norange circle represent points original data included bootstrap sample.\nblue circle represents point repeatedly resampled (therefore darker) bootstrap sample.\ngreen circle represents particular structure data observed original bootstrap samples.\nrepeatedly resampling, can see dozens bootstrapped slopes plot Figure 7.13.\nFigure 7.13: Repeated bootstrap resamples size 58 taken original data. bootstrapped linear model slightly different.\nRecall order create confidence interval slope, need find range values statistic (slope) takes different bootstrap samples.\nFigure 7.14 histogram relevant bootstrapped slopes.\ncan see 95% bootstrap percentile interval true population slope given (0.061, 0.52).\n95% confident model describing population crops peas wheat, one ton per hectare increase peas yield associated increase predicted average wheat yield 0.061 0.52 tonnes per hectare.\nFigure 7.14: original crop data wheat peas bootstrapped 1,000 times. histogram provides sense variability standard deviation linear model slope sample sample.\n","code":""},{"path":"inference-reg.html","id":"mathslope","chapter":"7 Inference for regression","heading":"7.1.3 Theory-based inferential methods for \\(\\beta_1\\)","text":"certain technical conditions apply, convenient use mathematical approximations test estimate slope parameter.\napproximations build \\(t\\)-distribution described Chapter 6.\nmathematical model often correct usually easy implement computationally.\nvalidity technical conditions considered detail Section 7.2.section, discuss uncertainty estimates slope\n\\(y\\)-intercept regression line. Just identified standard\nerrors point estimates previous chapters, first discuss\nstandard errors new estimates.","code":""},{"path":"inference-reg.html","id":"midterm-elections-and-unemployment","chapter":"7 Inference for regression","heading":"Midterm elections and unemployment","text":"Elections members United States House Representatives\noccur every two years, coinciding every four years U.S.\nPresidential election. set House elections occurring \nmiddle Presidential term called midterm elections. America’s two-party\nsystem (vast majority House members history either Republicans Democrats), one political theory suggests higher unemployment rate,\nworse President’s party midterm elections. 2020, 232 Democrats, 198 Republicans, 1 Libertarian House.assess validity claim, can compile historical data \nlook connection. consider every midterm election 1898 \n2018, exception elections Great Depression.\nHouse Representatives made 435 voting members. Figure 7.15 shows data \nleast-squares regression line:\n\\[\\begin{aligned}\n&\\text{predicted % change House seats President's party}  \\\\\n&\\qquad\\qquad= -7.36 - 0.89 \\times \\text{(unemployment rate)}\\end{aligned}\\]\nconsider percent change number seats President’s\nparty (e.g., percent change number seats Republicans \n2018) unemployment rate.observational units data set?165Examining data, clear deviations linearity substantial outliers (see Section 7.2 discussion using residuals visualize well linear model fits data).\ndata collected sequentially, separate analysis used check apparent correlation successive observations time; correlation found.\nFigure 7.15: percent change House seats President’s party midterm election 1898 2010 plotted unemployment rate. two points Great Depression removed, least squares regression line fit data.\ndata Great Depression (1934 1938) removed \nunemployment rate 21% 18%, respectively. agree \nremoved investigation? ?166There negative slope line shown Figure 7.15. However, slope (\ny-intercept) estimates parameter values. might\nwonder, convincing evidence “true” linear model \nnegative slope? , data provide strong evidence \npolitical theory accurate, unemployment rate useful\npredictor midterm election? can frame investigation \nstatistical hypothesis test:\\(H_0\\): \\(\\beta_1 = 0\\). true linear model zero slope.\\(H_A\\): \\(\\beta_1 < 0\\). true linear model negative slope. percent change House seats President’s party negatively correlated percent unemployment.assess hypotheses, identify standard error estimate, compute appropriate test statistic, identify p-value.","code":""},{"path":"inference-reg.html","id":"understanding-regression-output-from-software","chapter":"7 Inference for regression","heading":"Understanding regression output from software","text":"Just like point estimates seen , can compute \nstandard error test statistic \\(b_1\\). generally label \ntest statistic using \\(T\\), since follows \\(t\\)-distribution.rely statistical software compute standard error \nleave explanation standard error determined \nsecond third statistics course.\nTable 7.2 shows software output least\nsquares regression line Figure 7.15. row labeled unemp includes relevant information slope estimate (.e., coefficient unemployment variable).\nTable 7.2: Output statistical software regression\nline modeling midterm election losses \nPresident’s party response unemployment.\nExample 7.1  first second columns Table 7.2 represent?entries first column represent least squares estimates, \\(b_0\\) \\(b_1\\). Using estimates, write equation least\nsquare regression line \\[\\begin{aligned}\n  \\hat{y} = -7.36 - 0.89 x\n  \\end{aligned}\\] \\(\\hat{y}\\) case represents predicted\nchange number seats president’s party, \\(x\\)\nrepresents unemployment rate.values second column correspond standard errors \nestimate, \\(SE(b_0)\\) \\(SE(b_1)\\). use values computing test statistic confidence interval \\(\\beta_0\\) \\(\\beta_1\\).previously used \\(t\\)-test statistic hypothesis testing \ncontext numerical data. Regression similar. hypotheses\nconsider, null value slope 0, can compute \ntest statistic using T-score formula:\n\\[\\begin{aligned}\nT\n  = \\frac{\\text{estimate} - \\text{null value}}{\\text{SE(estimate)}}\n  = \\frac{-0.89 - 0}{0.835}\n  = -1.07\\end{aligned}\\] corresponds third column \nTable 7.2 .Example 7.2  Use Table 7.2 determine p-value \nhypothesis test.last column table gives p-value \ntwo-sided hypothesis test coefficient unemployment rate:\n0.296. However, test one-sided — interested detecting true slope coefficient negative. Since estimated slope coefficient negative, one-sided p-value just half two-sided p-value: 0.148.167 p-value, data provide convincing evidence \nhigher unemployment rate negative correlation percent seats lost house. words, significant evidence political theory higher unemployment rate, worse President’s party midterm elections.","code":""},{"path":"inference-reg.html","id":"intuition-vs.-formal-inference","chapter":"7 Inference for regression","heading":"Intuition vs. formal inference","text":"final step mathematical hypothesis test slope, use information provided make conclusion whether data come population true slope zero (.e., \\(\\beta_1 = 0\\)). evaluating formal hypothesis claim, sometimes important check intuition. Based everything ’ve seen examples describing variability line sample sample, linear relationship given data come population slope truly zero.Example 7.3  Elmhurst College Illinois released anonymized data family income financial support provided school Elmhurst’s first-year students 2011. Figure 7.16 shows least-squares regression line fit scatterplot sample data. sure slope \nstatistically significantly different zero? , think \nformal hypothesis test reject claim true slope \nline zero?relationship variables perfect, evident decreasing trend data.\nsuggests hypothesis test reject null claim slope zero.\nFigure 7.16: Gift aid family income random sample 50 first-year students Elmhurst College, shown regression line.\npoint tools section go beyond visual interpretation linear relationship toward formal mathematical claim statistical significance slope estimate.\nTable 7.3: Summary least squares fit Elmhurst College data, predicting gift aid university based family income students.\nTable 7.3 shows\nstatistical software output fitting least squares regression\nline shown Figure 7.16. Use output formally\nevaluate following hypotheses.\\(H_0\\): true coefficient family income zero.\\(H_A\\): true coefficient family income zero.168Inference regression.usually rely statistical software \nidentify point estimates, standard errors, test statistics, p-values\npractice. However, aware software generally check\nwhether method appropriate, meaning must still verify\nconditions met. See Section 7.2.","code":""},{"path":"inference-reg.html","id":"theory-based-confidence-interval-for-a-regression-coefficient","chapter":"7 Inference for regression","heading":"Theory-based confidence interval for a regression coefficient","text":"Similar can conduct hypothesis test model coefficient\nusing regression output, can also construct confidence interval \ncoefficient.Confidence intervals coefficientsConfidence intervals model\ncoefficients (e.g., \\(y\\)-intercept slope) can computed using \\(t\\)-distribution:\n\\[\\begin{aligned}\n  b_i \\ \\pm\\ t_{df}^{\\star} \\times SE(b_{})\n  \\end{aligned}\\] \\(t_{df}^{\\star}\\) appropriate \\(t\\)-value\ncorresponding confidence level model’s degrees \nfreedom. simple linear regression, model’s degrees freedom \\(n-1\\).Example 7.4  Compute 95% confidence interval slope coefficient using \nregression output Table 7.3.point estimate \\(b_1 = -0.0431\\) standard error \\(SE(b_1) = 0.0108\\). degrees freedom distribution \\(df = n - 2 = 48\\), typically noted regression output, allowing us identify \\(t_{48}^{\\star} = 2.01\\) use confidence interval.can now construct confidence interval usual way:\n\\[\\begin{aligned}\n  \\text{point estimate} &\\pm t_{48}^{\\star} \\times SE(\\text{point estimate}) \\\\\n  &\\qquad\\\\qquad \\\\\n    -0.0431  &\\pm 2.01 \\times 0.0108 \\\\\n    &\\qquad\\\\qquad\\\\\n    (-0.&0648, -0.0214)\n  \\end{aligned}\\]\n95% confident , $1000 increase \nfamily income, university’s gift aid predicted decrease average $21.40 $64.80.topic intervals book, ’ve focused exclusively \nconfidence intervals model parameters. However, \ntypes intervals may interest, including prediction\nintervals response value also confidence intervals mean\nresponse value context regression. intervals typically covered second course statistics.","code":""},{"path":"inference-reg.html","id":"tech-cond-linmod","chapter":"7 Inference for regression","heading":"7.2 Checking model conditions","text":"previous sections, used randomization bootstrapping perform inference mathematical model valid due violations technical conditions. section, ’ll provide details mathematical model appropriate discussion technical conditions needed randomization bootstrapping procedures.","code":""},{"path":"inference-reg.html","id":"what-are-the-technical-conditions","chapter":"7 Inference for regression","heading":"What are the technical conditions?","text":"fitting least squares line, generally requireLinearity. data show linear trend. nonlinear trend\n(e.g., first panel Figure 7.17, advanced regression\nmethod another book later course applied.Linearity. data show linear trend. nonlinear trend\n(e.g., first panel Figure 7.17, advanced regression\nmethod another book later course applied.Independent observations. cautious applying regression data, sequential\nobservations time stock price day. data may\nunderlying structure considered model\nanalysis. example data set successive observations\nindependent shown fourth panel \nFigure 7.17. also \ninstances correlations within data important, paired data described Section 6.2.Independent observations. cautious applying regression data, sequential\nobservations time stock price day. data may\nunderlying structure considered model\nanalysis. example data set successive observations\nindependent shown fourth panel \nFigure 7.17. also \ninstances correlations within data important, paired data described Section 6.2.Nearly normal residuals. Generally, residuals must nearly normal. condition\nfound unreasonable, usually outliers \nconcerns influential points, discussed \nSection 3.3. example \nresidual potentially concern shown \nFigure 7.17, one observation \nclearly much regression line others.Nearly normal residuals. Generally, residuals must nearly normal. condition\nfound unreasonable, usually outliers \nconcerns influential points, discussed \nSection 3.3. example \nresidual potentially concern shown \nFigure 7.17, one observation \nclearly much regression line others.Constant equal variability. variability points around least squares line remains\nroughly constant. example non-constant variability shown \nthird panel \nFigure 7.17, represents \ncommon pattern observed condition fails: \nvariability \\(y\\) larger \\(x\\) larger.Constant equal variability. variability points around least squares line remains\nroughly constant. example non-constant variability shown \nthird panel \nFigure 7.17, represents \ncommon pattern observed condition fails: \nvariability \\(y\\) larger \\(x\\) larger.\nFigure 7.17: Four examples showing methods chapter insufficient apply data. top set graphs represents \\(x\\) \\(y\\) relationship. bottom set graphs residual plot. First panel: linearity fails. Second panel: outliers, especially one point far away line. Third panel: variability errors related value \\(x\\). Fourth panel: time series data set shown, successive observations highly correlated.\nconcerns applying least squares regression \nElmhurst data Figure 3.13?169The technical conditions often remembered using LINE mnemonic.\nlinearity, normality, equality variance conditions usually can assessed residual plots, seen Figure 7.17.\ncareful consideration experimental design undertaken confirm observed values indeed independent.L: linear modelI: independent observationsN: points normally distributed around lineE: equal variability around line values explanatory variable","code":""},{"path":"inference-reg.html","id":"why-do-we-need-technical-conditions","chapter":"7 Inference for regression","heading":"Why do we need technical conditions?","text":"inferential techniques covered text, technical conditions don’t hold, possible make concluding claims population.\n, without technical conditions, T-score (Z-score) assumed t-distribution (standard normal Z distribution).\nsaid, almost always impossible check conditions precisely, look large deviations conditions.\nlarge deviations, unable trust calculated p-value endpoints resulting confidence interval.","code":""},{"path":"inference-reg.html","id":"what-about-linearity","chapter":"7 Inference for regression","heading":"What about Linearity?","text":"linearity condition among important goal understand linear model \\(x\\) \\(y\\).\nexample, value slope meaningful true relationship \\(x\\) \\(y\\) quadratic.\ncautious inference, model also accurate portrayal relationship variables. extended discussion different methods modeling functional forms linear outside scope text.","code":""},{"path":"inference-reg.html","id":"what-about-independence","chapter":"7 Inference for regression","heading":"What about Independence?","text":"technical condition describing independence observations often crucial also difficult diagnose. also extremely difficult gather dataset true random sample population interest. (Note: true randomized experiment fixed set individuals much easier implement, indeed, randomized experiments done medical studies days.)Dependent observations can bias results ways produce fundamentally flawed analyses. , hang gym measuring height weight, linear model surely representation students university. best model describing students use gym (also willing talk , use gym times measuring, etc.).lieu trying answer whether observations true random sample, might instead focus whether believe observations representative populations.\nHumans notoriously bad implementing random procedures, wary process used human intuition balance data respect , example, demographics individuals sample.","code":""},{"path":"inference-reg.html","id":"what-about-normality","chapter":"7 Inference for regression","heading":"What about Normality?","text":"normality condition requires points vary symmetrically around line, spreading bell-shaped fashion. consider “bell” normal distribution sitting top line (coming paper 3-D sense) indicate points dense close line disperse gradually get farther line.normality condition less important linearity independence reasons.\nFirst, linear model fit least squares still unbiased estimate true population model.\nHowever, standard errors associated variability line well estimated.\nFortunately Central Limit Theorem tells us inferential analyses (e.g., standard errors, p-values, confidence intervals) done using mathematical model still hold (even data normally distributed around line) long sample size large enough.One analysis method require normality, regardless sample size, creating intervals predict response individual outcomes given \\(x\\) value, using linear model, topic covered later courses.\nadditional reason worry slightly less normality neither randomization test bootstrapping procedures require data normal around line.","code":""},{"path":"inference-reg.html","id":"what-about-equal-variability","chapter":"7 Inference for regression","heading":"What about Equal variability?","text":"normality, equal variability condition (points spread similar ways around line values \\(x\\)) cause problems estimate linear model, randomization test, bootstrap confidence interval.\nHowever, data exhibit non-equal variance across range \\(x\\)-values potential seriously mis-estimate variability slope consequences inference results (.e., hypothesis tests confidence intervals).equal variability condition violated theory-based analysis (e.g., p-value T-score) needed, existing methods can easily handle unequal variance (e.g., weighted least squares analysis), covered later course.","code":""},{"path":"inference-reg.html","id":"r-inference-for-regression","chapter":"7 Inference for regression","heading":"7.3 R: Inference for regression","text":"","code":""},{"path":"inference-reg.html","id":"inference-using-r-and-catstats-2","chapter":"7 Inference for regression","heading":"7.3.1 Inference using R and catstats","text":"","code":""},{"path":"inference-reg.html","id":"simulation-based-inference-for-the-regression-slope","chapter":"7 Inference for regression","heading":"Simulation-based inference for the regression slope","text":"demonstration, apply simulation-based inference functions regression catstats package data change House seats President’s party midterm elections function national unemployment rate. need drop Great Depression years perform simulations:Now correct data, can perform randomization test slope simple linear regression.results give scatterplot observed data regression line superimposed, gives observed slope (match put as_extreme_as). Next scatterplot, null distribution slope coefficient, observed slope indicated vertical line values extreme highlighted red. caption gives number simulations resulting slope extreme observed: simulation 118/1000, approximate p-value 0.118.obtain confidence interval slope, use regression_bootstrap_CI(), core arguments regression_test().bootstrap distribution slope based observed data, upper lower bounds confidence interval highlighted red. confidence interval also given caption figure. , 95% confident true change number seats House Representatives additional percentage point unemployment decrease 2.6 percent seats increase 0.3 percent seats.Notice bootstrap distribution symmetric example! , bootstrap confidence interval different obtain theory-based methods: (-2.6, 0.8). LINE technical conditions satisfactorily met, though hard see scatterplot data.","code":"\n#load data\ndata(midterms_house)\n#Drop Great Depression years\nd <- midterms_house %>% \n  filter(!(year %in% c(1935, 1939)))\nlibrary(catstats)\nset.seed(621311)\nregression_test(\n  formula = house_change ~ unemp,  #Always use response ~ explanatory\n  data = d,  #name of data set\n  statistic = \"slope\", #Can also test correlation\n  direction = \"less\", #Direction of alternative hypothesis\n  as_extreme_as = -0.89, #Observed slope\n  number_repetitions = 1000  #Number of simulations\n)\nset.seed(31143518)\nregression_bootstrap_CI(\n  formula = house_change ~ unemp,  #Always use response ~ explanatory\n  data = d,  #name of data set\n  statistic = \"slope\", #Can also test correlation\n  confidence_level = 0.95, #confidence level as a proportion\n  number_repetitions = 1000  #Number of simulations\n)"},{"path":"inference-reg.html","id":"theory-based-inference-for-the-regression-slope","chapter":"7 Inference for regression","heading":"Theory-based inference for the regression slope","text":"demonstrate theory-based inference R, revisit gift aid income data. want know whether evidence suggest slope gift aid function family income non-zero. function linear regression R lm(). Unlike prob.test() t.test(), just running lm() doesn’t print information need. produce coefficient estimates.Instead, linear regression, want save regression results can get complete output using summary():produces lot output; focus Coefficients section. gives us estimated value slope, standard error estimate, t-statistic, p-value. want row labeled name explanatory variable, family_income. estimate slope -0.043 thousand dollars per additional thousand dollars family income, strong evidence null hypothesis slope 0.can compute confidence intervals hand using reported estimate, standard error, df. need compute t-value Chapter 6:can also use confint() function R compute confidence intervals regression coefficients.either case, 90% confident gift aid $24.90 $61.20 less per $1000 increase family income.","code":"#> \n#> Call:\n#> lm(formula = gift_aid ~ family_income, data = elmhurst)\n#> \n#> Coefficients:\n#>   (Intercept)  family_income  \n#>       24.3193        -0.0431\ngift_reg <- lm(gift_aid~family_income, #Always use reponse ~ explanatory\n               data = elmhurst)  #Name of data set\nsummary(gift_reg)  #Obtain full results for regression\n#> \n#> Call:\n#> lm(formula = gift_aid ~ family_income, data = elmhurst)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -10.113  -3.623  -0.216   3.159  11.571 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    24.3193     1.2915   18.83  < 2e-16 ***\n#> family_income  -0.0431     0.0108   -3.98  0.00023 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 4.78 on 48 degrees of freedom\n#> Multiple R-squared:  0.249,  Adjusted R-squared:  0.233 \n#> F-statistic: 15.9 on 1 and 48 DF,  p-value: 0.000229\n#Get t-star for 90% confidence interval\nqt(.95, df = 48)\n#> [1] 1.68\n#Lower confidence bound\n-0.04307 - 1.677224*0.01081\n#> [1] -0.0612\n\n#Upper confidence bound\n-0.04307 + 1.677224*0.01081\n#> [1] -0.0249\nconfint(gift_reg,   #name of regression results\n        level = 0.9)  #confidence level as a proportion\n#>                   5 %    95 %\n#> (Intercept)   22.1533 26.4854\n#> family_income -0.0612 -0.0249"},{"path":"inference-reg.html","id":"catstats-function-summary-2","chapter":"7 Inference for regression","heading":"7.3.2 catstats function summary","text":"previous section, introduced two new R\nfunctions catstats library. provide summary \nfunctions. can also access\nhelp files functions using ? command. \nexample, type ?regression_test R console bring \nhelp file regression_test function.\nregression_test: Simulation-based hypothesis test regression slope correlation two quantitative variables.\nformula = y ~ x y name quantitative response variable data set x name quantitative explanatory variable\ndata = data frame, columns variable\nstatistic = one \"slope\" \"correlation\" (quotations important !)\ndirection = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)\nas_extreme_as = value observed slope correlation\nnumber_repetitions = number simulated samples generate (least 1000!)\n\nregression_test: Simulation-based hypothesis test regression slope correlation two quantitative variables.formula = y ~ x y name quantitative response variable data set x name quantitative explanatory variabledata = data frame, columns variablestatistic = one \"slope\" \"correlation\" (quotations important !)direction = one \"greater\", \"less\", \"two-sided\" (quotations important !) match sign \\(H_A\\)as_extreme_as = value observed slope correlationnumber_repetitions = number simulated samples generate (least 1000!)\nregression_bootstrap_CI: Bootstrap confidence interval regression slope correlation.\nformula = y ~ x y name quantitative response variable data set x name quantitative explanatory variable\ndata = data frame, columns variable\nstatistic = one \"slope\" \"correlation\" (quotations important !)\nconfidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)\nnumber_repetitions = number simulated samples generate (least 1000!)\n\nregression_bootstrap_CI: Bootstrap confidence interval regression slope correlation.formula = y ~ x y name quantitative response variable data set x name quantitative explanatory variabledata = data frame, columns variablestatistic = one \"slope\" \"correlation\" (quotations important !)confidence_level = confidence level decimal (e.g., 0.90, 0.95, etc)number_repetitions = number simulated samples generate (least 1000!)\n","code":""},{"path":"inference-reg.html","id":"interactive-r-tutorials-5","chapter":"7 Inference for regression","heading":"7.3.3 Interactive R tutorials","text":"Navigate concepts ’ve learned chapter R using following self-paced tutorials.\nneed browser get started!Tutorial 8: Inference regressionTutorial 8 - Lesson 1: Inference regressionTutorial 8 - Lesson 2: Randomization test slopeTutorial 8 - Lesson 3: t-test slopeTutorial 8 - Lesson 4: Checking technical conditions slope inferenceTutorial 8 - Lesson 5: Inference beyond simple linear regression modelYou can also access full list tutorials supporting book .","code":""},{"path":"inference-reg.html","id":"r-labs-6","chapter":"7 Inference for regression","heading":"7.3.4 R labs","text":"apply concepts ’ve learned chapter R computational labs walk data analysis case study.Multiple linear regression - Grading professorFull list labs supporting OpenIntro::Introduction Modern Statistics","code":""},{"path":"inference-reg.html","id":"chp7-review","chapter":"7 Inference for regression","heading":"7.4 Chapter 7 review","text":"","code":""},{"path":"inference-reg.html","id":"terms-6","chapter":"7 Inference for regression","heading":"7.4.1 Terms","text":"introduced following terms chapter.\n’re sure terms mean, recommend go back text review definitions.\npurposefully presenting alphabetical order, instead order appearance, little challenging locate.\nHowever able easily spot bolded text.","code":""},{"path":"case-studies.html","id":"case-studies","chapter":"8 Appendix: Case studies","heading":"8 Appendix: Case studies","text":"week, work case study post responses discussion questions case studies D2L Discussion forums. recommend read case study post responses earlier week classroom community can engage meaningful discussion case study.","code":""},{"path":"activities.html","id":"activities","chapter":"9 Appendix: Activities","heading":"9 Appendix: Activities","text":"week, work -class activity team mates guidance instructor. recommend purchase printed copy activity coursepack MSU Bookstore bring class day. purchase printed coursepack, may print activities Appendix bring class .","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Agresti, Alan. 2007. Introduction Categorical Data Analysis. 2nd ed. Wiley.Chimowitz, Marc , Michael J Lynn, Colin P Derdeyn, Tanya N Turan, David Fiorella, Bethany F Lane, L Scott Janis, et al. 2011. “Stenting Versus Aggressive Medical Therapy Intracranial Arterial Stenosis.” New England Journal Medicine 365 (11): 993–1003. http://www.nejm.org/doi/full/10.1056/NEJMoa1105335.Slawson, D C, F Shaughnessy. 2002. “Teaching Information Mastery: Case Baby Jeff Importance Bayes’ Theorem.” Family Medicine 34 (2): 140–42.Tintle, Nathan L, Beth L Chance, George W Cobb, Allan J Rossman, Soma Roy, Todd M Swanson, Jill L VanderStoep. 2016. Introduction Statistical Investigations. 1st ed. Wiley.Wickham, Hadley, Garrett Grolemund. 2017. R Data Science. O’Reilly Media.Wickham, Hadley, others. 2014. “Tidy Data.” Journal Statistical Software 59 (10): 1–23.","code":""}]

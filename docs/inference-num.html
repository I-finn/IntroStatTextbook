<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Inference for numerical data | Montana State Introductory Statistics with R</title>
  <meta name="description" content="Open resources textbook for Stat 216 at Montana State University" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Inference for numerical data | Montana State Introductory Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Open resources textbook for Stat 216 at Montana State University" />
  <meta name="github-repo" content="MTstateIntroStats/IntroStatTextbook" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Inference for numerical data | Montana State Introductory Statistics with R" />
  
  <meta name="twitter:description" content="Open resources textbook for Stat 216 at Montana State University" />
  

<meta name="author" content="Nicole Carnegie, Stacey Hancock, Elijah Meyer, Jade Schmidt, Melinda Yager" />


<meta name="date" content="2020-08-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="inference-cat.html"/>
<link rel="next" href="inference-reg.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MSU Intro Stat with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#textbook-overview"><i class="fa fa-check"></i>Textbook overview</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a>
<ul>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html#montana-state-university-authors"><i class="fa fa-check"></i>Montana State University Authors</a></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html#openintro-authors"><i class="fa fa-check"></i>OpenIntro Authors</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="copyright.html"><a href="copyright.html"><i class="fa fa-check"></i>Copyright</a></li>
<li class="chapter" data-level="1" data-path="intro-to-data.html"><a href="intro-to-data.html"><i class="fa fa-check"></i><b>1</b> Introduction to data</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro-to-data.html"><a href="intro-to-data.html#basic-stents-strokes"><i class="fa fa-check"></i><b>1.1</b> Case study: using stents to prevent strokes</a></li>
<li class="chapter" data-level="1.2" data-path="intro-to-data.html"><a href="intro-to-data.html#data-basics"><i class="fa fa-check"></i><b>1.2</b> Data basics</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="intro-to-data.html"><a href="intro-to-data.html#observations-variables-and-data-frames"><i class="fa fa-check"></i><b>1.2.1</b> Observations, variables, and data frames</a></li>
<li class="chapter" data-level="1.2.2" data-path="intro-to-data.html"><a href="intro-to-data.html#variable-types"><i class="fa fa-check"></i><b>1.2.2</b> Types of variables</a></li>
<li class="chapter" data-level="1.2.3" data-path="intro-to-data.html"><a href="intro-to-data.html#variable-relations"><i class="fa fa-check"></i><b>1.2.3</b> Relationships between variables</a></li>
<li class="chapter" data-level="1.2.4" data-path="intro-to-data.html"><a href="intro-to-data.html#explanatory-and-response-variables"><i class="fa fa-check"></i><b>1.2.4</b> Explanatory and response variables</a></li>
<li class="chapter" data-level="1.2.5" data-path="intro-to-data.html"><a href="intro-to-data.html#introducing-observational-studies-and-experiments"><i class="fa fa-check"></i><b>1.2.5</b> Introducing observational studies and experiments</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="intro-to-data.html"><a href="intro-to-data.html#sampling-principles-strategies"><i class="fa fa-check"></i><b>1.3</b> Sampling principles and strategies</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="intro-to-data.html"><a href="intro-to-data.html#populations-and-samples"><i class="fa fa-check"></i><b>1.3.1</b> Populations and samples</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro-to-data.html"><a href="intro-to-data.html#anecdotal-evidence"><i class="fa fa-check"></i><b>1.3.2</b> Anecdotal evidence</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro-to-data.html"><a href="intro-to-data.html#sampling-from-a-population"><i class="fa fa-check"></i><b>1.3.3</b> Sampling from a population</a></li>
<li class="chapter" data-level="1.3.4" data-path="intro-to-data.html"><a href="intro-to-data.html#four-sampling-methods-special-topic"><i class="fa fa-check"></i><b>1.3.4</b> Four sampling methods (special topic)</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro-to-data.html"><a href="intro-to-data.html#observational-studies"><i class="fa fa-check"></i><b>1.4</b> Observational studies</a></li>
<li class="chapter" data-level="1.5" data-path="intro-to-data.html"><a href="intro-to-data.html#experiments"><i class="fa fa-check"></i><b>1.5</b> Experiments</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="intro-to-data.html"><a href="intro-to-data.html#principles-of-experimental-design"><i class="fa fa-check"></i><b>1.5.1</b> Principles of experimental design</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro-to-data.html"><a href="intro-to-data.html#reducing-bias-human-experiments"><i class="fa fa-check"></i><b>1.5.2</b> Reducing bias in human experiments</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="intro-to-data.html"><a href="intro-to-data.html#scope-of-inference"><i class="fa fa-check"></i><b>1.6</b> Scope of inference</a></li>
<li class="chapter" data-level="1.7" data-path="intro-to-data.html"><a href="intro-to-data.html#data-in-r"><i class="fa fa-check"></i><b>1.7</b> Data in <code>R</code></a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="intro-to-data.html"><a href="intro-to-data.html#dataframes-in-r"><i class="fa fa-check"></i><b>1.7.1</b> Dataframes in <code>R</code></a></li>
<li class="chapter" data-level="1.7.2" data-path="intro-to-data.html"><a href="intro-to-data.html#datastruc"><i class="fa fa-check"></i><b>1.7.2</b> Tidy structure of data</a></li>
<li class="chapter" data-level="1.7.3" data-path="intro-to-data.html"><a href="intro-to-data.html#using-the-pipe-to-chain"><i class="fa fa-check"></i><b>1.7.3</b> Using the pipe to chain</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="intro-to-data.html"><a href="intro-to-data.html#chapter-review"><i class="fa fa-check"></i><b>1.8</b> Chapter review</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>1.8.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="eda.html"><a href="eda.html"><i class="fa fa-check"></i><b>2</b> Exploratory data analysis</a>
<ul>
<li class="chapter" data-level="2.1" data-path="eda.html"><a href="eda.html#quantitative-data"><i class="fa fa-check"></i><b>2.1</b> Exploring quantitative data</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="eda.html"><a href="eda.html#scatterplots"><i class="fa fa-check"></i><b>2.1.1</b> Scatterplots for paired data</a></li>
<li class="chapter" data-level="2.1.2" data-path="eda.html"><a href="eda.html#dotplots"><i class="fa fa-check"></i><b>2.1.2</b> Dot plots and the mean</a></li>
<li class="chapter" data-level="2.1.3" data-path="eda.html"><a href="eda.html#histograms"><i class="fa fa-check"></i><b>2.1.3</b> Histograms and shape</a></li>
<li class="chapter" data-level="2.1.4" data-path="eda.html"><a href="eda.html#variance-sd"><i class="fa fa-check"></i><b>2.1.4</b> Variance and standard deviation</a></li>
<li class="chapter" data-level="2.1.5" data-path="eda.html"><a href="eda.html#box-plots-quartiles-and-the-median"><i class="fa fa-check"></i><b>2.1.5</b> Box plots, quartiles, and the median</a></li>
<li class="chapter" data-level="2.1.6" data-path="eda.html"><a href="eda.html#describing-and-comparing-quantitative-distributions"><i class="fa fa-check"></i><b>2.1.6</b> Describing and comparing quantitative distributions</a></li>
<li class="chapter" data-level="2.1.7" data-path="eda.html"><a href="eda.html#robust-statistics"><i class="fa fa-check"></i><b>2.1.7</b> Robust statistics</a></li>
<li class="chapter" data-level="2.1.8" data-path="eda.html"><a href="eda.html#transforming-data-special-topic"><i class="fa fa-check"></i><b>2.1.8</b> Transforming data (special topic)</a></li>
<li class="chapter" data-level="2.1.9" data-path="eda.html"><a href="eda.html#mapping-data"><i class="fa fa-check"></i><b>2.1.9</b> Mapping data</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="eda.html"><a href="eda.html#categorical-data"><i class="fa fa-check"></i><b>2.2</b> Exploring categorical data</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="eda.html"><a href="eda.html#contingency-tables-and-conditional-proportions"><i class="fa fa-check"></i><b>2.2.1</b> Contingency tables and conditional proportions</a></li>
<li class="chapter" data-level="2.2.2" data-path="eda.html"><a href="eda.html#bar-plots-and-mosaic-plots"><i class="fa fa-check"></i><b>2.2.2</b> Bar plots and mosaic plots</a></li>
<li class="chapter" data-level="2.2.3" data-path="eda.html"><a href="eda.html#why-not-pie-charts"><i class="fa fa-check"></i><b>2.2.3</b> Why not pie charts?</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="eda.html"><a href="eda.html#exploratory-data-analysis-in-r"><i class="fa fa-check"></i><b>2.3</b> Exploratory data analysis in <code>R</code></a></li>
<li class="chapter" data-level="2.4" data-path="eda.html"><a href="eda.html#chp2-review"><i class="fa fa-check"></i><b>2.4</b> Chapter 2 review</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>2.4.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="cor-reg.html"><a href="cor-reg.html"><i class="fa fa-check"></i><b>3</b> Correlation and regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="cor-reg.html"><a href="cor-reg.html#chp3-review"><i class="fa fa-check"></i><b>3.1</b> Chapter 3 review</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>3.1.1</b> Terms</a></li>
<li class="chapter" data-level="3.1.2" data-path="cor-reg.html"><a href="cor-reg.html#chapter-exercises"><i class="fa fa-check"></i><b>3.1.2</b> Chapter exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mult-reg.html"><a href="mult-reg.html"><i class="fa fa-check"></i><b>4</b> Multiple regression</a>
<ul>
<li class="chapter" data-level="4.1" data-path="mult-reg.html"><a href="mult-reg.html#num-vs.-whatever---mlr"><i class="fa fa-check"></i><b>4.1</b> Num vs. whatever - MLR</a></li>
<li class="chapter" data-level="4.2" data-path="mult-reg.html"><a href="mult-reg.html#parallel-slopes"><i class="fa fa-check"></i><b>4.2</b> Parallel slopes</a></li>
<li class="chapter" data-level="4.3" data-path="mult-reg.html"><a href="mult-reg.html#hint-at-interaction-planes-and-parallel-planes-but-not-quantify"><i class="fa fa-check"></i><b>4.3</b> Hint at interaction, planes, and parallel planes but not quantify</a></li>
<li class="chapter" data-level="4.4" data-path="mult-reg.html"><a href="mult-reg.html#chp4-review"><i class="fa fa-check"></i><b>4.4</b> Chapter 4 review</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>4.4.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inference-foundations.html"><a href="inference-foundations.html"><i class="fa fa-check"></i><b>5</b> Foundations of inference</a>
<ul>
<li class="chapter" data-level="5.1" data-path="inference-foundations.html"><a href="inference-foundations.html#inf-rand"><i class="fa fa-check"></i><b>5.1</b> Randomization test</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="inference-foundations.html"><a href="inference-foundations.html#caseStudyGenderDiscrimination"><i class="fa fa-check"></i><b>5.1.1</b> Gender discrimination case study</a></li>
<li class="chapter" data-level="5.1.2" data-path="inference-foundations.html"><a href="inference-foundations.html#caseStudyOpportunityCost"><i class="fa fa-check"></i><b>5.1.2</b> Opportunity cost case study</a></li>
<li class="chapter" data-level="5.1.3" data-path="inference-foundations.html"><a href="inference-foundations.html#HypothesisTesting"><i class="fa fa-check"></i><b>5.1.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="5.1.4" data-path="inference-foundations.html"><a href="inference-foundations.html#randomization-test-summary"><i class="fa fa-check"></i><b>5.1.4</b> Randomization test summary</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="inference-foundations.html"><a href="inference-foundations.html#boot-ci"><i class="fa fa-check"></i><b>5.2</b> Bootstrap confidence interval</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="inference-foundations.html"><a href="inference-foundations.html#sec-med-consult"><i class="fa fa-check"></i><b>5.2.1</b> Medical consultant case study</a></li>
<li class="chapter" data-level="5.2.2" data-path="inference-foundations.html"><a href="inference-foundations.html#tappers-and-listeners-case-study"><i class="fa fa-check"></i><b>5.2.2</b> Tappers and listeners case study</a></li>
<li class="chapter" data-level="5.2.3" data-path="inference-foundations.html"><a href="inference-foundations.html#ConfidenceIntervals"><i class="fa fa-check"></i><b>5.2.3</b> Confidence intervals</a></li>
<li class="chapter" data-level="5.2.4" data-path="inference-foundations.html"><a href="inference-foundations.html#bootstrap-summary"><i class="fa fa-check"></i><b>5.2.4</b> Bootstrap summary</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="inference-foundations.html"><a href="inference-foundations.html#inf-math"><i class="fa fa-check"></i><b>5.3</b> Mathematical model</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="inference-foundations.html"><a href="inference-foundations.html#CLTsection"><i class="fa fa-check"></i><b>5.3.1</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="5.3.2" data-path="inference-foundations.html"><a href="inference-foundations.html#normalDist"><i class="fa fa-check"></i><b>5.3.2</b> Normal Distribution</a></li>
<li class="chapter" data-level="5.3.3" data-path="inference-foundations.html"><a href="inference-foundations.html#ApplyingTheNormalModel"><i class="fa fa-check"></i><b>5.3.3</b> Hypothesis testing case studies</a></li>
<li class="chapter" data-level="5.3.4" data-path="inference-foundations.html"><a href="inference-foundations.html#confidence-interval-case-study"><i class="fa fa-check"></i><b>5.3.4</b> Confidence interval case study</a></li>
<li class="chapter" data-level="5.3.5" data-path="inference-foundations.html"><a href="inference-foundations.html#mathematical-model-summary"><i class="fa fa-check"></i><b>5.3.5</b> Mathematical model summary</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="inference-foundations.html"><a href="inference-foundations.html#chp5-review"><i class="fa fa-check"></i><b>5.4</b> Chapter 5 review</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>5.4.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inference-cat.html"><a href="inference-cat.html"><i class="fa fa-check"></i><b>6</b> Inference for categorical data</a>
<ul>
<li class="chapter" data-level="6.1" data-path="inference-cat.html"><a href="inference-cat.html#single-prop"><i class="fa fa-check"></i><b>6.1</b> One proportion</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="inference-cat.html"><a href="inference-cat.html#one-prop-null-boot"><i class="fa fa-check"></i><b>6.1.1</b> Bootstrap test for <span class="math inline">\(H_0: p = p_0\)</span></a></li>
<li class="chapter" data-level="6.1.2" data-path="inference-cat.html"><a href="inference-cat.html#one-prop-norm"><i class="fa fa-check"></i><b>6.1.2</b> Mathematical model</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="inference-cat.html"><a href="inference-cat.html#diff-two-prop"><i class="fa fa-check"></i><b>6.2</b> Difference of two proportions</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="inference-cat.html"><a href="inference-cat.html#two-prop-errors"><i class="fa fa-check"></i><b>6.2.1</b> Randomization test for <span class="math inline">\(H_0: p_1 - p_2 = 0\)</span></a></li>
<li class="chapter" data-level="6.2.2" data-path="inference-cat.html"><a href="inference-cat.html#two-prop-boot-ci"><i class="fa fa-check"></i><b>6.2.2</b> Bootstrap confidence interval for <span class="math inline">\(p_1 - p_2\)</span></a></li>
<li class="chapter" data-level="6.2.3" data-path="inference-cat.html"><a href="inference-cat.html#math-2prop"><i class="fa fa-check"></i><b>6.2.3</b> Mathematical model</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="inference-cat.html"><a href="inference-cat.html#independence-in-two-way-tables"><i class="fa fa-check"></i><b>6.3</b> Independence in two-way tables</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="inference-cat.html"><a href="inference-cat.html#randomization-test-of-h_0-independence"><i class="fa fa-check"></i><b>6.3.1</b> Randomization test of <span class="math inline">\(H_0:\)</span> independence</a></li>
<li class="chapter" data-level="6.3.2" data-path="inference-cat.html"><a href="inference-cat.html#mathematical-model"><i class="fa fa-check"></i><b>6.3.2</b> Mathematical model</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="inference-cat.html"><a href="inference-cat.html#chp6-review"><i class="fa fa-check"></i><b>6.4</b> Chapter 6 review</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>6.4.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="inference-num.html"><a href="inference-num.html"><i class="fa fa-check"></i><b>7</b> Inference for numerical data</a>
<ul>
<li class="chapter" data-level="7.1" data-path="inference-num.html"><a href="inference-num.html#one-mean"><i class="fa fa-check"></i><b>7.1</b> One mean</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="inference-num.html"><a href="inference-num.html#bootstrap-confidence-interval-for-mu"><i class="fa fa-check"></i><b>7.1.1</b> Bootstrap confidence interval for <span class="math inline">\(\mu\)</span></a></li>
<li class="chapter" data-level="7.1.2" data-path="inference-num.html"><a href="inference-num.html#one-mean-math"><i class="fa fa-check"></i><b>7.1.2</b> Mathematical model</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="inference-num.html"><a href="inference-num.html#paired-data"><i class="fa fa-check"></i><b>7.2</b> Paired difference</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="inference-num.html"><a href="inference-num.html#case-study"><i class="fa fa-check"></i><b>7.2.1</b> case study</a></li>
<li class="chapter" data-level="7.2.2" data-path="inference-num.html"><a href="inference-num.html#randomization-test-for-h_0-mu_d-0"><i class="fa fa-check"></i><b>7.2.2</b> Randomization test for <span class="math inline">\(H_0: \mu_d = 0\)</span></a></li>
<li class="chapter" data-level="7.2.3" data-path="inference-num.html"><a href="inference-num.html#bootstrap-confidence-interval-for-mu_d"><i class="fa fa-check"></i><b>7.2.3</b> Bootstrap confidence interval for <span class="math inline">\(\mu_d\)</span></a></li>
<li class="chapter" data-level="7.2.4" data-path="inference-cat.html"><a href="inference-cat.html#mathematical-model"><i class="fa fa-check"></i><b>7.2.4</b> Mathematical model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="inference-num.html"><a href="inference-num.html#difference-of-two-means"><i class="fa fa-check"></i><b>7.3</b> Difference of two means</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="inference-num.html"><a href="inference-num.html#case-study-1"><i class="fa fa-check"></i><b>7.3.1</b> case study</a></li>
<li class="chapter" data-level="7.3.2" data-path="inference-num.html"><a href="inference-num.html#randomization-test-for-h_0-mu_1---mu_2-0"><i class="fa fa-check"></i><b>7.3.2</b> Randomization test for <span class="math inline">\(H_0: \mu_1 - \mu_2 = 0\)</span></a></li>
<li class="chapter" data-level="7.3.3" data-path="inference-num.html"><a href="inference-num.html#bootstrap-confidence-interval-for-mu_1---mu_2"><i class="fa fa-check"></i><b>7.3.3</b> Bootstrap confidence interval for <span class="math inline">\(\mu_1 - \mu_2\)</span></a></li>
<li class="chapter" data-level="7.3.4" data-path="inference-num.html"><a href="inference-num.html#mathematical-model-1"><i class="fa fa-check"></i><b>7.3.4</b> Mathematical model</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="inference-num.html"><a href="inference-num.html#anovaAndRegrWithCategoricalVariables"><i class="fa fa-check"></i><b>7.4</b> Comparing many means with ANOVA</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="inference-num.html"><a href="inference-num.html#randomization-test-for-h_0-mu_1-mu_2-ldots-mu_k"><i class="fa fa-check"></i><b>7.4.1</b> Randomization test for <span class="math inline">\(H_0: \mu_1 = \mu_2 = \ldots = \mu_k\)</span></a></li>
<li class="chapter" data-level="7.4.2" data-path="inference-num.html"><a href="inference-num.html#mathematical-model-2"><i class="fa fa-check"></i><b>7.4.2</b> Mathematical model</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="inference-num.html"><a href="inference-num.html#chp7-review"><i class="fa fa-check"></i><b>7.5</b> Chapter 7 review</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>7.5.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="inference-reg.html"><a href="inference-reg.html"><i class="fa fa-check"></i><b>8</b> Inference for regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="inference-reg.html"><a href="inference-reg.html#chp8-review"><i class="fa fa-check"></i><b>8.1</b> Chapter 8 review</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="intro-to-data.html"><a href="intro-to-data.html#terms"><i class="fa fa-check"></i><b>8.1.1</b> Terms</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="case-studies.html"><a href="case-studies.html"><i class="fa fa-check"></i><b>9</b> Case Studies</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Montana State Introductory Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inference-num" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> Inference for numerical data</h1>

<div class="uptohere">
The content in this chapter is currently just placeholder. We will remove this banner once the chapter content has been updated and ready for review.
</div>

<div class="chapterintro">
<p>Focusing now on Statistical Inference for <strong>numerical data</strong>, again, we will revisit and expand upon the foundational aspects of hypothesis testing from Chapter <a href="inference-foundations.html#inference-foundations">5</a>.</p>
<p>The important data structure for this chapter is a numeric response variable (that is, the outcome is quantitative).
The four data structures we detail are one numeric response variable, one numeric response variable which is a difference across a pair of observations, a numeric response variable broken down by a binary explanatory variable, and a numeric response variable broken down by an explanatory variable that has two or more levels.
When appropriate, each of the data structures will be analyzed using the three methods from Chapter <a href="inference-foundations.html#inference-foundations">5</a>: randomization test, bootstrapping, and mathematical models.</p>
As we build on the inferential ideas, we will visit new foundational concepts in statistical inference. One key new idea rests in estimating how the sample mean (as opposed to the sample proportion) varies from sample to sample; the resulting value is referred to as the standard error of the mean. We will also introduce a new important mathematical model, the <span class="math inline">\(t\)</span>-distribution (as the foundation for the <span class="math inline">\(t\)</span>-test).
</div>
<p>In this chapter, we focus on the sample mean (instead of, for example, the sample median or the range of the observations) because of the well-studied mathematical model which describes the behavior of the sample mean.
We will not cover mathematical models which describe other statistics, but the bootstrap and randomization techniques described below are immediately extendable to any function of the observed data.
The sample mean will be calculated in one group, two paired groups, two independent groups, and many groups settings.
The techniques described for each setting will vary slightly, but you will be well served to find the structural similarities across the different settings.</p>
<div id="one-mean" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> One mean</h2>
<p>Similar to how we can model the behavior of the
sample proportion <span class="math inline">\(\hat{p}\)</span> using a normal distribution,
the sample mean <span class="math inline">\(\bar{x}\)</span> can also be modeled using
a normal distribution when certain conditions are met.

However, we’ll soon learn that a new distribution,
called the <span class="math inline">\(t\)</span>-distribution,
tends to be more useful when working with the sample mean.
We’ll first learn about this new distribution,
then we’ll use it to construct confidence intervals
and conduct hypothesis tests for the mean.</p>
<div id="bootstrap-confidence-interval-for-mu" class="section level3" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Bootstrap confidence interval for <span class="math inline">\(\mu\)</span></h3>
<p>As an employer who subsidizes housing for your employees, you need to know the average month rental price for a three bedroom flat in Edinburgh.
In order to walk through the example more clearly, let’s say that you are only able to randomly sample five Edinburgh flats (if this were a real example, you would surely be able to take a much larger sample size, possibly even being able to measure the entire population!).</p>
<div id="observed-data" class="section level4 unnumbered">
<h4>Observed data</h4>
<p>Figure <a href="inference-num.html#fig:5flats">7.1</a> presents the details of the random sample of observations where the monthly rent of five flats has been recorded.</p>
<div class="figure" style="text-align: center"><span id="fig:5flats"></span>
<img src="07/figures/5flats.png" alt="5 flats" width="75%" />
<p class="caption">
Figure 7.1: 5 flats
</p>
</div>
<p>The sample average monthly rent of £ 1648 is a first guess at the price of three bedroom flats. However, as a student of statistics, you understand that one sample mean based on a sample of five observations will not necessarily equal the true population average rent for all three bedroom flats in Edinburgh.
Indeed, you can see that the observed rent prices vary with a standard deviation of 340.232, and surely the average monthly rent would be different if a different sample of size five had been taken from the population.
Fortunately, as it did in previous chapters for the sample proportion, bootstrapping will approximate the variability of the sample mean from sample to sample.</p>
</div>
<div id="variability-of-the-statistic" class="section level4 unnumbered">
<h4>Variability of the statistic</h4>
<p>As with the inferential ideas covered in Chapter <a href="inference-foundations.html#inference-foundations">5</a>, the inferential analysis methods in this chapter are grounded in quantifying how one dataset differs from another when they are both taken from the same population.
Again, it still doesn’t make sense to to take repeated samples from the same population (same reasoning: if you have the means to take more samples, a larger sample size will benefit you more than then exact same sample twice).
Just like with proportions, we are going to use the observed data to</p>
<p>Most of the inferential procedures covered in this text are grounded in quantifying how one data set would differ from another when they are both taken from the same population.
It doesn’t make sense to take repeated samples from the same population because if you have the means to take more samples, a larger sample size will benefit you more than the exact same sample twice.
Instead, we measure how the samples behave under an estimate of the population. Figure <a href="inference-num.html#fig:bootquant1">7.2</a> shows how the unknown original population can be estimated by using the sample to approximate the distribution of</p>

<div class="todo">
need to fill in the example here
</div>
<div class="figure" style="text-align: center"><span id="fig:bootquant1"></span>
<img src="07/figures/bootquant1.png" alt="first figure with the ? pop, then sample, then estimate of the pop." width="75%" />
<p class="caption">
Figure 7.2: first figure with the ? pop, then sample, then estimate of the pop.
</p>
</div>
<p>By taking repeated samples from the estimated population, the variability from sample to sample can be observed. In Figure <a href="inference-foundations.html#fig:boot2">5.9</a> the repeated bootstrap samples are obviously different both from each other and from the original population.
Recall that the bootstrap samples were taken from the same (estimated) population, and so the differences are due entirely to natural variability in the sampling procedure.</p>
<div class="figure" style="text-align: center"><span id="fig:bootquant2"></span>
<img src="07/figures/bootquant2.png" alt="next fig, has the bootstrap samples" width="75%" />
<p class="caption">
Figure 7.3: next fig, has the bootstrap samples
</p>
</div>
<p>By summarizing each of the bootstrap samples (here, using the sample mean), we see, directly, the variability of the sample mean, <span class="math inline">\(\bar{x}\)</span>, from sample to sample.
The distribution of <span class="math inline">\(\hat{x}_{bs}\)</span> for the Edinburgh flats is shown in Figure <a href="inference-num.html#fig:bootquant3">7.4</a>.</p>

<div class="todo">
after the plot is made, describe the actual BS samples
</div>
<div class="figure" style="text-align: center"><span id="fig:bootquant3"></span>
<img src="07/figures/bootquant3.png" alt="WITH ADDED HISTOGRAM... boot samples, arrow, histogram of all of them" width="75%" />
<p class="caption">
Figure 7.4: WITH ADDED HISTOGRAM… boot samples, arrow, histogram of all of them
</p>
</div>

<div class="todo">
add the sampling with replacement part (????)
</div>
<p>Figure <a href="#fig:flatsbsmean"><strong>??</strong></a> summarizes one thousand bootstrap samples in a histogram of the bootstrap sample means.
The bootstrapped average rent prices vary from £ 1250 to £ 1995 (with a small observed sample of size 5, a bootstrap resample can sometimes, although rarely, include only repeated measurements of the same observation).
The bootstrap confidence interval is found by locating the middle 90% (for a 90% confidence interval) or a 95% (for a 95% confidence interval) of the bootstrapped statistics.</p>

<div class="example">
<p>Using Figure <a href="#fig:flatsbsmean"><strong>??</strong></a>, find the 90% and 95% confidence intervals for the true mean monthly rental price of a three bedroom flat in Edinburgh.</p>
<p>The SE of the bootstrapped means measures how variable the means are from resample to resample. The bootstrap SE is a good approximation to the SE of means as if we had taken repeated samples from the original population (which we agreed isn’t something we would do because of wasted resources).</p>
Logistically, we can find the standard deviation of the bootstrapped means using the same calculations from Chapter <a href="eda.html#eda">2</a>. That is, the bootstrapped means are the individual observations about which we measure the variability.
</div>

<div class="guidedpractice">
<p>It turns out that the standard deviation of the bootstrapped means from Figure <a href="#fig:flatsbsmean"><strong>??</strong></a> is £ 136.9. [Note: in R the calculation was done using the function <code>sd()</code>.] The average of the observed prices, the best guess point estimate for <span class="math inline">\(\mu\)</span>, is £ 1648.</p>
<p>Find and interpret the confidence interval for <span class="math inline">\(\mu\)</span> (the true average rental price of flats in Edinbugh) using the Bootstrap SE inverval formula.^[Using the formula for the boostrap SE interval, we find the 95% confidence interval for <span class="math inline">\(\mu\)</span> is:
<span class="math inline">\(1648 \pm 2 \cdot 136.9 \rightarrow\)</span> (£ 1374.2, £ 1921.8)</p>
We are 95% confident that the true average rent price for a three bedroom flat in Edinburgh is somewhere between £ 1374.2 and £ 1921.8.]
</div>

<div class="example">
<p>Compare and contrast the two different 95% confidence intervals for <span class="math inline">\(\mu\)</span> created by finding the percentiles of the bootstrapped means and created by finding the SE of the bootstrapped means. Do you think the intervals <em>should</em> be identical?</p>
<hr />
<ul>
<li>Percentile interval: (£ 1389.75, £ 1916)</li>
<li>SE interval: (£ 1374.2, £ 1921.8)</li>
</ul>
<p>The intervals were created using different methods, so it is not surprising that they are not identical. However, we are pleased to see that the two methods provide very similar interval approximations.</p>
The technical details surrounding which data structures are best for percentile intervals and which are best for SE intervals is beyond the scope of this text. However, the larger the samples are, the better the interval estimates will be.
</div>
</div>
<div id="bootstrap-confidence-interval-for-sigma" class="section level4 unnumbered">
<h4>Bootstrap confidence interval for <span class="math inline">\(\sigma\)</span></h4>
<p>Suppose that the research question at hand seeks to understand how variable the rental price of the flats are in Edinburgh.
That is, your interest is no longer in the average rental price of the flats but in the <em>standard deviation</em> of the rental prices of all three bedroom flats in Edinburgh, <span class="math inline">\(\sigma\)</span>.
You may have already realized that the sample standard deviation, <span class="math inline">\(s\)</span>, will work as a good <strong>point estimate</strong> for the parameter of interest: the population standard deviation, <span class="math inline">\(\sigma\)</span>.
The point estimate of the five observations is calculated to be <span class="math inline">\(s =\)</span> £ 340.23.
While <span class="math inline">\(s =\)</span> £ 340.23 might be a good guess for <span class="math inline">\(\sigma\)</span>, we prefer to have an interval
Although there is a mathematical model which describes how <span class="math inline">\(s\)</span> varies from sample to sample, the mathematical model will not be presented in this text.
But even without the mathematical model, bootstrapping can be used to find a confidence interval for the parameter <span class="math inline">\(\sigma\)</span>.</p>

<div class="example">
<p>Describe the bootstrap distribution for the standard deviation shown in Figure <a href="inference-num.html#fig:flatsbssd">7.5</a>.</p>
<hr />
The distribution is skewed left and centered near £ 340.23, which is the point estimate from the original data. Most observations in this distribution lie between £ 0 and £ 408.1.
</div>

<div class="guidedpractice">
Using Figure <a href="inference-num.html#fig:flatsbssd">7.5</a>, find <em>and interpret</em> a 90% confidence interval for the population standard deviation for three bedroom flat prices in Edinburgh.<a href="#fn93" class="footnote-ref" id="fnref93"><sup>93</sup></a>
</div>
<div class="figure" style="text-align: center"><span id="fig:flatsbssd"></span>
<img src="07-inference-num_files/figure-html/flatsbssd-1.png" alt="The original Edinburgh data is bootstrapped 1,000 times. The histogram provides a sense for the variability of the standard deviation of the rent values from sample to sample." width="70%" />
<p class="caption">
Figure 7.5: The original Edinburgh data is bootstrapped 1,000 times. The histogram provides a sense for the variability of the standard deviation of the rent values from sample to sample.
</p>
</div>
</div>
<div id="bootstrapping-is-not-a-solution-to-small-sample-sizes" class="section level4 unnumbered">
<h4>Bootstrapping is not a solution to small sample sizes!</h4>
<p>The example presented above is done for a sample with only five observations.
As with analysis techniques that build on mathematical models, bootstrapping works best when a large random sample has been taken from the population.
Bootstrapping is a method for capturing the variability of a statistic when the mathematical model is unknown (it is not a method for navigating small samples).
As you might guess, the larger the random sample, the more accurately that sample will represent the population of interest.</p>
</div>
</div>
<div id="one-mean-math" class="section level3" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Mathematical model</h3>
<p>As with the sample proportion, the variability of the sample mean is well described by the mathematical theory given by the Central Limit Theorem. However, because of missing information about the inherent variability in the population, a <span class="math inline">\(t\)</span>-distribution is used in place of the standard normal when performing hypothesis test or confidence interval analyses.</p>
<div id="a-mathematical-distribution-of-barx" class="section level4" number="7.1.2.1">
<h4><span class="header-section-number">7.1.2.1</span> A mathematical distribution of <span class="math inline">\(\bar{x}\)</span></h4>
<p>The sample mean tends to follow
a normal distribution centered at the population mean, <span class="math inline">\(\mu\)</span>,
when certain conditions are met.
Additionally, we can compute a standard error for the sample
mean using the population standard deviation <span class="math inline">\(\sigma\)</span>
and the sample size <span class="math inline">\(n\)</span>.</p>

<div class="onebox">
<strong>Central Limit Theorem for the sample mean</strong><br />
When we collect a sufficiently large sample of
<span class="math inline">\(n\)</span> independent observations from a population with
mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>,
the sampling distribution of <span class="math inline">\(\bar{x}\)</span> will be nearly
normal with
<span class="math display">\[\begin{align*}
  &amp;\text{Mean}=\mu
  &amp;&amp;\text{Standard Error }(SE) = \frac{\sigma}{\sqrt{n}}
  \end{align*}\]</span>
</div>
<p>Before diving into confidence intervals and hypothesis
tests using <span class="math inline">\(\bar{x}\)</span>, we first need to cover two topics:</p>
<ul>
<li>When we modeled <span class="math inline">\(\hat{p}\)</span> using the normal distribution,
certain conditions had to be satisfied.
The conditions for working with <span class="math inline">\(\bar{x}\)</span>
are a little more complex, and below, we will discuss
how to check conditions for inference using a mathematical model.</li>
<li>The standard error is dependent on the population
standard deviation, <span class="math inline">\(\sigma\)</span>.
However, we rarely know <span class="math inline">\(\sigma\)</span>, and instead
we must estimate it.
Because this estimation is itself imperfect,
we use a new distribution called the
<strong><span class="math inline">\(t\)</span>-distribution</strong>
to fix this problem, which we discuss in</li>
</ul>
</div>
<div id="evaluating-the-two-conditions-required-for-modeling-barx" class="section level4 unnumbered">
<h4>Evaluating the two conditions required for modeling <span class="math inline">\(\bar{x}\)</span></h4>
<p>Two conditions are required to apply the
Central Limit Theorem
for a sample mean <span class="math inline">\(\bar{x}\)</span>:<br />
* <strong>Independence.</strong> The sample observations must be independent,
The most common way to satisfy this condition is
when the sample is a simple random sample from the
population.
If the data come from a random process,
analogous to rolling a die,
this would also satisfy the independence condition.<br />
* <strong>Normality.</strong> When a sample is small,
we also require that the sample observations
come from a normally distributed population.
We can relax this condition more and more
for larger and larger sample sizes.
This condition is obviously vague,
making it difficult to evaluate,
so next we introduce a couple rules of thumb
to make checking this condition easier.</p>

<div class="onebox">
<p><strong>General rule: how to perform the normality check</strong></p>
<p>There is no perfect way to check the normality condition,
so instead we use two general rules:</p>
<ul>
<li><span class="math inline">\(\mathbf{n &lt; 30}\)</span>: If the sample size <span class="math inline">\(n\)</span> is less than 30
and there are no clear outliers in the data,
then we typically assume the data come from
a nearly normal distribution to satisfy the
condition.<br />
</li>
<li><span class="math inline">\(\mathbf{n \geq 30}\)</span>: If the sample size <span class="math inline">\(n\)</span> is at least 30
and there are no <em>particularly extreme</em> outliers,
then we typically assume the sampling distribution
of <span class="math inline">\(\bar{x}\)</span> is nearly normal, even if the underlying
distribution of individual observations is not.
</div></li>
</ul>
<p>In this first course in statistics, you aren’t expected
to develop perfect judgement on the normality condition.
However, you are expected to be able to handle
clear cut cases based on the rules of thumb.<a href="#fn94" class="footnote-ref" id="fnref94"><sup>94</sup></a></p>

<div class="example">
<p>Consider the following two plots
that come from simple random samples from
different populations.
Their sample sizes are <span class="math inline">\(n_1 = 15\)</span> and <span class="math inline">\(n_2 = 50\)</span>.</p>
<p>Are the independence and normality conditions met
in each case?</p>
<hr />
<p>Each samples is from a simple random sample of its
respective population, so the independence condition
is satisfied.
Let’s next check the normality condition for
each using the rule of thumb.</p>
<p>The first sample has fewer than 30 observations,
so we are watching for any clear outliers.
None are present; while there is a small gap in the
histogram on the right, this gap is small and
20% of the observations in this small sample
are represented in that far right bar of the histogram,
so we can hardly call these clear outliers.
With no clear outliers, the normality condition
is reasonably met.</p>
The second sample has a sample size greater than 30 and
includes an outlier that appears to be roughly 5 times
further from the center of the distribution than the
next furthest observation.
This is an example of a particularly extreme outlier,
so the normality condition would not be satisfied.
</div>
<p><img src="07-inference-num_files/figure-html/outliersandsscondition-1.png" width="70%" style="display: block; margin: auto;" /><img src="07-inference-num_files/figure-html/outliersandsscondition-2.png" width="70%" style="display: block; margin: auto;" /></p>
<p>In practice, it’s typical to also do a mental check to evaluate
whether we have reason to believe the underlying population
would have moderate skew (if <span class="math inline">\(n &lt; 30\)</span>)
or have particularly extreme outliers (<span class="math inline">\(n \geq 30\)</span>)
beyond what we observe in the data.
For example, consider the number of followers
for each individual account on Twitter,
and then imagine this distribution.
The large majority of accounts have built up
a couple thousand followers or fewer,
while a relatively tiny fraction have amassed
tens of millions of followers,
meaning the distribution is extremely skewed.
When we know the data come from such an extremely
skewed distribution,
it takes some effort to understand what sample
size is large enough for the normality condition
to be satisfied.</p>
<p></p>
</div>
<div id="introducing-the-t-distribution" class="section level4 unnumbered">
<h4>Introducing the <span class="math inline">\(t\)</span>-distribution</h4>
<p>
</p>
<p>In practice, we cannot directly calculate the standard error
for <span class="math inline">\(\bar{x}\)</span> since we do not know the population standard
deviation, <span class="math inline">\(\sigma\)</span>.
We encountered a similar issue when computing the standard
error for a sample proportion, which relied on the population
proportion, <span class="math inline">\(p\)</span>.
Our solution in the proportion context was to use sample
value in place
of the population value when computing the standard error.
We’ll employ a similar strategy for computing the standard
error of <span class="math inline">\(\bar{x}\)</span>, using the sample
standard deviation <span class="math inline">\(s\)</span> in place of <span class="math inline">\(\sigma\)</span>:
<span class="math display">\[\begin{align*}
SE = \frac{\sigma}{\sqrt{n}} \approx \frac{s}{\sqrt{n}}
\end{align*}\]</span>
This strategy tends to work well when we have
a lot of data and can estimate <span class="math inline">\(\sigma\)</span> using <span class="math inline">\(s\)</span> accurately.
However, the estimate is less precise with smaller samples,
and this leads to problems when using the normal
distribution to model <span class="math inline">\(\bar{x}\)</span>.</p>
<p>We’ll find it useful to use a new distribution for
inference calculations called the <strong><span class="math inline">\(t\)</span>-distribution</strong>.
A <span class="math inline">\(t\)</span>-distribution, shown as a solid line in
Figure <a href="inference-num.html#fig:tDistCompareToNormalDist">7.6</a>, has a bell shape.
However, its tails are thicker than the normal distribution’s,
meaning observations are more likely to fall beyond two
standard deviations from the mean than under the normal
distribution.</p>
<p>The extra thick tails of the <span class="math inline">\(t\)</span>-distribution are exactly
the correction needed to resolve the problem of using <span class="math inline">\(s\)</span>
in place of <span class="math inline">\(\sigma\)</span> in the <span class="math inline">\(SE\)</span> calculation.</p>
<div class="figure" style="text-align: center"><span id="fig:tDistCompareToNormalDist"></span>
<img src="07-inference-num_files/figure-html/tDistCompareToNormalDist-1.png" alt="Comparison of a $t$-distribution and a normal distribution." width="70%" />
<p class="caption">
Figure 7.6: Comparison of a <span class="math inline">\(t\)</span>-distribution and a normal distribution.
</p>
</div>
<p>The <span class="math inline">\(t\)</span>-distribution is always centered at zero and
has a single parameter: degrees of freedom.
The <strong>degrees of freedom</strong> 
{degrees of freedom (<span class="math inline">\(df\)</span>)!<span class="math inline">\(t\)</span>-distribution}
describes the precise form of the bell-shaped <span class="math inline">\(t\)</span>-distribution.
Several <span class="math inline">\(t\)</span>-distributions are shown in
Figure <a href="inference-num.html#fig:tDistConvergeToNormalDist">7.7</a>
in comparison to the normal distribution.</p>
<p>In general, we’ll use a <span class="math inline">\(t\)</span>-distribution
with <span class="math inline">\(df = n - 1\)</span> to model the sample mean
when the sample size is <span class="math inline">\(n\)</span>.
That is, when we have more observations,
the degrees of freedom will be larger and
the <span class="math inline">\(t\)</span>-distribution will look more like the
standard normal distribution;
when the degrees of freedom is about 30 or more,
the <span class="math inline">\(t\)</span>-distribution is nearly indistinguishable
from the normal distribution.</p>
<div class="figure" style="text-align: center"><span id="fig:tDistConvergeToNormalDist"></span>
<img src="07-inference-num_files/figure-html/tDistConvergeToNormalDist-1.png" alt="The larger the degrees of freedom, the more closely the $t$-distribution resembles the standard normal distribution." width="70%" />
<p class="caption">
Figure 7.7: The larger the degrees of freedom, the more closely the <span class="math inline">\(t\)</span>-distribution resembles the standard normal distribution.
</p>
</div>

<div class="onebox">
<p><strong>Degrees of freedom: df</strong></p>
<p>The degrees of freedom describes the shape of the
<span class="math inline">\(t\)</span>-distribution.
The larger the degrees of freedom, the more closely
the distribution approximates the normal model.</p>
When modeling <span class="math inline">\(\bar{x}\)</span> using the <span class="math inline">\(t\)</span>-distribution,
use <span class="math inline">\(df = n - 1\)</span>.
</div>
<p>The <span class="math inline">\(t\)</span>-distribution allows us greater flexibility than
the normal distribution when analyzing numerical data.
In practice, it’s common to use statistical software,
such as R, Python, or SAS for these analyses.
In R, the function used for calculating probabilities under a <span class="math inline">\(t\)</span>-distribution is <code>pt()</code> (which should seem similar to previous R functions, <code>pnorm()</code> and <code>pchisq()</code>).
Don’t forget that with the <span class="math inline">\(t\)</span>-distribution, the degrees of freedom must always be specified!</p>
<!--
Alternatively, a graphing calculator or a
\termsub{$\pmb{t}$-table}{t-table@$t$-table} may be used;
the $t$-table is similar to the normal distribution table,
and it may be found in Appendix \ref{tDistributionTable},
which includes usage instructions and examples
for those who wish to use this option.
-->
<p>No matter the approach you choose, apply your method
using the examples below to confirm your working
understanding of the <span class="math inline">\(t\)</span>-distribution.</p>

<div class="example">
<p>What proportion of the <span class="math inline">\(t\)</span>-distribution
with 18 degrees of freedom falls below -2.10?</p>
<hr />
<p>Just like a normal probability problem, we first draw
the picture in Figure <a href="inference-num.html#fig:tDistDF18LeftTail2Point10">7.8</a>
and shade the area below -2.10.</p>
Using statistical software, we can obtain a precise
value: 0.0250.
</div>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="inference-num.html#cb17-1" aria-hidden="true"></a><span class="co"># using pt() to find probability under the $t$-distribution</span></span>
<span id="cb17-2"><a href="inference-num.html#cb17-2" aria-hidden="true"></a><span class="kw">pt</span>(<span class="op">-</span><span class="fl">2.10</span>, <span class="dt">df =</span> <span class="dv">18</span>)</span>
<span id="cb17-3"><a href="inference-num.html#cb17-3" aria-hidden="true"></a><span class="co">#&gt; [1] 0.025</span></span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:tDistDF18LeftTail2Point10"></span>
<img src="07-inference-num_files/figure-html/tDistDF18LeftTail2Point10-1.png" alt="The $t$-distribution with 18 degrees of freedom. The area below -2.10 has been shaded." width="70%" />
<p class="caption">
Figure 7.8: The <span class="math inline">\(t\)</span>-distribution with 18 degrees of freedom. The area below -2.10 has been shaded.
</p>
</div>

<div class="example">
<p>A <span class="math inline">\(t\)</span>-distribution with 20 degrees of freedom
is shown in the top panel of
Figure <a href="inference-num.html#fig:tDistDF20RightTail1Point65">7.9</a>.
Estimate the proportion of the distribution falling
above 1.65.</p>
<hr />
With a normal distribution, this would correspond to
about 0.05, so we should expect the <span class="math inline">\(t\)</span>-distribution
to give us a value in this neighborhood.
Using statistical software: 0.0573.
</div>
<div class="figure" style="text-align: center"><span id="fig:tDistDF20RightTail1Point65"></span>
<img src="07-inference-num_files/figure-html/tDistDF20RightTail1Point65-1.png" alt="Top: The $t$-distribution with 20 degrees of freedom, with the area above 1.65 shaded. Bottom: The $t$-distribution with 2 degrees of freedom, with the area further than 3 units from 0 shaded." width="70%" /><img src="07-inference-num_files/figure-html/tDistDF20RightTail1Point65-2.png" alt="Top: The $t$-distribution with 20 degrees of freedom, with the area above 1.65 shaded. Bottom: The $t$-distribution with 2 degrees of freedom, with the area further than 3 units from 0 shaded." width="70%" />
<p class="caption">
Figure 7.9: Top: The <span class="math inline">\(t\)</span>-distribution with 20 degrees of freedom, with the area above 1.65 shaded. Bottom: The <span class="math inline">\(t\)</span>-distribution with 2 degrees of freedom, with the area further than 3 units from 0 shaded.
</p>
</div>

<div class="example">
<p>A <span class="math inline">\(t\)</span>-distribution with 2 degrees of freedom
is shown in the bottom panel of
Figure <a href="inference-num.html#fig:tDistDF20RightTail1Point65">7.9</a>.
Estimate the proportion of the distribution falling more
than 3 units from the mean (above or below).</p>
<hr />
With so few degrees of freedom, the <span class="math inline">\(t\)</span>-distribution will
give a more notably different value than the normal
distribution.
Under a normal distribution, the area would be about
0.003 using the 68-95-99.7 rule.
For a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df = 2\)</span>, the area in both
tails beyond 3 units totals 0.0955.
This area is dramatically different than what
we obtain from the normal distribution.
</div>

<div class="guidedpractice">
What proportion of the <span class="math inline">\(t\)</span>-distribution with 19 degrees
of freedom falls above -1.79 units?
Use your preferred method for finding tail areas.<a href="#fn95" class="footnote-ref" id="fnref95"><sup>95</sup></a>
</div>
<p>
</p>
</div>
<div id="one-sample-t-confidence-intervals" class="section level4 unnumbered">
<h4>One sample <span class="math inline">\(t\)</span>-confidence intervals</h4>
<p></p>
<p>Let’s get our first taste of applying the <span class="math inline">\(t\)</span>-distribution
in the context of an example about the mercury content
of dolphin muscle.
Elevated mercury concentrations are an important problem
for both dolphins
and other animals, like humans, who occasionally eat them.</p>
<div class="figure" style="text-align: center"><span id="fig:rissosDolphin"></span>
<img src="07/figures/rissosDolphin.jpg" alt="A Risso's dolphin. Photo by Mike Baird, www.bairdphotos.com" width="75%" />
<p class="caption">
Figure 7.10: A Risso’s dolphin. Photo by Mike Baird, www.bairdphotos.com
</p>
</div>
</div>
<div id="observed-data-1" class="section level4 unnumbered">
<h4>Observed data</h4>
<p>We will identify a confidence interval for the average mercury content in dolphin muscle using a sample of 19 Risso’s dolphins from the Taiji area in Japan. The data are summarized in Table <a href="inference-num.html#tab:summaryStatsOfHgInMuscleOfRissosDolphins">7.1</a>. The minimum and maximum observed values can be used to evaluate whether or not there are clear outliers.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:summaryStatsOfHgInMuscleOfRissosDolphins">Table 7.1: </span>Summary of mercury content in the muscle of 19 Risso’s dolphins from the Taiji area. Measurements are in micrograms of mercury per wet gram
of muscle (<span class="math inline">\(\mu\)</span>g/wet g).
</caption>
<thead>
<tr>
<th style="text-align:right;">
<span class="math inline">\(n\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\bar{x}\)</span>
</th>
<th style="text-align:right;">
s
</th>
<th style="text-align:right;">
minimum
</th>
<th style="text-align:right;">
maximum
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
19
</td>
<td style="text-align:right;">
4.4
</td>
<td style="text-align:right;">
2.3
</td>
<td style="text-align:right;">
1.7
</td>
<td style="text-align:right;">
9.2
</td>
</tr>
</tbody>
</table>

<div class="example">
<p>Are the independence and
normality conditions satisfied for this data set?</p>
<hr />
The observations are a simple random sample,
therefore independence is reasonable.
The summary statistics in
Table <a href="inference-num.html#tab:summaryStatsOfHgInMuscleOfRissosDolphins">7.1</a>
do not suggest any clear outliers, with
all observations are within 2.5 standard deviations
of the mean.
Based on this evidence, the normality condition
seems reasonable.
</div>
<p>In the normal model, we used <span class="math inline">\(z^{\star}\)</span> and the standard error to determine the width of a confidence interval. We revise the confidence interval formula slightly when using the <span class="math inline">\(t\)</span>-distribution:
<span class="math display">\[\begin{align*}
&amp;\text{point estimate} \ \pm\  t^{\star}_{df} \times SE
&amp;&amp;\to
&amp;&amp;\bar{x} \ \pm\  t^{\star}_{df} \times \frac{s}{\sqrt{n}}
\end{align*}\]</span></p>

<div class="example">
<p>Using the summary statistics in
Table <a href="inference-num.html#tab:summaryStatsOfHgInMuscleOfRissosDolphins">7.1</a>,
compute the standard error for the average
mercury content in the <span class="math inline">\(n = 19\)</span> dolphins.</p>
<hr />
We plug in <span class="math inline">\(s\)</span> and <span class="math inline">\(n\)</span> into the formula:
<span class="math inline">\(SE  = s / \sqrt{n}  = 2.3 / \sqrt{19}  = 0.528\)</span>.
</div>
<p>The value <span class="math inline">\(t^{\star}_{df}\)</span> is a cutoff we obtain based on the
confidence level and the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df\)</span> degrees
of freedom.
That cutoff is found in the same way as with a normal
distribution: we find <span class="math inline">\(t^{\star}_{df}\)</span> such that
the fraction of the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df\)</span> degrees
of freedom within a distance <span class="math inline">\(t^{\star}_{df}\)</span>
of 0 matches the confidence level of interest.</p>

<div class="example">
<p>When <span class="math inline">\(n = 19\)</span>, what is the appropriate
degrees of freedom?
Find <span class="math inline">\(t^{\star}_{df}\)</span> for this degrees of freedom
and the confidence level of 95%</p>
<hr />
<p>The degrees of freedom is easy to calculate:
<span class="math inline">\(df = n - 1 = 18\)</span>.</p>
Using statistical software, we find the cutoff where
the upper tail is equal to 2.5%:
<span class="math inline">\(t^{\star}_{18} = 2.10\)</span>.
The area below -2.10 will also be equal to 2.5%.
That is, 95% of the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df = 18\)</span>
lies within 2.10 units of 0.
</div>

<div class="onebox">
<p><strong>Degrees of freedom for a single sample.</strong></p>
If the sample has <span class="math inline">\(n\)</span> observations and we are examining a single mean, then we use the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df=n-1\)</span> degrees of freedom.
</div>
<p>%In our current example, we should use the <span class="math inline">\(t\)</span>-distribution
%with <span class="math inline">\(df=19-1=18\)</span> degrees of freedom.
%We can generally identify <span class="math inline">\(t_{18}^{\star}\)</span>
%using statistical software.
%Alternatively, we could use the <span class="math inline">\(t\)</span>-table in
%Appendix .
%Generally the value of <span class="math inline">\(t^{\star}_{df}\)</span> is slightly larger
%than what we would get under the normal model with <span class="math inline">\(z^{\star}\)</span>.</p>

<div class="example">
<p>Compute and interpret the 95% confidence interval
for the average mercury content in Risso’s dolphins.</p>
<hr />
We can construct the confidence interval as
<span class="math display">\[\begin{align*}
  \bar{x} \ \pm\  t^{\star}_{18} \times SE
    \quad \to \quad 4.4 \ \pm\  2.10 \times 0.528
    \quad \to \quad (3.29, 5.51)
  \end{align*}\]</span>
We are 95% confident the average mercury content of muscles
in Risso’s dolphins is between 3.29 and 5.51 <span class="math inline">\(\mu\)</span>g/wet gram,
which is considered extremely high.
</div>
<p></p>

<div class="onebox">
<p><strong>Finding a <span class="math inline">\(t\)</span>-confidence interval for the mean, <span class="math inline">\(\mu\)</span>.</strong></p>
Based on a sample of <span class="math inline">\(n\)</span> independent and nearly normal
observations, a confidence interval for the population
mean is
<span class="math display">\[\begin{align*}
  &amp;\text{point estimate} \ \pm\  t^{\star}_{df} \times SE
  &amp;&amp;\to
  &amp;&amp;\bar{x} \ \pm\  t^{\star}_{df} \times \frac{s}{\sqrt{n}}
  \end{align*}\]</span>
where <span class="math inline">\(\bar{x}\)</span> is the sample mean, <span class="math inline">\(t^{\star}_{df}\)</span>
corresponds to the confidence level and degrees of freedom
<span class="math inline">\(df\)</span>, and <span class="math inline">\(SE\)</span> is the standard error as estimated by
the sample.
</div>

<div class="guidedpractice">
The FDA’s webpage provides some data on mercury content of fish.
Based on a sample of 15 croaker white fish (Pacific),
a sample mean and standard deviation were computed as 0.287
and 0.069 ppm (parts per million), respectively.
The 15 observations ranged from 0.18 to 0.41 ppm.
We will assume these observations are independent.
Based on the summary statistics of the data,
do you have any objections to the normality condition
of the individual observations?<a href="#fn96" class="footnote-ref" id="fnref96"><sup>96</sup></a>
</div>
<p></p>

<div class="example">
<p>Estimate the standard error of
<span class="math inline">\(\bar{x} = 0.287\)</span> ppm using the data summaries in the previous Guided Practice. If we are to use the <span class="math inline">\(t\)</span>-distribution to create a
90% confidence interval for the actual mean of the
mercury content, identify the degrees of freedom
and <span class="math inline">\(t^{\star}_{df}\)</span>.</p>
<hr />
<p>The standard error: <span class="math inline">\(SE = \frac{0.069}{\sqrt{15}} = 0.0178\)</span>.</p>
<p>Degrees of freedom: <span class="math inline">\(df = n - 1 = 14\)</span>.</p>
Since the goal is a 90% confidence interval,
we choose <span class="math inline">\(t_{14}^{\star}\)</span> so that the two-tail area
is 0.1:
<span class="math inline">\(t^{\star}_{14} = 1.76\)</span>.
</div>
<!--
\begin{onebox}{Confidence interval for a single mean}
  Once you've determined a one-mean confidence interval
  would be helpful for an application,
  there are four steps to constructing the interval:
  \begin{description}
  \item[Prepare.]
      Identify $\bar{x}$, $s$, $n$, and determine what
      confidence level you wish to use.
  \item[Check.]
      Verify the conditions to ensure $\bar{x}$
      is nearly normal.
  \item[Calculate.]
      If the conditions hold, compute $SE$,
      find $t_{df}^{\star}$, and construct the interval.
  \item[Conclude.]
      Interpret the confidence interval in the context
      of the problem.
  \end{description}
\end{onebox}
-->

<div class="guidedpractice">
Using the information and results of the previous Guided Practice and Example, compute a 90% confidence interval for the average mercury content of croaker white fish (Pacific).<a href="#fn97" class="footnote-ref" id="fnref97"><sup>97</sup></a>
</div>

<div class="guidedpractice">
The 90% confidence interval from the previous
Guided Practice is 0.256 ppm to 0.318 ppm.
Can we say that 90% of croaker white fish (Pacific)
have mercury levels between 0.256 and 0.318 ppm?<a href="#fn98" class="footnote-ref" id="fnref98"><sup>98</sup></a>
</div>
<p></p>
</div>
<div id="one-sample-t-tests" class="section level4 unnumbered">
<h4>One sample <span class="math inline">\(t\)</span>-tests</h4>
<p>Now that we’ve used the <span class="math inline">\(t\)</span>-distribution for making a confidence
intervals for a mean, let’s speed on through to
hypothesis tests for the mean.</p>
<!--
\newcommand{\cherryblossomn}{100}
\newcommand{\cherryblossommean}{97.32}
\newcommand{\cherryblossomnull}{93.29}
\newcommand{\cherryblossomsd}{16.98}
\newcommand{\cherryblossomse}{1.70}
\newcommand{\cherryblossomz}{2.37}
-->
<p>Is the typical US runner getting faster or slower over time? We consider this question in the context of the Cherry Blossom Race, which is a 10-mile race in Washington, DC each spring.</p>
<p>The average time for all runners who finished the Cherry Blossom Race in 2006 was 93.29 minutes (93 minutes and about 17 seconds). We want to determine using data from 100 participants in the 2017 Cherry Blossom Race whether runners in this race are getting faster or slower, versus the other possibility that there has been no change.</p>

<div class="guidedpractice">
What are appropriate hypotheses for this context?<a href="#fn99" class="footnote-ref" id="fnref99"><sup>99</sup></a>
</div>

<div class="guidedpractice">
The data come from a simple random sample of all participants,
so the observations are independent.
However, should we be worried about the normality condition?
See Figure <a href="inference-num.html#fig:run10SampTimeHistogram">7.11</a> for a histogram
of the differences and evaluate if we can move
forward.<a href="#fn100" class="footnote-ref" id="fnref100"><sup>100</sup></a>
</div>
<div class="figure" style="text-align: center"><span id="fig:run10SampTimeHistogram"></span>
<img src="07-inference-num_files/figure-html/run10SampTimeHistogram-1.png" alt="A histogram of `time` for the sample Cherry Blossom Race data." width="70%" />
<p class="caption">
Figure 7.11: A histogram of <code>time</code> for the sample Cherry Blossom Race data.
</p>
</div>
<p>When completing a hypothesis test for the one-sample mean,
the process is nearly identical to completing a hypothesis
test for a single proportion.
First, we find the Z score using the observed value,
null value, and standard error;
however, we call it a <strong>T score</strong> since we use
a <span class="math inline">\(t\)</span>-distribution for calculating the tail area.
Then we finding the p-value using the same ideas we used
previously: find the one-tail area under the sampling
distribution, and double it.</p>

<div class="example">
<p>With both the independence
and normality conditions satisfied,
we can proceed with a hypothesis test using
the <span class="math inline">\(t\)</span>-distribution.
The sample mean and sample standard deviation
of the sample
of 100 runners from the
2017 Cherry Blossom Race
are 97.32 and 16.98 minutes,
respectively.
Recall that the sample size is 100
and the average run time in 2006 was 93.29 minutes.
Find the test statistic and p-value.
What is your conclusion?</p>
<hr />
<p>To find the test statistic (T score),
we first must determine the standard error:
<span class="math display">\[\begin{align*}
  SE
    = 16.98 / \sqrt{100}
    = 1.70
  \end{align*}\]</span>
Now we can compute the 
using the sample mean (97.32),
null value (98.29), and <span class="math inline">\(SE\)</span>:
<span class="math display">\[\begin{align*}
  T
    = \frac{97.32 - 93.29}{1.70}
    = 2.37
  \end{align*}\]</span>
For <span class="math inline">\(df = 100 - 1 = 99\)</span>,
we can determine using statistical software
(or a <span class="math inline">\(t\)</span>-table, see below) that the one-tail area is 0.01,
which we double to get the p-value: 0.02.</p>
Because the p-value is smaller than 0.05,
we reject the null hypothesis.
That is, the data provide strong evidence that the average
run time for the Cherry Blossom Run in 2017 is different
than the 2006 average.
Since the observed value is above the null value
and we have rejected the null hypothesis, we would conclude
that runners in the race were slower on average in 2017
than in 2006.
</div>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="inference-num.html#cb18-1" aria-hidden="true"></a><span class="co"># using pt() to find the p-value</span></span>
<span id="cb18-2"><a href="inference-num.html#cb18-2" aria-hidden="true"></a><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pt</span>(<span class="fl">2.37</span>, <span class="dt">df =</span> <span class="dv">99</span>)</span>
<span id="cb18-3"><a href="inference-num.html#cb18-3" aria-hidden="true"></a><span class="co">#&gt; [1] 0.00986</span></span></code></pre></div>

<div class="onebox">
<p><strong>When using a <span class="math inline">\(t\)</span>-distribution, we use a T score (same as Z score).</strong></p>
To help us remember to use the <span class="math inline">\(t\)</span>-distribution,
we use a <span class="math inline">\(T\)</span> to represent the test statistic,
and we often call this a <strong>T score</strong>.
The Z score and T score are computed in the exact same way
and are conceptually identical:
each represents how many standard errors the observed value
is from the null value.
</div>
<!--
\begin{onebox}{Hypothesis testing for a single mean}
  Once you've determined a one-mean hypothesis test is the
  correct procedure, there are four steps to completing the
  test:
  \begin{description}
  \item[Prepare.]
      Identify the parameter of interest,
      list out hypotheses,
      identify the significance level,
      and identify $\bar{x}$, $s$, and $n$.
  \item[Check.]
      Verify conditions to ensure $\bar{x}$ is nearly normal.
  \item[Calculate.]
      If the conditions hold, compute $SE$,
      compute the T score, and identify the p-value.
  \item[Conclude.]
      Evaluate the hypothesis test by comparing the p-value
      to $\alpha$, and provide a conclusion in the context
      of the problem.
  \end{description}
\end{onebox}
-->
<!--
\CalculatorVideos{confidence intervals and hypothesis tests for a single mean}


{\input{ch_inference_for_means/TeX/one-sample_means_with_the_t-distribution.tex}}
-->
</div>
</div>
</div>
<div id="paired-data" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Paired difference</h2>
<div id="case-study" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> case study</h3>
</div>
<div id="randomization-test-for-h_0-mu_d-0" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Randomization test for <span class="math inline">\(H_0: \mu_d = 0\)</span></h3>
<p>for randomization, idea of coin flipping</p>
<div id="observed-data-2" class="section level5 unnumbered">
<h5>Observed data</h5>
</div>
<div id="variability-of-the-statistic-1" class="section level5 unnumbered">
<h5>Variability of the statistic</h5>
</div>
<div id="observed-statistic-vs.-null-statistics" class="section level5 unnumbered">
<h5>Observed statistic vs. null statistics</h5>
</div>
</div>
<div id="bootstrap-confidence-interval-for-mu_d" class="section level3" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Bootstrap confidence interval for <span class="math inline">\(\mu_d\)</span></h3>
<p>for bootstrap and mathematical model there is not much to do here except go through a full example. tie the ideas back to the one-sample problem.</p>
<div id="observed-data-3" class="section level5 unnumbered">
<h5>Observed data</h5>
</div>
<div id="variability-of-the-statistic-2" class="section level5 unnumbered">
<h5>Variability of the statistic</h5>
</div>
</div>
<div id="mathematical-model" class="section level3" number="7.2.4">
<h3><span class="header-section-number">7.2.4</span> Mathematical model</h3>
<p>for bootstrap and mathematical model there is not much to do here except go through a full example. tie the ideas back to the one-sample problem.</p>
<div id="observed-data-4" class="section level5 unnumbered">
<h5>Observed data</h5>
</div>
<div id="variability-of-the-statistic-3" class="section level5 unnumbered">
<h5>Variability of the statistic</h5>
</div>
<div id="observed-statistic-vs.-null-statistics-1" class="section level5 unnumbered">
<h5>Observed statistic vs. null statistics</h5>
</div>
</div>
</div>
<div id="difference-of-two-means" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Difference of two means</h2>
<div id="case-study-1" class="section level3" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> case study</h3>
</div>
<div id="randomization-test-for-h_0-mu_1---mu_2-0" class="section level3" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> Randomization test for <span class="math inline">\(H_0: \mu_1 - \mu_2 = 0\)</span></h3>
<p>need to talk about the way to randomize is almost identical to chapter 5 &amp; 6. a new plot will probably help (but again, very similar to 5.7)</p>
<div id="observed-data-5" class="section level5 unnumbered">
<h5>Observed data</h5>
</div>
<div id="variability-of-the-statistic-4" class="section level5 unnumbered">
<h5>Variability of the statistic</h5>
</div>
<div id="observed-statistic-vs.-null-statistics-2" class="section level5 unnumbered">
<h5>Observed statistic vs. null statistics</h5>
</div>
</div>
<div id="bootstrap-confidence-interval-for-mu_1---mu_2" class="section level3" number="7.3.3">
<h3><span class="header-section-number">7.3.3</span> Bootstrap confidence interval for <span class="math inline">\(\mu_1 - \mu_2\)</span></h3>
<p>tie back to idea in chapter 6 for two proportion CI.</p>
<div id="observed-data-6" class="section level5 unnumbered">
<h5>Observed data</h5>
</div>
<div id="variability-of-the-statistic-5" class="section level5 unnumbered">
<h5>Variability of the statistic</h5>
</div>
</div>
<div id="mathematical-model-1" class="section level3" number="7.3.4">
<h3><span class="header-section-number">7.3.4</span> Mathematical model</h3>
<p>t-test. mention that there are lots of nuances outside the scope of this book.</p>
<div id="observed-data-7" class="section level5 unnumbered">
<h5>Observed data</h5>
</div>
<div id="variability-of-the-statistic-6" class="section level5 unnumbered">
<h5>Variability of the statistic</h5>
</div>
<div id="observed-statistic-vs.-null-statistics-3" class="section level5 unnumbered">
<h5>Observed statistic vs. null statistics</h5>
</div>
</div>
</div>
<div id="anovaAndRegrWithCategoricalVariables" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Comparing many means with ANOVA</h2>
<p></p>
<p>Sometimes we want to compare means across many groups.
We might initially think to do pairwise comparisons.
For example, if there were three groups, we might be tempted
to compare the first mean with the second,
then with the third,
and then finally compare the second and third means for
a total of three comparisons.
However, this strategy can be treacherous.
If we have many groups and do many comparisons,
it is likely that we will eventually find a difference
just by chance, even if there is no difference in the
populations.
Instead, we should apply a holistic test to check whether
there is evidence that at least one pair groups are
in fact different, and this is where <strong>ANOVA</strong> saves
the day.</p>
<p>In this section, we will learn a new method called
<strong>analysis of variance (ANOVA)</strong> and a new test
statistic called <span class="math inline">\(F\)</span> (which we will introduce in our discussion of mathematical models).
ANOVA uses a single hypothesis test to check whether
the means across many groups are equal:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: The mean outcome is the same across all groups. In statistical notation, <span class="math inline">\(\mu_1 = \mu_2 = \cdots = \mu_k\)</span> where <span class="math inline">\(\mu_i\)</span> represents the mean of the outcome for observations in category <span class="math inline">\(i\)</span>.<br />
</li>
<li><span class="math inline">\(H_A\)</span>: At least one mean is different.</li>
</ul>
<p>Generally we must check three conditions on the data before performing ANOVA:<br />
* the observations are independent within and across groups,<br />
* the data within each group are nearly normal, and<br />
* the variability across the groups is about equal.</p>
<p>When these three conditions are met, we may perform an ANOVA to determine whether the data provide strong evidence against the null hypothesis that all the <span class="math inline">\(\mu_i\)</span> are equal.</p>

<div class="example">
<p>College departments commonly run multiple
lectures of the same introductory course each semester
because of high demand.
Consider a statistics department that runs three lectures
of an introductory statistics course.
We might like to determine whether there are statistically
significant differences in first exam scores in these three
classes (<span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, and <span class="math inline">\(C\)</span>).
Describe appropriate hypotheses to determine whether
there are any differences between the three classes.</p>
<hr />
The hypotheses may be written in the following form:<br />
* <span class="math inline">\(H_0\)</span>: The average score is identical in all lectures.
Any observed difference is due to chance.<br />
Notationally, we write <span class="math inline">\(\mu_A=\mu_B=\mu_C\)</span>.
* <span class="math inline">\(H_A\)</span>: The average score varies by class.
We would reject the null hypothesis in favor of the
alternative hypothesis if there were larger differences
among the class averages than what we might expect
from chance alone.
</div>
<p>Strong evidence favoring the alternative hypothesis in ANOVA
is described by unusually large differences among the group means.
We will soon learn that assessing the variability of the group
means relative to the variability among individual observations
within each group is key to ANOVA’s success.</p>

<div class="example">
<p>Examine Figure <a href="inference-num.html#fig:toyANOVA">7.12</a>. Compare groups I, II, and III.
Can you visually determine if the differences in the group centers is due to chance or not?
Now compare groups IV, V, and VI.
Do these differences appear to be due to chance?</p>
<hr />
Any real difference in the means of groups I, II, and III
is difficult to discern, because the data within each group
are very volatile relative to any differences in the
average outcome.
On the other hand, it appears there are differences
in the centers of groups IV, V, and VI.
For instance, group V appears to have a higher mean than
that of the other two groups.
Investigating groups IV, V, and VI, we see the differences
in the groups’ centers are noticeable because those
differences are large <em>relative to the variability
in the individual observations within each group</em>.
</div>
<div class="figure" style="text-align: center"><span id="fig:toyANOVA"></span>
<img src="07-inference-num_files/figure-html/toyANOVA-1.png" alt="Side-by-side dot plot for the outcomes for six groups." width="70%" />
<p class="caption">
Figure 7.12: Side-by-side dot plot for the outcomes for six groups.
</p>
</div>
<div id="batting-case-study" class="section level4 unnumbered">
<h4>Batting case study</h4>
<p></p>
<!--
\newcommand{\mlbdata}{\data{bat18}}
\newcommand{\mlbN}{429}
\newcommand{\mlbK}{3}
\newcommand{\mlbMinAB}{100}
\newcommand{\mlbDFA}{2}
\newcommand{\mlbDFB}{426}
\newcommand{\mlbF}{5.077}
\newcommand{\mlbPvalue}{0.0066}
-->
<p>We would like to discern whether there are real differences
between the batting performance of baseball players according
to their position:
outfielder (OF), infielder (IF),
%designated hitter (DH),
and catcher (C).
We will use a data set called <code>mlb_players_18</code>,
which includes batting records of 429 Major League
Baseball (MLB) players from the 2018 season who had
at least 100 at bats.
Six of the 429 cases represented in <code>mlb_players_18</code>
are shown in Figure <a href="inference-num.html#tab:mlbBat18DataMatrix">7.2</a>,
and descriptions for each variable are provided
in Figure <a href="inference-num.html#tab:mlbBat18Variables">7.3</a>.
The measure we will use for the player batting
performance (the outcome variable) is on-base
percentage (<code>OBP</code>).
The on-base percentage roughly represents the fraction
of the time a player successfully gets on base or hits
a home run.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:mlbBat18DataMatrix">Table 7.2: </span>Six cases from the <code>mlb_players_18</code> data matrix.
</caption>
<thead>
<tr>
<th style="text-align:left;">
name
</th>
<th style="text-align:left;">
team
</th>
<th style="text-align:left;">
position
</th>
<th style="text-align:right;">
games
</th>
<th style="text-align:right;">
AB
</th>
<th style="text-align:right;">
R
</th>
<th style="text-align:right;">
H
</th>
<th style="text-align:right;">
doubles
</th>
<th style="text-align:right;">
triples
</th>
<th style="text-align:right;">
HR
</th>
<th style="text-align:right;">
RBI
</th>
<th style="text-align:right;">
walks
</th>
<th style="text-align:right;">
strike_outs
</th>
<th style="text-align:right;">
stolen_bases
</th>
<th style="text-align:right;">
caught_stealing_base
</th>
<th style="text-align:right;">
AVG
</th>
<th style="text-align:right;">
OBP
</th>
<th style="text-align:right;">
SLG
</th>
<th style="text-align:right;">
OPS
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Abreu, J
</td>
<td style="text-align:left;">
CWS
</td>
<td style="text-align:left;">
1B
</td>
<td style="text-align:right;">
128
</td>
<td style="text-align:right;">
499
</td>
<td style="text-align:right;">
68
</td>
<td style="text-align:right;">
132
</td>
<td style="text-align:right;">
36
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
78
</td>
<td style="text-align:right;">
37
</td>
<td style="text-align:right;">
109
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.265
</td>
<td style="text-align:right;">
0.325
</td>
<td style="text-align:right;">
0.473
</td>
<td style="text-align:right;">
0.798
</td>
</tr>
<tr>
<td style="text-align:left;">
Acuna Jr., R
</td>
<td style="text-align:left;">
ATL
</td>
<td style="text-align:left;">
LF
</td>
<td style="text-align:right;">
111
</td>
<td style="text-align:right;">
433
</td>
<td style="text-align:right;">
78
</td>
<td style="text-align:right;">
127
</td>
<td style="text-align:right;">
26
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
26
</td>
<td style="text-align:right;">
64
</td>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
123
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0.293
</td>
<td style="text-align:right;">
0.366
</td>
<td style="text-align:right;">
0.552
</td>
<td style="text-align:right;">
0.917
</td>
</tr>
<tr>
<td style="text-align:left;">
Adam, J
</td>
<td style="text-align:left;">
KC
</td>
<td style="text-align:left;">
P
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:left;">
Adames, W
</td>
<td style="text-align:left;">
TB
</td>
<td style="text-align:left;">
SS
</td>
<td style="text-align:right;">
85
</td>
<td style="text-align:right;">
288
</td>
<td style="text-align:right;">
43
</td>
<td style="text-align:right;">
80
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
34
</td>
<td style="text-align:right;">
31
</td>
<td style="text-align:right;">
95
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0.278
</td>
<td style="text-align:right;">
0.348
</td>
<td style="text-align:right;">
0.406
</td>
<td style="text-align:right;">
0.754
</td>
</tr>
<tr>
<td style="text-align:left;">
Adams, A
</td>
<td style="text-align:left;">
WSH
</td>
<td style="text-align:left;">
P
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
<tr>
<td style="text-align:left;">
Adams, C
</td>
<td style="text-align:left;">
NYY
</td>
<td style="text-align:left;">
P
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
</tr>
</tbody>
</table>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:mlbBat18Variables">Table 7.3: </span>Variables and their descriptions for the mlb_players_18 data set.
</caption>
<thead>
<tr>
<th style="text-align:left;">
variable
</th>
<th style="text-align:left;">
description
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
name
</td>
<td style="text-align:left;">
Player name
</td>
</tr>
<tr>
<td style="text-align:left;">
team
</td>
<td style="text-align:left;">
The abbreviated name of the player’s team
</td>
</tr>
<tr>
<td style="text-align:left;">
position
</td>
<td style="text-align:left;">
The player’s primary field position (OF, IF, C)
</td>
</tr>
<tr>
<td style="text-align:left;">
AB
</td>
<td style="text-align:left;">
Number of opportunities at bat
</td>
</tr>
<tr>
<td style="text-align:left;">
H
</td>
<td style="text-align:left;">
Number of hits
</td>
</tr>
<tr>
<td style="text-align:left;">
HR
</td>
<td style="text-align:left;">
Number of home runs
</td>
</tr>
<tr>
<td style="text-align:left;">
RBI
</td>
<td style="text-align:left;">
Number of runs batted in
</td>
</tr>
<tr>
<td style="text-align:left;">
AVG
</td>
<td style="text-align:left;">
Batting average, which is equal to H/AB
</td>
</tr>
<tr>
<td style="text-align:left;">
OBP
</td>
<td style="text-align:left;">
On-base percentage, which is roughly equal to the fraction of times a player gets on base or hits a home run
</td>
</tr>
</tbody>
</table>

<div class="guidedpractice">
The null hypothesis under consideration is the following:
<span class="math inline">\(\mu_{OF} = \mu_{IF} = %\mu_{DH} =  \mu_{C}\)</span>.
Write the null and corresponding alternative hypotheses
in plain language.<a href="#fn101" class="footnote-ref" id="fnref101"><sup>101</sup></a>
</div>

<div class="example">
<p>The player positions have been divided
into three groups: outfield (OF), infield (IF),
and catcher (C).
What would be an appropriate point estimate of the on-base
percentage by outfielders, <span class="math inline">\(\mu_{OF}\)</span>?</p>
<hr />
A good estimate of the on-base percentage by outfielders would
be the sample average of <code>OBP</code> for just those players
whose position is outfield: <span class="math inline">\(\bar{x}_{OF} = 0.320\)</span>.
</div>
</div>
<div id="randomization-test-for-h_0-mu_1-mu_2-ldots-mu_k" class="section level3" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> Randomization test for <span class="math inline">\(H_0: \mu_1 = \mu_2 = \ldots = \mu_k\)</span></h3>
<p>Table <a href="inference-num.html#tab:mlbHRPerABSummaryTable">7.4</a> provides summary statistics for each group. A side-by-side box plot for the on-base percentage is shown in Figure <a href="inference-num.html#fig:mlbANOVABoxPlot">7.13</a>. Notice that the variability appears to be approximately constant across groups; nearly constant variance across groups is an important assumption that must be satisfied before we consider the ANOVA approach.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:mlbHRPerABSummaryTable">Table 7.4: </span>Summary statistics of on-base percentage, split by player position.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
OF
</th>
<th style="text-align:right;">
IF
</th>
<th style="text-align:right;">
C
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Sample size (<span class="math inline">\(n_i\)</span>)
</td>
<td style="text-align:right;">
160.000
</td>
<td style="text-align:right;">
205.000
</td>
<td style="text-align:right;">
64.000
</td>
</tr>
<tr>
<td style="text-align:left;">
Sample mean (<span class="math inline">\(\bar{x}_i\)</span>)
</td>
<td style="text-align:right;">
0.320
</td>
<td style="text-align:right;">
0.318
</td>
<td style="text-align:right;">
0.302
</td>
</tr>
<tr>
<td style="text-align:left;">
Sample SD (<span class="math inline">\(s_i\)</span>)
</td>
<td style="text-align:right;">
0.043
</td>
<td style="text-align:right;">
0.038
</td>
<td style="text-align:right;">
0.038
</td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span id="fig:mlbANOVABoxPlot"></span>
<img src="07-inference-num_files/figure-html/mlbANOVABoxPlot-1.png" alt="Side-by-side box plot of the on-base percentage for 429 players across four groups. There is one prominent outlier visible in the infield group, but with 154 observations in the infield group, this outlier is not a concern." width="70%" />
<p class="caption">
Figure 7.13: Side-by-side box plot of the on-base percentage for 429 players across four groups. There is one prominent outlier visible in the infield group, but with 154 observations in the infield group, this outlier is not a concern.
</p>
</div>

<div class="example">
<p>The largest difference between the sample means
is between the catcher and the outfielder positions.
Consider again the original hypotheses:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mu_{OF} = \mu_{IF} = \mu_{C}\)</span></p></li>
<li><p><span class="math inline">\(H_A\)</span>: The average on-base percentage (<span class="math inline">\(\mu_i\)</span>) varies
across some (or all) groups.</p>
<p>Why might it be inappropriate to run the test by simply
estimating whether the difference of <span class="math inline">\(\mu_{C}\)</span> and
<span class="math inline">\(\mu_{OF}\)</span> is statistically significant at a 0.05
significance level?<br />
—</p></li>
</ul>
<p>The primary issue here is that we are inspecting the data
before picking the groups that will be compared.
It is inappropriate to examine all data by eye
(informal testing) and only afterwards decide which parts
to formally test.
This is called <strong>data snooping</strong> or <strong>data fishing</strong>.
Naturally, we would pick the groups with the large
differences for the formal test, and this would leading
to an inflation in the Type 1 Error rate.
To understand this better, let’s consider a slightly
different problem.</p>
Suppose we are to measure the aptitude for students in
20 classes in a large elementary school at the beginning
of the year.
In this school, all students are randomly assigned to
classrooms, so any differences we observe between the
classes at the start of the year are completely due
to chance.
However, with so many groups, we will probably observe
a few groups that look rather different from each other.
If we select only these classes that look so different
and then perform a formal test,
we will probably make the wrong conclusion that the
assignment wasn’t random.
While we might only formally test differences
for a few pairs of classes, we informally evaluated
the other classes by eye before choosing the most extreme
cases for a comparison.
</div>
<p>For additional information on the ideas expressed above, we recommend
reading about the
<strong>prosecutor’s fallacy</strong>.<a href="#fn102" class="footnote-ref" id="fnref102"><sup>102</sup></a></p>
<div id="observed-data-8" class="section level4 unnumbered">
<h4>Observed data</h4>
<p>In the next section we will learn how to use the <span class="math inline">\(F\)</span> statistic
to test whether observed differences in sample means
could have happened just by chance even if there was no
difference in the respective population means.</p>
<p>The method of analysis of variance in this context focuses
on answering one question:
is the variability in the sample means so large that it seems
unlikely to be from chance alone?
This question is different from earlier testing procedures
since we will <em>simultaneously</em> consider many groups,
and evaluate whether their sample means differ more than
we would expect from natural variation.
We call this variability the
<strong>mean square between groups (<span class="math inline">\(MSG\)</span>)</strong>,
and it has an associated degrees of freedom,
<span class="math inline">\(df_{G} = k - 1\)</span> when there are
<span class="math inline">\(k\)</span> groups.
The <span class="math inline">\(MSG\)</span> can be thought of as a scaled variance formula
for means.
If the null hypothesis is true, any variation in the sample
means is due to chance and shouldn’t be too large.
Details of <span class="math inline">\(MSG\)</span> calculations are provided in the
footnote.<a href="#fn103" class="footnote-ref" id="fnref103"><sup>103</sup></a></p>
<p>However, we typically use software for these computations.</p>
<p></p>
<p>The mean square between the groups is, on its own, quite useless
in a hypothesis test.
We need a benchmark value for how much variability should
be expected among the sample means if the null hypothesis is true.
To this end, we compute a pooled variance estimate,
often abbreviated as the <strong>mean square error (<span class="math inline">\(MSE\)</span>)</strong>,
which has an associated degrees of freedom value <span class="math inline">\(df_E = n - k\)</span>.
It is helpful to think of <span class="math inline">\(MSE\)</span> as a measure of the variability
within the groups.
Details of the computations of the <span class="math inline">\(MSE\)</span> and a link to an
extra online section for ANOVA calculations are provided
in the footnote^[Let <span class="math inline">\(\bar{x}\)</span> represent the mean
of outcomes across all groups.
Then the <strong>sum of squares total (<span class="math inline">\(SST\)</span>)</strong> is computed as
<span class="math display">\[\begin{align*}
  SST = \sum_{i=1}^{n} \left(x_{i} - \bar{x}\right)^2
  \end{align*}\]</span>
where the sum is over all observations in the data set.
Then we compute the <strong>sum of squared errors (<span class="math inline">\(SSE\)</span>)</strong>
in one of two equivalent ways:
<span class="math display">\[\begin{align*}
  SSE &amp;= SST - SSG \\
  	&amp;= (n_1-1)s_1^2 + (n_2-1)s_2^2 + \cdots + (n_k-1)s_k^2
  \end{align*}\]</span>
where <span class="math inline">\(s_i^2\)</span> is the sample variance (square of the standard
deviation) of the residuals in group <span class="math inline">\(i\)</span>.
Then the <span class="math inline">\(MSE\)</span> is the standardized form of <span class="math inline">\(SSE\)</span>:
<span class="math inline">\(MSE = \frac{1}{df_{E}}SSE\)</span>.</p>
<p>See <a href="www.openintro.org/d?file=stat_extra_anova_calculations">additional details on ANOVA calculations</a>
for interested readers.</p>
</div>
<div id="variability-of-the-statistic-7" class="section level4 unnumbered">
<h4>Variability of the statistic</h4>
<p>When the null hypothesis is true, any differences among the
sample means are only due to chance, and the <span class="math inline">\(MSG\)</span> and <span class="math inline">\(MSE\)</span>
should be about equal.
As a test statistic for ANOVA, we examine the fraction of <span class="math inline">\(MSG\)</span>
and <span class="math inline">\(MSE\)</span>:
<span class="math display">\[\begin{align*}
F = \frac{MSG}{MSE}
\end{align*}\]</span>
The <span class="math inline">\(MSG\)</span> represents a measure of the between-group variability,
and <span class="math inline">\(MSE\)</span> measures the variability within each of the groups.</p>

<div class="guidedpractice">
For the baseball data, <span class="math inline">\(MSG = 0.00803\)</span> and <span class="math inline">\(MSE=0.00158\)</span>.
Identify the degrees of freedom associated with MSG and
MSE and verify the <span class="math inline">\(F\)</span> statistic is approximately
5.077.<a href="#fn104" class="footnote-ref" id="fnref104"><sup>104</sup></a>
</div>
</div>
<div id="observed-statistic-vs.-null-statistic" class="section level4 unnumbered">
<h4>Observed statistic vs. null statistic</h4>
<p>We can use the <span class="math inline">\(F\)</span> statistic to evaluate the hypotheses in
what is called an F-test.
A p-value can be computed from the <span class="math inline">\(F\)</span> statistic using
an <span class="math inline">\(F\)</span> distribution, which has two associated parameters:
<span class="math inline">\(df_{1}\)</span> and <span class="math inline">\(df_{2}\)</span>.
For the <span class="math inline">\(F\)</span> statistic in ANOVA,
<span class="math inline">\(df_{1} = df_{G}\)</span> and <span class="math inline">\(df_{2} = df_{E}\)</span>.
An <span class="math inline">\(F\)</span> distribution with 2 and 426 degrees
of freedom, corresponding to the <span class="math inline">\(F\)</span> statistic for the
baseball hypothesis test, is shown in
Figure <a href="inference-num.html#fig:fDist2And423Shaded">7.14</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:fDist2And423Shaded"></span>
<img src="07-inference-num_files/figure-html/fDist2And423Shaded-1.png" alt="An $F$ distribution with $df_1=3$ and $df_2=323$." width="70%" /><img src="07-inference-num_files/figure-html/fDist2And423Shaded-2.png" alt="An $F$ distribution with $df_1=3$ and $df_2=323$." width="70%" />
<p class="caption">
Figure 7.14: An <span class="math inline">\(F\)</span> distribution with <span class="math inline">\(df_1=3\)</span> and <span class="math inline">\(df_2=323\)</span>.
</p>
</div>

<div class="todo">
need a simulation method here where the data gets randomized, the F statistic is calculted, and the p-value is obtained from the histogram.
</div>
</div>
</div>
<div id="mathematical-model-2" class="section level3" number="7.4.2">
<h3><span class="header-section-number">7.4.2</span> Mathematical model</h3>
<div id="the-anova-f-test" class="section level4 unnumbered">
<h4>The ANOVA F-test</h4>
<div id="variability-of-the-statistic-8" class="section level5 unnumbered">
<h5>Variability of the statistic</h5>
<p>The larger the observed variability in the sample
means (<span class="math inline">\(MSG\)</span>) relative to the within-group observations (<span class="math inline">\(MSE\)</span>),
the larger <span class="math inline">\(F\)</span> will be and the stronger the evidence against
the null hypothesis.
Because larger values of <span class="math inline">\(F\)</span> represent stronger evidence against
the null hypothesis, we use the upper tail of the distribution
to compute a p-value.</p>

<div class="onebox">
<strong>The F statistic and the F-test.</strong>
Analysis of variance (ANOVA) is used to test whether
the mean outcome differs across 2 or more groups.
ANOVA uses a test statistic <span class="math inline">\(F\)</span>, which represents
a standardized ratio of variability in the sample means
relative to the variability within the groups.
If <span class="math inline">\(H_0\)</span> is true and the model conditions are satisfied,
the statistic <span class="math inline">\(F\)</span> follows an <span class="math inline">\(F\)</span> distribution with
parameters <span class="math inline">\(df_{1} = k - 1\)</span> and <span class="math inline">\(df_{2} = n - k\)</span>.
The upper tail of the <span class="math inline">\(F\)</span> distribution is used to
represent the p-value.
</div>
</div>
<div id="observed-statistic-vs.-null-statistics-4" class="section level5 unnumbered">
<h5>Observed statistic vs. null statistics</h5>

<div class="example">
<p>The p-value corresponding to
the shaded area in
Figure <a href="inference-num.html#fig:fDist2And423Shaded">7.14</a>
is equal to about 0.0066.
Does this provide strong evidence against the
null hypothesis?</p>
<hr />
The p-value is smaller than 0.05, indicating the evidence
is strong enough to reject the null hypothesis
at a significance level of 0.05.
That is, the data provide strong evidence that the average
on-base percentage varies by player’s primary field position.
</div>
<p>Note that the small p-value indicates that there is a significant difference between the average batting averages of the different positions. However, the ANOVA test does not provide a mechanism for knowing <em>which</em> group is driving the significant differences. The follow-up questions surrounding individual group comparisons is called a problem of <strong>multiple comparisons</strong> and is outside the scope of this text. We encourage you to learn more about multiple comparisons, however, so that additional comparisons after a significant ANOVA test does not lead to undue false positive conclusions.</p>
</div>
</div>
<div id="reading-an-anova-table-from-software" class="section level4 unnumbered">
<h4>Reading an ANOVA table from software</h4>
<p>The calculations required to perform an ANOVA by hand are
tedious and prone to human error.
For these reasons, it is common to use statistical software
to calculate the <span class="math inline">\(F\)</span> statistic and p-value.</p>
<p>An ANOVA can be summarized in a table very similar to that
of a regression summary, which we saw in
Chapters <a href="cor-reg.html#cor-reg">3</a>
and <a href="mult-reg.html#mult-reg">4</a>.
Table <a href="inference-num.html#tab:anovaSummaryTableForOBPAgainstPosition">7.5</a>
shows an ANOVA summary to test whether the mean of on-base
percentage varies by player positions in the MLB.
Many of these values should look familiar;
in particular, the <span class="math inline">\(F\)</span>-test statistic and p-value
can be retrieved from the last two columns.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:anovaSummaryTableForOBPAgainstPosition">Table 7.5: </span>ANOVA summary for testing whether the average on-base percentage differs across player positions.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Df
</th>
<th style="text-align:right;">
Sum Sq
</th>
<th style="text-align:right;">
Mean Sq
</th>
<th style="text-align:right;">
F value
</th>
<th style="text-align:right;">
Pr(&gt;F)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
position
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0.0161
</td>
<td style="text-align:right;">
0.0080
</td>
<td style="text-align:right;">
5.08
</td>
<td style="text-align:right;">
0.0066
</td>
</tr>
<tr>
<td style="text-align:left;">
Residuals
</td>
<td style="text-align:right;">
426
</td>
<td style="text-align:right;">
0.6740
</td>
<td style="text-align:right;">
0.0016
</td>
<td style="text-align:right;">
</td>
<td style="text-align:right;">
</td>
</tr>
</tbody>
<tfoot>
<tr>
<td style="padding: 0; border:0;" colspan="100%">
<sup>a</sup> <span class="math inline">\(s_{pooled} = 0.040\)</span> on <span class="math inline">\(df = 423\)</span>
</td>
</tr>
</tfoot>
</table>
</div>
<div id="conditions-for-an-anova-analysis" class="section level4 unnumbered">
<h4>Conditions for an ANOVA analysis</h4>
<p>There are three conditions we must check for an ANOVA analysis:
all observations must be independent,
the data in each group must be nearly normal,
and the variance within each group must be approximately equal.</p>
<ul>
<li><p><strong>Independence.</strong> If the data are a simple random sample,
this condition is satisfied.
For processes and experiments, carefully consider whether
the data may be independent (e.g. no pairing).
For example, in the MLB data, the data were not sampled.
However, there are not obvious reasons why independence
would not hold for most or all observations.</p></li>
<li><p><strong>Approximately normal.</strong> As with one- and two-sample testing for means,
the normality assumption is especially important
when the sample size is quite small when it is
ironically difficult to check for non-normality.
A histogram of the observations from each group
is shown in Figure <a href="inference-num.html#fig:mlbANOVADiagNormalityGroups">7.15</a>.
Since each of the groups we’re considering have
relatively large sample sizes,
what we’re looking for are major outliers.
None are apparent, so this conditions is reasonably met.</p></li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:mlbANOVADiagNormalityGroups"></span>
<img src="07-inference-num_files/figure-html/mlbANOVADiagNormalityGroups-1.png" alt="Histograms of OBP for each field position." width="70%" /><img src="07-inference-num_files/figure-html/mlbANOVADiagNormalityGroups-2.png" alt="Histograms of OBP for each field position." width="70%" /><img src="07-inference-num_files/figure-html/mlbANOVADiagNormalityGroups-3.png" alt="Histograms of OBP for each field position." width="70%" />
<p class="caption">
Figure 7.15: Histograms of OBP for each field position.
</p>
</div>
<ul>
<li><strong>Constant variance.</strong> The last assumption is that the variance in the
groups is about equal from one group to the next.
This assumption can be checked by examining a
side-by-side box plot of the outcomes across the
groups, as in Figure <a href="inference-num.html#fig:mlbANOVABoxPlot">7.13</a>.
In this case, the variability is similar in the
four groups but not identical.
We see in Table <a href="inference-num.html#tab:mlbHRPerABSummaryTable">7.4</a>
that the standard deviation doesn’t vary much
from one group to the next.</li>
</ul>
<p></p>

<div class="onebox">
<p><strong>Diagnostics for an ANOVA analysis.</strong></p>
Independence is always important to an ANOVA analysis.
The normality condition is very important when the sample
sizes for each group are relatively small.
The constant variance condition is especially important
when the sample sizes differ between groups.
</div>
<p></p>
</div>
</div>
</div>
<div id="chp7-review" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Chapter 7 review</h2>

<div class="todo">
need to expand on the technical condition as the last row. also, is it helpful for the rest of the table to be repeated?
</div>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:chp7summary">Table 7.6: </span>Summary and comparison of Randomization Tests, Bootstrapping, and Mathematical Models as inferential statistical methods.
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Randomization Test
</th>
<th style="text-align:left;">
Bootstrapping
</th>
<th style="text-align:left;">
Mathematical Model
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
What does it do?
</td>
<td style="text-align:left;">
Shuffles the explanatory variable to mimic the natural variability found in a randomized experiment.
</td>
<td style="text-align:left;">
Resamples (with replacement) from the observed data to mimic the sampling variability found by collecting data.
</td>
<td style="text-align:left;">
Uses theory (primarily the Central Limit Theorem) to describe the hypothetical variability resulting from either repeated randomized experiments or random samples.
</td>
</tr>
<tr>
<td style="text-align:left;">
What is the random process described?
</td>
<td style="text-align:left;">
randomized experiment
</td>
<td style="text-align:left;">
random sampling
</td>
<td style="text-align:left;">
either / both
</td>
</tr>
<tr>
<td style="text-align:left;">
Is there flexibility?
</td>
<td style="text-align:left;">
Yes, can be used to describe random sampling in an observational model
</td>
<td style="text-align:left;">
Yes, can be used to describe random allocation in an experiment
</td>
<td style="text-align:left;">
Yes
</td>
</tr>
<tr>
<td style="text-align:left;">
What is it best for?
</td>
<td style="text-align:left;">
Hypothesis Testing (can be used for Confidence Intervals, but not covered in this text).
</td>
<td style="text-align:left;">
Confidence Intervals (HT for one proportion covered in Chapter 6).
</td>
<td style="text-align:left;">
Quick analyses through, for example, calculating a Z score.
</td>
</tr>
<tr>
<td style="text-align:left;">
What physical object represents the simulation process?
</td>
<td style="text-align:left;">
shuffling cards
</td>
<td style="text-align:left;">
pulling balls from a bag
</td>
<td style="text-align:left;">
NA
</td>
</tr>
<tr>
<td style="text-align:left;">
What are the technical conditions?
</td>
<td style="text-align:left;">
independence
</td>
<td style="text-align:left;">
independence, big n
</td>
<td style="text-align:left;">
independence, big n
</td>
</tr>
</tbody>
</table>
<div id="terms" class="section level3" number="7.5.1">
<h3><span class="header-section-number">7.5.1</span> Terms</h3>
<p>We introduced the following terms in the chapter.
If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions.
We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate.
However you should be able to easily spot them as <strong>bolded text</strong>.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="93">
<li id="fn93"><p>By looking at the percentile values in Figure <a href="inference-num.html#fig:flatsbssd">7.5</a>, the middle 90% of the bootstrap standard deviations are given by the 5 percentile (£ 153.9) and 95 percentile (£ 385.6). That is, we are 90% confident that the true standard deviation of rent prices is between £ 153.9 and £ 385.6.<a href="inference-num.html#fnref93" class="footnote-back">↩︎</a></p></li>
<li id="fn94"><p>More
nuanced guidelines would consider further relaxing
the <em>particularly extreme outlier</em> check when the
sample size is very large.
However, we’ll leave further discussion here to a future course.<a href="inference-num.html#fnref94" class="footnote-back">↩︎</a></p></li>
<li id="fn95"><p>We want to find the shaded area <em>above</em> -1.79 (we leave the picture to you).
The lower tail area has an area of 0.0447,
so the upper area would have an area of <span class="math inline">\(1 - 0.0447 = 0.9553\)</span>.<a href="inference-num.html#fnref95" class="footnote-back">↩︎</a></p></li>
<li id="fn96"><p>The sample size is under 30,
so we check for obvious outliers:
since all observations are within 2 standard deviations
of the mean, there are no such clear outliers.<a href="inference-num.html#fnref96" class="footnote-back">↩︎</a></p></li>
<li id="fn97"><p><span class="math inline">\(\bar{x} \ \pm\ t^{\star}_{14} \times SE  \ \to\  0.287 \ \pm\  1.76 \times 0.0178  \ \to\ (0.256, 0.318)\)</span>.
We are 90% confident that the average mercury content
of croaker white fish (Pacific) is between 0.256 and 0.318 ppm.<a href="inference-num.html#fnref97" class="footnote-back">↩︎</a></p></li>
<li id="fn98"><p>No, a confidence interval only provides a range
of plausible values for a population parameter,
in this case the population mean.
It does not describe what we might observe
for individual observations.<a href="inference-num.html#fnref98" class="footnote-back">↩︎</a></p></li>
<li id="fn99"><p><span class="math inline">\(H_0\)</span>: The average 10-mile run time was the same for 2006 and 2017. <span class="math inline">\(\mu = 93.29\)</span> minutes. <span class="math inline">\(H_A\)</span>: The average 10-mile run time for 2017 was  than that of 2006. <span class="math inline">\(\mu \neq 93.29\)</span> minutes.<a href="inference-num.html#fnref99" class="footnote-back">↩︎</a></p></li>
<li id="fn100"><p>With a sample of 100,
we should only be concerned if there is are particularly
extreme outliers.
The histogram of the data doesn’t show any outliers of concern
(and arguably, no outliers at all).<a href="inference-num.html#fnref100" class="footnote-back">↩︎</a></p></li>
<li id="fn101"><p><span class="math inline">\(H_0\)</span>: The average on-base percentage is equal
across the four positions.
<span class="math inline">\(H_A\)</span>: The average on-base percentage varies across some
(or all) groups.<a href="inference-num.html#fnref101" class="footnote-back">↩︎</a></p></li>
<li id="fn102"><p>See, for example,
[textbook-prosecutors_fallacy](andrewgelman.com/2007/05/18/the_prosecutors.<a href="inference-num.html#fnref102" class="footnote-back">↩︎</a></p></li>
<li id="fn103"><p>Let <span class="math inline">\(\bar{x}\)</span> represent the mean of
outcomes across all groups.
Then the mean square between groups is computed as
<span class="math display">\[\begin{align*}
  MSG
    = \frac{1}{df_{G}}SSG
    = \frac{1}{k-1}\sum_{i=1}^{k} n_{i}
        \left(\bar{x}_{i} - \bar{x}\right)^2
  \end{align*}\]</span>
where <span class="math inline">\(SSG\)</span> is called the <strong>sum of squares between groups</strong>
and <span class="math inline">\(n_{i}\)</span> is the sample size of group <span class="math inline">\(i\)</span>.<a href="inference-num.html#fnref103" class="footnote-back">↩︎</a></p></li>
<li id="fn104"><p>There are <span class="math inline">\(k = 3\)</span> groups,
so <span class="math inline">\(df_{G} = k - 1 = 2\)</span>.
There are <span class="math inline">\(n = n_1 + n_2 + n_3 = 429\)</span> total observations,
so <span class="math inline">\(df_{E} = n - k = 426\)</span>.
Then the <span class="math inline">\(F\)</span> statistic is computed as the ratio of <span class="math inline">\(MSG\)</span>
and <span class="math inline">\(MSE\)</span>:
<span class="math inline">\(F  = \frac{MSG}{MSE}  = \frac{0.00803}{0.00158}  = 5.082  \approx 5.077\)</span>.
(<span class="math inline">\(F = 5.077\)</span> was computed by using values for <span class="math inline">\(MSG\)</span>
and <span class="math inline">\(MSE\)</span> that were not rounded.)<a href="inference-num.html#fnref104" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="inference-cat.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inference-reg.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/MTstateIntroStats/IntroStatTextbook/edit/master/07-inference-num.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["stat216-textbook.pdf", "stat216-textbook.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 12 Errors, power, and practical importance | Montana State Introductory Statistics with R</title>
<meta name="author" content="Nicole Carnegie, Stacey Hancock, Elijah Meyer, Jade Schmidt, Melinda Yager">
<meta name="description" content="Using data to make inferential decisions about larger populations is not a perfect process. As seen in Chapter 9, a small p-value typically leads the researcher to a decision to reject the null...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="Chapter 12 Errors, power, and practical importance | Montana State Introductory Statistics with R">
<meta property="og:type" content="book">
<meta property="og:url" content="https://mtstateintrostats.github.io/IntroStatTextbook/foundations-errors.html">
<meta property="og:description" content="Using data to make inferential decisions about larger populations is not a perfect process. As seen in Chapter 9, a small p-value typically leads the researcher to a decision to reject the null...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 12 Errors, power, and practical importance | Montana State Introductory Statistics with R">
<meta name="twitter:description" content="Using data to make inferential decisions about larger populations is not a perfect process. As seen in Chapter 9, a small p-value typically leads the researcher to a decision to reject the null...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet">
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script><script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script><script src="libs/plotly-binding-4.10.0/plotly.js"></script><script src="libs/typedarray-0.1/typedarray.min.js"></script><link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script><link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script><script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="css/ims-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Montana State Introductory Statistics with R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="authors.html">Authors</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="rstudio.html">Preliminaries: Getting started in RStudio</a></li>
<li class="book-part">Introduction to data</li>
<li><a class="" href="data-hello.html"><span class="header-section-number">1</span> Hello data</a></li>
<li><a class="" href="data-design.html"><span class="header-section-number">2</span> Study design</a></li>
<li><a class="" href="data-applications.html"><span class="header-section-number">3</span> Applications: Data</a></li>
<li class="book-part">Exploratory data analysis</li>
<li><a class="" href="explore-categorical.html"><span class="header-section-number">4</span> Exploring categorical data</a></li>
<li><a class="" href="explore-numerical.html"><span class="header-section-number">5</span> Exploring quantitative data</a></li>
<li><a class="" href="explore-regression.html"><span class="header-section-number">6</span> Correlation and regression</a></li>
<li><a class="" href="explore-mult-reg.html"><span class="header-section-number">7</span> Multivariable models</a></li>
<li><a class="" href="explore-applications.html"><span class="header-section-number">8</span> Applications: Explore</a></li>
<li class="book-part">Foundations of inference</li>
<li><a class="" href="foundations-randomization.html"><span class="header-section-number">9</span> Hypothesis testing with randomization</a></li>
<li><a class="" href="foundations-bootstrapping.html"><span class="header-section-number">10</span> Confidence intervals with bootstrapping</a></li>
<li><a class="" href="foundations-mathematical.html"><span class="header-section-number">11</span> Inference with mathematical models</a></li>
<li><a class="active" href="foundations-errors.html"><span class="header-section-number">12</span> Errors, power, and practical importance</a></li>
<li><a class="" href="foundations-applications.html"><span class="header-section-number">13</span> Applications: Foundations</a></li>
<li class="book-part">Inference for categorical data</li>
<li><a class="" href="inference-one-prop.html"><span class="header-section-number">14</span> Inference for a single proportion</a></li>
<li><a class="" href="inference-two-props.html"><span class="header-section-number">15</span> Inference for comparing two proportions</a></li>
<li><a class="" href="inference-categ-applications.html"><span class="header-section-number">16</span> Applications: Infer categorical</a></li>
<li class="book-part">Inference for quantitative data</li>
<li><a class="" href="inference-one-mean.html"><span class="header-section-number">17</span> Inference for a single mean</a></li>
<li><a class="" href="inference-paired-means.html"><span class="header-section-number">18</span> Inference for comparing paired means</a></li>
<li><a class="" href="inference-two-means.html"><span class="header-section-number">19</span> Inference for comparing two independent means</a></li>
<li><a class="" href="inference-num-applications.html"><span class="header-section-number">20</span> Applications: Infer quantitative</a></li>
<li class="book-part">Inference for regression</li>
<li><a class="" href="inference-reg.html"><span class="header-section-number">21</span> Inference for correlation and slope</a></li>
<li><a class="" href="inference-reg-applications.html"><span class="header-section-number">22</span> Applications: Infer regression</a></li>
<li class="book-part">Probability</li>
<li><a class="" href="probability.html"><span class="header-section-number">23</span> Probability with tables</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/MTstateIntroStats/IntroStatTextbook">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="foundations-errors" class="section level1" number="12">
<h1>
<span class="header-section-number">12</span> Errors, power, and practical importance<a class="anchor" aria-label="anchor" href="#foundations-errors"><i class="fas fa-link"></i></a>
</h1>
<!-- ```{block2, type="todo", echo=TRUE} -->
<!-- - power -->
<!-- - what affects power -->
<!-- - what affects width of CIs -->
<!-- - statistical significance vs practical importance -->
<!-- - effect size? -->
<!-- ``` -->
<div class="chapterintro">
<p>Using data to make inferential decisions about larger populations is not a perfect process.
As seen in Chapter <a href="foundations-randomization.html#foundations-randomization">9</a>, a small p-value typically leads the researcher to a decision to reject the null claim or hypothesis.
Sometimes, however, data can produce a small p-value when the null hypothesis is actually true and the data are just inherently variable.
Here we describe the errors which can arise in hypothesis testing, how to define and quantify the different errors, and suggestions for mitigating errors if possible.</p>
</div>
<div id="decerr" class="section level2" number="12.1">
<h2>
<span class="header-section-number">12.1</span> Decision errors<a class="anchor" aria-label="anchor" href="#decerr"><i class="fas fa-link"></i></a>
</h2>
<p>In Chapter <a href="foundations-randomization.html#foundations-randomization">9</a>, we explored the concept of a p-value as a continuum of strength of evidence against a null hypothesis, from 0 (extremely strong evidence against the null) to 1 (no evidence against the null). In some cases, however, a <strong>decision</strong> to the hypothesis test is needed, with the two possible decisions as follows:</p>
<ul>
<li>Reject the null hypothesis</li>
<li>Fail to reject the null hypothesis</li>
</ul>
<div class="workedexample">
<p>For which values of the p-value should you “reject” a null hypothesis? “fail to reject” a null hypothesis?</p>
<hr>
<p>Since a smaller p-value gives you stronger evidence <em>against</em> the null hypothesis, we reject <span class="math inline">\(H_0\)</span> when the p-value is very small, and fail to reject <span class="math inline">\(H_0\)</span> when the p-value is not small.</p>
</div>
<p>Hypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free. Similarly, data can point to the wrong conclusion. However, what distinguishes statistical hypothesis tests from a court system is that our framework allows us to quantify and control how often the data lead us to the incorrect conclusion.</p>
<p>In a hypothesis test, there are two competing hypotheses: the null and the alternative. We make a statement about which one might be true, but we might choose incorrectly.
There are four possible scenarios in a hypothesis test, which are summarized in Table <a href="foundations-errors.html#tab:fourHTScenarios">12.1</a>.</p>
<div class="inline-table"><table class="table table-striped table-condensed" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:fourHTScenarios">Table 12.1: </span>Four different scenarios for hypothesis tests.
</caption>
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="1">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Test conclusion
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
Truth
</th>
<th style="text-align:left;">
Reject null hypothesis
</th>
<th style="text-align:left;">
Fail to reject null hypothesis
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;width: 15em; ">
Null hypothesis is true
</td>
<td style="text-align:left;width: 8em; ">
Type 1 Error
</td>
<td style="text-align:left;width: 8em; ">
Good decision
</td>
</tr>
<tr>
<td style="text-align:left;width: 15em; ">
Alternative hypothesis is true
</td>
<td style="text-align:left;width: 8em; ">
Good decision
</td>
<td style="text-align:left;width: 8em; ">
Type 2 Error
</td>
</tr>
</tbody>
</table></div>
<p>A <strong>Type 1 Error</strong> is rejecting the null hypothesis when <span class="math inline">\(H_0\)</span> is actually true.
Since we rejected the null hypothesis in the <a href="foundations-randomization.html#Martian">Martian alphabet example</a> and <a href="foundations-randomization.html#caseStudySexDiscrimination">sex discrimination case study</a>, it is possible that we made a Type 1 Error in one or both of those studies.</p>
<p>A <strong>Type 2 Error</strong> is failing to reject the null hypothesis when the alternative is actually true. Since we failed to reject the null hypothesis in the <a href="#id_#case-study-med-consult-test">medical consultant</a>, it is possible that we made a Type 2 Error in that study.</p>
<div class="workedexample">
<p>In a US court, the defendant is either innocent (<span class="math inline">\(H_0\)</span>) or guilty (<span class="math inline">\(H_A\)</span>). What does a Type 1 Error represent in this context? What does a Type 2 Error represent? Table <a href="foundations-errors.html#tab:fourHTScenarios">12.1</a> may be useful.</p>
<hr>
<p>If the court makes a Type 1 Error, this means the defendant is innocent (<span class="math inline">\(H_0\)</span> true) but wrongly convicted. A Type 2 Error means the court failed to reject <span class="math inline">\(H_0\)</span> (i.e., failed to convict the person) when they were in fact guilty (<span class="math inline">\(H_A\)</span> true).</p>
</div>
<div class="guidedpractice">
<p>Consider the Martian alphabet study where we concluded students were more likely to say that Bumba was the figure on the left. What would a Type 1 Error represent in this context?<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Making a Type 1 Error in this context would mean that students are equally likely to associate Bumba with the figure on the left as the figure on the right, despite the strong evidence (the data suggesting otherwise) found in the experiment. Notice that this does &lt;em&gt;not&lt;/em&gt; necessarily mean something was wrong with the data or that we made a computational mistake. Sometimes data simply point us to the wrong conclusion, which is why scientific studies are often repeated to check initial findings.&lt;/p&gt;"><sup>107</sup></a></p>
</div>
<div class="workedexample">
<p>How could we reduce the Type 1 Error rate in US courts? What influence would this have on the Type 2 Error rate?</p>
<hr>
<p>To lower the Type 1 Error rate, we might raise our standard for conviction from “beyond a reasonable doubt” to “beyond a conceivable doubt” so fewer people would be wrongly convicted. However, this would also make it more difficult to convict the people who are actually guilty, so we would make more Type 2 Errors.</p>
</div>
<div class="workedexample">
<p>How could we reduce the Type 1 Error rate in US courts?
What influence would this have on the Type 2 Error rate?</p>
<hr>
<p>To lower the Type 1 Error rate, we might raise our standard for conviction from “beyond a reasonable doubt” to “beyond a conceivable doubt” so fewer people would be wrongly convicted.
However, this would also make it more difficult to convict the people who are actually guilty, so we would make more Type 2 Errors.</p>
</div>
<div class="guidedpractice">
<p>How could we reduce the Type 2 Error rate in US courts?
What influence would this have on the Type 1 Error rate?<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;To lower the Type 2 Error rate, we want to convict more guilty people.
We could lower the standards for conviction from “beyond a reasonable doubt” to “beyond a little doubt”.
Lowering the bar for guilt will also result in more wrongful convictions, raising the Type 1 Error rate.&lt;/p&gt;"><sup>108</sup></a></p>
</div>
<p>The example and guided practice above provide an important lesson: if we reduce how often we make one type of error, we generally make more of the other type.</p>
</div>
<div id="significance-level" class="section level2" number="12.2">
<h2>
<span class="header-section-number">12.2</span> Significance level<a class="anchor" aria-label="anchor" href="#significance-level"><i class="fas fa-link"></i></a>
</h2>
<p>The <strong>significance level</strong> provides the cutoff for the p-value which will lead to a decision of “reject the null hypothesis”.
When the p-value is less than the significance level, we say the results are <strong>statistically significant</strong>.
This means the data provide such strong evidence against <span class="math inline">\(H_0\)</span> that we reject the null hypothesis in favor of the alternative hypothesis.</p>
<div class="onebox">
<p><strong>Significance level = probability of making a Type 1 error.</strong></p>
<p>We reject a null hypothesis if the p-value is less than a chosen significance level, <span class="math inline">\(\alpha\)</span>. Therefore, if the null hypothesis is true, but we end up with really unusual data just by chance—a p-value less than <span class="math inline">\(\alpha\)</span>—then we mistakenly reject the null hypothesis, making a Type 1 error.</p>
</div>
<p>The significance level should be chosen depending on the field or the application and the real-life consequences of an incorrect decision.
The traditional level is 0.05, but, as discussed in Section <a href="foundations-randomization.html#p-value-stat-signif">9.3.2</a>, this choice is somewhat arbitrary—there is nothing special about this particular value.
We should select a level that is smaller or larger than 0.05 depending on the consequences of any conclusions reached from the test.</p>
<p>If making a Type 1 Error is dangerous or especially costly, we should choose a small significance level (e.g., 0.01 or 0.001).
If we want to be very cautious about rejecting the null hypothesis, we demand very strong evidence favoring the alternative <span class="math inline">\(H_A\)</span> before we would reject <span class="math inline">\(H_0.\)</span></p>
<p>If a Type 2 Error is relatively more dangerous or much more costly than a Type 1 Error, then we should choose a higher significance level (e.g., 0.10).
Here we want to be cautious about failing to reject <span class="math inline">\(H_0\)</span> when the null is actually false.</p>
<div class="tip">
<p><strong>Significance levels should reflect consequences of errors.</strong></p>
<p>The significance level selected for a test should reflect the real-world consequences associated with making a Type 1 or Type 2 Error.</p>
</div>
</div>
<div id="two-sided-tests" class="section level2" number="12.3">
<h2>
<span class="header-section-number">12.3</span> Two-sided hypotheses<a class="anchor" aria-label="anchor" href="#two-sided-tests"><i class="fas fa-link"></i></a>
</h2>
<p></p>
<p>In Chapter <a href="foundations-randomization.html#foundations-randomization">9</a> we explored whether women were discriminated against.
In this case study, however, we have ignored the possibility that <em>men</em> are actually discriminated against. This possibility wasn’t considered in our original hypotheses or analyses.
The disregard of the extra alternatives may have seemed natural since we expected the data to point in the direction in which we framed the problem.
However, there are two dangers if we ignore possibilities that disagree with prior beliefs or that conflict with our world view:</p>
<ol style="list-style-type: decimal">
<li><p>Framing an alternative hypothesis simply to match the direction that the data are expected to point will generally inflate the Type 1 Error rate.
After all the work we have done (and will continue to do) to rigorously control the error rates in hypothesis tests, careless construction of the alternative hypotheses can disrupt that hard work.</p></li>
<li><p>If we only use alternative hypotheses that agree with our worldview, then we are going to be subjecting ourselves to <strong>confirmation bias</strong>, which means we are looking for data that supports our ideas.
That’s not very scientific, and we can do better!</p></li>
</ol>
<p>The hypotheses we have seen in the past two chapters are called <strong>one-sided hypothesis tests</strong> because they only explored one direction of possibilities.
Such hypotheses are appropriate when we are exclusively interested in the single direction, but usually we want to consider all possibilities.</p>
<p>Consider the situation of the <a href="foundations-bootstrapping.html#case-study-med-consult-test">medical consultant</a>. The setting has been framed in the context of the consultant being helpful. This original hypothesis is a one-sided hypothesis test because it only explored whether the consultant’s patients had a complication rate <em>below</em> 10%.</p>
<p>But what if the consultant actually performed <em>worse</em> than the average? Would we care? More than ever! Since it turns out that we care about a finding in either direction, we should run a two-sided hypothesis test.</p>
<div class="workedexample">
<p>Form hypotheses to conduct a two-sided test for the medical consultant case study in plain and statistical language. Let <span class="math inline">\(\pi\)</span> represent the true complication rate of organ donors who work with this medical consultant.</p>
<hr>
<p>We want to understand whether the medical consultant is helpful or harmful. We’ll consider both of these possibilities using a two-sided hypothesis test.</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: There is no association between the consultant’s contributions and the clients’ complication rate, i.e., <span class="math inline">\(\pi = 0.10\)</span></p></li>
<li><p><span class="math inline">\(H_A\)</span>: There is an association, either positive or negative, between the consultant’s contributions and the clients’ complication rate, i.e., <span class="math inline">\(\pi \neq 0.10\)</span>.</p></li>
</ul>
<p>Compare this to the one-sided hypothesis test, when the hypotheses were:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: There is no association between the consultant’s contributions and the clients’ complication rate, i.e., <span class="math inline">\(\pi = 0.10\)</span>.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: Patients who work with the consultant tend to have a complication rate lower than 10%, i.e., <span class="math inline">\(\pi &lt; 0.10\)</span>.</p></li>
</ul>
</div>
<p>There were 62 patients who worked with this medical consultant, 3 of which developed complications from their organ donation, for a point estimate of <span class="math inline">\(\hat{p} = \frac{3}{62} = 0.048\)</span>.</p>
<p>According to the point estimate, the complication rate for clients of this medical consultant is 5.2% below the expected complication rate of 10%. However, we wonder if this difference could be easily explainable by chance.</p>
<p>Recall in Section <a href="foundations-bootstrapping.html#case-study-med-consult-test">10.3</a>, we simulated what proportions we might see from chance alone under the null hypothesis. By using marbles, cards, or a spinner to reflect the null hypothesis, we can simulate what would happen to 62 ‘patients’ <em>if the true complication rate is 10%</em>. After repeating this simulation 10,000 times, we can build a <strong>null distribution</strong> of the sample proportions shown in Figure <a href="foundations-errors.html#fig:nullDistForPHatIfLiverTransplantConsultantIsNotHelpful2">12.1</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:nullDistForPHatIfLiverTransplantConsultantIsNotHelpful2"></span>
<img src="12-foundations-errors_files/figure-html/nullDistForPHatIfLiverTransplantConsultantIsNotHelpful2-1.png" alt="The null distribution for $\hat{p}$, created from 10,000 simulated studies. " width="90%"><p class="caption">
Figure 12.1: The null distribution for <span class="math inline">\(\hat{p}\)</span>, created from 10,000 simulated studies.
</p>
</div>
<p>The original hypothesis, investigating if the medical consultant was helpful, was a one-sided hypothesis test (<span class="math inline">\(H_A: \pi &lt; 0.10\)</span>) so we only counted the simulations <em>below</em> our observed proportion of 0.048 in order to calculate the p-value of 0.1222 or 12.22%. However, the p-value of this two-sided hypothesis test investigating if the medical consultant is helpful or harmful is not 0.1222!</p>
<p>The p-value is defined as the chance we observe a result <em>at least as favorable to the alternative hypothesis as the result</em> (i.e., the proportion) we observe. For a two-sided hypothesis test, that means finding the proportion of simulations <em>further in either tail</em> than the observed result, or beyond a point that is <em>equi-distant from the null hypothesis</em> as the observed result.</p>
<p>In this case, the observed proportion of 0.048 is 0.052 below the null hypothesized value of 0.10. So while we will continue to count the 0.1222 simulations at or below 0.048, we must add to that the proportion of simulations that are <strong>at or above</strong> <span class="math inline">\(0.10 + 0.052 = 0.152\)</span> in order to obtain the p-value.</p>
<p>In Figure <a href="foundations-errors.html#fig:Medical-consultant-two-sided">12.2</a> we’ve also shaded these differences in the right tail of the distribution. These two shaded tails provide a visual representation of the p-value for a two-sided test.</p>
<!--
%There is something different in this study than in the past studies: in this study, we are particularly interested in whether blood thinners increase *or* decrease the risk of death in patients who undergo CPR before arriving at the hospital.\footnote{Realistically, we probably are interested in either direction in the past studies as well, and so we should have used the approach we now discuss in this section. However, for simplicity and the sake of not introducing too many concepts at once, we skipped over these details in earlier sections.} For example, there are chance differences of $\hat{p}_t - \hat{p}_c = -0.14$, that would have been stronger evidence against the null hypothesis as our observed difference of +0.13. Likewise, anything less than or equal -0.13 would provide as much evidence against the null hypothesis as +0.13, and for this reason, we must count both tails towards the p-value, as shown in Figure \ref{CPR-study-p-value}.
-->
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:Medical-consultant-two-sided"></span>
<img src="12-foundations-errors_files/figure-html/Medical-consultant-two-sided-1.png" alt="The null distribution for $\hat{p}$, created from 10,000 simulated studies. All simulations that are at least as far from the null value of 0.10 as the observed proportion (i.e., those below 0.048 and those above 0.152) are shaded." width="90%"><p class="caption">
Figure 12.2: The null distribution for <span class="math inline">\(\hat{p}\)</span>, created from 10,000 simulated studies. All simulations that are at least as far from the null value of 0.10 as the observed proportion (i.e., those below 0.048 and those above 0.152) are shaded.
</p>
</div>
<p>From our previous simulation, we know that 12.22% of the simulations lie at or below the observed proportion of 0.048. Figure <a href="foundations-errors.html#fig:Medical-consultant-two-sided">12.2</a> shows that an additional 0.0811 or 8.11% of simulations fall at or above 0.152. This indicates the p-value for this two-sided test is <span class="math inline">\(0.1222 + 0.0811 = 0.2033\)</span>. With this large p-value, we do not find statistically significant evidence that the medical consultant’s patients had a complication rate different from 10%.</p>
<p>In Section <a href="foundations-mathematical.html#CLTsection">11.1</a>, we learned that the null distribution will be symmetric under certain conditions. When the null distribution is symmetric, we can find a two-sided p-value by merely taking the single tail (in this case, 0.1222) and double it to get the two-sided p-value: 0.2444. Note that the example here does not satisfy the conditions and the null distribution in Figure <a href="foundations-errors.html#fig:Medical-consultant-two-sided">12.2</a> is not symmetric. Thus, the result of a ‘doubled’ one-sided p-value of 0.2444 is not a good estimate of the actual two-sided p-value of 0.2033.</p>
<p></p>
<div class="onebox">
<p><strong>Default to a two-sided test.</strong></p>
<p>We want to be rigorous and keep an open mind when we analyze data and evidence. Use a one-sided hypothesis test only if you truly have interest in only one direction.</p>
</div>
<div class="onebox">
<p><strong>Computing a p-value for a two-sided test.</strong></p>
<p>If your null distribution is symmetric, first compute the p-value for one tail of the distribution, then double that value to get the two-sided p-value. That’s it!<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;If the null distribution is not symmetric, then the computer will have to count the proportions in each tail separately, since the two tail proportions may differ.&lt;/p&gt;"><sup>109</sup></a></p>
</div>
<p>Generally, to find a two-sided p-value we double the single tail area, which remains a reasonable approach even when the sampling distribution is asymmetric. However, the approach can result in p-values larger than 1 when the point estimate is very near the mean in the null distribution; in such cases, we write that the p-value is 1. Also, very large p-values computed in this way (e.g., 0.85), may also be slightly inflated. Typically, we do not worry too much about the precision of very large p-values because they lead to the same analysis conclusion, even if the value is slightly off.</p>
</div>
<div id="controlling-the-type-1-error-rate" class="section level2" number="12.4">
<h2>
<span class="header-section-number">12.4</span> Controlling the Type 1 error rate<a class="anchor" aria-label="anchor" href="#controlling-the-type-1-error-rate"><i class="fas fa-link"></i></a>
</h2>
<p>Now that we understand the difference between one-sided and two-sided tests, we must recognize when to use each type of test.
Because of the result of increased error rates, it is never okay to change two-sided tests to one-sided tests after observing the data.
We explore the consequences of ignoring this advice in the next example.</p>
<div class="workedexample">
<p>Using <span class="math inline">\(\alpha=0.05,\)</span> we show that freely switching from two-sided tests to one-sided tests will lead us to make twice as many Type 1 Errors as intended.</p>
<hr>
<p>Suppose we are interested in finding any difference from 0.
We’ve created a smooth-looking <strong>null distribution</strong> representing differences due to chance in Figure <a href="foundations-errors.html#fig:type1ErrorDoublingExampleFigure">12.3</a>.</p>
<p>Suppose the sample difference was larger than 0.
Then if we can flip to a one-sided test, we would use <span class="math inline">\(H_A:\)</span> difference <span class="math inline">\(&gt; 0.\)</span> Now if we obtain any observation in the upper 5% of the distribution, we would reject <span class="math inline">\(H_0\)</span> since the p-value would just be a the single tail.
Thus, if the null hypothesis is true, we incorrectly reject the null hypothesis about 5% of the time when the sample mean is above the null value, as shown in Figure <a href="foundations-errors.html#fig:type1ErrorDoublingExampleFigure">12.3</a>.</p>
<p>Suppose the sample difference was smaller than 0.
Then if we change to a one-sided test, we would use <span class="math inline">\(H_A:\)</span> difference <span class="math inline">\(&lt; 0.\)</span> If the observed difference falls in the lower 5% of the figure, we would reject <span class="math inline">\(H_0.\)</span> That is, if the null hypothesis is true, then we would observe this situation about 5% of the time.</p>
<p>By examining these two scenarios, we can determine that we will make a Type 1 Error <span class="math inline">\(5\%+5\%=10\%\)</span> of the time if we are allowed to swap to the “best” one-sided test for the data.
This is twice the error rate we prescribed with our significance level: <span class="math inline">\(\alpha=0.05\)</span> (!).</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:type1ErrorDoublingExampleFigure"></span>
<img src="12-foundations-errors_files/figure-html/type1ErrorDoublingExampleFigure-1.png" alt="The shaded regions represent areas where we would reject $H_0$ under the bad practices considered in when $\alpha = 0.05.$" width="90%"><p class="caption">
Figure 12.3: The shaded regions represent areas where we would reject <span class="math inline">\(H_0\)</span> under the bad practices considered in when <span class="math inline">\(\alpha = 0.05.\)</span>
</p>
</div>
<div class="caution">
<p><strong>Hypothesis tests should be set up <em>before</em> seeing the data.</strong></p>
<p>After observing data, it is tempting to turn a two-sided test into a one-sided test.
Avoid this temptation.
Hypotheses should be set up <em>before</em> observing the data.</p>
</div>
</div>
<div id="power" class="section level2" number="12.5">
<h2>
<span class="header-section-number">12.5</span> Power<a class="anchor" aria-label="anchor" href="#power"><i class="fas fa-link"></i></a>
</h2>
<p>Although we won’t go into extensive detail here, power is an important topic for follow-up consideration after understanding the basics of hypothesis testing.
A good power analysis is a vital preliminary step to any study as it will inform whether the data you collect are sufficient for being able to conclude your research broadly.</p>
<p>Often times in experiment planning, there are two competing considerations:</p>
<ul>
<li>We want to collect enough data that we can detect important effects.</li>
<li>Collecting data can be expensive, and, in experiments involving people, there may be some risk to patients.</li>
</ul>
<p>When planning a study, we want to know how likely we are to detect an effect we care about.
In other words, if there is a real effect, and that effect is large enough that it has practical value, then what is the probability that we detect that effect?
This probability is called the <strong>power</strong>, and we can compute it for different sample sizes or different effect sizes.</p>
<div class="important">
<p><strong>Power.</strong></p>
<p>The power of the test is the probability of rejecting the null claim when the alternative claim is true.</p>
<p>How easy it is to detect the effect depends on both how big the effect is (e.g., how good the medical treatment is) as well as the sample size.</p>
</div>
<p>We think of power as the probability that you will become rich and famous from your science.
In order for your science to make a splash, you need to have good ideas!
That is, you won’t become famous if you happen to find a single Type 1 error which rejects the null hypothesis.
Instead, you’ll become famous if your science is very good and important (that is, if the alternative hypothesis is true).
The better your science is (i.e., the better the medical treatment), the larger the <em>effect size</em> and the easier it will be for you to convince people of your work.</p>
<p>Not only does your science need to be solid, but you also need to have evidence (i.e., data) that shows the effect.
A few observations (e.g., <span class="math inline">\(n = 2)\)</span> is unlikely to be convincing because of well known ideas of natural variability.
Indeed, the larger the data set which provides evidence for your scientific claim, the more likely you are to convince the community that your idea is correct.</p>
<div class="underconstruction">
<p>TODO - sections below are from old Chapter 5 - go through and cut/move</p>
</div>
<p>When the null hypothesis is true, the probability of a Type 1 error is our chosen significance level, <span class="math inline">\(\alpha\)</span>, which means the probability of a correct decision is <span class="math inline">\(1 - \alpha\)</span>.</p>
<p>If an alternative hypothesis is true, the probability of a Type 2 error, which we denote by <span class="math inline">\(\beta\)</span>, depends on several components:</p>
<ul>
<li>significance level</li>
<li>sample size</li>
<li>whether the alternative hypothesis is one-sided or two-sided</li>
<li>standard deviation of the statistic</li>
<li>how far the alternative parameter value is from the null value</li>
</ul>
<p>Only the first three of these components are within the control of the researcher.</p>
<p>The probability of a correct decision when the alternative hypothesis is true, <span class="math inline">\(1 - \beta\)</span>, is called the <strong>power</strong> of the test. Higher <strong>power</strong> means we are more likely to detect an effect that actually exists.</p>
<div class="onebox">
<p><strong>Power.</strong></p>
<p>The <strong>power</strong> of a test is the probability of rejecting a false null hypothesis.</p>
</div>
<div class="workedexample">
<p>Suppose we would like to test whether less than 65% of a large population approves of a new law: <span class="math inline">\(H_0: \pi = 0.65\)</span> versus <span class="math inline">\(H_A: \pi &lt; 0.65\)</span>. We collect a random sample of <span class="math inline">\(n = 200\)</span> individuals from this population. For what values of the sample proportion, <span class="math inline">\(\hat{p}\)</span>, would we reject <span class="math inline">\(H_0\)</span> using a significance level of <span class="math inline">\(\alpha = 0.05\)</span>?</p>
<hr>
<p>Under the assumption of the null hypothesis, the standard deviation of <span class="math inline">\(\hat{p}\)</span> is <span class="math inline">\(\sqrt{0.65(1-0.65)/200} = 0.0337\)</span>. Thus, by the Central Limit Theorem, sample proportions vary according to an approximate normal distribution with mean 0.65 and standard deviation 0.0337. We will reject the null hypothesis that the true proportion is 0.65 if the sample proportion is so low that its probability is less than 0.05, shown in Figure <a href="foundations-errors.html#fig:power-example">12.4</a>.</p>
<p>To be precise, we will reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\hat{p}\)</span> is less than the 5th percentile of the null distribution: <code>qnorm(0.05, 0.65, 0.0337)</code> = 0.59.</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:power-example"></span>
<img src="05/figures/PowerExample.png" alt="Shaded area on a null distribution where we would reject the null hypothesis. This area is equal to the significance level." width="80%"><p class="caption">
Figure 12.4: Shaded area on a null distribution where we would reject the null hypothesis. This area is equal to the significance level.
</p>
</div>
<p>To calculate power, we need to know the true value of the parameter. In the previous example, the alternative was <span class="math inline">\(H_0: \pi &lt; 0.65\)</span>, so if we just say the alternative hypothesis is true, we still do not know the value of <span class="math inline">\(\pi\)</span>. Thus, power calculations are done for a specific value of the parameter, and the power changes if the value of the parameter changes.</p>
<div class="workedexample">
<p>Consider again the test of whether less than 65% of a large population approves of a new law: <span class="math inline">\(H_0: \pi = 0.65\)</span> versus <span class="math inline">\(H_A: \pi &lt; 0.65\)</span>. Suppose the population approval rate is actually <span class="math inline">\(\pi = 0.58\)</span>. What is the probability that we will detect this effect?</p>
<hr>
<p>This example asks us to calculate the power – the probability our test will provide evidence that <span class="math inline">\(\pi &lt; 0.65\)</span> when the true value of <span class="math inline">\(\pi\)</span> is 0.58. Recall from the previous example that we will reject the null if <span class="math inline">\(\hat{p} &lt; 0.59\)</span>. Thus, the power is the probability that <span class="math inline">\(\hat{p}\)</span> will be less than 0.59 when the true proportion is 0.58: <code>pnorm(0.59, 0.58, 0.0337)</code> = 0.62. There is only a 62% chance that the data we collect will provide strong enough evidence to conclude <span class="math inline">\(\pi &lt; 0.65\)</span>. This probability is represented by the red area in Figure <a href="foundations-errors.html#fig:power-example-2">12.5</a>.</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:power-example-2"></span>
<img src="05/figures/PowerExample2.png" alt="The blue distribution is the distribution of sample proportions if the null hypothesis is true, $\pi = 0.65$ -- the blue shaded area represents the probability we reject a true null hypothesis. The red distribution is the distribution of sample proportions under a particular alternative hypothesis, that $\pi = 0.58$ -- the red shaded area represents the power." width="80%"><p class="caption">
Figure 12.5: The blue distribution is the distribution of sample proportions if the null hypothesis is true, <span class="math inline">\(\pi = 0.65\)</span> – the blue shaded area represents the probability we reject a true null hypothesis. The red distribution is the distribution of sample proportions under a particular alternative hypothesis, that <span class="math inline">\(\pi = 0.58\)</span> – the red shaded area represents the power.
</p>
</div>
<div class="onebox">
<p><strong>Increasing power.</strong></p>
<p>The <strong>power</strong> of a test will <em>increase</em> when:</p>
<ul>
<li>the significance level <em>increases</em>
</li>
<li>the sample size <em>increases</em>
</li>
<li>we change from a two-sided to a one-sided test</li>
<li>the standard deviation of the statistic <em>decreases</em>
</li>
<li>how far the alternative parameter value is from the null value <em>increases</em>
</li>
</ul>
</div>
<div class="inline-figure"><img src="05/images/whalberg.png" width="40%" style="display: block; margin: auto;"></div>
</div>
<div id="statistical-significance-vs.-practical-importance" class="section level2" number="12.6">
<h2>
<span class="header-section-number">12.6</span> Statistical significance vs. practical importance<a class="anchor" aria-label="anchor" href="#statistical-significance-vs.-practical-importance"><i class="fas fa-link"></i></a>
</h2>
<div class="workedexample">
<p>An Austrian study of heights of 507,125 military recruits reported that men born in spring were statistically significantly taller than men born in the fall (p-value &lt; 0.0001). A confidence interval for the true difference in mean height between men born in spring and men born in fall was (0.598, 0.602) cm. Is this result practically important?</p>
<hr>
<p>No, these results don’t mean much in this context – a difference in average height of around 0.6 cm would not even be noticeable by the human eye! Just because a result is statistically significant does not mean that it is necessarily practically important – meaningful in the context of the problem.</p>
</div>
<p>In the previous example, we saw two groups of men that differed in average height, and that difference was <strong>statistically significant</strong> – that is, the observed difference in sample means of 0.6 cm is very unlikely to occur <em>if the true difference in average height was zero</em>. But, a difference of 0.6 cm in height is not meaningful – not <strong>practically important</strong>.</p>
<p>Why did this happen? Recall that the variability in sample statistics decreases as the sample size increases. For example, unknown to you, suppose a slight majority of a population, say 50.5%, support a new ballot measure. You want to test <span class="math inline">\(H_0: \pi = 0.50\)</span> versus <span class="math inline">\(H_0: \pi &gt; 0.50\)</span> for this population. Since the true proportion is not exactly 0.50, you can make your p-value smaller than any given significance level as long as you choose a large enough sample size!</p>
<p>Figure <a href="foundations-errors.html#fig:stat-signif">12.6</a> displays this scenario. The distribution of possible sample proportions who support the new ballot measure in samples of size <span class="math inline">\(n = 100,000\)</span> when 50.5% of the population supports the measure is represented by the black normal curve. The dotted red normal curve is the null distribution of sample proportions for <span class="math inline">\(H_0: \pi = 0.5\)</span>. There is very little overlap between the two distributions due to the very large sample size. The shaded blue area represents the power of the test of <span class="math inline">\(H_0: \pi = 0.5\)</span> versus <span class="math inline">\(H_A: \pi &gt; 0.5\)</span> when <span class="math inline">\(\alpha = 0.05\)</span> – 0.885! That is, we have an 88.5% chance that our p-value will be less than 0.05, even though the true proportion is only 0.05 above 0.5!</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:stat-signif"></span>
<img src="12-foundations-errors_files/figure-html/stat-signif-1.png" alt="Black curve: sampling distribution of sample proportions from samples of size 100,000 when the true proportion is 0.505. Red curve: null distribution of sample proportions for a null value of 0.50." width="100%"><p class="caption">
Figure 12.6: Black curve: sampling distribution of sample proportions from samples of size 100,000 when the true proportion is 0.505. Red curve: null distribution of sample proportions for a null value of 0.50.
</p>
</div>
<!-- Someday - annotate these plots better -->
<div class="guidedpractice">
<p>If p-values can be made arbitrarily small with large sample sizes, what might tend to happen with small sample sizes? Would small sample sizes be more likely to give practically important results that are not statistically significant? or statistically significant results that are not practically important?<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Since hypothesis tests with small sample sizes typically have low power, small sample sizes can result in practically important results that are not statistically significant.&lt;/p&gt;"><sup>110</sup></a></p>
</div>
<p>Consider the opposite scenario – small sample sizes with a meaningful difference. Suppose again that you would like to determine if a majority of a population support a new ballot measure. However, you only have the time and money to survey 20 people in the community. Unknown to you, 65% of the population support the measure.</p>
<p>Examine Figure <a href="foundations-errors.html#fig:prac-signif">12.7</a>. The distribution of possible sample proportions who support the new ballot measure in samples of size <span class="math inline">\(n = 20\)</span> when 65% of the population supports the measure is represented by the black normal curve. The dotted red normal curve is the null distribution of sample proportions for <span class="math inline">\(H_0: \pi = 0.5\)</span>. Even though 0.65 is quite a bit higher than 0.50, there is still a lot of overlap between the two distributions due to the small sample size. The shaded blue area represents the power of the test of <span class="math inline">\(H_0: \pi = 0.5\)</span> versus <span class="math inline">\(H_A: \pi &gt; 0.5\)</span> when <span class="math inline">\(\alpha = 0.05\)</span> – only 0.29! That is, even though 65% of the population supports the measure (much higher than 50%), we only have a 29% chance of detecting that difference with our small sample size.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:prac-signif"></span>
<img src="12-foundations-errors_files/figure-html/prac-signif-1.png" alt="Black curve: approximate sampling distribution of sample proportions from samples of size 20 when the true proportion is 0.65. Red curve: approximate null distribution of sample proportions for a null value of 0.50." width="100%"><p class="caption">
Figure 12.7: Black curve: approximate sampling distribution of sample proportions from samples of size 20 when the true proportion is 0.65. Red curve: approximate null distribution of sample proportions for a null value of 0.50.
</p>
</div>
<div class="onebox">
<p><strong>Statistical significance versus practical importance.</strong></p>
<p><em>For large sample sizes, results may be statistically significant, but not practically important.</em> Since sample statistics vary very little among samples with large sample sizes, it is easy for a hypothesis test to result in a very small p-value, even if the observed effect is practically meaningless.</p>
<p><em>For small sample sizes, results may be practically important, but not statistically significant.</em> Since studies with small sample sizes tend to have very low power, it is difficult for a hypothesis test to result in a very small p-value, even if the observed effect is quite large.</p>
</div>
</div>
<div id="chp12-review" class="section level2" number="12.7">
<h2>
<span class="header-section-number">12.7</span> Chapter review<a class="anchor" aria-label="anchor" href="#chp12-review"><i class="fas fa-link"></i></a>
</h2>
<div id="summary-9" class="section level3 unnumbered">
<h3>Summary<a class="anchor" aria-label="anchor" href="#summary-9"><i class="fas fa-link"></i></a>
</h3>
<div class="underconstruction">
<p>TODO</p>
</div>
</div>
<div id="terms-10" class="section level3 unnumbered">
<h3>Terms<a class="anchor" aria-label="anchor" href="#terms-10"><i class="fas fa-link"></i></a>
</h3>
<p>We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as <strong>bolded text</strong>.</p>
<div class="inline-table"><table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;"><tbody>
<tr>
<td style="text-align:left;">
confirmation bias
</td>
<td style="text-align:left;">
practical importance
</td>
<td style="text-align:left;">
two-sided hypothesis test
</td>
</tr>
<tr>
<td style="text-align:left;">
one-sided hypothesis test
</td>
<td style="text-align:left;">
significance level
</td>
<td style="text-align:left;">
Type 1 Error
</td>
</tr>
<tr>
<td style="text-align:left;">
power
</td>
<td style="text-align:left;">
statistical significance
</td>
<td style="text-align:left;">
Type 2 Error
</td>
</tr>
</tbody></table></div>
</div>
<div id="key-ideas-9" class="section level3 unnumbered">
<h3>Key ideas<a class="anchor" aria-label="anchor" href="#key-ideas-9"><i class="fas fa-link"></i></a>
</h3>
<div class="underconstruction">
<p>TODO</p>
</div>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="foundations-mathematical.html"><span class="header-section-number">11</span> Inference with mathematical models</a></div>
<div class="next"><a href="foundations-applications.html"><span class="header-section-number">13</span> Applications: Foundations</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#foundations-errors"><span class="header-section-number">12</span> Errors, power, and practical importance</a></li>
<li><a class="nav-link" href="#decerr"><span class="header-section-number">12.1</span> Decision errors</a></li>
<li><a class="nav-link" href="#significance-level"><span class="header-section-number">12.2</span> Significance level</a></li>
<li><a class="nav-link" href="#two-sided-tests"><span class="header-section-number">12.3</span> Two-sided hypotheses</a></li>
<li><a class="nav-link" href="#controlling-the-type-1-error-rate"><span class="header-section-number">12.4</span> Controlling the Type 1 error rate</a></li>
<li><a class="nav-link" href="#power"><span class="header-section-number">12.5</span> Power</a></li>
<li><a class="nav-link" href="#statistical-significance-vs.-practical-importance"><span class="header-section-number">12.6</span> Statistical significance vs. practical importance</a></li>
<li>
<a class="nav-link" href="#chp12-review"><span class="header-section-number">12.7</span> Chapter review</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#summary-9">Summary</a></li>
<li><a class="nav-link" href="#terms-10">Terms</a></li>
<li><a class="nav-link" href="#key-ideas-9">Key ideas</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/MTstateIntroStats/IntroStatTextbook/blob/master/12-foundations-errors.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/MTstateIntroStats/IntroStatTextbook/edit/master/12-foundations-errors.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Montana State Introductory Statistics with R</strong>" was written by Nicole Carnegie, Stacey Hancock, Elijah Meyer, Jade Schmidt, Melinda Yager. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>

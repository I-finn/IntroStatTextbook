<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 14 Inference for a single proportion | Montana State Introductory Statistics with R</title>
<meta name="author" content="Stacey Hancock, Nicole Carnegie, Elijah Meyer, Jade Schmidt, Melinda Yager">
<meta name="description" content="Focusing now on statistical inference for categorical data, we will revisit many of the foundational aspects of hypothesis testing from Chapter 9. The three data structures we detail are one...">
<meta name="generator" content="bookdown 0.27 with bs4_book()">
<meta property="og:title" content="Chapter 14 Inference for a single proportion | Montana State Introductory Statistics with R">
<meta property="og:type" content="book">
<meta property="og:url" content="https://mtstateintrostats.github.io/IntroStatTextbook/inference-one-prop.html">
<meta property="og:description" content="Focusing now on statistical inference for categorical data, we will revisit many of the foundational aspects of hypothesis testing from Chapter 9. The three data structures we detail are one...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 14 Inference for a single proportion | Montana State Introductory Statistics with R">
<meta name="twitter:description" content="Focusing now on statistical inference for categorical data, we will revisit many of the foundational aspects of hypothesis testing from Chapter 9. The three data structures we detail are one...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet">
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script><script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script><script src="libs/plotly-binding-4.10.0/plotly.js"></script><script src="libs/typedarray-0.1/typedarray.min.js"></script><link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script><link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script><script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="css/ims-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Montana State Introductory Statistics with R</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="authors.html">Authors</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li><a class="" href="rstudio.html">Preliminaries: Getting started in RStudio</a></li>
<li class="book-part">Introduction to data</li>
<li><a class="" href="data-hello.html"><span class="header-section-number">1</span> Hello data</a></li>
<li><a class="" href="data-design.html"><span class="header-section-number">2</span> Study design</a></li>
<li><a class="" href="data-applications.html"><span class="header-section-number">3</span> Applications: Data</a></li>
<li class="book-part">Exploratory data analysis</li>
<li><a class="" href="explore-categorical.html"><span class="header-section-number">4</span> Exploring categorical data</a></li>
<li><a class="" href="explore-numerical.html"><span class="header-section-number">5</span> Exploring quantitative data</a></li>
<li><a class="" href="explore-regression.html"><span class="header-section-number">6</span> Correlation and regression</a></li>
<li><a class="" href="explore-mult-reg.html"><span class="header-section-number">7</span> Multivariable models</a></li>
<li><a class="" href="explore-applications.html"><span class="header-section-number">8</span> Applications: Explore</a></li>
<li class="book-part">Foundations of inference</li>
<li><a class="" href="foundations-randomization.html"><span class="header-section-number">9</span> Hypothesis testing with randomization</a></li>
<li><a class="" href="foundations-bootstrapping.html"><span class="header-section-number">10</span> Confidence intervals with bootstrapping</a></li>
<li><a class="" href="foundations-mathematical.html"><span class="header-section-number">11</span> Inference with mathematical models</a></li>
<li><a class="" href="foundations-errors.html"><span class="header-section-number">12</span> Errors, power, and practical importance</a></li>
<li><a class="" href="foundations-applications.html"><span class="header-section-number">13</span> Applications: Foundations</a></li>
<li class="book-part">Inference for categorical data</li>
<li><a class="active" href="inference-one-prop.html"><span class="header-section-number">14</span> Inference for a single proportion</a></li>
<li><a class="" href="inference-two-props.html"><span class="header-section-number">15</span> Inference for comparing two proportions</a></li>
<li><a class="" href="inference-categ-applications.html"><span class="header-section-number">16</span> Applications: Infer categorical</a></li>
<li class="book-part">Inference for quantitative data</li>
<li><a class="" href="inference-one-mean.html"><span class="header-section-number">17</span> Inference for a single mean</a></li>
<li><a class="" href="inference-paired-means.html"><span class="header-section-number">18</span> Inference for comparing paired means</a></li>
<li><a class="" href="inference-two-means.html"><span class="header-section-number">19</span> Inference for comparing two independent means</a></li>
<li><a class="" href="inference-num-applications.html"><span class="header-section-number">20</span> Applications: Infer quantitative</a></li>
<li class="book-part">Inference for regression</li>
<li><a class="" href="inference-reg.html"><span class="header-section-number">21</span> Inference for correlation and slope</a></li>
<li><a class="" href="inference-reg-applications.html"><span class="header-section-number">22</span> Applications: Infer regression</a></li>
<li class="book-part">Probability</li>
<li><a class="" href="probability.html"><span class="header-section-number">23</span> Probability with tables</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/MTstateIntroStats/IntroStatTextbook">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="inference-one-prop" class="section level1" number="14">
<h1>
<span class="header-section-number">14</span> Inference for a single proportion<a class="anchor" aria-label="anchor" href="#inference-one-prop"><i class="fas fa-link"></i></a>
</h1>
<div class="chapterintro">
<p>Focusing now on statistical inference for categorical data, we will revisit many of the foundational aspects of hypothesis testing from Chapter <a href="foundations-randomization.html#foundations-randomization">9</a>.</p>
<p>The three data structures we detail are one binary variable, summarized using a single proportion, and two binary variables, summarized using a difference of two proportions.
When appropriate, each of the data structures will be analyzed using the simulation-based inferential methods described in Chapters <a href="foundations-randomization.html#foundations-randomization">9</a> and <a href="foundations-bootstrapping.html#foundations-bootstrapping">10</a>, and the theory-based methods covered in Chapter <a href="foundations-mathematical.html#foundations-mathematical">11</a>.</p>
<p>As we build on the inferential ideas, we will visit new foundational concepts in statistical inference.
For example, we will cover the conditions for when a normal model is appropriate; the two different error rates in hypothesis testing; and choosing the confidence level for a confidence interval.</p>
</div>
<p>We encountered statistical inference methods for a single proportion in the Martian alphabet example of Section <a href="foundations-randomization.html#Martian">9.1</a> and the Medical consultant case study of Section <a href="foundations-bootstrapping.html#case-study-med-consult">10.1</a>, exploring point estimates, confidence intervals, and hypothesis tests.
In this section, we’ll do a review of these topics when collecting data for single proportion contexts.</p>
<p>Note that there is only one variable being measured in a study which focuses on one proportion.
For each observational unit, the single variable is measured as either a “success” or “failure” (e.g., “surgical complication” vs. “no surgical complication”)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;The terms “success” and “failure” may not actually represent outcomes we view as successful or not, but it is the typical generic way to referring to the possible outcomes of a binary variable. The “success” is whatever we count when calculating our sample proportion.&lt;/p&gt;"><sup>112</sup></a>.
Because the nature of the research question at hand focuses on only a single variable, there is not a way to randomize the variable across a different (explanatory) variable.
For this reason, we will not use randomization as an analysis tool when focusing on a single proportion.
Instead, we will apply bootstrapping techniques to test a given hypothesis, and we will also revisit the associated mathematical models.</p>
<p>Below we summarize the notation used throughout this chapter.</p>
<div class="onebox">
<p><strong>Notation for a single categorical variable</strong>.</p>
<ul>
<li>
<span class="math inline">\(n\)</span> = sample size (number of observational units in the data set)</li>
<li>
<span class="math inline">\(\hat{p}\)</span> = sample proportion (number of “successes” divided by the sample size)</li>
<li>
<span class="math inline">\(\pi\)</span> = population proportion<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;When you see &lt;span class="math inline"&gt;\(\pi\)&lt;/span&gt; in this textbook, it will always symbolize a (typically unknown) population proportion, not the value 3.14….&lt;/p&gt;'><sup>113</sup></a>
</li>
</ul>
</div>
<div id="one-prop-null-boot" class="section level2" number="14.1">
<h2>
<span class="header-section-number">14.1</span> Simulation-based test for <span class="math inline">\(H_0: \pi = \pi_0\)</span><a class="anchor" aria-label="anchor" href="#one-prop-null-boot"><i class="fas fa-link"></i></a>
</h2>
<p>At the end of Chapter <a href="foundations-randomization.html#foundations-randomization">9</a>, we introduced the general steps of a hypothesis test:</p>
<div class="onebox">
<p><strong>General steps of a hypothesis test.</strong> Every hypothesis test follows these same general steps:</p>
<ol style="list-style-type: decimal">
<li>Frame the research question in terms of hypotheses.</li>
<li>Collect and summarize data using a test statistic.</li>
<li>Assume the null hypothesis is true, and simulate or mathematically model a null distribution for the test statistic.</li>
<li>Compare the observed test statistic to the null distribution to calculate a p-value.</li>
<li>Make a conclusion based on the p-value, and write a conclusion in context, in plain language, and in terms of the alternative hypothesis.</li>
</ol>
</div>
<p>In this section, we will demonstrate the steps of a hypothesis test for the medical consultant case study from Section <a href="foundations-bootstrapping.html#case-study-med-consult">10.1</a>.
Recall that in this case study, a medical consultant tried
to attract patients by noting the average complication rate for liver donor surgeries in the US is about 10%, but her clients have had only 3 complications in the 62 liver donor surgeries she has facilitated.
Do these data provide evidence that her rate of complications is <em>less</em> than the average complication rate in the US of 10%?</p>
<div id="steps-1-and-2-hypotheses-and-test-statistic" class="section level3 unnumbered">
<h3>Steps 1 and 2: Hypotheses and test statistic<a class="anchor" aria-label="anchor" href="#steps-1-and-2-hypotheses-and-test-statistic"><i class="fas fa-link"></i></a>
</h3>
<p>Regardless of if we use simulation-based methods or theory-based methods, the first two steps of a hypothesis test start out the same: setting up hypotheses and summarizing data with a test statistic.</p>
<p>Let <span class="math inline">\(\pi\)</span> represent the true complication rate for liver donors working with this consultant. This “true” complication probability is called the <strong>parameter</strong> of interest.</p>
<p>The sample proportion for the complication rate is 3 complications divided by the 62 surgeries the consultant has worked on: <span class="math inline">\(\hat{p} = 3/62 = 0.048\)</span>. Since this value is estimated from sample data, it is called a <strong>statistic</strong>. The statistic <span class="math inline">\(\hat{p}\)</span> is also our <strong>point estimate</strong>, or “best guess,” for <span class="math inline">\(\pi\)</span>, and we will use it as our <strong>test statistic</strong>.</p>
<div class="protip">
<p>Summary measures that summarize a sample of data, such as <span class="math inline">\(\hat{p}\)</span>, are called <strong>statistics</strong>. Numbers that summarize an entire population, such as <span class="math inline">\(\pi\)</span>, are called <strong>parameters</strong>. You can remember this distinction by looking at the first letter of each term:</p>
<blockquote>
<p><strong><em>S</em></strong>tatistics summarize <strong><em>S</em></strong>amples.<br><strong><em>P</em></strong>arameters summarize <strong><em>P</em></strong>opulations.</p>
</blockquote>
<p>We typically use Roman letters to symbolize statistics (e.g., <span class="math inline">\(\bar{x}\)</span>, <span class="math inline">\(\hat{p}\)</span>), and Greek letters to symbolize parameters (e.g., <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\pi\)</span>). Since we rarely can measure the entire population, and thus rarely know the actual parameter values, we like to say, “We don’t know Greek, and we don’t know parameters!”</p>
</div>
<div class="workedexample">
<p>Write out hypotheses in both plain and statistical language to test whether the medical consultant’s true rate of complications, <span class="math inline">\(\pi\)</span>, is <em>less</em> than the average complication rate in the US of 10%.</p>
<hr>
<p>In words:</p>
<blockquote>
<p><span class="math inline">\(H_0\)</span>: There is no association between the consultant’s contributions and the clients’ complication rate.<br><span class="math inline">\(H_A\)</span>: Patients who work with the consultant tend to have a complication rate lower than 10%.</p>
</blockquote>
<p>In statistical language:</p>
<blockquote>
<p><span class="math inline">\(H_0: \pi=0.10\)</span><br><span class="math inline">\(H_A: \pi&lt;0.10\)</span></p>
</blockquote>
</div>
</div>
<div id="steps-3-and-4-null-distribution-and-p-value" class="section level3 unnumbered">
<h3>Steps 3 and 4: Null distribution and p-value<a class="anchor" aria-label="anchor" href="#steps-3-and-4-null-distribution-and-p-value"><i class="fas fa-link"></i></a>
</h3>
<p>To assess these hypotheses, we need to evaluate the possibility of getting a sample proportion as far below the <strong>null value</strong>, <span class="math inline">\(0.10\)</span>, as what was observed (<span class="math inline">\(\hat{p} = 0.048\)</span>), <em>if the null hypothesis were true</em>.</p>
<div class="onebox">
<p><strong>Null value of a hypothesis test.</strong></p>
<p>The <strong>null value</strong> is the reference value for the parameter in <span class="math inline">\(H_0\)</span>, and it is sometimes represented with the parameter’s label with a subscript 0 (or “null”), e.g., <span class="math inline">\(\pi_0\)</span> (just like <span class="math inline">\(H_0\)</span>).</p>
</div>
<p>The deviation of the sample statistic from the null hypothesized parameter is usually quantified with a p-value<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Now would be a good time to review the definition of a p-value in Section &lt;a href="foundations-randomization.html#p-value-stat-signif"&gt;9.3.2&lt;/a&gt;!&lt;/p&gt;'><sup>114</sup></a>. The p-value is computed based on the null distribution, which is the distribution of the test statistic if the null hypothesis is true. Supposing the null hypothesis is true, we can compute the p-value by identifying the chance of observing a test statistic that favors the alternative hypothesis at least as strongly as the observed test statistic.</p>
<div class="onebox">
<p><strong>Null distribution.</strong></p>
<p>The <strong>null distribution</strong> of a test statistic is the sampling distribution of that statistic <em>under the assumption of the null hypothesis</em>. It describes how that statistic would vary from sample to sample, if the null hypothesis were true.</p>
<p>The null distribution can be estimated through simulation (simulation-based methods), as in this section, or can be modeled by a mathematical function (theory-based methods), as in Section <a href="inference-one-prop.html#theory-prop">14.3</a>.</p>
</div>
<p>We want to identify the sampling distribution of the test statistic (<span class="math inline">\(\hat{p}\)</span>) if the null hypothesis was true. In other words, we want to see how the sample proportion changes due to chance alone. Then we plan to use this information to decide whether there is enough evidence to reject the null hypothesis.</p>
<p>Under the null hypothesis, 10% of liver donors have complications during or after surgery. Suppose this rate was really no different for the consultant’s clients (for <em>all</em> the consultant’s clients, not just the 62 previously measured). If this was the case, we could <em>simulate</em> 62 clients to get a sample proportion for the complication rate from the null distribution.</p>
<p>This is a similar scenario to the one we encountered in Section <a href="foundations-randomization.html#Martian">9.1</a>, with one important difference—the null value is 0.10, not 0.50. Thus, flipping a coin to simulate whether a client had complications would not be simulating under the correct null hypothesis.</p>
<div class="guidedpractice">
<p>What physical object could you use to simulate a random sample of 62 clients who had a 10% chance of complications? How would you use this object?<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;One option would be to use a spinner with 10% shaded red, and the rest shaded green. Each spin of the spinner would represent one client. Spin the spinner 62 times and count the number of times the spinner lands on red. The proportion of times the spinner lands on red represents a simulated &lt;span class="math inline"&gt;\(\hat{p}\)&lt;/span&gt; under the assumption that &lt;span class="math inline"&gt;\(\pi = 0.10\)&lt;/span&gt;. Other objects include: a bag of marbles with 10% red marbles and 90% white marbles, or 10 cards where 1 is red and 9 are white. Sampling 62 times with replacement from these collections would simulate one sample of clients.&lt;/p&gt;'><sup>115</sup></a></p>
</div>
<p>Assuming the true complication rate for the consultant’s clients is 10%, each client can be simulated using a bag of marbles with 10% red marbles and 90% white marbles. Sampling a marble from the bag (with 10% red marbles) is one way of simulating whether a patient has a complication <em>if the true complication rate is 10%</em> for the data. If we select 62 marbles (with replacement) and then compute the proportion of patients with complications in the simulation, <span class="math inline">\(\hat{p}_{sim}\)</span>, then the resulting sample proportion is calculated exactly from a sample from the null distribution.</p>
<p>Many simulations are needed to get a sense of the variability of sample proportions in the null distribution. Figure <a href="inference-one-prop.html#fig:nullDistForPHatIfLiverTransplantConsultantIsNotHelpful">14.1</a> shows the results of 10,000 simulated studies. The proportions that are equal to or less than <span class="math inline">\(\hat{p}=0.048\)</span> are shaded. The shaded areas represent sample proportions under the null distribution that provide at least as much evidence as <span class="math inline">\(\hat{p}\)</span> favoring the alternative hypothesis. There were 1222 simulated sample proportions with <span class="math inline">\(\hat{p}_{sim} \leq 0.048\)</span>. We use these to construct the null distribution’s left-tail area and find the p-value: <span class="math display">\[\begin{align}
\text{left tail area }\label{estOfPValueBasedOnSimulatedNullForSingleProportion}
    &amp;= \frac{\text{Number of observed simulations with }\hat{p}_{sim}\leq\text{ 0.048}}{10000}
\end{align}\]</span> Of the 10,000 simulated <span class="math inline">\(\hat{p}_{sim}\)</span>, 1222 were equal to or smaller than <span class="math inline">\(\hat{p}\)</span>. Since the hypothesis test is one-sided, the estimated p-value is equal to this tail area: 0.1222.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:nullDistForPHatIfLiverTransplantConsultantIsNotHelpful"></span>
<img src="14-categorical-one-prop_files/figure-html/nullDistForPHatIfLiverTransplantConsultantIsNotHelpful-1.png" alt="The null distribution for $\hat{p}$, created from 10,000 simulated studies. The left tail, representing the p-value for the hypothesis test, contains 12.22% of the simulations." width="90%"><p class="caption">
Figure 14.1: The null distribution for <span class="math inline">\(\hat{p}\)</span>, created from 10,000 simulated studies. The left tail, representing the p-value for the hypothesis test, contains 12.22% of the simulations.
</p>
</div>
</div>
<div id="step-5-conclusion-and-scope-of-inference" class="section level3 unnumbered">
<h3>Step 5: Conclusion and scope of inference<a class="anchor" aria-label="anchor" href="#step-5-conclusion-and-scope-of-inference"><i class="fas fa-link"></i></a>
</h3>
<div class="workedexample">
<p>Because the estimated p-value is 0.1222, which is not small, we have little to no evidence against the null hypothesis. That is, our <em>decision</em> of the test is to fail to reject the null hypothesis.</p>
<p>Based on this decision, what is the <em>conclusion</em> of the hypothesis test?</p>
<hr>
<p>There isn’t sufficiently strong evidence to support the claim that fewer than 10% of the consultant’s clients experience complications. That is, there isn’t sufficiently strong evidence to support an association between the consultant’s work and fewer surgery complications.</p>
</div>
<p>Let’s consider the <em>scope of inference</em> for this study—to which population can we generalize? Since these data come from a sample of the medical consultant’s past clients, we can only generalize our results to clients similar to those the medical consultant has had in the past.</p>
<p>Note that since this study only involves a single variable (whether a liver donor surgery patient experienced a complication), we do not consider the other component of scope of inference: whether we can conclude cause and effect, or only an association. This is because cause and effect or an association requires two variables.</p>
<div class="workedexample">
<p>This consultant tried to attract patients by noting the average complication rate for liver donor surgeries in the US is about 10%, but her clients have had only 3 complications in the 62 liver donor surgeries she has facilitated. These data, however, did not provide sufficient evidence that the consultant’s complication rate was less than 10%, since the p-value was approximately 0.122. Does this mean we can conclude that the consultant’s complication rate was equal to 10%?</p>
<hr>
<p>No! Though we did not find strong evidence against the null hypothesis, this does not mean we have evidence <em>for</em> the null hypothesis—we cannot “accept” the null. The sample proportion was <span class="math inline">\(\hat{p} = 3/62 = 0.048\)</span>, which is our point estimate—or “best guess”—of <span class="math inline">\(\pi\)</span>. It wouldn’t make sense that a sample complication rate of 4.8% gives us evidence that the true complication rate was exactly 10%. It’s plausible that the true complication rate is 10%, but there are a range of plausible values for <span class="math inline">\(\pi\)</span>. In the next section, we will review the how to use <strong>bootstrapping</strong> to generate this range of plausible values for <span class="math inline">\(\pi\)</span> using the observed data.</p>
</div>
<!--
Add this to an Appendix someday!

#### Generating the exact null distribution and p-value  {-} {#exactNullDistributionUsingBinomialModel}

The number of successes in $n$ independent cases can be described using the binomial model, which was introduced in Section \ref{binomialModel}. Recall that the probability of observing exactly $k$ successes is given by
\begin{align} \label{binomialEquationShownForFindingNullDistributionInSmallSamplePropTest}
P(k\text{ successes}) = {n\choose k} p^{k}(1-p)^{n-k} = \frac{n!}{k!(n-k)!} p^{k}(1-p)^{n-k}
\end{align}
where $p$ is the true probability of success. The expression ${n\choose k}$ is read as \emph{$n$ choose $k$}, and the exclamation points represent factorials. For instance, $3!$ is equal to $3\times 2\times 1=6$, $4!$ is equal to $4\times 3\times 2\times 1 = 24$, and so on (see Section \ref{binomialModel}).

The tail area of the null distribution is computed by adding up the probability in Equation \eqref{binomialEquationShownForFindingNullDistributionInSmallSamplePropTest} for each $k$ that provides at least as strong of evidence favoring the alternative hypothesis as the data. If the hypothesis test is one-sided, then the p-value is represented by a single tail area. If the test is two-sided, compute the single tail area and double it to get the p-value, just as we have done in the past.

\begin{example}{Compute the exact p-value to check the consultant's claim that her clients' complication rate is below 105.}
Exactly $k=3$ complications were observed in the $n=62$ cases cited by the consultant. Since we are testing against the 10% national average, our null hypothesis is $p=0.10$. We can compute the p-value by adding up the cases where there are 3 or fewer complications:
\begin{align*}
\text{p-value}
    &= \sum_{j=0}^{3} {n\choose j} p^{j}(1-p)^{n-j} \\
    &= \sum_{j=0}^{3} {62\choose j} 0.1^{j}(1-0.1)^{62-j} \\
    &= {62\choose 0} 0.1^{0}(1-0.1)^{62-0} +
        {62\choose 1} 0.1^{1}(1-0.1)^{62-1} \\
    & \qquad + {62\choose 2} 0.1^{2}(1-0.1)^{62-2} +
        {62\choose 3} 0.1^{3}(1-0.1)^{62-3} \\
    &= 0.0015 + 0.0100 + 0.0340 + 0.0755 \\
    &= 0.1210
\end{align*}
This exact p-value is very close to the p-value based on the simulations (0.1222), and we come to the same conclusion. We do not reject the null hypothesis, and there is not statistically significant evidence to support the association.

If it were plotted, the exact null distribution would look almost identical to the simulated null distribution shown in Figure \ref{nullDistForPHatIfLiverTransplantConsultantIsNotHelpful} on page \pageref{nullDistForPHatIfLiverTransplantConsultantIsNotHelpful}.
\end{example}

-->
</div>
</div>
<div id="boot-ci-prop" class="section level2" number="14.2">
<h2>
<span class="header-section-number">14.2</span> Bootstrap confidence interval for <span class="math inline">\(\pi\)</span><a class="anchor" aria-label="anchor" href="#boot-ci-prop"><i class="fas fa-link"></i></a>
</h2>
<p>A confidence interval provides a range of plausible values for the parameter <span class="math inline">\(\pi\)</span>. If the goal is to produce a range of possible values for a population value, then in an ideal world, we would sample data from the population again and recompute the sample proportion. Then we could do it again. And again. And so on until we have a good sense of the variability of our original estimate. The ideal world where sampling data is free or extremely cheap is almost never the case, and taking repeated samples from a population is usually impossible. So, instead of using a “resample from the population” approach, bootstrapping uses a “resample from the sample” approach.</p>
<div class="onebox">
<p><strong>Bootstrapping from one sample</strong>.</p>
<ol style="list-style-type: decimal">
<li>Take a random sample of size <span class="math inline">\(n\)</span> from the original sample, <em>with replacement</em>. This is called a <strong>bootstrapped resample</strong>.</li>
<li>Record the sample proportion (or statistic of interest) from the bootstrapped resample. This is called a <strong>bootstrapped statistic</strong>.</li>
<li>Repeat steps (1) and (2) 1000s of times to create a distribution of bootstrapped statistics.</li>
</ol>
</div>
<div class="workedexample">
<p>In Section <a href="foundations-bootstrapping.html#case-study-med-consult">10.1</a>, we found a 95% bootstrapped confidence interval for the true complication rate of the consultant of (0%, 11.3%). How would we interpret this interval in context of the problem?</p>
<hr>
<p>Since this interval is a range of plausible values for <span class="math inline">\(\pi\)</span>, the unknown true complication rate for this consultant, our interpretation of the interval is in terms of this parameter:</p>
<blockquote>
<p>We are 95% confident that the true complication rate of this medical consultant is between 0% and 11.3%.</p>
</blockquote>
</div>
<p>As in the Example above, any <em>interpretation of a confidence interval</em> should include the following three components:</p>
<ul>
<li>How confident are you? e.g., “We are 95% confident…”</li>
<li>What parameter are you estimating? e.g., “…the true complication rate of this medical consultant…”</li>
<li>What is the interval? e.g., “…between 0% and 11.3%”</li>
</ul>
<p>You can find confidence intervals of difference confidence levels by changing the percent of the distribution you take, e.g., locate the middle 90% of the bootstrapped statistics for a 90% confidence interval.</p>
<div class="guidedpractice">
<p>To find the middle 90% of a distribution, which two percentiles would form its boundaries?<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;The the middle 95% of a distribution would range from the 5&lt;sup&gt;th&lt;/sup&gt; percentile (the value with 5% of the distribution below) to the 95&lt;sup&gt;th&lt;/sup&gt; percentile (the value with 5% of the distribution above).&lt;/p&gt;"><sup>116</sup></a></p>
</div>
</div>
<div id="theory-prop" class="section level2" number="14.3">
<h2>
<span class="header-section-number">14.3</span> Theory-based inferential methods for <span class="math inline">\(\pi\)</span><a class="anchor" aria-label="anchor" href="#theory-prop"><i class="fas fa-link"></i></a>
</h2>
<p>In Chapter <a href="foundations-mathematical.html#foundations-mathematical">11</a>, we introduced the normal distribution and showed how it can be used as a mathematical model to describe the variability of a sample mean or sample proportion as a result of the Central Limit Theorem. Theory-based hypothesis tests and confidence intervals for proportions use the normal distribution to calculate the p-value and to determine the width of the confidence interval.</p>
<div class="onebox">
<p><strong>Central Limit Theorem for the sample proportion.</strong></p>
<p>When we collect a sufficiently large sample of <span class="math inline">\(n\)</span> independent observations of a categorical variable from a population with <span class="math inline">\(\pi\)</span> proportion of successes, the sampling distribution of <span class="math inline">\(\hat{p}\)</span> will be nearly normal with <span class="math display">\[\begin{align*}
  &amp;\text{Mean}=\pi
  &amp;&amp;\text{Standard Deviation }(SD) = \sqrt{\frac{\pi(1-\pi)}{n}}
  \end{align*}\]</span></p>
</div>
<div id="evaluating-the-two-conditions-required-for-modeling-hatp-using-theory-based-methods" class="section level3" number="14.3.1">
<h3>
<span class="header-section-number">14.3.1</span> Evaluating the two conditions required for modeling <span class="math inline">\(\hat{p}\)</span> using theory-based methods<a class="anchor" aria-label="anchor" href="#evaluating-the-two-conditions-required-for-modeling-hatp-using-theory-based-methods"><i class="fas fa-link"></i></a>
</h3>
<p>There are two conditions required to apply the Central Limit Theorem for a sample proportion <span class="math inline">\(\hat{p}\)</span>. When the sample observations are independent and the sample size is sufficiently large, the normal model will describe the variability in sample proportions quite well; when the observations violate the conditions, the normal model can be inaccurate.</p>
<div class="onebox">
<p><strong>Conditions for the sampling distribution of</strong> <span class="math inline">\(\hat{p}\)</span> to be approximately normal.</p>
<p>The sampling distribution for <span class="math inline">\(\hat{p}\)</span> based on a sample of size <span class="math inline">\(n\)</span> from a population with a true proportion <span class="math inline">\(\pi\)</span> can be modeled using a normal distribution when:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Independence condition.</strong> The sample observations are independent, i.e., the outcome of one observation does not influence the outcome of another. This condition is met if data come from a simple random sample of the target population.</p></li>
<li>
<p><strong>Success-failure condition.</strong> We expected to see at least 10 successes and 10 failures in the sample, i.e., <span class="math inline">\(n\pi\geq10\)</span> and <span class="math inline">\(n(1-\pi)\geq10\)</span>.</p>
<ul>
<li>Since <span class="math inline">\(\pi\)</span> is typically unknown, we consider this condition met if we have at least 10 successes and 10 failures in the observed data. That is, if <span class="math inline">\(n\hat{p} \geq 10\)</span> and <span class="math inline">\(n(1-\hat{p}) \geq 10\)</span>.</li>
</ul>
</li>
</ol>
</div>
<div class="protip">
<p>The success-failure condition listed above is only necessary for the sampling distribution of <span class="math inline">\(\hat{p}\)</span> to be approximately normal. The mean of the sampling distribution of <span class="math inline">\(\hat{p}\)</span> is <span class="math inline">\(\pi\)</span>, and the standard deviation is <span class="math inline">\(\sqrt{\frac{\ \pi(1-\pi)\ }{n}}\)</span>, regardless of the sample size.</p>
</div>
<p>Typically we don’t know the true proportion <span class="math inline">\(\pi\)</span>, so we substitute some value to check the success-failure condition and to estimate the standard deviation of the sampling distribution of <span class="math inline">\(\hat{p}\)</span>. When we plug in <span class="math inline">\(\hat{p}\)</span> for <span class="math inline">\(\pi\)</span> in the standard deviation of the sampling distribution, we call this the <strong>standard error of the sample proportion</strong>:
<span class="math display">\[
SE(\hat{p}) = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}.
\]</span></p>
<p>The independence condition is a more nuanced requirement. When it isn’t met, it is important to understand how and why it isn’t met. For example, <em>there exist no statistical methods available to truly correct the inherent biases of data from a convenience sample.</em> On the other hand, if we took a cluster random sample (see Section <a href="data-design.html#samp-methods">2.1.5</a>), the observations wouldn’t be independent, but suitable statistical methods are available for analyzing the data (but they are beyond the scope of even most second or third courses in statistics)<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;While this book is scoped to well-constrained statistical problems, do remember that this is just the first book in what is a large library of statistical methods that are suitable for a very wide range of data and contexts.&lt;/p&gt;"><sup>117</sup></a>.</p>
<div class="workedexample">
<p>In the examples based on large sample theory, we modeled <span class="math inline">\(\hat{p}\)</span> using the normal distribution. Why is this not appropriate for the study on the medical consultant?</p>
<hr>
<p>The independence assumption may be reasonable if each of the surgeries is from a different surgical team. However, the success-failure condition is not satisfied. We only observed 3 complications in 62 patients. We would need to have observed at least 10 complications (and at least 10 without complications) in order to meet the success-failure condition required for the normal approximation.</p>
</div>
<p>Since theory-based methods cannot be used on the medical consultant example, we’ll turn to another example to demonstrate these methods, where conditions for approximating the distribution of <span class="math inline">\(\hat{p}\)</span> by a normal distribution are met.</p>
</div>
<div id="payday-lenders" class="section level3" number="14.3.2">
<h3>
<span class="header-section-number">14.3.2</span> Theory-based test for <span class="math inline">\(H_0: \pi = \pi_0\)</span><a class="anchor" aria-label="anchor" href="#payday-lenders"><i class="fas fa-link"></i></a>
</h3>
<p>One possible regulation for payday lenders is that they would be required to do a credit check and evaluate debt payments against the borrower’s finances. We would like to know: would borrowers support this form of regulation?</p>
<div class="workedexample">
<p>Set up hypotheses to evaluate whether borrowers have a majority support for this type of regulation. We take “majority” to mean greater than 50% of the population.</p>
<hr>
<p>In words,</p>
<ul>
<li>
<span class="math inline">\(H_0\)</span>: there is not majority support for the regulation</li>
<li>
<span class="math inline">\(H_A\)</span>: the majority of borrowers support the regulation</li>
</ul>
<p>In statistical notation,</p>
<ul>
<li>
<span class="math inline">\(H_0\)</span>: <span class="math inline">\(\pi = 0.50\)</span>
</li>
<li>
<span class="math inline">\(H_A\)</span>: <span class="math inline">\(\pi &gt; 0.50\)</span>,</li>
</ul>
<p>where <span class="math inline">\(\pi\)</span> represents the proportion of <em>all</em> payday loan borrowers that would support the regulation.</p>
</div>
<div class="protip">
<p>Note that the null hypothesis above was stated as <span class="math inline">\(H_0: \pi = 0.50\)</span>, even though saying there is “not majority support” would imply <span class="math inline">\(\pi \leq 0.50\)</span>. Indeed, some textbooks would write <span class="math inline">\(H_0: \pi \leq 0.50\)</span> in this case, and it is not an incorrect statement. However, when calculating the p-value, we need to assume a particular value for <span class="math inline">\(\pi\)</span> under the null hypothesis, so in this textbook, our null hypothesis will always be of the form:</p>
<p><span class="math display">\[
H_0: \mbox{ parameter } = \mbox{ null value}
\]</span></p>
</div>
<p>To apply the normal distribution to model the null distribution, the independence and success-failure conditions must be satisfied.</p>
<div class="workedexample">
<p>Do payday loan borrowers support a regulation that would require lenders to pull their credit report and evaluate their debt payments? From a random sample of 826 borrowers, 51% said they would support such a regulation. Is it reasonable use a normal distribution to model <span class="math inline">\(\hat{p}\)</span> for a hypothesis test here?</p>
<hr>
<p>Independence holds since the poll is based on a random sample. Thus, we can assume that the response of one borrower does not influence the response of another.</p>
<p>The success-failure condition also holds, since <span class="math inline">\(n\hat{p} = 826(0.51) = 421.26 &gt; 10\)</span> and <span class="math inline">\(n(1-\hat{p}) = 826(1-0.51) = 404.74 &gt; 10\)</span>.</p>
</div>
<div class="workedexample">
<p>Continuing the previous Example, evaluate whether the poll on lending regulations provides convincing evidence that a majority of payday loan borrowers support a new regulation that would require lenders to pull credit reports and evaluate debt payments.</p>
<hr>
<p>With hypotheses already set up and conditions checked, we can move onto calculations. The <strong>null standard error</strong> in the context of a one proportion hypothesis test is computed using the null value, <span class="math inline">\(\pi_0\)</span>: <span class="math display">\[\begin{align*}
  SE_0(\hat{p}) = \sqrt{\frac{\pi_0 (1 - \pi_0)}{n}}
      = \sqrt{\frac{0.5 (1 - 0.5)}{826}}
      = 0.017
  \end{align*}\]</span> A picture of the normal model for the null distribution of sample proportions in this scenario is shown below in Figure <a href="inference-one-prop.html#fig:paydayCC-norm-pvalue">14.2</a>, with the p-value represented by the shaded region. Note that this null distribution is centered at 0.50, the null value, and has standard deviation 0.017.</p>
<p>Under <span class="math inline">\(H_0\)</span>, the probability of observing <span class="math inline">\(\hat{p} = 0.51\)</span> or higher is 0.278, the area above 0.51 on the null distribution.</p>
<p>With a p-value of 0.278, the poll does not provide convincing evidence that a majority of payday loan borrowers support regulations around credit checks and evaluation of debt payments.</p>
<p>You’ll note that this conclusion is somewhat unsatisfactory because there is no conclusion, as is the case with larger p-values. That is, there is no resolution one way or the other about public opinion. We cannot claim that exactly 50% of people support the regulation, but we cannot claim a majority support it either.</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:paydayCC-norm-pvalue"></span>
<img src="14-categorical-one-prop_files/figure-html/paydayCC-norm-pvalue-1.png" alt="Approximate sampling distribution of $\hat{p}$ across all possible samples assuming $\pi = 0.50$. The shaded area represents the p-value corresponding to an observed sample proportion of 0.51." width="90%"><p class="caption">
Figure 14.2: Approximate sampling distribution of <span class="math inline">\(\hat{p}\)</span> across all possible samples assuming <span class="math inline">\(\pi = 0.50\)</span>. The shaded area represents the p-value corresponding to an observed sample proportion of 0.51.
</p>
</div>
<p>Often, with theory-based methods, we use a <strong>standardized statistic</strong> rather than the original statistic as our test statistic. A standardized statistic is computed by subtracting the mean of the null distribution from the original statistic, then dividing by the standard error: <span class="math display">\[
\mbox{standardized statistic} = \frac{\mbox{observed statistic} - \mbox{null value}}{\mbox{null standard error}}
\]</span> The <strong>null standard error</strong> (<span class="math inline">\(SE_0(\text{statistic})\)</span>) of the observed statistic is its estimated standard deviation assuming the null hypothesis is true. We can interpret the standardized statistic as <em>the number of standard errors our observed statistic is above (if positive) or below (if negative) the null value</em>. When we are modeling the null distribution with a normal distribution, this standardized statistic is called <span class="math inline">\(Z\)</span>, since it is the Z-score of the sample proportion.</p>
<div class="onebox">
<p><strong>Standardized sample proportion.</strong></p>
<p>The <strong>standardized statistic</strong> for theory-based methods for one proportion is <span class="math display">\[
Z = \frac{\hat{p} - \pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}} = \frac{\hat{p} - \pi_0}{SE_0(\hat{p})}
\]</span> where <span class="math inline">\(\pi_0\)</span> is the null value. The denominator, <span class="math inline">\(SE_0(\hat{p}) = \sqrt{\frac{\pi_0(1-\pi_0)}{n}}\)</span>, is called the <strong>null standard error</strong> of the sample proportion.</p>
</div>
<p>With the standardized statistic as our test statistic, we can find the p-value as the area under a standard normal distribution at or more extreme than our observed <span class="math inline">\(Z\)</span> value.</p>
<div class="workedexample">
<p>Do payday loan borrowers support a regulation that would require lenders to pull their credit report and evaluate their debt payments? From a random sample of 826 borrowers, 51% said they would support such a regulation. We set up hypotheses and checked conditions previously. Now calculate and interpret the standardized statistic, then use the standard normal distribution to calculate the approximate p-value.</p>
<hr>
<p>Our sample proportion is <span class="math inline">\(\hat{p} = 0.51\)</span>. Since our null value is <span class="math inline">\(\pi_0 = 0.50\)</span>,<br>
the null standard error is <span class="math display">\[\begin{align*}
  SE_0(\hat{p}) = \sqrt{\frac{\pi_0 (1 - \pi_0)}{n}}
      = \sqrt{\frac{0.5 (1 - 0.5)}{826}}
      = 0.017
  \end{align*}\]</span></p>
<p>The standardized statistic is <span class="math display">\[\begin{align*}
Z = \frac{0.51 - 0.50}{0.017} = 0.59
\end{align*}\]</span></p>
<p>Interpreting this value, we can say that our sample proportion of 0.51 was only 0.59 standard errors above the null value of 0.50.</p>
<p>Shown in Figure <a href="inference-one-prop.html#fig:paydayCC-stdnorm-pvalue">14.3</a>, the p-value is the area above <span class="math inline">\(Z = 0.59\)</span> on a standard normal distribution—0.278—the same p-value we would obtain by finding the area above <span class="math inline">\(\hat{p} = 0.51\)</span> on a normal distribution with mean 0.50 and standard deviation 0.017, as in Figure <a href="inference-one-prop.html#fig:paydayCC-norm-pvalue">14.2</a>.</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:paydayCC-stdnorm-pvalue"></span>
<img src="14-categorical-one-prop_files/figure-html/paydayCC-stdnorm-pvalue-1.png" alt="Approximate sampling distribution of \(Z\) across all possible samples assuming \(\pi = 0.50\). The shaded area represents the p-value corresponding to an observed standardized statistic of 0.59. Compare to Figure 14.2." width="90%"><p class="caption">
Figure 14.3: Approximate sampling distribution of <span class="math inline">\(Z\)</span> across all possible samples assuming <span class="math inline">\(\pi = 0.50\)</span>. The shaded area represents the p-value corresponding to an observed standardized statistic of 0.59. Compare to Figure <a href="inference-one-prop.html#fig:paydayCC-norm-pvalue">14.2</a>.
</p>
</div>

<div class="onebox">
<p><strong>Theory-based hypothesis test for a proportion: one-sample</strong> <span class="math inline">\(Z\)</span>-test.</p>
<ol style="list-style-type: decimal">
<li>Frame the research question in terms of hypotheses.</li>
<li>Using the null value, <span class="math inline">\(\pi_0\)</span>, verify the conditions for using the normal distribution to approximate the null distribution.</li>
<li>Calculate the test statistic: <span class="math display">\[
Z = \frac{\hat{p} - \pi_0}{\sqrt{\frac{\pi_0(1-\pi_0)}{n}}} = \frac{\hat{p} - \pi_0}{SE_0(\hat{p})}
\]</span>
</li>
<li>Use the test statistic and the standard normal distribution to calculate the p-value.</li>
<li>Make a conclusion based on the p-value, and write a conclusion in context, in plain language, and in terms of the alternative hypothesis.</li>
</ol>
</div>
</div>
<div id="theory-based-confidence-interval-for-pi" class="section level3" number="14.3.3">
<h3>
<span class="header-section-number">14.3.3</span> Theory-based confidence interval for <span class="math inline">\(\pi\)</span><a class="anchor" aria-label="anchor" href="#theory-based-confidence-interval-for-pi"><i class="fas fa-link"></i></a>
</h3>
<p>A confidence interval provides a range of plausible values for the parameter <span class="math inline">\(\pi\)</span>. A point estimate is our best guess for the value of the parameter, so it makes sense to build the confidence interval around that value. The standard error, which is a measure of the uncertainty associated with the point estimate, provides a guide for how large we should make the confidence interval. When <span class="math inline">\(\hat{p}\)</span> can be modeled using a normal distribution, the 68-95-99.7 rule tells us that, in general, 95% of observations are within 2 standard errors of the mean. Here, we use the value 1.96 to be slightly more precise. The confidence interval for <span class="math inline">\(\pi\)</span> then takes the form <span class="math display">\[\begin{align*}
\hat{p} \pm z^{\star} \times SE(\hat{p}).
\end{align*}\]</span></p>
<p>We have seen <span class="math inline">\(\hat{p}\)</span> to be the sample proportion. The value <span class="math inline">\(z^{\star}\)</span> comes from a standard normal distribution and is determined by the chosen confidence level. The value of the standard error of <span class="math inline">\(\hat{p}\)</span>, <span class="math inline">\(SE(\hat{p})\)</span>, approximates how far we would expect the sample proportion to fall from <span class="math inline">\(\pi\)</span>, and depends heavily on the sample size.</p>
<div class="onebox">
<p><strong>Standard error of one proportion,</strong> <span class="math inline">\(\hat{p}\)</span>.</p>
<p>When the conditions are met so that the distribution for <span class="math inline">\(\hat{p}\)</span> is nearly normal, the <strong>variability</strong> of a single proportion, <span class="math inline">\(\hat{p}\)</span> is well described by its standard deviation:</p>
<p><span class="math display">\[SD(\hat{p}) = \sqrt{\frac{\pi(1-\pi)}{n}}\]</span></p>
<p>Note that we almost never know the true value of <span class="math inline">\(\pi\)</span>, but we can substitute our best guess of <span class="math inline">\(\pi\)</span> to obtain an approximate standard deviation, called the <strong>standard error</strong> of <span class="math inline">\(\hat{p}\)</span>:</p>
<p><span class="math display">\[SD(\hat{p}) \approx \hspace{3mm} SE(\hat{p}) = \sqrt{\frac{(\mbox{best guess of }\pi)(1 - \mbox{best guess of }\pi)}{n}}\]</span></p>
<p>For hypothesis testing, we often use <span class="math inline">\(\pi_0\)</span> as the best guess of <span class="math inline">\(\pi\)</span>. For confidence intervals, we typically use <span class="math inline">\(\hat{p}\)</span> as the best guess of <span class="math inline">\(\pi\)</span>.</p>
</div>
<p></p>
<!--
\newcommand{\paydayN}{826}
\newcommand{\paydayNHalf}{413}
\newcommand{\paydayRegPerc}{70\%}
\newcommand{\paydayRegProp}{0.70}
\newcommand{\paydayRegSE}{0.016}
\newcommand{\paydayRegSEPerc}{1.6\%}
\newcommand{\paydayRegLower}{0.669}
\newcommand{\paydayRegUpper}{0.731}
\newcommand{\paydayRegLowerPerc}{66.9\%}
\newcommand{\paydayRegUpperPerc}{73.1\%}
% https://www.pewtrusts.org/-/media/assets/2017/04/payday-loan-customers-want-more-protections-methodology.pdf

did search and replace for each term above.  for example 826 for 826

-->
<div class="guidedpractice">
<p>Consider taking many polls of registered voters (i.e., random samples) of size 300 and asking them if they support legalized marijuana. It is suspected that about 2/3 of all voters support legalized marijuana. To understand how the sample proportion (<span class="math inline">\(\hat{p}\)</span>) would vary across the samples, calculate the standard error of <span class="math inline">\(\hat{p}\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Because the &lt;span class="math inline"&gt;\(\pi\)&lt;/span&gt; is unknown but expected to be around 2/3, we will use 2/3 in place of &lt;span class="math inline"&gt;\(\pi\)&lt;/span&gt; in the formula for the standard deviation of &lt;span class="math inline"&gt;\(\hat{p}\)&lt;/span&gt; and calculate&lt;br&gt;&lt;span class="math inline"&gt;\(SE(\hat{p}) = \sqrt{\frac{2/3 (1 - 2/3)} {300}} = 0.027\)&lt;/span&gt;.&lt;/p&gt;'><sup>118</sup></a></p>
</div>
<div class="workedexample">
<p>A simple random sample of 826 payday loan borrowers was surveyed to better understand their interests around regulation and costs. 51% of the responses supported new regulations on payday lenders.</p>
<ol style="list-style-type: decimal">
<li><p>Is it reasonable to model the variability of <span class="math inline">\(\hat{p}\)</span> from sample to sample using a normal distribution?</p></li>
<li><p>Calculate the standard error of <span class="math inline">\(\hat{p}\)</span>.</p></li>
<li><p>Construct a 95% confidence interval for <span class="math inline">\(\pi\)</span>, the proportion of <em>all</em> payday borrowers who support increased regulation for payday lenders.</p></li>
</ol>
<hr>
<ol style="list-style-type: decimal">
<li>
<p>The data are a random sample, so the observations are independent and representative of the population of interest.</p>
<p>We also must check the success-failure condition, which we do using <span class="math inline">\(\hat{p}\)</span> in place of <span class="math inline">\(\pi\)</span> when computing a confidence interval:</p>
<p>Support: <span class="math inline">\(n \hat{p} = 826 \times 0.51 \approx 421 &gt; 10\)</span></p>
<p>Not: <span class="math inline">\(n (1 - \hat{p}) = 826 \times (1 - 0.51) \approx 405 &gt; 10\)</span></p>
<p>Since both values are at least 10, we can use the normal distribution to model the sampling distribution of <span class="math inline">\(\hat{p}\)</span>.</p>
</li>
<li>
<p>Because <span class="math inline">\(\pi\)</span> is unknown and the standard error is for a confidence interval, use <span class="math inline">\(\hat{p}\)</span> as our best guess of <span class="math inline">\(\pi\)</span> in the formula.</p>
<p><span class="math inline">\(SE(\hat{p}) = \sqrt{\frac{0.51 (1 - 0.51)} {826}} = 0.017\)</span>.</p>
</li>
<li><p>Using the point estimate <span class="math inline">\(0.51\)</span>, <span class="math inline">\(z^{\star} = 1.96\)</span> for a 95% confidence interval, and the standard error <span class="math inline">\(SE = 0.017\)</span> from the previous Guided Practice, the confidence interval is <span class="math display">\[\begin{align*}
  \text{point estimate} &amp;\pm\ z^{\star} \times SE \\
   \quad\to\quad
   0.51 \ &amp;\pm\ 1.96 \times 0.017 \\
   \quad\to\quad
   (0.477, &amp;0.543)
  \end{align*}\]</span> We are 95% confident that the true proportion of payday borrowers who supported regulation at the time of the poll was between 0.477 and 0.543.</p></li>
</ol>
</div>
<div class="onebox">
<p><strong>Constructing a confidence interval for a single proportion.</strong></p>
<p>There are four steps to constructing a confidence interval for <span class="math inline">\(p\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Check independence and the success-failure condition using <span class="math inline">\(\hat{p}\)</span>. If the conditions are met, the sampling distribution of <span class="math inline">\(\hat{p}\)</span> may be well-approximated by the normal model.</li>
<li>Construct the standard error: <span class="math display">\[
SE(\hat{p}) = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
  \]</span>
</li>
<li>Use statistical software to find the multiplier <span class="math inline">\(z^{\star}\)</span> corresponding to the confidence level.</li>
<li>Apply the general confidence interval formula <span class="math inline">\(\mbox{statistic} \pm (\mbox{multiplier}) \times SE\)</span>: <span class="math display">\[
\hat{p} \pm z^{\star}\times \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
  \]</span>
</li>
</ol>
</div>
<div id="zstar-and-the-confidence-level" class="section level4 unnumbered">
<h4>
<span class="math inline">\(z^{\star}\)</span> and the confidence level<a class="anchor" aria-label="anchor" href="#zstar-and-the-confidence-level"><i class="fas fa-link"></i></a>
</h4>
<p></p>
<p>Suppose we want to consider confidence intervals where the confidence level is somewhat higher than 95%: perhaps we would like a confidence level of 99%. Think back to the analogy about trying to catch a fish: if we want to be more sure that we will catch the fish, we should use a wider net. To create a 99% confidence level, we must also widen our 95% interval. On the other hand, if we want an interval with lower confidence, such as 90%, we could make our original 95% interval slightly slimmer.</p>
<p>The 95% confidence interval structure provides guidance in how to make intervals with new confidence levels. Below is a general 95% confidence interval for a parameter whose point estimate has a nearly normal distribution: <span class="math display">\[\begin{eqnarray}
\text{point estimate}\ \pm\ 1.96\times SE
\end{eqnarray}\]</span> There are three components to this interval: the point estimate, “1.96”, and the standard error. The choice of <span class="math inline">\(1.96\times SE\)</span> was based on capturing 95% of the sampling distribution of statistics since the point estimate is within 1.96 standard errors of the true parameter about 95% of the time. The choice of 1.96 corresponds to a 95% <strong>confidence level</strong>.</p>
<div class="guidedpractice">
<p>If <span class="math inline">\(X\)</span> is a normally distributed random variable, how often will <span class="math inline">\(X\)</span> be within 2.58 standard deviations of the mean?<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;This is equivalent to asking how often the &lt;span class="math inline"&gt;\(Z\)&lt;/span&gt; score will be larger than -2.58 but less than 2.58. (For a picture, see Figure &lt;a href="inference-one-prop.html#fig:choosingZForCI"&gt;14.4&lt;/a&gt;.) To determine this probability, look up -2.58 and 2.58 in the normal probability table (0.0049 and 0.9951). Thus, there is a &lt;span class="math inline"&gt;\(0.9951-0.0049 \approx 0.99\)&lt;/span&gt; probability that the unobserved random variable &lt;span class="math inline"&gt;\(X\)&lt;/span&gt; will be within 2.58 standard deviations of the mean.&lt;/p&gt;'><sup>119</sup></a></p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:choosingZForCI"></span>
<img src="14-categorical-one-prop_files/figure-html/choosingZForCI-1.png" alt="The area between -$z^{\star}$ and $z^{\star}$ increases as $|z^{\star}|$ becomes larger. If the confidence level is 99%, we choose $z^{\star}$ such that 99% of the normal curve is between -$z^{\star}$ and $z^{\star}$, which corresponds to 0.5% in the lower tail and 0.5% in the upper tail: $z^{\star}=2.58$." width="90%"><p class="caption">
Figure 14.4: The area between -<span class="math inline">\(z^{\star}\)</span> and <span class="math inline">\(z^{\star}\)</span> increases as <span class="math inline">\(|z^{\star}|\)</span> becomes larger. If the confidence level is 99%, we choose <span class="math inline">\(z^{\star}\)</span> such that 99% of the normal curve is between -<span class="math inline">\(z^{\star}\)</span> and <span class="math inline">\(z^{\star}\)</span>, which corresponds to 0.5% in the lower tail and 0.5% in the upper tail: <span class="math inline">\(z^{\star}=2.58\)</span>.
</p>
</div>
<p></p>
<p>To create a 99% confidence interval, change 1.96 in the 95% confidence interval formula to be 2.58. The previous Guided Practice highlights that 99% of the time a normal random variable will be within 2.58 standard deviations of its mean. This approach—using the Z-scores in the normal model to compute confidence levels—is appropriate when the point estimate is associated with a normal distribution and we can properly compute the standard error. Thus, the formula for a 99% confidence interval is:</p>
<span class="math display">\[\begin{eqnarray*}
\text{point estimate}\ \pm\ 2.58\times SE
\end{eqnarray*}\]</span>
<!--
label for previous equation?
\label{99PercCIForMean}
\label{99PercCIForNormalPointEstimate}

%\Comment{I don't know where the equation number above gets referenced. Might drop the equation number.}
-->
<p>The normal approximation is crucial to the precision of the <span class="math inline">\(z^\star\)</span> confidence intervals. When the normal model is not a good fit, we will use alternative distributions that better characterize the sampling distribution or we will use bootstrapping procedures.</p>
<div class="guidedpractice">
<p>Create a 99% confidence interval for the impact of the stent on the risk of stroke using the data from Section <a href="data-hello.html#basic-stents-strokes">1.1</a>. The point estimate is 0.090, and the standard error is <span class="math inline">\(SE = 0.028\)</span>. It has been verified for you that the point estimate can reasonably be modeled by a normal distribution.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Since the necessary conditions for applying the normal model have already been checked for us, we can go straight to the construction of the confidence interval: &lt;span class="math inline"&gt;\(\text{point estimate}\ \pm\ 2.58 \times SE \rightarrow (0.018, 0.162)\)&lt;/span&gt;. We are 99% confident that implanting a stent in the brain of a patient who is at risk of stroke increases the risk of stroke within 30 days by a rate of 0.018 to 0.162 (assuming the patients are representative of the population).&lt;/p&gt;'><sup>120</sup></a></p>
</div>
<div class="onebox">
<p><strong>Theory-based</strong> <span class="math inline">\((1-\alpha)\times 100\)</span>% confidence interval.</p>
<p>If the statistic follows the normal model with standard error <span class="math inline">\(SE\)</span>, then a confidence interval for the population parameter is <span class="math display">\[\begin{eqnarray*}
\text{statistic}\ \pm\ z^{\star} \times SE
\end{eqnarray*}\]</span> where <span class="math inline">\(z^{\star}\)</span> corresponds to the confidence level selected: the middle <span class="math inline">\((1-\alpha)\times 100\)</span>% of a standard normal distribution lies between <span class="math inline">\(-z^{\star}\)</span> and <span class="math inline">\(z^{\star}\)</span>.</p>
</div>
</div>
<div id="using-r-to-find-zstar" class="section level4 unnumbered">
<h4>Using R to find <span class="math inline">\(z^{\star}\)</span><a class="anchor" aria-label="anchor" href="#using-r-to-find-zstar"><i class="fas fa-link"></i></a>
</h4>
<p>Figure <a href="inference-one-prop.html#fig:choosingZForCI">14.4</a> provides a picture of how to identify <span class="math inline">\(z^{\star}\)</span> based on a confidence level. We select <span class="math inline">\(z^{\star}\)</span> so that the area between -<span class="math inline">\(z^{\star}\)</span> and <span class="math inline">\(z^{\star}\)</span> in the normal model corresponds to the confidence level. In R, you can find <span class="math inline">\(z^{\star}\)</span> using the <code><a href="https://rdrr.io/r/stats/Normal.html">qnorm()</a></code> function:</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># z* for 90% --&gt; alpha = 0.15 --&gt; need 5% on each side:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">.90</span> <span class="op">+</span> <span class="fl">.05</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1.645</span></span>
<span></span>
<span><span class="co"># z* for 95% --&gt; alpha = 0.05 --&gt; need 2.5% on each side:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">.95</span> <span class="op">+</span> <span class="fl">.025</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1.96</span></span>
<span></span>
<span><span class="co"># z* for 99% --&gt; alpha = 0.01 --&gt; need .5% on each side:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fl">.99</span> <span class="op">+</span> <span class="fl">.005</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 2.576</span></span></code></pre></div>
<div class="guidedpractice">
<p>Previously, we found that implanting a stent in the brain of a patient at risk for a stroke <em>increased</em> the risk of a stroke. The study estimated a 9% increase in the number of patients who had a stroke, and the standard error of this estimate was about <span class="math inline">\(SE = 2.8%\)</span>. Compute a 90% confidence interval for the effect.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;We must find &lt;span class="math inline"&gt;\(z^{\star}\)&lt;/span&gt; such that 90% of the distribution falls between -&lt;span class="math inline"&gt;\(z^{\star}\)&lt;/span&gt; and &lt;span class="math inline"&gt;\(z^{\star}\)&lt;/span&gt; in the standard normal model, &lt;span class="math inline"&gt;\(N(\mu=0, \sigma=1)\)&lt;/span&gt;. We can find -&lt;span class="math inline"&gt;\(z^{\star}\)&lt;/span&gt; from a standard normal distribution by looking for a lower tail of 5% (the other 5% is in the upper tail), thus &lt;span class="math inline"&gt;\(z^{\star}=1.645\)&lt;/span&gt;. The 90% confidence interval can then be computed as &lt;span class="math inline"&gt;\(\text{point estimate}\ \pm\ 1.65\times SE \to (4.4\%, 13.6\%)\)&lt;/span&gt;. (Note: The conditions for normality had earlier been confirmed for us.) That is, we are 90% confident that implanting a stent in a stroke patient’s brain increased the risk of stroke within 30 days by 4.4% to 13.6%.&lt;/p&gt;'><sup>121</sup></a></p>
</div>
</div>
</div>
<div id="violating-conditions" class="section level3" number="14.3.4">
<h3>
<span class="header-section-number">14.3.4</span> Violating conditions<a class="anchor" aria-label="anchor" href="#violating-conditions"><i class="fas fa-link"></i></a>
</h3>
<p>We’ve spent a lot of time discussing conditions for when <span class="math inline">\(\hat{p}\)</span> can be reasonably modeled by a normal distribution. What happens when the success-failure condition fails? What about when the independence condition fails? In either case, the general ideas of confidence intervals and hypothesis tests remain the same, but the strategy or technique used to generate the interval or p-value change.</p>
<p>Regardless of the statistical method chosen, the p-value is always derived by analyzing the null distribution of the test statistic. The normal model poorly approximates the null distribution for <span class="math inline">\(\hat{p}\)</span> when the success-failure condition is not satisfied. When the success-failure condition isn’t met, we can simulate the null distribution of <span class="math inline">\(\hat{p}\)</span> using the null value, <span class="math inline">\(\pi_0\)</span>, as seen in Section <a href="inference-one-prop.html#one-prop-null-boot">14.1</a>, and use this distribution to compute the tail area, i.e., the p-value. Neither the p-value approximated by the normal distribution nor the simulated p-value are exact, because the normal distribution and simulated null distribution themselves are not exact, only a close approximation<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;An exact p-value for hypothesis tests involving a single proportion can be generated using the binomial distribution, but that method will not be covered in this text.&lt;/p&gt;"><sup>122</sup></a>.</p>
<p>Unfortunately, <strong>the independence condition must also hold for simulation-based methods</strong>. Methods for dealing with observations which are not independent are outside the scope of this book.</p>
</div>
</div>
<div id="chp14-review" class="section level2" number="14.4">
<h2>
<span class="header-section-number">14.4</span> Chapter review<a class="anchor" aria-label="anchor" href="#chp14-review"><i class="fas fa-link"></i></a>
</h2>
<!-- ### Summary {-} -->
<!-- ::: {.underconstruction} -->
<!-- TODO -->
<!-- ::: -->
<div id="terms-11" class="section level3 unnumbered">
<h3>Terms<a class="anchor" aria-label="anchor" href="#terms-11"><i class="fas fa-link"></i></a>
</h3>
<p>We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as <strong>bolded text</strong>.</p>
<div class="inline-table"><table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;"><tbody>
<tr>
<td style="text-align:left;">
confidence level
</td>
<td style="text-align:left;">
parameter
</td>
<td style="text-align:left;">
success-failure condition
</td>
</tr>
<tr>
<td style="text-align:left;">
independence condition
</td>
<td style="text-align:left;">
point estimate
</td>
<td style="text-align:left;">
test statistic
</td>
</tr>
<tr>
<td style="text-align:left;">
null distribution
</td>
<td style="text-align:left;">
standard error of the sample proportion
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
null value
</td>
<td style="text-align:left;">
statistic
</td>
<td style="text-align:left;">
</td>
</tr>
</tbody></table></div>
<!-- ### Key ideas {-} --><!-- ::: {.underconstruction} --><!-- TODO --><!-- ::: -->
</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="foundations-applications.html"><span class="header-section-number">13</span> Applications: Foundations</a></div>
<div class="next"><a href="inference-two-props.html"><span class="header-section-number">15</span> Inference for comparing two proportions</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#inference-one-prop"><span class="header-section-number">14</span> Inference for a single proportion</a></li>
<li>
<a class="nav-link" href="#one-prop-null-boot"><span class="header-section-number">14.1</span> Simulation-based test for \(H_0: \pi = \pi_0\)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#steps-1-and-2-hypotheses-and-test-statistic">Steps 1 and 2: Hypotheses and test statistic</a></li>
<li><a class="nav-link" href="#steps-3-and-4-null-distribution-and-p-value">Steps 3 and 4: Null distribution and p-value</a></li>
<li><a class="nav-link" href="#step-5-conclusion-and-scope-of-inference">Step 5: Conclusion and scope of inference</a></li>
</ul>
</li>
<li><a class="nav-link" href="#boot-ci-prop"><span class="header-section-number">14.2</span> Bootstrap confidence interval for \(\pi\)</a></li>
<li>
<a class="nav-link" href="#theory-prop"><span class="header-section-number">14.3</span> Theory-based inferential methods for \(\pi\)</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#evaluating-the-two-conditions-required-for-modeling-hatp-using-theory-based-methods"><span class="header-section-number">14.3.1</span> Evaluating the two conditions required for modeling \(\hat{p}\) using theory-based methods</a></li>
<li><a class="nav-link" href="#payday-lenders"><span class="header-section-number">14.3.2</span> Theory-based test for \(H_0: \pi = \pi_0\)</a></li>
<li><a class="nav-link" href="#theory-based-confidence-interval-for-pi"><span class="header-section-number">14.3.3</span> Theory-based confidence interval for \(\pi\)</a></li>
<li><a class="nav-link" href="#violating-conditions"><span class="header-section-number">14.3.4</span> Violating conditions</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#chp14-review"><span class="header-section-number">14.4</span> Chapter review</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#terms-11">Terms</a></li></ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/MTstateIntroStats/IntroStatTextbook/blob/master/14-categorical-one-prop.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/MTstateIntroStats/IntroStatTextbook/edit/master/14-categorical-one-prop.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Montana State Introductory Statistics with R</strong>" was written by Stacey Hancock, Nicole Carnegie, Elijah Meyer, Jade Schmidt, Melinda Yager. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>

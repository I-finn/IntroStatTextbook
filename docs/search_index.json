[
["inference-cat.html", "Chapter 5 Inference for categorical data 5.1 Foundations of inference 5.2 The normal distribution 5.3 One proportion 5.4 Difference of two proportions 5.5 Summary of \\(z\\)-procedures 5.6 R: Inference for categorical data 5.7 Chapter 5 review", " Chapter 5 Inference for categorical data Statistical inference is primarily concerned with understanding and quantifying the uncertainty of parameter estimates—that is, how variable is a sample statistic from sample to sample? While the equations and details change depending on the setting, the foundations for inference are the same throughout all of statistics. We will begin this chapter with a discussion of the foundations of inference, and introduce the two primary vehicles of inference: the hypothesis test and confidence interval. The rest of this chapter focuses statistical inference for categorical data. The two data structures we detail are: one binary variable, summarized using a single proportion, and two binary variables, summarized using a difference (or ratio) of two proportions. We will also introduce a new important mathematical model, the normal distribution (as the foundation for the \\(z\\)-test). Throughout the book so far, you have worked with data in a variety of contexts. You have learned how to summarize and visualize the data as well as how to model multiple variables at the same time. Sometimes the data set at hand represents the entire research question. But more often than not, the data have been collected to answer a research question about a larger group of which the data are a (hopefully) representative subset. You may agree that there is almost always variability in data (one data set will not be identical to a second data set even if they are both collected from the same population using the same methods). However, quantifying the variability in the data is neither obvious nor easy to do (how different is one data set from another?). Suppose your professor splits the students in class into two groups: students on the left and students on the right. If \\(\\hat{p}_{_L}\\) and \\(\\hat{p}_{_R}\\) represent the proportion of students who own an Apple product on the left and right, respectively, would you be surprised if \\(\\hat{p}_{_L}\\) did not exactly equal \\(\\hat{p}_{_R}\\)? While the proportions would probably be close to each other, it would be unusual for them to be exactly the same. We would probably observe a small difference due to chance. If we don’t think the side of the room a person sits on in class is related to whether the person owns an Apple product, what assumption are we making about the relationship between these two variables? (Reminder: for these Guided Practice questions, you can check your answer in the footnote.)1 Studying randomness of this form is a key focus of statistics. Throughout this chapter, and those that follow, we provide two different approaches for quantifying the variability inherent in data: simulation-based methods and theory-based methods (mathematical models). Using the methods provided in this and future chapters, we will be able to draw conclusions beyond the data set at hand to research questions about larger populations. 5.1 Foundations of inference Given results seen in a sample, the process of determining what we can infer to the population based on sample results is called statistical inference. Statistical inferential methods enable us to understand and quantify the uncertainty of our sample results. Statistical inference helps us answer two questions about the population: How strong is the evidence of an effect? How large is the effect? The first question is answered through a hypothesis test, while the second is addressed with a confidence interval. 5.1.1 Motivating example: Martian alphabet How well can humans distinguish one “Martian” letter from another? The Figure 5.1 displays two Martian letters—one is Kiki and the another is Bumba. Which do you think is Kiki and which do you think is Bumba?2 Figure 5.1: Two Martian letters: Bumba and Kiki. Do you think the letter Bumba is on the left or the right?3 This same image and question were presented to an introductory statistics class of 38 students. In that class, 34 students correctly identified Bumba as the Martian letter on the left. Assuming we can’t read Martian, is this result surprising? One of two possibilities occurred: We can’t read Martian, and these results just occurred by chance. We can read Martian, and these results reflect this ability. To decide between these two possibilities, we could calculate the probability of observing such results in a randomly selected sample of 38 students, under the assumption that students were just guessing. If this probability is very low, we’d have reason to reject the first possibility in favor of the second. We can calculate this probability using one of two methods: Simulation-based method: simulate lots of samples (Classes) of 38 students under the assumption that students are just guessing, then calculate the proportion of these simulated samples where we saw 34 or more students guessing correctly, or Theory-based method: develop a mathematical model for the sample proportion in this scenario and use the model to calculate the probability. How could you use a coin or cards to simulate the guesses of one sample of 38 students who cannot read Martian?4 For this situation—since “just guessing” means you have a 50% chance of guessing correctly—we could simulate a sample of 38 students’ guesses by flipping a coin 38 times and counting the number of times it lands on heads. Using a computer to repeat this process 1,000 times, we create the dot plot in Figure 5.2. Figure 5.2: A dot plot of 1,000 sample proportions; each calculated by flipping a coin 38 times and calculating the proportion of times the coin landed on heads. None of the 1,000 simulations had sample proportion of at least 89%, which was the proportion observed in the study. None of our simulated samples produce 34 of 38 correct guesses! That is, if students were just guessing, it is nearly impossible to observe 34 or more correct guesses in a sample of 38 students. Given this low probability, the more plausible possibility is 2. We can read Martian, and these results reflect this ability. We’ve just completed our first hypothesis test! Now, obviously no one can read Martian, so a more realistic possibility is that humans tend to choose Bumba on the left more often than the right—there is a greater than 50% chance of choosing Bumba as the letter on the left. Even though we may think we’re guessing just by chance, we have a preference for Bumba on the left. It turns out that the explanation for this preference is called synesthesia, a tendency for humans to correlate sharp sounding noises (e.g., Kiki) with sharp looking images.5 But wait—we’re not done! We have evidence that humans tend to prefer Bumba on the left, but by how much? To answer this, we need a confidence interval—an interval of plausible values for the true probability humans will selecting Bumba as the left letter. The width of this interval is determined by how variable sample proportions are from sample to sample. It turns out, there is a mathematical model for this variability that we will explore later in this chapter. For now, let’s take the standard deviation from our simulated sample proportions as an estimate for this variability: 0.08. Since the simulated distribution of proportions is bell-shaped, we know about 95% of sample proportions should fall within two standard deviations of the true proportion, so we can add and subtract this margin of error to our sample proportion to calculate an approximate 95% confidence interval6: \\[ \\frac{34}{38} \\pm 2\\times 0.08 = 0.89 \\pm 0.16 = (0.73, 1) \\] Thus, based on this data, we are 95% confident that the probability a human guesses Bumba on the left is somewhere between 73% and 100%. 5.1.2 Variability in a statistic There are two approaches to modeling how a statistic may vary from sample to sample. In the Martian alphabet example, we used a simulation-based approach to model this variability, using the standard deviation of the simulated distribution of sample proportions as a quantitative measure of this sampling variability. Simulation-based methods include the randomization tests and bootstrapping methods we will use in this textbook. We can also use a theory-based approach—one which makes use of mathematical modeling—and involves the normal and \\(t\\) probability distributions. All of the theory-based methods discussed in this book work (under certain conditions) because of a very important theorem in Statistics called the Central Limit Theorem. Central Limit Theorem. For large sample sizes, the sampling distribution of a sample proportion (or sample mean) will appear to follow a bell-shaped curve called the normal distribution. An example of a perfect normal distribution is shown in Figure 5.3. While the mean (center) and standard deviation (variability) may change for different scenarios, the general shape remains roughly intact. Figure 5.3: A normal curve. Recall from Chapter 2 that a distribution of a variable is a description of the possible values it takes and how frequently each value occurs. In a sampling distribution, our “variable” is a sample statistic, and the sampling distribution is a description of the possible values a sample statistic takes and how frequently each value occurs when looking across many many possible samples. It is quite amazing that something like a sample proportion, summarizing a categorical variable, will have a bell-shaped sampling distribution if we sample large enough samples! Theory-based methods also give us mathematical expressions for the standard deviation of a sampling distribution. For instance, if the true population proportion is \\(\\pi\\), then the standard deviation of the sampling distribution of sample proportions is \\[ SD(\\hat{p}) = \\sqrt{\\frac{\\pi(1-\\pi)}{n}}. \\] In the case of two samples, a sample of size \\(n_1\\) taken from a population with proportion \\(\\pi_1\\), and the other of size \\(n_2\\) from a population with proportion \\(\\pi_2\\), the standard deviation of the sampling distribution of the difference in sample proportions is \\[ SD(\\hat{p}_1 - \\hat{p}_2) = \\sqrt{\\frac{\\pi_1(1-\\pi_1)}{n_1} + \\frac{\\pi_2(1-\\pi_2)}{n}} \\] Typically, the values of the parameters \\(\\pi\\), \\(\\pi_1\\), and \\(\\pi_2\\) are unknown, so we are unable to calculate these standard deviations. In this case, we substitute our “best guess” for \\(\\pi\\) in the formulas, either from a hypothesis or from a point estimate. Standard error. The standard deviation of a sampling distribution for a statistic represents how far away we would expect the statistic to land from the parameter. Since the formulas for these standard deviations depend on unknown parameters, we substitute our “best guess” for \\(\\pi\\) in the formulas, either from a hypothesis or from a point estimate. The resulting estimated standard deviation is called the standard error of the statistic. 5.1.3 Hypothesis tests In the Martian alphabet example, we utilized a hypothesis test, which is a formal technique for evaluating two competing possibilities. Each hypothesis test involves a null hypothesis, which represents either a skeptical perspective or a perspective of no difference or no effect, and an alternative hypothesis, which represents a new perspective such as the possibility that there has been a change or that there is a treatment effect in an experiment. The alternative hypothesis is usually the reason the scientists set out to do the research in the first place. Null and alternative hypotheses. The null hypothesis (\\(H_0\\)) often represents either a skeptical perspective or a claim to be tested. The alternative hypothesis (\\(H_A\\)) represents an alternative claim under consideration and is often represented by a range of possible values for the parameter of interest. In the Martian alphabet example, which of the two competing possibilities was the null hypothesis? the alternative hypothesis?7 The hypothesis testing framework is a very general tool, and we often use it without a second thought. If a person makes a somewhat unbelievable claim, we are initially skeptical. However, if there is sufficient evidence that supports the claim, we set aside our skepticism. The hallmarks of hypothesis testing are also found in the US court system. The US court system A US court considers two possible claims about a defendant: they are either innocent or guilty. If we set these claims up in a hypothesis framework, which would be the null hypothesis and which the alternative? The jury considers whether the evidence is so convincing (strong) that there is no reasonable doubt regarding the person’s guilt. That is, the skeptical perspective (null hypothesis) is that the person is innocent until evidence is presented that convinces the jury that the person is guilty (alternative hypothesis). Analogously, in a hypothesis test, we assume the null hypothesis until evidence is presented that convinces us the alternative hypothesis is true. Jurors examine the evidence to see whether it convincingly shows a defendant is guilty. Notice that if a jury finds a defendant not guilty, this does not necessarily mean the jury is confident in the person’s innocence. They are simply not convinced of the alternative that the person is guilty. This is also the case with hypothesis testing: even if we fail to reject the null hypothesis, we typically do not accept the null hypothesis as truth. Failing to find strong evidence for the alternative hypothesis is not equivalent to providing evidence that the null hypothesis is true. p-value In the Martian alphabet example, we performed a simulation-based hypothesis test of the hypotheses: \\(H_0\\): The chance a human chooses Bumba on the left is 50%. \\(H_A\\): Humans have a preference for choosing Bumba on the left. The research question—can humans read Martian?—was framed in the context of these hypotheses. The null hypothesis (\\(H_0\\)) was a perspective of no effect (no ability to read Martian). The student data provided a point estimate of 89.5% (\\(34/38 \\times 100\\)%) for the true probability of choosing Bumba on the left. We determined that observing such a sample proportion from chance alone (assuming \\(H_0\\)) would be rare—it would only happen in less than 1 out of 1000 samples. When results like these are inconsistent with \\(H_0\\), we reject \\(H_0\\) in favor of \\(H_A\\). Here, we concluded there humans have a preference for choosing Bumba on the left. The less than 1-in-1000 chance is what we call a p-value, which is a probability quantifying the strength of the evidence against the null hypothesis and in favor of the alternative. p-value. The p-value is the probability of observing data at least as favorable to the alternative hypothesis as our current data set, if the null hypothesis were true. We typically use a summary statistic of the data, such as a proportion or difference in proportions, to help compute the p-value and evaluate the hypotheses. This summary value that is used to compute the p-value is often called the test statistic. When interpreting a p-value, remember that the definition of a p-value has three components. It is a (1) probability. What it is the probability of? It is the probability of (2) our observed sample statistic or one more extreme. Assuming what? It is the probability of our observed sample statistic or one more extreme, (3) assuming the null hypothesis is true: probability data8 null hypothesis What was the test statistic in the Martian alphabet example? The test statistic in the the Martian alphabet example was the sample proportion, \\(\\frac{34}{38} = 0.895\\) (or 89.5%). This is also the point estimate of the true probability that humans would choose Bumba on the left. Since the p-value is a probability, its value will always be between 0 and 1. The closer the p-value is to 0, the stronger the evidence we have against the null hypothesis. Why? A small p-value means that our data are unlikely to occur, if the null hypothesis is true. We take that to mean that the null hypothesis isn’t a plausible assumption, and we reject it. This process mimics the scientific method—it is easier to disprove a theory than prove it. If scientists want to find evidence that a new drug reduces the risk of stroke, then they assume it doesn’t reduce the risk of stroke and then show that the observed data are so unlikely to occur that the more plausible explanation is that the drug works. Think of the p-value as a continuum of strength of evidence against the null. You may use Table 5.1 as a general guide, but remember that there are no hard and fast cutoffs on this scale—the strength of evidence against the null with a p-value of 0.049 is the same as with a p-value of 0.051. Table 5.1: The p-value as a continuum of strength of evidence against the null–a general guide. p-value range Strength of evidence against \\(H_0\\) p-value &lt; 0.01 very strong 0.01 &lt; p-value &lt; 0.05 strong 0.05 &lt; p-value &lt; 0.10 moderate p-value &gt; 0.10 little to no evidence Regardless of the data structure or analysis method, the hypothesis testing framework always follows the same steps—only the details for how we model randomness in the data change. Steps of a hypothesis test. Every hypothesis test follows these same general steps: Frame the research question in terms of hypotheses. Collect and summarize data using a test statistic. Assume the null hypothesis is true, and simulate or mathematically model a null distribution for the test statistic. Compare the observed test statistic to the null distribution to calculate a p-value. Make a conclusion based on the p-value, and write a conclusion in context, in plain language, and in terms of the alternative hypothesis. Decisions and statistical significance In some cases, a decision to the hypothesis test is needed, with the two possible decisions as follows: Reject the null hypothesis Fail to reject the null hypothesis For which values of the p-value should you “reject” a null hypothesis? “fail to reject” a null hypothesis?9 In order to decide between these two options, we need a previously set threshold for our p-value: when the p-value is less than a previously set threshold, we reject \\(H_0\\); otherwise, we fail to reject \\(H_0\\). This threshold is called the significance level, and when the p-value is less than the significance level, we say the results are statistically significant. This means the data provide such strong evidence against \\(H_0\\) that we reject the null hypothesis in favor of the alternative hypothesis. The significance level, often represented by \\(\\alpha\\) (the Greek letter alpha), is typically set to \\(\\alpha = 0.05\\), but can vary depending on the field or the application and the real-life consequences of an incorrect decision. Using a significance level of \\(\\alpha = 0.05\\) in the Martian alphabet study, we can say that the data provided statistically significant evidence against the null hypothesis. Statistical significance. We say that the data provide statistically significant evidence against the null hypothesis if the p-value is less than some reference value called the significance level, denoted by \\(\\alpha\\). What’s so special about 0.05? We often use a threshold of 0.05 to determine whether a result is statistically significant. But why 0.05? Maybe we should use a bigger number, or maybe a smaller number. If you’re a little puzzled, that probably means you’re reading with a critical eye—good job! The OpenIntro authors have a video to help clarify why 0.05: https://www.openintro.org/book/stat/why05/ Sometimes it’s also a good idea to deviate from the standard. We’ll discuss when to choose a threshold different than 0.05 in Section ??. Statistical significance has been a hot topic in the news, related to the “reproducibility crisis” in some scientific fields. We encourage you to read more about the debate on the use of p-values and statistical significance. A good place to start would be the Nature article, “Scientists rise up against statistical significance,” from March 20, 2019. 5.1.4 Confidence intervals A point estimate provides a single plausible value for a parameter. However, a point estimate is rarely perfect—usually there is some error in the estimate. In addition to supplying a point estimate of a parameter, a next logical step would be to provide a plausible range of values for the parameter. A plausible range of values for the population parameter is called a confidence interval. Using only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net. We can throw a spear where we saw a fish, but we will probably miss. On the other hand, if we toss a net in that area, we have a good chance of catching the fish. If we report a point estimate, we probably will not hit the exact population parameter. On the other hand, if we report a range of plausible values—a confidence interval—we have a good shot at capturing the parameter. This reasoning also explains why we can never prove a null hypothesis. Sample statistics will vary from sample to sample. While we can quantify this uncertainty (e.g., we are 95% sure the statistic is within 0.15 of the parameter), we can never be certain that the parameter is an exact value. For example, suppose want to test whether a coin is a fair coin, i.e., \\(H_0: \\pi = 0.50\\) versus \\(H_0: \\pi \\neq 0.50\\), so you toss the coin 10 times to collect data. In those 10 tosses, 6 land on heads and 4 land on tails, resulting in a p-value of 0.75410. We don’t have enough evidence to show that the coin is biased, but surely we wouldn’t say we just proved the coin is fair! There are only two possible decisions in a hypothesis test: (1) reject \\(H_0\\), or (2) fail to reject \\(H_0\\). Since one can never prove a null hypothesis—we can only disprove11 it—we never have the ability to “accept the null.” You may have seen this phrase in other textbooks or articles, but it is incorrect. If we want to be very certain we capture the population parameter, should we use a wider interval or a smaller interval?12 In Section 5.3.3.1 we will discuss different percentages for the confidence interval (e.g., 90% confidence interval or 99% confidence interval). Section ?? also provides a longer discussion on what “95% confidence” actually means. 5.2 The normal distribution Among all the distributions we see in statistics, one is overwhelmingly the most common. The symmetric, unimodal, bell curve is ubiquitous throughout statistics. It is so common that people know it as a variety of names including the normal curve, normal model, or normal distribution.13 Under certain conditions, sample proportions, sample means, and sample differences can be modeled using the normal distribution—the basis for our theory-based inference methods. Additionally, some variables such as SAT scores and heights of US adult males closely follow the normal distribution. Normal distribution facts. Many summary statistics and variables are nearly normal, but none are exactly normal. Thus the normal distribution, while not perfect for any single problem, is very useful for a variety of problems. We will use it in data exploration and to solve important problems in statistics. In this section, we will discuss the normal distribution in the context of data to become more familiar with normal distribution techniques. 5.2.1 Normal distribution model The normal distribution always describes a symmetric, unimodal, bell-shaped curve. However, normal curves can look different depending on the details of the model. Specifically, the normal model can be adjusted using two parameters: mean and standard deviation. As you can probably guess, changing the mean shifts the bell curve to the left or right, while changing the standard deviation stretches or constricts the curve. Figure 5.4 shows the normal distribution with mean \\(0\\) and standard deviation \\(1\\) (which is commonly referred to as the standard normal distribution) on top. A normal distribution with mean \\(19\\) and standard deviation \\(4\\) is shown on the bottom. Figure 5.5 shows the same two normal distributions on the same axis. Figure 5.4: Both curves represent the normal distribution, however, they differ in their center and spread. The normal distribution with mean 0 and standard deviation 1 is called the standard normal distribution. Figure 5.5: The two normal models shown above and now plotted together on the same scale. If a normal distribution has mean \\(\\mu\\) and standard deviation \\(\\sigma\\), we may write the distribution as \\(N(\\mu, \\sigma)\\). The two distributions in Figure 5.5 can be written as \\[\\begin{align*} N(\\mu=0,\\sigma=1)\\quad\\text{and}\\quad N(\\mu=19,\\sigma=4) \\end{align*}\\] Because the mean and standard deviation describe a normal distribution exactly, they are called the distribution’s parameters. Write down the short-hand for a normal distribution with (a) mean 5 and standard deviation 3, (b) mean -100 and standard deviation 10, and (c) mean 2 and standard deviation 9.14 5.2.2 Standardizing with Z-scores Table 5.2 shows the mean and standard deviation for total scores on the SAT and ACT. The distribution of SAT and ACT scores are both nearly normal. Suppose Ann scored 1800 on her SAT and Tom scored 24 on his ACT. Who performed better?15 Table 5.2: Mean and standard deviation for the SAT and ACT. SAT ACT Mean 1500 21 SD 300 5 Figure 5.6: Ann’s and Tom’s scores shown with the distributions of SAT and ACT scores. The solution to the previous example relies on a standardization technique called a Z-score, a method most commonly employed for nearly normal observations (but that may be used with any distribution). The Z-score of an observation is defined as the number of standard deviations it falls above or below the mean. If the observation is one standard deviation above the mean, its Z-score is 1. If it is 1.5 standard deviations below the mean, then its Z-score is -1.5. If \\(x\\) is an observation from a distribution \\(N(\\mu, \\sigma)\\), we define the Z-score mathematically as \\[\\begin{eqnarray*} Z = \\frac{x-\\mu}{\\sigma} \\end{eqnarray*}\\] Using \\(\\mu_{SAT}=1500\\), \\(\\sigma_{SAT}=300\\), and \\(x_{Ann}=1800\\), we find Ann’s Z-score: \\[\\begin{eqnarray*} Z_{Ann} = \\frac{x_{Ann} - \\mu_{SAT}}{\\sigma_{SAT}} = \\frac{1800-1500}{300} = 1 \\end{eqnarray*}\\] The Z-score. The Z-score of an observation is the number of standard deviations it falls above or below the mean. We compute the Z-score for an observation \\(x\\) that follows a distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) by first subtracting its mean, then dividing by its standard deviation: \\[\\begin{eqnarray*} Z = \\frac{x-\\mu}{\\sigma} \\end{eqnarray*}\\] Use Tom’s ACT score, 24, along with the ACT mean and standard deviation to compute his Z-score.16 Observations above the mean always have positive Z-scores while those below the mean have negative Z-scores. If an observation is equal to the mean (e.g., SAT score of 1500), then the Z-score is \\(0\\). Let \\(X\\) represent a random variable from \\(N(\\mu=3, \\sigma=2)\\), and suppose we observe \\(x=5.19\\). (a) Find the Z-score of \\(x\\). (b) Use the Z-score to determine how many standard deviations above or below the mean \\(x\\) falls.17 Head lengths of brushtail possums follow a nearly normal distribution with mean 92.6 mm and standard deviation 3.6 mm. Compute the Z-scores for possums with head lengths of 95.4 mm and 85.8 mm.18 We can use Z-scores to roughly identify which observations are more unusual than others. One observation \\(x_1\\) is said to be more unusual than another observation \\(x_2\\) if the absolute value of its Z-score is larger than the absolute value of the other observation’s Z-score: \\(|Z_1| &gt; |Z_2|\\). This technique is especially insightful when a distribution is symmetric. Which of the two brushtail possum observations in the previous guided practice is more unusual?19 5.2.3 Normal probability calculations in R Ann from the SAT Guided Practice earned a score of 1800 on her SAT with a corresponding \\(Z=1\\). She would like to know what percentile she falls in among all SAT test-takers. Ann’s percentile is the percentage of people who earned a lower SAT score than Ann. We shade the area representing those individuals in Figure 5.7. The total area under the normal curve is always equal to 1, and the proportion of people who scored below Ann on the SAT is equal to the area shaded in Figure 5.7: 0.8413. In other words, Ann is in the \\(84^{th}\\) percentile of SAT takers. Figure 5.7: The normal model for SAT scores, shading the area of those individuals who scored below Ann. We can use the normal model to find percentiles or probabilities. In R, the function to calculate normal probabilities is pnorm. The normTail function is available in the openintro R package and will draw the associated curve if it is helpful. In the code below, we find the percentile of \\(Z=0.43\\) is 0.6664, or the \\(66.64^{th}\\) percentile. pnorm(0.43, m = 0, s = 1) #&gt; [1] 0.666 openintro::normTail(0.43, m = 0, s = 1) We can also find the Z-score associated with a percentile. For example, to identify Z for the \\(80^{th}\\) percentile, we use qnorm which identifies the quantile for a given percentage. The quantile represents the cutoff value. (To remember the function qnorm as providing a cutoff, notice that both qnorm and “cutoff” start with the sound “kuh”. To remember the pnorm function as providing a probability from a given cutoff, notice that both pnorm and probability start with the sound “puh”.) We determine the Z-score for the \\(80^{th}\\) percentile using qnorm: 0.84. qnorm(0.80, m = 0, s = 1) #&gt; [1] 0.842 openintro::normTail(0.80, m = 0, s = 1) We can use these functions with other normal distributions than the standard normal distribution by specifying the mean as the argument for m and the standard deviation as the argument for s. Here we determine the proportion of ACT test takers who scored worse than Tom on the ACT: 0.73. pnorm(24, m = 21, s = 5) #&gt; [1] 0.726 openintro::normTail(24, m = 21, s = 5) Determine the proportion of SAT test takers who scored better than Ann on the SAT.20 5.2.4 Normal probability examples Cumulative SAT scores are approximated well by a normal model, \\(N(\\mu=1500, \\sigma=300)\\). Shannon is a randomly selected SAT taker, and nothing is known about Shannon’s SAT aptitude. What is the probability that Shannon scores at least 1630 on her SATs? First, always draw and label a picture of the normal distribution. (Drawings need not be exact to be useful.) We are interested in the chance she scores above 1630, so we shade the upper tail. See the normal curve below. The picture shows the mean and the values at 2 standard deviations above and below the mean. The simplest way to find the shaded area under the curve makes use of the Z-score of the cutoff value. With \\(\\mu=1500\\), \\(\\sigma=300\\), and the cutoff value \\(x=1630\\), the Z-score is computed as \\[\\begin{eqnarray*} Z = \\frac{x - \\mu}{\\sigma} = \\frac{1630 - 1500}{300} = \\frac{130}{300} = 0.43 \\end{eqnarray*}\\] We use software to find the percentile of \\(Z=0.43\\), which yields 0.6664. However, the percentile describes those who had a Z-score lower than 0.43. To find the area above \\(Z=0.43\\), we compute one minus the area of the lower tail, as seen below. The probability Shannon scores at least 1630 on the SAT is 0.3336. Always draw a picture first, and find the Z-score second. For any normal probability situation, always always always draw and label the normal curve and shade the area of interest first. The picture will provide an estimate of the probability. After drawing a figure to represent the situation, identify the Z-score for the observation of interest. If the probability of Shannon scoring at least 1630 is 0.3336, then what is the probability she scores less than 1630? Draw the normal curve representing this exercise, shading the lower region instead of the upper one.21 Edward earned a 1400 on his SAT. What is his percentile? First, a picture is needed. Edward’s percentile is the proportion of people who do not get as high as a 1400. These are the scores to the left of 1400. Identifying the mean \\(\\mu=1500\\), the standard deviation \\(\\sigma=300\\), and the cutoff for the tail area \\(x=1400\\) makes it easy to compute the Z-score: \\[\\begin{eqnarray*} Z = \\frac{x - \\mu}{\\sigma} = \\frac{1400 - 1500}{300} = -0.33 \\end{eqnarray*}\\] Using the normal probability table, identify the row of \\(-0.3\\) and column of \\(0.03\\), which corresponds to the probability \\(0.3707\\). Edward is at the \\(37^{th}\\) percentile. Use the results of the previous example to compute the proportion of SAT takers who did better than Edward. Also draw a new picture.22 Areas to the right. The normal probability table in most books gives the area to the left. If you would like the area to the right, first find the area to the left and then subtract this amount from one. Stuart earned an SAT score of 2100. Draw a picture for each part. (a) What is his percentile? (b) What percent of SAT takers did better than Stuart?23 Based on a sample of 100 men,24 the heights of male adults between the ages 20 and 62 in the US is nearly normal with mean 70.0’’ and standard deviation 3.3’’. Mike is 5’7’’ and Jim is 6’4’’. (a) What is Mike’s height percentile? (b) What is Jim’s height percentile? Also draw one picture for each part.25 The last several problems have focused on finding the probability or percentile for a particular observation. What if you would like to know the observation corresponding to a particular percentile? Erik’s height is at the \\(40^{th}\\) percentile. How tall is he? As always, first draw the picture (see below). In this case, the lower tail probability is known (0.40), which can be shaded on the diagram. We want to find the observation that corresponds to this value. As a first step in this direction, we determine the Z-score associated with the \\(40^{th}\\) percentile. Because the percentile is below 50%, we know \\(Z\\) will be negative. Looking in the negative part of the normal probability table, we search for the probability inside the table closest to 0.4000. We find that 0.4000 falls in row \\(-0.2\\) and between columns \\(0.05\\) and \\(0.06\\). Since it falls closer to \\(0.05\\), we take this one: \\(Z=-0.25\\). Knowing \\(Z_{Erik}=-0.25\\) and the population parameters \\(\\mu=70\\) and \\(\\sigma=3.3\\) inches, the Z-score formula can be set up to determine Erik’s unknown height, labeled \\(x_{Erik}\\): \\[\\begin{eqnarray*} -0.25 = Z_{Erik} = \\frac{x_{Erik} - \\mu}{\\sigma} = \\frac{x_{Erik} - 70}{3.3} \\end{eqnarray*}\\] Solving for \\(x_{Erik}\\) yields the height 69.18 inches. That is, Erik is about 5’9’’ (this is notation for 5-feet, 9-inches). qnorm(0.4, m = 0, s = 1) #&gt; [1] -0.253 What is the adult male height at the \\(82^{nd}\\) percentile? Again, we draw the figure first (see below). Next, we want to find the Z-score at the \\(82^{nd}\\) percentile, which will be a positive value. Using qnorm(), the \\(82^{nd}\\) percentile corresponds to \\(Z=0.92\\). Finally, the height \\(x\\) is found using the Z-score formula with the known mean \\(\\mu\\), standard deviation \\(\\sigma\\), and Z-score \\(Z=0.92\\): \\[\\begin{eqnarray*} 0.92 = Z = \\frac{x-\\mu}{\\sigma} = \\frac{x - 70}{3.3} \\end{eqnarray*}\\] This yields 73.04 inches or about 6’1’’ as the height at the \\(82^{nd}\\) percentile. qnorm(0.82, m = 0, s = 1) #&gt; [1] 0.915 What is the \\(95^{th}\\) percentile for SAT scores? What is the \\(97.5^{th}\\) percentile of the male heights? As always with normal probability problems, first draw a picture.26 What is the probability that a randomly selected male adult is at least 6’2’’ (74 inches)? What is the probability that a male adult is shorter than 5’9’’ (69 inches)?27 What is the probability that a random adult male is between 5’9’’ and 6’2’’? These heights correspond to 69 inches and 74 inches. First, draw the figure. The area of interest is no longer an upper or lower tail. The total area under the curve is 1. If we find the area of the two tails that are not shaded (from the previous Guided Practice, these areas are \\(0.3821\\) and \\(0.1131\\)), then we can find the middle area: That is, the probability of being between 5’9’’ and 6’2’’ is 0.5048. What percent of SAT takers get between 1500 and 2000?28 What percent of adult males are between 5’5’’ and 5’7’’?29 5.2.5 68-95-99.7 rule Here, we present a useful general rule for the probability of falling within 1, 2, and 3 standard deviations of the mean in the normal distribution. The rule will be useful in a wide range of practical settings, especially when trying to make a quick estimate without a calculator or Z table. Figure 5.8: Probabilities for falling within 1, 2, and 3 standard deviations of the mean in a normal distribution. Use pnorm to confirm that about 68%, 95%, and 99.7% of observations fall within 1, 2, and 3, standard deviations of the mean in the normal distribution, respectively. For instance, first find the area that falls between \\(Z=-1\\) and \\(Z=1\\), which should have an area of about 0.68. Similarly there should be an area of about 0.95 between \\(Z=-2\\) and \\(Z=2\\).30 It is possible for a normal random variable to fall 4, 5, or even more standard deviations from the mean. However, these occurrences are very rare if the data are nearly normal. The probability of being further than 4 standard deviations from the mean is about 1-in-30,000. For 5 and 6 standard deviations, it is about 1-in-3.5 million and 1-in-1 billion, respectively. SAT scores closely follow the normal model with mean \\(\\mu = 1500\\) and standard deviation \\(\\sigma = 300\\). (a) About what percent of test takers score 900 to 2100? (b) What percent score between 1500 and 2100?31 5.3 One proportion Notation. \\(n\\) = sample size (number of observational units in the data set) \\(\\hat{p}\\) = sample proportion (number of “successes” divided by the sample size) \\(\\pi\\) = population proportion32 A single proportion is used to summarize data when we measured a single categorical variable on each observational unit—the single variable is measured as either a success or failure (e.g., “surgical complication” vs. “no surgical complication”)33. 5.3.1 Hypothesis test for \\(H_0: \\pi = \\pi_0\\) Regardless of if we use simulation-based methods or theory-based methods, the first few steps of a hypothesis test start out the same: Frame the research question in terms of hypotheses. Collect and summarize data using a test statistic. People providing an organ for donation sometimes seek the help of a special medical consultant. These consultants assist the patient in all aspects of the surgery, with the goal of reducing the possibility of complications during the medical procedure and recovery. Patients might choose a consultant based in part on the historical complication rate of the consultant’s clients. One consultant tried to attract patients by noting the average complication rate for liver donor surgeries in the US is about 10%, but her clients have had only 3 complications in the 62 liver donor surgeries she has facilitated. She claims this is strong evidence that her work meaningfully contributes to reducing complications (and therefore she should be hired!). Using these data, is it possible to assess the consultant’s claim that her work meaningfully contributes to reducing complications? No. The claim is that there is a causal connection, but the data are observational, so we must be on the lookout for confounding variables. For example, maybe patients who can afford a medical consultant can afford better medical care, which can also lead to a lower complication rate. While it is not possible to assess the causal claim, it is still possible to understand the consultant’s true rate of complications. We will let \\(\\pi\\) represent the true complication rate for liver donors working with this consultant. This “true” complication probability is called the parameter of interest34.) The sample proportion for the complication rate is 3 complications divided by the 62 surgeries the consultant has worked on: \\(\\hat{p} = 3/62 = 0.048\\). Since this value is estimated from sample data, it is called a statistic. The statistic \\(\\hat{p}\\) is our point estimate, or “best guess,” for \\(\\pi\\). Parameters and statistics. A parameter is the “true” value of interest. We typically estimate the parameter using a statistic or point estimate from a sample of data. For example, we estimate the probability \\(\\pi\\) of a complication for a client of the medical consultant by examining the past complications rates of her clients: \\[\\hat{p} = 3 / 62 = 0.048\\qquad\\text{is used to estimate}\\qquad \\pi\\] Summary measures that summarize a sample of data, such as \\(\\hat{p}\\), are called statistics. Numbers that summarize an entire population, such as \\(\\pi\\), are called parameters. You can remember this distinction by looking at the first letter of each term: Statistics summarize Samples; Parameters summarize Populations. We typically use Roman letters to symbolize statistics (e.g., \\(\\bar{x}\\), \\(\\hat{p}\\)), and Greek letters to symbolize parameters (e.g., \\(\\mu\\), \\(\\pi\\)). Since we rarely can measure the entire population, and thus rarely know the actual parameter values, we like to say, “We don’t know Greek, and we don’t know parameters!” Write out hypotheses in both plain and statistical language to test for the association between the consultant’s work and the true complication rate, \\(\\pi\\), for the consultant’s clients. In words: \\(H_0\\): There is no association between the consultant’s contributions and the clients’ complication rate. \\(H_A\\): Patients who work with the consultant tend to have a complication rate lower than 10%. In statistical language In statistical language: \\(H_0: \\pi=0.10\\) \\(H_A: \\pi&lt;0.10\\) To assess these hypotheses, we need to evaluate the possibility of a sample value (\\(\\hat{p}\\)) as far below the null value, \\(\\pi_0=0.10\\) as what was observed. Null value of a hypothesis test. The null value is the reference value for the parameter in \\(H_0\\), and it is sometimes represented with the parameter’s label with a subscript 0 (or “null”), e.g., \\(\\pi_0\\) (just like \\(H_0\\)). The deviation of the sample statistic from the null hypothesized parameter is usually quantified with a p-value. The p-value is computed based on the null distribution, which is the distribution of the test statistic if the null hypothesis is true. Supposing the null hypothesis is true, we can compute the p-value by identifying the chance of observing a test statistic that favors the alternative hypothesis at least as strongly as the observed test statistic. The null distribution can be created through simulation (simulation-based methods), or can be modeled by a mathematical function (theory-based methods). Simulation-based method for calculating the p-value We want to identify the sampling distribution of the test statistic (\\(\\hat{p}\\)) if the null hypothesis was true. In other words, we want to see how the sample proportion changes due to chance alone. Then we plan to use this information to decide whether there is enough evidence to reject the null hypothesis. Under the null hypothesis, 10% of liver donors have complications during or after surgery. Suppose this rate was really no different for the consultant’s clients (for all the consultant’s clients, not just the 62 previously measured). If this was the case, we could simulate 62 clients to get a sample proportion for the complication rate from the null distribution. This is a similar scenario to the one we encountered in Section 5.1.1, with one important difference—the null value is 0.10, not 0.50. Thus, a flipping a coin to simulate whether a client had complications would not be simulating under the correct null hypothesis. What physical object could you use to simulate a random sample of 62 clients who had a 10% chance of complications? How would you use this object?35 Assuming the true complication rate for the consultant’s clients is 10%, each client can be simulated using a bag of marbles with 10% red marbles and 90% white marbles. Sampling a marble from the bag (with 10% red marbles) is one way of simulating whether a patient has a complication if the true complication rate is 10% for the data. If we select 62 marbles and then compute the proportion of patients with complications in the simulation, \\(\\hat{p}_{sim}\\), then the resulting sample proportion is calculated exactly from a sample from the null distribution. An undergraduate student was paid $2 to complete this simulation. There were 5 simulated cases with a complication and 57 simulated cases without a complication, i.e., \\(\\hat{p}_{sim} = 5/62 = 0.081\\). Is this one simulation enough to determine whether or not we should reject the null hypothesis? No. To assess the hypotheses, we need to see a distribution of many \\(\\hat{p}_{sim}\\), not just a single draw from this sampling distribution. One simulation isn’t enough to get a sense of the null distribution; many simulation studies are needed. Roughly 10,000 seems sufficient. However, paying someone to simulate 10,000 studies by hand is a waste of time and money. Instead, simulations are typically programmed into a computer, which is much more efficient. Figure 5.9 shows the results of 10,000 simulated studies. The proportions that are equal to or less than \\(\\hat{p}=0.048\\) are shaded. The shaded areas represent sample proportions under the null distribution that provide at least as much evidence as \\(\\hat{p}\\) favoring the alternative hypothesis. There were 1222 simulated sample proportions with \\(\\hat{p}_{sim} \\leq 0.048\\). We use these to construct the null distribution’s left-tail area and find the p-value: \\[\\begin{align} \\text{left tail area }\\label{estOfPValueBasedOnSimulatedNullForSingleProportion} &amp;= \\frac{\\text{Number of observed simulations with }\\hat{p}_{sim}\\leq\\text{ 0.048}}{10000} \\end{align}\\] Of the 10,000 simulated \\(\\hat{p}_{sim}\\), 1222 were equal to or smaller than \\(\\hat{p}\\). Since the hypothesis test is one-sided, the estimated p-value is equal to this tail area: 0.1222. Figure 5.9: The null distribution for \\(\\hat{p}\\), created from 10,000 simulated studies. The left tail, representing the p-value for the hypothesis test, contains 12.22% of the simulations. Because the estimated p-value is 0.1222, which is not small, we have little to no evidence against the null hypothesis. Explain what this means in plain language in the context of the problem.36 Does the conclusion in the previous Guided Practice imply there is no real association between the surgical consultant’s work and the risk of complications? Explain.37 Regardless of the statistical method chosen, the p-value is always derived by analyzing the null distribution of the test statistic. The normal model poorly approximates the null distribution for \\(\\hat{p}\\) when the success-failure condition is not satisfied. As a substitute, we can generate the null distribution using simulated sample proportions and use this distribution to compute the tail area, i.e., the p-value. Neither the p-value approximated by the normal distribution nor the simulated p-value are exact, because the normal distribution and simulated null distribution themselves are not exact, only a close approximation. An exact p-value can be generated using the binomial distribution, but that method will not be covered in this text. Theory-based method for calculating the p-value In Section 5.1.2, we introduced the normal distribution and showed how it can be used as a mathematical model to describe the variability of a sample mean or sample proportion as a result of the Central Limit Theorem. We explored the normal distribution further in Section 5.2. There are conditions under which a sample proportion \\(\\hat{p}\\) is well-modeled using a normal distribution. When the sample observations are independent and the sample size is sufficiently large, the normal model will describe the variability quite well; when the observations violate the conditions, the normal model can be inaccurate. Sampling distribution of \\(\\hat{p}\\). The sampling distribution for \\(\\hat{p}\\) based on a sample of size \\(n\\) from a population with a true proportion \\(\\pi\\) is nearly normal when: The sample’s observations are independent, e.g., are from a simple random sample. We expected to see at least 10 successes and 10 failures in the sample, i.e., \\(n\\pi\\geq10\\) and \\(n(1-\\pi)\\geq10\\). This is called the success-failure condition. When these conditions are met, then the sampling distribution of \\(\\hat{p}\\) is nearly normal with mean \\(\\pi\\) and standard deviation \\(\\sqrt{\\frac{\\ \\pi(1-\\pi)\\ }{n}}\\). The success-failure condition listed above is only necessary for the sampling distribution of \\(\\hat{p}\\) to be approximately normal. The mean of the sampling distribution of \\(\\hat{p}\\) is \\(\\pi\\), and the standard deviation is \\(\\sqrt{\\frac{\\ \\pi(1-\\pi)\\ }{n}}\\), regardless of the sample size. Typically we don’t know the true proportion \\(\\pi\\), so we substitute some value to check the success-failure condition and to estimate the standard deviation of the sampling distribution of \\(\\hat{p}\\). The independence condition is a more nuanced requirement. When it isn’t met, it is important to understand how and why it isn’t met. For example, there exist no statistical methods available to truly correct the inherent biases of data from a convenience sample. On the other hand, if we took a cluster random sample (see Section ??), the observations wouldn’t be independent, but suitable statistical methods are available for analyzing the data (but they are beyond the scope of even most second or third courses in statistics)38. In the examples based on large sample theory, we modeled \\(\\hat{p}\\) using the normal distribution. Why is this not appropriate for the study on the medical consultant? The independence assumption may be reasonable if each of the surgeries is from a different surgical team. However, the success-failure condition is not satisfied. Under the null hypothesis, we would anticipate seeing \\(62\\times 0.10=6.2\\) complications, not the 10 required for the normal approximation. Since theory-based methods cannot be used on the medical consultant example, we’ll turn to another example to demonstrate these methods, where conditions for approximating the distribution of \\(\\hat{p}\\) by a normal distribution are met. One possible regulation for payday lenders is that they would be required to do a credit check and evaluate debt payments against the borrower’s finances. We would like to know: would borrowers support this form of regulation? Set up hypotheses to evaluate whether borrowers have a majority support for this type of regulation.39 To apply the normal distribution to model the null distribution, the independence and success-failure conditions must be satisfied. In a hypothesis test, the success-failure condition is checked using the null proportion: we verify \\(np_0\\) and \\(n(1-p_0)\\) are at least 10, where \\(p_0\\) is the null value. Do payday loan borrowers support a regulation that would require lenders to pull their credit report and evaluate their debt payments? From a random sample of 826 borrowers, 51% said they would support such a regulation. Is it reasonable use a normal distribution to model \\(\\hat{p}\\) for a hypothesis test here?40 Using the hypotheses and data from the previous two Guided Practices, evaluate whether the poll on lending regulations provides convincing evidence that a majority of payday loan borrowers support a new regulation that would require lenders to pull credit reports and evaluate debt payments. With hypotheses already set up and conditions checked, we can move onto calculations. The standard error in the context of a one-proportion hypothesis test is computed using the null value, \\(p_0\\): \\[\\begin{align*} SE = \\sqrt{\\frac{p_0 (1 - p_0)}{n}} = \\sqrt{\\frac{0.5 (1 - 0.5)}{826}} = 0.017 \\end{align*}\\] A picture of the normal model for null distribution of sample proportions is shown below in Figure ??, with the p-value represented by the shaded region. Note that this distribution is centered at 0.50, the null value, and has standard deviation 0.017. Under \\(H_0\\), the probability of observing \\(\\hat{p} = 0.51\\) or higher is 0.278, the area above 0.51 on the null distribution. With a p-value of 0.278, the poll does not provide convincing evidence that a majority of payday loan borrowers support regulations around credit checks and evaluation of debt payments. You’ll note that this conclusion is somewhat unsatisfactory because there is no conclusion, as is the case with larger p-values. That is, there is no resolution one way or the other about public opinion. We cannot claim that exactly 50% of people support the regulation, but we cannot claim a majority support it either. Often, with theory-based methods, we use a standardized statistic rather than the original statistic. A standardized statistic is computed by subtracting the mean of the null distribution from the original statistic, then dividing by the standard error. When we are modeling the null distribution with a normal distribution, this standardized statistic is called \\(Z\\), since it is the \\(Z-score\\) of the sample proportion. Standardized sample proportion. The standardized statistic for theory-based methods for one proportion is \\[ Z = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}} \\] where \\(p_0\\) is the null value. The denominator, \\(\\sqrt{\\frac{p_0(1-p_0)}{n}}\\), is called the null standard error. With the standardized statistic as our test statistic, we can find the p-value as the area under a standard normal distribution. Do payday loan borrowers support a regulation that would require lenders to pull their credit report and evaluate their debt payments? From a random sample of 826 borrowers, 51% said they would support such a regulation. We set up hypotheses and checked conditions previously. Now calculate and interpret the standardized statistic, then use the standard normal distribution to calculate the approximate p-value. Our sample proportion is \\(\\hat{p} = 0.51\\). Since our null value is \\(p_0 = 0.50\\), the null standard error is \\[\\begin{align*} SE = \\sqrt{\\frac{p_0 (1 - p_0)}{n}} = \\sqrt{\\frac{0.5 (1 - 0.5)}{826}} = 0.017 \\end{align*}\\] The standardized statistic is \\[\\begin{align*} Z = \\frac{0.51 - 0.50}{0.017} = 0.57 \\end{align*}\\] Interpreting this value, we can say that our sample proportion of 0.51 was only 0.57 standard errors above the null value of 0.50. The p-value is the area above \\(Z = 0.57\\) on a standard normal distribution—0.278—the same p-value we would obtain by finding the area above \\(\\hat{p} = 0.51\\) on a normal distribution with mean 0.50 and standard deviation 0.017. Theory-based hypothesis test for a proportion: one-sample \\(Z\\)-test. Frame the research question in terms of hypotheses. Using the null value, \\(p_0\\), verify the conditions for using the normal distribution to approximate the null distribution. Calculate the test statistic: \\[ Z = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}} \\] Use the test statistic and the standard normal distribution to calculate the p-value. Make a conclusion based on the p-value, and write a conclusion in context, in plain language, and in terms of the alternative hypothesis. Violating conditions We’ve spent a lot of time discussing conditions for when \\(\\hat{p}\\) can be reasonably modeled by a normal distribution. What happens when the success-failure condition fails? What about when the independence condition fails? In either case, the general ideas of confidence intervals and hypothesis tests remain the same, but the strategy or technique used to generate the interval or p-value change. When the success-failure condition isn’t met for a hypothesis test, we can simulate the null distribution of \\(\\hat{p}\\) using the null value, \\(p_0\\), as seen in Section 5.3.1. Unfortunately, methods for dealing with observations which are not independent are outside the scope of this book. 5.3.2 Two-sided hypotheses 5.3.3 Confidence interval for \\(\\pi\\) A confidence interval provides a range of plausible values for the parameter \\(p\\), and when \\(\\hat{p}\\) can be modeled using a normal distribution, the confidence interval for \\(p\\) takes the form \\[\\begin{align*} \\hat{p} \\pm z^{\\star} \\times SE. \\end{align*}\\] We have seen \\(\\hat{p}\\) to be the sample proportion. The value \\(z^{\\star}\\) determines the confidence level (previously set to be 1.96) and will be discussed in detail in the examples following. The value of the standard error, \\(SE\\), depends heavily on the sample size. Standard Error of one proportion, \\(\\hat{p}\\) When the conditions are met so that the distribution fo \\(\\hat{p}\\) is nearly normal, the variability of a single proportion, \\(\\hat{p}\\) is well described by: \\[SE(\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}\\] Note that we almost never know the true value of \\(p\\). A more helpful formula to use is: \\[SE(\\hat{p}) \\approx \\sqrt{\\frac{(\\mbox{best guess of }p)(1 - \\mbox{best guess of }p)}{n}}\\] For hypothesis testing, we often use \\(p_0\\) as the best guess of \\(p\\). For confidence intervals, we typically use \\(\\hat{p}\\) as the best guess of \\(p\\). Consider taking many polls of registered voters (i.e., random samples) of size 300 asking them if they support legalized marijuana. It is suspected that about 2/3 of all voters support legalized marijuana. To understand how the sample proportion (\\(\\hat{p}\\)) would vary across the samples, calculate the standard error of \\(\\hat{p}\\).41 Variability of the statistic A simple random sample of 826 payday loan borrowers was surveyed to better understand their interests around regulation and costs. 70% of the responses supported new regulations on payday lenders. Is it reasonable to model the variability of \\(\\hat{p}\\) from sample to sample using a normal distribution? Estimate the standard error of \\(\\hat{p}\\). Construct a 95% confidence interval for \\(p\\), the proportion of payday borrowers who support increased regulation for payday lenders. The data are a random sample, so the observations are independent and representative of the population of interest. We also must check the success-failure condition, which we do using \\(\\hat{p}\\) in place of \\(p\\) when computing a confidence interval: \\[\\begin{align*} \\text{Support: } n p &amp; \\approx 826 \\times 0.70 = 578 &amp;\\text{Not: } n (1 - p) &amp; \\approx 826 \\times (1 - 0.70) = 248 \\end{align*}\\] Since both values are at least 10, we can use the normal distribution to model \\(\\hat{p}\\). Because \\(p\\) is unknown and the standard error is for a confidence interval, use \\(\\hat{p}\\) in place of \\(p\\) in the formula. \\(SE = \\sqrt{\\frac{p(1-p)}{n}} \\approx \\sqrt{\\frac{0.70 (1 - 0.70)} {826}} = 0.016\\). Using the point estimate 0.70, \\(z^{\\star} = 1.96\\) for a 95% confidence interval, and the standard error \\(SE = 0.016\\) from the pervious Guided Practice, the confidence interval is \\[\\begin{eqnarray*} \\text{point estimate} \\ \\pm\\ z^{\\star} \\times SE \\quad\\to\\quad 0.70 \\ \\pm\\ 1.96 \\times 0.016 \\quad\\to\\quad (0.669, 0.731) \\end{eqnarray*}\\] We are 95% confident that the true proportion of payday borrowers who supported regulation at the time of the poll was between 0.669 and 0.731. Constructing a confidence interval for a single proportion. There are three steps to constructing a confidence interval for \\(p\\). Check independence and the success-failure condition using \\(\\hat{p}\\). If the conditions are met, the sampling distribution of \\(\\hat{p}\\) may be well-approximated by the normal model. Construct the standard error using \\(\\hat{p}\\) in place of \\(p\\) in the standard error formula. Apply the general confidence interval formula. 5.3.3.1 Changing the confidence level Suppose we want to consider confidence intervals where the confidence level is somewhat higher than 95%: perhaps we would like a confidence level of 99%. Think back to the analogy about trying to catch a fish: if we want to be more sure that we will catch the fish, we should use a wider net. To create a 99% confidence level, we must also widen our 95% interval. On the other hand, if we want an interval with lower confidence, such as 90%, we could make our original 95% interval slightly slimmer. The 95% confidence interval structure provides guidance in how to make intervals with new confidence levels. Below is a general 95% confidence interval for a point estimate that comes from a nearly normal distribution: \\[\\begin{eqnarray} \\text{point estimate}\\ \\pm\\ 1.96\\times SE \\end{eqnarray}\\] There are three components to this interval: the point estimate, “1.96”, and the standard error. The choice of \\(1.96\\times SE\\) was based on capturing 95% of the data since the estimate is within 1.96 standard errors of the true value about 95% of the time. The choice of 1.96 corresponds to a 95% confidence level. If \\(X\\) is a normally distributed random variable, how often will \\(X\\) be within 2.58 standard deviations of the mean?42 Figure 5.10: The area between -\\(z^{\\star}\\) and \\(z^{\\star}\\) increases as \\(|z^{\\star}|\\) becomes larger. If the confidence level is 99%, we choose \\(z^{\\star}\\) such that 99% of the normal curve is between -\\(z^{\\star}\\) and \\(z^{\\star}\\), which corresponds to 0.5% in the lower tail and 0.5% in the upper tail: \\(z^{\\star}=2.58\\). To create a 99% confidence interval, change 1.96 in the 95% confidence interval formula to be \\(2.58\\). The previous Guided Practice highlights that 99% of the time a normal random variable will be within 2.58 standard deviations of its mean. This approach – using the Z-scores in the normal model to compute confidence levels – is appropriate when the point estimate is associated with a normal distribution and we can properly compute the standard error. Thus, the formula for a 99% confidence interval is: \\[\\begin{eqnarray*} \\text{point estimate}\\ \\pm\\ 2.58\\times SE \\end{eqnarray*}\\] The normal approximation is crucial to the precision of the \\(z^\\star\\) confidence intervals (in contrast to the bootstrap confidence intervals). When the normal model is not a good fit, we will use alternative distributions that better characterize the sampling distribution or we will use bootstrapping procedures. Create a 99% confidence interval for the impact of the stent on the risk of stroke using the data from Section 1.1. The point estimate is 0.090, and the standard error is \\(SE = 0.028\\). It has been verified for you that the point estimate can reasonably be modeled by a normal distribution.43 Mathematical model confidence interval for any confidence level. If the point estimate follows the normal model with standard error \\(SE\\), then a confidence interval for the population parameter is \\[\\begin{eqnarray*} \\text{point estimate}\\ \\pm\\ z^{\\star} \\times SE \\end{eqnarray*}\\] where \\(z^{\\star}\\) corresponds to the confidence level selected. Figure 5.10 provides a picture of how to identify \\(z^{\\star}\\) based on a confidence level. We select \\(z^{\\star}\\) so that the area between -\\(z^{\\star}\\) and \\(z^{\\star}\\) in the normal model corresponds to the confidence level. Previously, we found that implanting a stent in the brain of a patient at risk for a stroke increased the risk of a stroke. The study estimated a 9% increase in the number of patients who had a stroke, and the standard error of this estimate was about \\(SE = 2.8%\\). Compute a 90% confidence interval for the effect.44 Add tappers and listeners case study example from old Ch. 5 here. 5.4 Difference of two proportions Notation. \\(n_1\\), \\(n_2\\) = sample sizes of two independent samples \\(\\hat{p}_1\\), \\(\\hat{p}_2\\) = sample proportions of two independent samples \\(\\pi_1\\), \\(\\pi_2\\) = population proportions of two independent samples We now extend the methods from Section 5.3 to apply confidence intervals and hypothesis tests to differences in population proportions that come from two groups: \\(\\pi_1 - \\pi_2\\). In our investigations, we’ll identify a reasonable point estimate of \\(p_1 - p_2\\) based on the sample, and you may have already guessed its form: \\(\\hat{p}_1 - \\hat{p}_2\\). Then we’ll look at the inferential analysis in three different ways: using a randomization test, applying bootstrapping for interval estimates, and, if we verify that the point estimate can be modeled using a normal distribution, we compute the estimate’s standard error, and we apply the mathematical framework. 5.4.1 Randomization test for \\(H_0: p_1 - p_2 = 0\\) Observed data We consider a study on a new malaria vaccine called PfSPZ. In this study, volunteer patients were randomized into one of two experiment groups: 14 patients received an experimental vaccine or 6 patients received a placebo vaccine. Nineteen weeks later, all 20 patients were exposed to a drug-sensitive malaria virus strain; the motivation of using a drug-sensitive strain of virus here is for ethical considerations, allowing any infections to be treated effectively. The results are summarized in Table 5.3, where 9 of the 14 treatment patients remained free of signs of infection while all of the 6 patients in the control group patients showed some baseline signs of infection. Table 5.3: Summary results for the malaria vaccine experiment. outcome infection no infection Total vaccine 5 9 14 treatment placebo 6 0 6 Total 11 9 20 Is this an observational study or an experiment? What implications does the study type have on what can be inferred from the results?45 In this study, a smaller proportion of patients who received the vaccine showed signs of an infection (35.7% versus 100%). However, the sample is very small, and it is unclear whether the difference provides convincing evidence that the vaccine is effective. As we saw in Section ??, we can randomize the responses (infection or no infection) to the treatment conditions under the null hypothesis of independence and compute possible differences in proportions. The process by which we randomize observations to two groups is summarized and visualized in Figure ??. Variability of the statistic Figure 5.11 shows a stacked plot of the differences found from 100 randomization simulations (i.e., repeated iterations as described in Figure ??), where each dot represents a simulated difference between the infection rates (control rate minus treatment rate). Figure 5.11: A stacked dot plot of differences from 100 simulations produced under the independence model \\(H_0\\), where in these simulations infections are unaffected by the vaccine. Two of the 100 simulations had a difference of at least 64.3%, the difference observed in the study. Observed statistic vs null statistics Note that the distribution of these simulated differences is centered around 0. We simulated the differences assuming that the independence model was true, and under this condition, we expect the difference to be near zero with some random fluctuation, where near is pretty generous in this case since the sample sizes are so small in this study. How often would you observe a difference of at least 64.3% (0.643) according to Figure 5.11? Often, sometimes, rarely, or never? It appears that a difference of at least 64.3% due to chance alone would only happen about 2% of the time according to Figure 5.11. Such a low probability indicates a rare event. The difference of 64.3% being a rare event suggests two possible interpretations of the results of the study: \\(H_0\\) Independence model. The vaccine has no effect on infection rate, and we just happened to observe a difference that would only occur on a rare occasion. \\(H_A\\) Alternative model. The vaccine has an effect on infection rate, and the difference we observed was actually due to the vaccine being effective at combating malaria, which explains the large difference of 64.3%. Based on the simulations, we have two options: We conclude that the study results do not provide strong evidence against the independence model. That is, we do not have sufficiently strong evidence to conclude the vaccine had an effect in this clinical setting. We conclude the evidence is sufficiently strong to reject \\(H_0\\) and assert that the vaccine was useful. When we conduct formal studies, usually we reject the notion that we just happened to observe a rare event.46 In this case, we reject the independence model in favor of the alternative. That is, we are concluding the data provide strong evidence that the vaccine provides some protection against malaria in this clinical setting. Statistical inference, is built on evaluating whether such differences are due to chance. In statistical inference, data scientists evaluate which model is most reasonable given the data. Errors do occur, just like rare events, and we might choose the wrong model. While we do not always choose correctly, statistical inference gives us tools to control and evaluate how often these errors occur. Decision errors Hypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free. Similarly, data can point to the wrong conclusion. However, what distinguishes statistical hypothesis tests from a court system is that our framework allows us to quantify and control how often the data lead us to the incorrect conclusion. In a hypothesis test, there are two competing hypotheses: the null and the alternative. We make a statement about which one might be true, but we might choose incorrectly. There are four possible scenarios in a hypothesis test, which are summarized in Table 5.4. Table 5.4: Four different scenarios for hypothesis tests. Test conclusion \\(H_0\\) true good decision Type 1 Error Truth \\(H_A\\) true Type 2 Error good decision A Type 1 Error is rejecting the null hypothesis when \\(H_0\\) is actually true. Since we rejected the null hypothesis in the gender discrimination and opportunity cost studies, it is possible that we made a Type 1 Error in one or both of those studies. A Type 2 Error is failing to reject the null hypothesis when the alternative is actually true. In a US court, the defendant is either innocent (\\(H_0\\)) or guilty (\\(H_A\\)). What does a Type 1 Error represent in this context? What does a Type 2 Error represent? Table 5.4 may be useful. If the court makes a Type 1 Error, this means the defendant is innocent (\\(H_0\\) true) but wrongly convicted. A Type 2 Error means the court failed to reject \\(H_0\\) (i.e., failed to convict the person) when they were in fact guilty (\\(H_A\\) true). Consider the opportunity cost study where we concluded students were less likely to make a DVD purchase if they were reminded that money not spent now could be spent later. What would a Type 1 Error represent in this context?47 How could we reduce the Type 1 Error rate in US courts? What influence would this have on the Type 2 Error rate? To lower the Type 1 Error rate, we might raise our standard for conviction from “beyond a reasonable doubt” to “beyond a conceivable doubt” so fewer people would be wrongly convicted. However, this would also make it more difficult to convict the people who are actually guilty, so we would make more Type 2 Errors. How could we reduce the Type 2 Error rate in US courts? What influence would this have on the Type 1 Error rate?48 The example and guided practice above provide an important lesson: if we reduce how often we make one type of error, we generally make more of the other type. Choosing a significance level Choosing a significance level for a test is important in many contexts, and the traditional level is 0.05. However, it is sometimes helpful to adjust the significance level based on the application. We may select a level that is smaller or larger than 0.05 depending on the consequences of any conclusions reached from the test. If making a Type 1 Error is dangerous or especially costly, we should choose a small significance level (e.g., 0.01 or 0.001). If we want to be very cautious about rejecting the null hypothesis, we demand very strong evidence favoring the alternative \\(H_A\\) before we would reject \\(H_0\\). If a Type 2 Error is relatively more dangerous or much more costly than a Type 1 Error, then we should choose a higher significance level (e.g., 0.10). Here we want to be cautious about failing to reject \\(H_0\\) when the null is actually false. Significance levels should reflect consequences of errors. The significance level selected for a test should reflect the real-world consequences associated with making a Type 1 or Type 2 Error. Two-sided hypotheses In Section ?? we explored whether women were discriminated against and whether a simple trick could make students a little thriftier. In these two case studies, we’ve actually ignored some possibilities: What if men are actually discriminated against? What if the money trick actually makes students spend more? These possibilities weren’t considered in our original hypotheses or analyses. The disregard of the extra alternatives may have seemed natural since the data pointed in the directions in which we framed the problems. However, there are two dangers if we ignore possibilities that disagree with our data or that conflict with our world view: Framing an alternative hypothesis simply to match the direction that the data point will generally inflate the Type 1 Error rate. After all the work we’ve done (and will continue to do) to rigorously control the error rates in hypothesis tests, careless construction of the alternative hypotheses can disrupt that hard work. If we only use alternative hypotheses that agree with our worldview, then we’re going to be subjecting ourselves to confirmation bias, which means we are looking for data that supports our ideas. That’s not very scientific, and we can do better! The original hypotheses we’ve seen are called one-sided hypothesis tests because they only explored one direction of possibilities. Such hypotheses are appropriate when we are exclusively interested in the single direction, but usually we want to consider all possibilities. To do so, let’s learn about two-sided hypothesis tests in the context of a new study that examines the impact of using blood thinners on patients who have undergone CPR. Cardiopulmonary resuscitation (CPR) is a procedure used on individuals suffering a heart attack when other emergency resources are unavailable. This procedure is helpful in providing some blood circulation to keep a person alive, but CPR chest compressions can also cause internal injuries. Internal bleeding and other injuries that can result from CPR complicate additional treatment efforts. For instance, blood thinners may be used to help release a clot that is causing the heart attack once a patient arrives in the hospital. However, blood thinners negatively affect internal injuries. Here we consider an experiment with patients who underwent CPR for a heart attack and were subsequently admitted to a hospital.49 Each patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group). The outcome variable of interest was whether the patient survived for at least 24 hours. Form hypotheses for this study in plain and statistical language. Let \\(p_c\\) represent the true survival rate of people who do not receive a blood thinner (corresponding to the control group) and \\(p_t\\) represent the survival rate for people receiving a blood thinner (corresponding to the treatment group). We want to understand whether blood thinners are helpful or harmful. We’ll consider both of these possibilities using a two-sided hypothesis test. \\(H_0\\): Blood thinners do not have an overall survival effect, i.e., the survival proportions are the same in each group. \\(p_t - p_c = 0\\). \\(H_A\\): Blood thinners have an impact on survival, either positive or negative, but not zero. \\(p_t - p_c \\neq 0\\). Note that if we had done a one-sided hypothesis test, the resulting hypotheses would have been: \\(H_0\\): Blood thinners do not have a positive overall survival effect, i.e., the survival proportions for the blood thinner group is the same or lower than the control group. \\(p_t - p_c \\leq 0\\). \\(H_A\\): Blood thinners have a positive impact on survival. \\(p_t - p_c &gt; 0\\). There were 50 patients in the experiment who did not receive a blood thinner and 40 patients who did. The study results are shown in Table 5.5. Table 5.5: Results for the CPR study. Patients in the treatment group were given a blood thinner, and patients in the control group were not Survived Died Total Control 11 39 50 Treatment 14 26 40 Total 25 65 90 What is the observed survival rate in the control group? And in the treatment group? Also, provide a point estimate of the difference in survival proportions of the two groups: \\(\\hat{p}_t - \\hat{p}_c\\).50 According to the point estimate, for patients who have undergone CPR outside of the hospital, an additional 13% of these patients survive when they are treated with blood thinners. However, we wonder if this difference could be easily explainable by chance. As we did in our past two studies this chapter, we will simulate what type of differences we might see from chance alone under the null hypothesis. By randomly assigning “simulated treatment” and “simulated control” stickers to the patients’ files, we get a new grouping. If we repeat this simulation 10,000 times, we can build a null distribution of the differences shown in Figure 5.12. Figure 5.12: Null distribution of the point estimate for the difference in proportions, \\(\\hat{p}_t - \\hat{p}_c\\). The shaded right tail shows observations that are at least as large as the observed difference, 0.13. The right tail area is 0.131. (Note: it is only a coincidence that we also have \\(\\hat{p}_t - \\hat{p}_c=0.13\\).) However, contrary to how we calculated the p-value in previous studies, the p-value of this test is not 0.131! The p-value is defined as the chance we observe a result at least as favorable to the alternative hypothesis as the result (i.e., the difference) we observe. In this case, any differences less than or equal to -0.13 would also provide equally strong evidence favoring the alternative hypothesis as a difference of +0.13 did. A difference of -0.13 would correspond to 13% higher survival rate in the control group than the treatment group. In Figure 5.13 we’ve also shaded these differences in the left tail of the distribution. These two shaded tails provide a visual representation of the p-value for a two-sided test. Figure 5.13: Null distribution of the point estimate for the difference in proportions, \\(\\hat{p}_t - \\hat{p}_c\\). All values that are at least as extreme as +0.13 but in either direction away from 0 are shaded. For a two-sided test, take the single tail (in this case, 0.131) and double it to get the p-value: 0.262. Since this p-value is larger than 0.05, we do not reject the null hypothesis. That is, we do not find statistically significant evidence that the blood thinner has any influence on survival of patients who undergo CPR prior to arriving at the hospital. Default to a two-sided test. We want to be rigorous and keep an open mind when we analyze data and evidence. Use a one-sided hypothesis test only if you truly have interest in only one direction. Computing a p-value for a two-sided test. First compute the p-value for one tail of the distribution, then double that value to get the two-sided p-value. That’s it! Consider the situation of the medical consultant. Now that you know about one-sided and two-sided tests, which type of test do you think is more appropriate? The setting has been framed in the context of the consultant being helpful (which is what led us to a one-sided test originally), but what if the consultant actually performed worse than the average? Would we care? More than ever! Since it turns out that we care about a finding in either direction, we should run a two-sided test. The p-value for the two-sided test is double that of the one-sided test, here the simulated p-value would be 0.2444. Generally, to find a two-sided p-value we double the single tail area, which remains a reasonable approach even when the sampling distribution is asymmetric. However, the approach can result in p-values larger than 1 when the point estimate is very near the mean in the null distribution; in such cases, we write that the p-value is 1. Also, very large p-values computed in this way (e.g., 0.85), may also be slightly inflated. Typically, we do not worry too much about the precision of very large p-values because they lead to the same analysis conclusion, even if the value is slightly off. Controlling the Type 1 Error rate Now that we understand the difference between one-sided and two-sided tests, we must recognize when to use each type of test. Because of the result of increased error rates, it is never okay to change two-sided tests to one-sided tests after observing the data. We explore the consequences of ignoring this advice in the next example. Using \\(\\alpha=0.05\\), we show that freely switching from two-sided tests to one-sided tests will lead us to make twice as many Type 1 Errors as intended. Suppose we are interested in finding any difference from 0. We’ve created a smooth-looking null distribution representing differences due to chance in Figure 5.14. Suppose the sample difference was larger than 0. Then if we can flip to a one-sided test, we would use \\(H_A\\): difference \\(&gt; 0\\). Now if we obtain any observation in the upper 5% of the distribution, we would reject \\(H_0\\) since the p-value would just be a the single tail. Thus, if the null hypothesis is true, we incorrectly reject the null hypothesis about 5% of the time when the sample mean is above the null value, as shown in Figure 5.14. Suppose the sample difference was smaller than 0. Then if we change to a one-sided test, we would use \\(H_A\\): difference \\(&lt; 0\\). If the observed difference falls in the lower 5% of the figure, we would reject \\(H_0\\). That is, if the null hypothesis is true, then we would observe this situation about 5% of the time. By examining these two scenarios, we can determine that we will make a Type 1 Error \\(5\\%+5\\%=10\\%\\) of the time if we are allowed to swap to the “best” one-sided test for the data. This is twice the error rate we prescribed with our significance level: \\(\\alpha=0.05\\) (!). Figure 5.14: The shaded regions represent areas where we would reject \\(H_0\\) under the bad practices considered in when \\(\\alpha = 0.05\\). Hypothesis tests should be set up before seeing the data. After observing data, it is tempting to turn a two-sided test into a one-sided test. Avoid this temptation. Hypotheses should be set up before observing the data. 5.4.2 Bootstrap confidence interval for \\(p_1 - p_2\\) The key will be to use two different bags to simulate from the original data. Use the CPR data. After we find the CI (use percentile and SE methods), write the interval values down below in the math section that describes the generic confidence interval method. In Section 5.4.1, we worked with the randomization distribution to understand the distribution of \\(\\hat{p}_1 - \\hat{p}_2\\) when the null hypothesis \\(H_0: p_1 - p_2 = 0\\) is true. Now, through bootstrapping, we study the variability of \\(\\hat{p}_1 - \\hat{p}_2\\) without the null assumption. Observed data Reconsider the CPR data from Section 5.4.1 which is provided in Table 5.5. The experiment consisted of two treatments on patients who underwent CPR for a heart attack and were subsequently admitted to a hospital. Each patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group). The outcome variable of interest was whether the patient survived for at least 24 hours. Again, we use the difference in sample proportions as the observed statistic of interest. Here, the value of the statistic is: \\(\\hat{p}_t - \\hat{p}_c = 0.35 - 0.22 = 0.13\\). Variability of the statistic The bootstrap method applied to two samples is an extension of the method described in Section ??. Now, we have two samples, so each sample estimates the population from which they came. In the CPR setting, the treatment sample estimates the population of all individuals who have gotten (or will get) the treatment; the control sample estimate the population of all individuals who do not get the treatment and are controls. Figure 5.15 extends Figure ?? to show the bootstrapping process from two samples simultaneously. Figure 5.15: (probably populations only, no BS samples) two sample estimating two populations (two infinite popuations) The variability of the statistic (the difference in sample proportions) can be calculated by taking one treatment bootstrap sample and one control bootstrap sample and calculating the difference of the bootstrap survival proportions. One sample from each of the estimated populations has been taken with the sample proportions calculated for the treatment bootstrap sample and the control bootstrap sample. once the image is in, we need to describe above (and below) the values (proportions, differences) in the image explicitly. Figure 5.16: some way to connect the first BS sample on the left wiht the first BS sample on the right. As always, the variability of the difference in proportions can only be estimated by repeated simulations, in this case, repeated bootstrap samples. Figure 5.16 shows multiple bootstrap differences calculated for each of the repeated bootstrap samples. Figure 5.17: in this graph, some kind of connection between each of the two sides Do we also want to visualize “sampling with replacement” in the two sample case? Repeated bootstrap simulations lead to a bootstrap sampling distribution of the statistic of interest, here the difference in sample proportions. Figure 5.18 shows 1000 bootstrap differences in proportions for the CPR data. Figure 5.18: A histogram of differences in proportions from 1000 bootstrap simulations. Percentile vs. SE bootstrap confidence intervals Figure 5.18 provides an estimate for the variability of the difference in survival proportions from sample to sample, The values in the histogram can be used in two different ways to create a confidence interval for the parameter of interest: \\(p_1 - p_2\\). Percentile bootstrap interval As in Section ??, the bootstrap confidence interval can be calculated directly from the bootstrapped differences in Figure 5.18. The interval created from the percentiles of the distribution is called the percentile interval. Note that here we calculate the 90% confidence interval by finding the \\(5^{th}\\) and \\(95^{th}\\) percentile values from the bootstrapped differences. The bootstrap 5 percentile proportion is -0.155 and the 95 percentile is 0.167. The result is: we are 90% confident that, in the population, the true difference in probability of survival is between -0.155 and 0.167. The interval shows that we do not have much definitive evidence of the affect of blood thinners, one way or another. (#fig:CPR percentile interval)The CPR data is bootstrapped 1000 times. Each simulation creates a sample from the original data where the probability of survival in the treatment group is \\(\\hat{p}_{t} = 14/40\\) and the probability of survival in the control group is \\(\\hat{p}_{c} = 11/50\\). SE bootstrap interval Alternatively, we can use the variability in the bootstrapped differences to calculate a standard error of the difference. The resulting interval is called the SE interval. Section 5.4.3 details the mathematical model for the standard error of the difference in sample proportions, but the bootstrap distribution typically does an excellent job of estimating the variability. \\[SE(\\hat{p}_t - \\hat{p}_c) \\approx SD(\\hat{p}_{bs,t} - \\hat{p}_{bs,c}) = 0.0975\\] The calculation above was performed in R using the sd() function, but any statistical software will calculate the standard deviation of the differences, here, the exact quantity we hope to approximate. Because we don’t know the true distribution of \\(\\hat{p}_t - \\hat{p}_c\\), we will use a rough approximation to find a confidence interval for \\(p_t - p_c\\). A 95% confidence interval for \\(p_t - p_c\\) is given by: \\[\\begin{align} \\hat{p}_t - \\hat{p}_c &amp;\\pm&amp; 2 SE(\\hat{p}_t - \\hat{p}_c)\\\\ 14/40 - 11/50 &amp;\\pm&amp; 0.0975\\\\ &amp;&amp;(-0.065, 0.325) \\end{align}\\] We are 95% confident that the true value of \\(p_t - p_c\\) is between -0.065 and 0.325. Again, the wide confidence interval that overlaps zero indicates that the study provides very little evidence about the effectiveness of blood thinners. 5.4.3 Mathematical model Variability of \\(\\hat{p}_1 - \\hat{p}_2\\) Like with \\(\\hat{p}\\), the difference of two sample proportions \\(\\hat{p}_1 - \\hat{p}_2\\) can be modeled using a normal distribution when certain conditions are met. First, we require a broader independence condition, and secondly, the success-failure condition must be met by both groups. Conditions for the sampling distribution of \\(\\hat{p}_1 -\\hat{p}_2\\) to be normal. The difference \\(\\hat{p}_1 - \\hat{p}_2\\) can be modeled using a normal distribution when Independence, extended. The data are independent within and between the two groups. Generally this is satisfied if the data come from two independent random samples or if the data come from a randomized experiment. Success-failure condition. The success-failure condition holds for both groups, where we check successes and failures in each group separately. When these conditions are satisfied, the standard error of \\(\\hat{p}_1 - \\hat{p}_2\\) is \\[\\begin{eqnarray*} SE = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}} \\end{eqnarray*}\\] where \\(p_1\\) and \\(p_2\\) represent the population proportions, and \\(n_1\\) and \\(n_2\\) represent the sample sizes. Confidence interval for \\(p_1 - p_2\\) We can apply the generic confidence interval formula for a difference of two proportions, where we use \\(\\hat{p}_1 - \\hat{p}_2\\) as the point estimate and substitute the \\(SE\\) formula: \\[\\begin{align*} &amp;\\text{point estimate} \\ \\pm\\ z^{\\star} \\times SE &amp;&amp;\\to &amp;&amp;\\hat{p}_1 - \\hat{p}_2 \\ \\pm\\ z^{\\star} \\times \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}} \\end{align*}\\] Standard Error of the difference in two proportions, \\(\\hat{p}_1 -\\hat{p}_2\\). When the conditions are met so that the distribution fo \\(\\hat{p}\\) is nearly normal, the variability of the difference in proportions, \\(\\hat{p}_1 -\\hat{p}_2\\), is well described by: \\[\\begin{eqnarray*} SE(\\hat{p}_1 -\\hat{p}_2) = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}} \\end{eqnarray*}\\] We reconsider the experiment for patients who underwent cardiopulmonary resuscitation (CPR) for a heart attack and were subsequently admitted to a hospital. These patients were randomly divided into a treatment group where they received a blood thinner or the control group where they did not receive a blood thinner. The outcome variable of interest was whether the patients survived for at least 24 hours. The results are shown in Table 5.5. Check whether we can model the difference in sample proportions using the normal distribution. We first check for independence: since this is a randomized experiment, this condition is satisfied. Next, we check the success-failure condition for each group. We have at least 10 successes and 10 failures in each experiment arm (11, 14, 39, 26), so this condition is also satisfied. With both conditions satisfied, the difference in sample proportions can be reasonably modeled using a normal distribution for these data. Create and interpret a 90% confidence interval of the difference for the survival rates in the CPR study. We’ll use \\(p_t\\) for the survival rate in the treatment group and \\(p_c\\) for the control group: \\[\\begin{align*} \\hat{p}_{t} - \\hat{p}_{c} = \\frac{14}{40} - \\frac{11}{50} = 0.35 - 0.22 = 0.13 \\end{align*}\\] We use the standard error formula previously provided. As with the one-sample proportion case, we use the sample estimates of each proportion in the formula in the confidence interval context: \\[\\begin{align*} SE \\approx \\sqrt{\\frac{0.35 (1 - 0.35)}{40} + \\frac{0.22 (1 - 0.22)}{50}} = 0.095 \\end{align*}\\] For a 90% confidence interval, we use \\(z^{\\star} = 1.65\\): \\[\\begin{align*} \\text{point estimate} \\ \\pm\\ z^{\\star} \\times SE \\quad \\to \\quad 0.13 \\ \\pm\\ 1.65 \\times 0.095 \\quad \\to \\quad (-0.027, 0.287) \\end{align*}\\] We are 90% confident that blood thinners have a difference of -2.7% to +28.7% percentage point impact on survival rate for patients who are like those in the study. Because 0% is contained in the interval, we do not have enough information to say whether blood thinners help or harm heart attack patients who have been admitted after they have undergone CPR. not sure why the footnote in the guidedpractice below doesn’t show up? should be right after: “Also interpret the interval in the context of the study.” A 5-year experiment was conducted to evaluate the effectiveness of fish oils on reducing cardiovascular events, where each subject was randomized into one of two treatment groups. We’ll consider heart attack outcomes in the patients listed in Table 5.6. Create a 95% confidence interval for the effect of fish oils on heart attacks for patients who are well-represented by those in the study. Also interpret the interval in the context of the study.^[Because the patients were randomized, the subjects are independent, both within and between the two groups. The success-failure condition is also met for both groups as all counts are at least 10. This satisfies the conditions necessary to model the difference in proportions using a normal distribution. Compute the sample proportions (\\(\\hat{p}_{\\text{fish oil}} = 0.0112\\), \\(\\hat{p}_{\\text{placebo}} = 0.0155\\)), point estimate of the difference (\\(0.0112 - 0.0155 = -0.0043\\)), and standard error \\(SE = \\sqrt{\\frac{0.0112 \\times 0.9888}{12933} + \\frac{0.0155 \\times 0.9845}{12938}} = 0.00145\\). Next, plug the values into the general formula for a confidence interval, where we’ll use a 95% confidence level with \\(z^{\\star} = 1.96\\): \\[\\begin{align*} -0.0043 \\pm 1.96 \\times 0.00145 \\quad \\to \\quad (-0.0071, -0.0015) \\end{align*}\\] We are 95% confident that fish oils decreases heart attacks by 0.15 to 0.71 percentage points (off of a baseline of about 1.55%) over a 5-year period for subjects who are similar to those in the study. Because the interval is entirely below 0, and the treatment was randomly assigned the data provide strong evidence that fish oil supplements reduce heart attacks in patients like those in the study.] Table 5.6: Results for the study on n-3 fatty acid supplement and related health benefits. heart attack no event Total fish oil 145 12788 12933 placebo 200 12738 12938 Hypothesis test for \\(H_0: p_1 - p_2 = 0\\) A mammogram is an X-ray procedure used to check for breast cancer. Whether mammograms should be used is part of a controversial discussion, and it’s the topic of our next example where we learn about 2-proportion hypothesis tests when \\(H_0\\) is \\(p_1 - p_2 = 0\\) (or equivalently, \\(p_1 = p_2\\)). A 30-year study was conducted with nearly 90,000 female participants. During a 5-year screening period, each woman was randomized to one of two groups: in the first group, women received regular mammograms to screen for breast cancer, and in the second group, women received regular non-mammogram breast cancer exams. No intervention was made during the following 25 years of the study, and we’ll consider death resulting from breast cancer over the full 30-year period. Results from the study are summarized in Figure 5.7. If mammograms are much more effective than non-mammogram breast cancer exams, then we would expect to see additional deaths from breast cancer in the control group. On the other hand, if mammograms are not as effective as regular breast cancer exams, we would expect to see an increase in breast cancer deaths in the mammogram group. Table 5.7: Summary results for breast cancer study. Death from breast cancer? Yes No Mammogram 500 44,425 Control 505 44,405 Is this study an experiment or an observational study?51 Set up hypotheses to test whether there was a difference in breast cancer deaths in the mammogram and control groups.52 Using the previous example, we will check the conditions for using a normal distribution to analyze the results of the study. The details are very similar to that of confidence intervals. However, when the null hypothesis is that \\(p_1 - p_2 = 0\\), we use a special proportion called the pooled proportion to check the success-failure condition: \\[\\begin{align*} \\hat{p}_{\\textit{pool}} &amp;= \\frac {\\text{# of patients who died from breast cancer in the entire study}} {\\text{# of patients in the entire study}} \\\\ &amp;= \\frac{500 + 505}{500 + \\text{44,425} + 505 + \\text{44,405}} \\\\ &amp;= 0.0112 \\end{align*}\\] This proportion is an estimate of the breast cancer death rate across the entire study, and it’s our best estimate of the proportions \\(p_{mgm}\\) and \\(p_{ctrl}\\) if the null hypothesis is true that \\(p_{mgm} = p_{ctrl}\\). We will also use this pooled proportion when computing the standard error. Is it reasonable to model the difference in proportions using a normal distribution in this study? Because the patients are randomized, they can be treated as independent, both within and between groups. We also must check the success-failure condition for each group. Under the null hypothesis, the proportions \\(p_{mgm}\\) and \\(p_{ctrl}\\) are equal, so we check the success-failure condition with our best estimate of these values under \\(H_0\\), the pooled proportion from the two samples, \\(\\hat{p}_{\\textit{pool}} = 0.0112\\): \\[\\begin{align*} \\hat{p}_{\\textit{pool}} \\times n_{mgm} &amp;= 0.0112 \\times \\text{44,925} = 503 &amp; (1 - \\hat{p}_{\\textit{pool}}) \\times n_{mgm} &amp;= 0.9888 \\times \\text{44,925} = \\text{44,422} \\\\ \\hat{p}_{\\textit{pool}} \\times n_{ctrl} &amp;= 0.0112 \\times \\text{44,910} = 503 &amp; (1 - \\hat{p}_{\\textit{pool}}) \\times n_{ctrl} &amp;= 0.9888 \\times \\text{44,910} = \\text{44,407} \\end{align*}\\] The success-failure condition is satisfied since all values are at least 10. With both conditions satisfied, we can safely model the difference in proportions using a normal distribution. Use the pooled proportion when \\(H_0\\) is \\(p_1 - p_2 = 0\\). When the null hypothesis is that the proportions are equal, use the pooled proportion (\\(\\hat{p}_{\\textit{pooled}}\\)) to verify the success-failure condition and estimate the standard error: \\[\\begin{eqnarray*} \\hat{p}_{\\textit{pooled}} = \\frac{\\text{number of ``successes&quot;}} {\\text{number of cases}} = \\frac{\\hat{p}_1 n_1 + \\hat{p}_2 n_2}{n_1 + n_2} \\end{eqnarray*}\\] Here \\(\\hat{p}_1 n_1\\) represents the number of successes in sample 1 since \\[\\begin{eqnarray*} \\hat{p}_1 = \\frac{\\text{number of successes in sample 1}}{n_1} \\end{eqnarray*}\\] Similarly, \\(\\hat{p}_2 n_2\\) represents the number of successes in sample 2. In the previous example, the pooled proportion was used to check the success-failure condition53. In the next example, we see an additional place where the pooled proportion comes into play: the standard error calculation. Compute the point estimate of the difference in breast cancer death rates in the two groups, and use the pooled proportion \\(\\hat{p}_{\\textit{pool}} = 0.0112\\) to calculate the standard error. The point estimate of the difference in breast cancer death rates is \\[\\begin{align*} \\hat{p}_{mgm} - \\hat{p}_{ctrl} &amp;= \\frac{500}{500 + 44,425} - \\frac{505}{505 + 44,405} \\\\ &amp;= 0.01113 - 0.01125 \\\\ &amp;= -0.00012 \\end{align*}\\] The breast cancer death rate in the mammogram group was 0.012% less than in the control group. Next, the standard error is calculated , \\(\\hat{p}_{\\textit{pool}}\\): \\[\\begin{align*} SE = \\sqrt{ \\frac{\\hat{p}_{\\textit{pool}}(1-\\hat{p}_{\\textit{pool}})} {n_{mgm}} + \\frac{\\hat{p}_{\\textit{pool}}(1-\\hat{p}_{\\textit{pool}})} {n_{ctrl}} } = 0.00070 \\end{align*}\\] Using the point estimate \\(\\hat{p}_{mgm} - \\hat{p}_{ctrl} = -0.00012\\) and standard error \\(SE = 0.00070\\), calculate a p-value for the hypothesis test and write a conclusion. Just like in past tests, we first compute a test statistic and draw a picture: \\[\\begin{align*} Z = \\frac{\\text{point estimate} - \\text{null value}}{SE} = \\frac{-0.00012 - 0}{0.00070} = -0.17 \\end{align*}\\] The lower tail area is 0.4325, which we double to get the p-value: 0.8650. Because this p-value is larger than 0.05, we do not reject the null hypothesis. That is, the difference in breast cancer death rates is reasonably explained by chance, and we do not observe benefits or harm from mammograms relative to a regular breast exam. Can we conclude that mammograms have no benefits or harm? Here are a few considerations to keep in mind when reviewing the mammogram study as well as any other medical study: We do not accept the null hypothesis, which means we don’t have sufficient evidence to conclude that mammograms reduce or increase breast cancer deaths. If mammograms are helpful or harmful, the data suggest the effect isn’t very large. Are mammograms more or less expensive than a non-mammogram breast exam? If one option is much more expensive than the other and doesn’t offer clear benefits, then we should lean towards the less expensive option. The study’s authors also found that mammograms led to over-diagnosis of breast cancer, which means some breast cancers were found (or thought to be found) but that these cancers would not cause symptoms during patients’ lifetimes. That is, something else would kill the patient before breast cancer symptoms appeared. This means some patients may have been treated for breast cancer unnecessarily, and this treatment is another cost to consider. It is also important to recognize that over-diagnosis can cause unnecessary physical or emotional harm to patients. These considerations highlight the complexity around medical care and treatment recommendations. Experts and medical boards who study medical treatments use considerations like those above to provide their best recommendation based on the current evidence. 5.5 Summary of \\(z\\)-procedures So far in this chapter, we have seen the normal distribution applied as the appropriate mathematical model in two distinct settings. Although the two data structures are different, their similarities and differences are worth pointing out. We provide Table 5.8 partly as a mechanism for understanding \\(z\\)-procedures and partly to highlight the extremely common usage of the normal distribution in practice. You will often hear the following two \\(z\\)-procedures referred to as a one sample \\(z\\)-test (\\(z\\)-interval) and two sample \\(z\\)-test (\\(z\\)-interval). Table 5.8: Similarities of \\(z\\)-methods across one sample and two independent samples analysis of a categorical response variable. one sample two indep. samples response variable binary binary explanatory variable none binary parameter of interest proportion: \\(\\pi\\) diff in props:\\(\\pi_1 - \\pi_2\\) statistic of interest proportion: \\(\\hat{p}\\) diff in props: \\(\\hat{p}_1 - \\hat{p}_2\\) null standard error \\(\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\) \\(\\sqrt{\\hat{p}_{pool}(1-\\hat{p}_{pool})\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}\\) standard error \\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) \\(\\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\\) conditions independence, 2. large samples (at least 10 successes and 10 failures) independence, 2. large samples (at least 10 successes and 10 failures in each sample) Hypothesis tests. When applying the normal distribution for a hypothesis test, we proceed as follows: Write appropriate hypotheses. Verify conditions for using the normal distribution. One-sample: the observations must be independent, and you must have at least 10 successes and 10 failures. For a difference of proportions: each sample must separately satisfy the one-sample conditions for the normal distribution, and the data in the groups must also be independent. Compute the statistic of interest, the null standard error, and the degrees of freedom. For \\(df\\), use \\(n-1\\) for one sample, and for two samples use either statistical software or the smaller of \\(n_1 - 1\\) and \\(n_2 - 1\\). Compute the T-score using the general formula: \\[ T = \\frac{\\mbox{statistic} - \\mbox{null value}}{\\mbox{null standard error}} \\] Use the statistical software to find the p-value using the standard normal distribution: Sign in \\(H_a\\) is \\(&lt;\\): p-value = area below Z-score Sign in \\(H_a\\) is \\(&gt;\\): p-value = area above Z-score Sign in \\(H_a\\) is \\(\\neq\\): p-value = 2 \\(\\times\\) area below \\(-|\\mbox{Z-score}|\\) Make a conclusion based on the p-value, and write a conclusion in context, in plain language, and in terms of the alternative hypothesis. Confidence intervals. Similarly, the following is how we generally compute a confidence interval using a normal distribution: Verify conditions for using the normal distribution. (See above.) Compute the statistic of interest, the standard error, and \\(z^{\\star}\\). Calculate the confidence interval using the general formula: \\[ \\mbox{statistic} \\pm\\ z^{\\star} SE. \\] Put the conclusions in context and in plain language so even non-data scientists can understand the results. 5.6 R: Inference for categorical data 5.7 Chapter 5 review 5.7.1 Terms We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as bolded text. alternative hypothesis null distribution pooled proportion test statistic Central Limit Theorem null hypothesis sampling distribution two sample \\(z\\)-test confidence interval one sample \\(z\\)-test SE interval two-sided hypothesis test confirmation bias one-sided hypothesis test standard error Type 1 Error hypothesis test p-value standard error for difference in proportions Type 2 Error margin of error parameter standard error of single proportion Z-score normal curve percentile standard normal distribution normal distribution percentile interval statistical inference normal model point estimate success-failure condition We would be assuming that these two variables are independent.↩︎ If you are a STAT 216 student, you will recognize this from our first week’s in-class activity.↩︎ Bumba is the Martian letter on the left!↩︎ A fair coin has a 50% chance of landing on heads, which is the chance a student would guess Bumba correctly if they were just guessing. Thus, toss a coin 38 times with heads representing “guess correctly”; then calculate the proportion of tosses that landed on heads. Another option would be to 10 black cards and 10 red cards, letting red represent “guess correctly”. Shuffle the cards and draw one card, record if it is red or black, then replace the card and shuffle again. Do this 38 times and calculate the proportion of red cards observed.↩︎ To explore this further, watch this TED Talk by neurologist Vilayanur Ramachandran (The synesthesia part begins at roughly 17:40 minutes).↩︎ If you carry out the calculations, you’ll note that the upper bound is actually \\(0.89 + 0.16 = 1.05\\), but since a sample proportion cannot be greater than 1, we truncated the interval to 1.↩︎ The first possibility (We can’t read Martian, and these results just occurred by chance.) was the null hypothesis; the second possibility (We can read Martian, and these results reflect this ability.) was the alternative hypothesis.↩︎ Technically, the observed sample statistic or one more extreme in the direction of our alternative. But it is helpful to just remember this as “the data”.↩︎ Since a smaller p-value gives you stronger evidence against the null hypothesis, we reject \\(H_0\\) when the p-value is very small, and fail to reject \\(H_0\\) when the p-value is not small.↩︎ You will get more practice calculating p-values such as these in this Chapter.↩︎ Since statistical methods are grounded in probability, technically we can only find strong evidence against a hypothesis, not disprove it.↩︎ If we want to be more certain we will capture the fish, we might use a wider net. Likewise, we use a wider confidence interval if we want to be more certain that we capture the parameter.↩︎ It is also introduced as the Gaussian distribution after Frederic Gauss, the first person to formalize its mathematical expression.↩︎ (a) \\(N(\\mu=5,\\sigma=3)\\). (b) \\(N(\\mu=-100, \\sigma=10)\\). (c) \\(N(\\mu=2, \\sigma=9)\\).↩︎ We use the standard deviation as a guide. Ann is 1 standard deviation above average on the SAT: \\(1500 + 300=1800\\). Tom is 0.6 standard deviations above the mean on the ACT: \\(21+0.6\\times 5=24\\). In Figure 5.6, we can see that Ann tends to do better with respect to everyone else than Tom did, so her score was better.↩︎ \\(Z_{Tom} = \\frac{x_{Tom} - \\mu_{ACT}}{\\sigma_{ACT}} = \\frac{24 - 21}{5} = 0.6\\)↩︎ (a) Its Z-score is given by \\(Z = \\frac{x-\\mu}{\\sigma} = \\frac{5.19 - 3}{2} = 2.19/2 = 1.095\\). (b) The observation \\(x\\) is 1.095 standard deviations above the mean. We know it must be above the mean since \\(Z\\) is positive.↩︎ For \\(x_1=95.4\\) mm: \\(Z_1 = \\frac{x_1 - \\mu}{\\sigma} = \\frac{95.4 - 92.6}{3.6} = 0.78\\). For \\(x_2=85.8\\) mm: \\(Z_2 = \\frac{85.8 - 92.6}{3.6} = -1.89\\).↩︎ Because the absolute value of Z-score for the second observation is larger than that of the first, the second observation has a more unusual head length.↩︎ If 84% had lower scores than Ann, the number of people who had better scores must be 16%. (Generally ties are ignored when the normal model, or any other continuous distribution, is used.)↩︎ We found the probability to be 0.6664. A picture for this exercise is represented by the shaded area below “0.6664”.↩︎ If Edward did better than 37% of SAT takers, then about 63% must have done better than him. ↩︎ Numerical answers: (a) 0.9772. (b) 0.0228.↩︎ This sample was taken from the USDA Food Commodity Intake Database.↩︎ First put the heights into inches: 67 and 76 inches. Figures are shown below. (a) \\(Z_{Mike} = \\frac{67 - 70}{3.3} = -0.91\\ \\to\\ 0.1814\\). (b) \\(Z_{Jim} = \\frac{76 - 70}{3.3} = 1.82\\ \\to\\ 0.9656\\). \\↩︎ Remember: draw a picture first, then find the Z-score. (We leave the pictures to you.) The Z-score can be found by using the percentiles and the normal probability table. (a) We look for 0.95 in the probability portion (middle part) of the normal probability table, which leads us to row 1.6 and (about) column 0.05, i.e., \\(Z_{95}=1.65\\). Knowing \\(Z_{95}=1.65\\), \\(\\mu = 1500\\), and \\(\\sigma = 300\\), we setup the Z-score formula: \\(1.65 = \\frac{x_{95} - 1500}{300}\\). We solve for \\(x_{95}\\): \\(x_{95} = 1995\\). (b) Similarly, we find \\(Z_{97.5} = 1.96\\), again setup the Z-score formula for the heights, and calculate \\(x_{97.5} = 76.5\\).↩︎ Numerical answers: (a) 0.1131. (b) 0.3821.↩︎ This is an abbreviated solution. (Be sure to draw a figure!) First find the percent who get below 1500 and the percent that get above 2000: \\(Z_{1500} = 0.00 \\to 0.5000\\) (area below), \\(Z_{2000} = 1.67 \\to 0.0475\\) (area above). Final answer: \\(1.0000-0.5000 - 0.0475 = 0.4525\\).↩︎ 5’5’’ is 65 inches. 5’7’’ is 67 inches. Numerical solution: \\(1.000 - 0.0649 - 0.8183 = 0.1168\\), i.e., 11.68%.↩︎ First draw the pictures. To find the area between \\(Z=-1\\) and \\(Z=1\\), use pnorm to determine the areas below \\(Z=-1\\) and above \\(Z=1\\). Next verify the area between \\(Z=-1\\) and \\(Z=1\\) is about 0.68. Repeat this for \\(Z=-2\\) to \\(Z=2\\) and also for \\(Z=-3\\) to \\(Z=3\\).↩︎ (a) 900 and 2100 represent two standard deviations above and below the mean, which means about 95% of test takers will score between 900 and 2100. (b) Since the normal model is symmetric, then half of the test takers from part (a) (\\(\\frac{95\\%}{2} = 47.5\\%\\) of all test takers) will score 900 to 1500 while 47.5% score between 1500 and 2100.↩︎ When you see \\(\\pi\\) in this textbook, it will always symbolize a (typically unknown) population proportion, not the value 3.14….↩︎ The terms “success” and “failure” may not actually represent outcomes we view as successful or not, but it is the typical generic way to referring to the possible outcomes of a binary variable. The “success” is whatever we count when calculating our sample proportion.↩︎ Parameters were first introduced in Section 2.2.2↩︎ One option would be to use a spinner with 10% shaded red, and the rest shaded green. Each spin of the spinner would represent one client. Spin the spinner 62 times and count the number of times the spinner lands on red. The proportion of times the spinner lands on red represents a simulated \\(\\hat{p}\\) under the assumption that \\(\\pi = 0.10\\). Other objects include: a bag of marbles with 10% red marbles and 90% white marbles, or 10 cards where 1 is red and 9 are white. Sampling 62 times with replacement from these collections would simulate one sample of clients.↩︎ There isn’t sufficiently strong evidence to support the claim that fewer than 10% of the consultant’s clients experience complications. That is, there isn’t sufficiently strong evidence to support an association between the consultant’s work and fewer surgery complications.↩︎ No. It might be that the consultant’s work is associated with a reduction but that there isn’t enough data to convincingly show this connection.↩︎ While this book is scoped to well-constrained statistical problems, do remember that this is just the first book in what is a large library of statistical methods that are suitable for a very wide range of data and contexts.↩︎ \\(H_0\\): there is not majority support for the regulation; \\(H_0\\): \\(\\pi \\leq 0.50\\). \\(H_A\\): the majority of borrowers support the regulation; \\(H_A\\): \\(\\pi &gt; 0.50\\).↩︎ Independence holds since the poll is based on a random sample. The success-failure condition also holds, which is checked using the null value (\\(p_0 = 0.5\\)) from \\(H_0\\): \\(np_0 = 826 \\times 0.5 = 413\\), \\(n(1 - p_0) = 826 \\times 0.5 = 413\\). Recall that here, the best guess for \\(\\pi\\) is \\(p_0\\) which comes from the null hypothesis (because we assume the null hypothesis is true when performing the testing procedure steps). \\(H_0\\): there is not support for the regulation; \\(H_0\\): \\(\\pi \\leq 0.50\\). \\(H_A\\): the majority of borrowers support the regulation; \\(H_A\\): \\(\\pi &gt; 0.50\\).↩︎ Because the \\(p\\) is unknown but expected to be around 2/3, we will use 2/3 in place of \\(p\\) in the formula for the standard error. \\(SE = \\sqrt{\\frac{p(1-p)}{n}} \\approx \\sqrt{\\frac{2/3 (1 - 2/3)} {300}} = 0.027\\).↩︎ This is equivalent to asking how often the \\(Z\\) score will be larger than -2.58 but less than 2.58. (For a picture, see Figure 5.10.) To determine this probability, look up -2.58 and 2.58 in the normal probability table (0.0049 and 0.9951). Thus, there is a \\(0.9951-0.0049 \\approx 0.99\\) probability that the unobserved random variable \\(X\\) will be within 2.58 standard deviations of the mean.↩︎ Since the necessary conditions for applying the normal model have already been checked for us, we can go straight to the construction of the confidence interval: \\(\\text{point estimate}\\ \\pm\\ 2.58 \\times SE \\rightarrow (0.018, 0.162)\\). We are 99% confident that implanting a stent in the brain of a patient who is at risk of stroke increases the risk of stroke within 30 days by a rate of 0.018 to 0.162 (assuming the patients are representative of the population).↩︎ We must find \\(z^{\\star}\\) such that 90% of the distribution falls between -\\(z^{\\star}\\) and \\(z^{\\star}\\) in the standard normal model, \\(N(\\mu=0, \\sigma=1)\\). We can look up -\\(z^{\\star}\\) in the normal probability table by looking for a lower tail of 5% (the other 5% is in the upper tail), thus \\(z^{\\star}=1.65\\). The 90% confidence interval can then be computed as \\(\\text{point estimate}\\ \\pm\\ 1.65\\times SE \\to (4.4\\%, 13.6\\%)\\). (Note: the conditions for normality had earlier been confirmed for us.) That is, we are 90% confident that implanting a stent in a stroke patient’s brain increased the risk of stroke within 30 days by 4.4% to 13.6%.↩︎ The study is an experiment, as patients were randomly assigned an experiment group. Since this is an experiment, the results can be used to evaluate a causal relationship between the malaria vaccine and whether patients showed signs of an infection.↩︎ This reasoning does not generally extend to anecdotal observations. Each of us observes incredibly rare events every day, events we could not possibly hope to predict. However, in the non-rigorous setting of anecdotal evidence, almost anything may appear to be a rare event, so the idea of looking for rare events in day-to-day activities is treacherous. For example, we might look at the lottery: there was only a 1 in 292 million chance that the Powerball numbers for the largest jackpot in history (January 13th, 2016) would be (04, 08, 19, 27, 34) with a Powerball of (10), but nonetheless those numbers came up! However, no matter what numbers had turned up, they would have had the same incredibly rare odds. That is, any set of numbers we could have observed would ultimately be incredibly rare. This type of situation is typical of our daily lives: each possible event in itself seems incredibly rare, but if we consider every alternative, those outcomes are also incredibly rare. We should be cautious not to misinterpret such anecdotal evidence.↩︎ Making a Type 1 Error in this context would mean that reminding students that money not spent now can be spent later does not affect their buying habits, despite the strong evidence (the data suggesting otherwise) found in the experiment. Notice that this does not necessarily mean something was wrong with the data or that we made a computational mistake. Sometimes data simply point us to the wrong conclusion, which is why scientific studies are often repeated to check initial findings.↩︎ To lower the Type 2 Error rate, we want to convict more guilty people. We could lower the standards for conviction from “beyond a reasonable doubt” to “beyond a little doubt”. Lowering the bar for guilt will also result in more wrongful convictions, raising the Type 1 Error rate.↩︎ B\\(\\ddot{\\text{o}}\\)ttiger et al. “Efficacy and safety of thrombolytic therapy after initially unsuccessful cardiopulmonary resuscitation: a prospective clinical trial.” The Lancet, 2001.↩︎ Observed control survival rate: \\(p_c = \\frac{11}{50} = 0.22\\). Treatment survival rate: \\(p_t = \\frac{14}{40} = 0.35\\). Observed difference: \\(\\hat{p}_t - \\hat{p}_c = 0.35 - 0.22 = 0.13\\).↩︎ This is an experiment. Patients were randomized to receive mammograms or a standard breast cancer exam. We will be able to make causal conclusions based on this study.↩︎ \\(H_0\\): the breast cancer death rate for patients screened using mammograms is the same as the breast cancer death rate for patients in the control, \\(p_{mgm} - p_{ctrl} = 0\\). \\(H_A\\): the breast cancer death rate for patients screened using mammograms is different than the breast cancer death rate for patients in the control, \\(p_{mgm} - p_{ctrl} \\neq 0\\).↩︎ For an example of a two-proportion hypothesis test that does not require the success-failure condition to be met, see Section 5.4.1.↩︎ "]
]

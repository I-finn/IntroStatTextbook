[
["index.html", "Montana State Introductory Statistics with R Welcome Textbook overview Acknowledgements", " Montana State Introductory Statistics with R Nicole Carnegie, Stacey Hancock, Elijah Meyer, Jade Schmidt, Melinda Yager 2020-07-30 Welcome We hope readers will take away three ideas from this book in addition to forming a foundation of statistical thinking and methods. Statistics is an applied field with a wide range of practical applications. You don’t have to be a math guru to learn from interesting, real data. Data are messy, and statistical tools are imperfect. However, when you understand the strengths and weaknesses of these tools, you can use them to learn interesting things about the world. Textbook overview Introduction to data. Data structures, variables, and basic data collection techniques. Exploratory data analysis. Data visualization and summarisation for one and two variables, with a taste of probability. Correlation and regression. Visualizing, describing, and quantifying relationships between two quantitative variables. Multiple regression. Descriptive summaries for quantifying the relationship between many variables. Foundations for inference. Case studies are used to introduce the ideas of statistical inference with randomization and simulations. Inference for categorical data. Inference for one or two proportions using simulation and randomization techniques as well as the normal distribution. Inference for numerical data. Inference for one or two means using simulation and randomization techniques as well as the \\(t\\)-distribution. Inference for regression. Inference for a regression slope or correlation using simulation and randomization techniques as well as the \\(t\\)-distribution. Acknowledgements This resource is largely a derivative of the 1st and 2nd editions of the OpenIntro textbook Introductory Statistics with Randomization and Simulation, without which this effort would not have been possible. The authors would also like to thank the Montana State University Library, who generously funded this project. "],
["about-the-authors.html", "About the Authors Montana State University Authors OpenIntro Authors", " About the Authors Montana State University Authors Nicole Carnegie Associate Professor of Statistics nicole.carnegie@montana.edu Stacey Hancock Assistant Professor of Statistics stacey.hancock@montana.edu Elijah Meyer PhD Graduate Student elijah.meyer@montana.edu Jade Schmidt Student Success Coordinator for Statistics jade.schmidt2@montana.edu Melinda Yager Assistant Coordinator for Statistics melinda.yager@montana.edu OpenIntro Authors Mine Çetinkaya-Rundel mine@openintro.org University of Edinburgh, Duke University, RStudio Johanna Hardin jo@openintro.org Pomona College David Diez david@openintro.org Google/YouTube Christopher D Barr Yale School of Management "],
["copyright.html", "Copyright", " Copyright Copyright © 2020. This textbook is available under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported license (CC BY-NC-SA): http://creativecommons.org/licenses/by-nc-sa/3.0/ This textbook was derived from the 1st and 2nd editions of the OpenIntro Introductory Statistics with Randomization and Simulation textbook. Visit the following link for further copyright information: http://www.openintro.org/perm/stat2nd_v1.txt "],
["intro-to-data.html", "Chapter 1 Introduction to data 1.1 Case study: using stents to prevent strokes 1.2 Data basics 1.3 Sampling principles and strategies 1.4 Experiments 1.5 Data in R 1.6 Chapter review", " Chapter 1 Introduction to data Scientists seek to answer questions using rigorous methods and careful observations. These observations – collected from the likes of field notes, surveys, and experiments – form the backbone of a statistical investigation and are called data. Statistics is the study of how best to collect, analyze, and draw conclusions from data, and in this first chapter, we focus on both the properties of data and on the collection of data. 1.1 Case study: using stents to prevent strokes In this section, we introduce a classic challenge in statistics: evaluating the efficacy of a medical treatment. Terms in this section, and indeed much of this chapter, will all be revisited later in the text. The plan for now is simply to get a sense of the role statistics can play in practice. Here, we will consider an experiment that studies effectiveness of stents in treating patients at risk of stroke (Chimowitz et al. 2011). Stents are small mesh tubes that are placed inside narrow or weak arteries to assist in patient recovery after cardiac events and reduce the risk of an additional heart attack or death. Many doctors have hoped that there would be similar benefits for patients at risk of stroke. We start by writing the principal question the researchers hope to answer: Does the use of stents reduce the risk of stroke? The researchers who asked this question conducted an experiment with 451 at-risk patients. Each volunteer patient was randomly assigned to one of two groups: Treatment group. Patients in the treatment group received a stent and medical management. The medical management included medications, management of risk factors, and help in lifestyle modification. Control group. Patients in the control group received the same medical management as the treatment group, but they did not receive stents. Researchers randomly assigned 224 patients to the treatment group and 227 to the control group. In this study, the control group provides a reference point against which we can measure the medical impact of stents in the treatment group. Researchers studied the effect of stents at two time points: 30 days after enrollment and 365 days after enrollment. The data collected on 5 of these patients are summarized in Table 1.1. Patient outcomes are recorded as stroke or no event, representing whether or not the patient had a stroke during that time period. The stent30 and stent365 data sets from this study can be found in the openintro package. Table 1.1: Results for five patients from the stent study. group 30 days 365 days patient treatment no event no event 1 treatment stroke stroke 2 treatment no event no event 3 treatment no event no event 4 control no event no event 5 Considering data from each patient individually would be a long, cumbersome path towards answering the original research question. Instead, performing a statistical data analysis allows us to consider all of the data at once. Table 1.2 summarizes the raw data in a more helpful way. In this table, we can quickly see what happened over the entire study. For instance, to identify the number of patients in the treatment group who had a stroke within 30 days after the treatment, we look in the leftmost column (30 days), at the intersection of treatment and stroke: 33. To identify the number of control patients who did not have a stroke after 365 days after receiving treatment, we look at the rightmost column (365 days), at the intersection of control and no event: 199. Table 1.2: Descriptive statistics for the stent study. 30 days 365 days stroke no event stroke no event treatment 33 191 45 179 control 13 214 28 199 Total 46 405 73 378 The data summarized in this table can also be visualized with a barplot, seen in Figure 1.1: Figure 1.1: Segmented barplot of outcomes in stent study by group and time. Of the 224 patients in the treatment group, 45 had a stroke by the end of the first year. Using these two numbers, compute the proportion of patients in the treatment group who had a stroke by the end of their first year. (Please note: answers to all Guided Practice exercises are provided using footnotes.)1 We can compute summary statistics from the table to give us a better idea of how the impact of the stent treatment differed between the two groups. A summary statistic is a single number summarizing a large amount of data. For instance, the primary results of the study after 1 year could be described by two summary statistics: the proportion of people who had a stroke in the treatment and control groups. Proportion who had a stroke in the treatment (stent) group: \\(45/224 = 0.20 = 20\\%\\). Proportion who had a stroke in the control group: \\(28/227 = 0.12 = 12\\%\\). These two summary statistics are useful in looking for differences in the groups, and we are in for a surprise: an additional 8% of patients in the treatment group had a stroke! This is important for two reasons. First, it is contrary to what doctors expected, which was that stents would reduce the rate of strokes. Second, it leads to a statistical question: do the data show a “real” difference between the groups? This second question is subtle, and is the basis of what we call statistical inference. Suppose you flip a coin 100 times. While the chance a coin lands heads in any given coin flip is 50%, we probably won’t observe exactly 50 heads. This type of fluctuation is part of almost any type of data generating process. It is possible that the 8% difference in the stent study is due to this natural variation. However, the larger the difference we observe (for a particular sample size), the less believable it is that the difference is due to chance. So what we are really asking is the following: is the difference so large that we should reject the notion that it was due to chance? While we don’t yet have our statistical tools to fully address this question on our own, we can comprehend the conclusions of the published analysis: there was compelling evidence of harm by stents in this study of stroke patients. Be careful: Do not generalize the results of this study to all patients and all stents. This study looked at patients with very specific characteristics who volunteered to be a part of this study and who may not be representative of all stroke patients. In addition, there are many types of stents and this study only considered the self-expanding Wingspan stent (Boston Scientific). However, this study does leave us with an important lesson: we should keep our eyes open for surprises. 1.2 Data basics Effective presentation and description of data is a first step in most analyses. This section introduces one structure for organizing data as well as some terminology that will be used throughout this book. 1.2.1 Observations, variables, and data frames Table 1.3 displays six rows of a data set for 50 randomly sampled loans offered through Lending Club, which is a peer-to-peer lending company. These observations will be referred to as the loan50 data set. The loan50 data can be found in the openintro package. Each row in the table represents a single loan. The formal name for a row is a case or observational unit. The columns represent characteristics of each loan, where each column is referred to as a variable. A variable is something that can be measured on an individual observational unit. Be careful not to confuse summary statistics—calculated from a group of observational units—with variables. For example, the first row represents a loan of $7,500 with an interest rate of 7.34%, where the borrower is based in Maryland (MD) and has an income of $70,000. What is the grade of the first loan in Table 1.3? And what is the home ownership status of the borrower for that first loan? Reminder: for these Guided Practice questions, you can check your answer in the footnote.2 In practice, it is especially important to ask clarifying questions to ensure important aspects of the data are understood. For instance, it is always important to be sure we know what each variable means and its units of measurement. Descriptions of the variables in the loan50 data set are given in Table 1.4. Table 1.3: Six rows from the loan50 data set loan_amount interest_rate term grade state total_income homeownership 1 22000 10.90 60 B NJ 59000 rent 2 6000 9.92 36 B CA 60000 rent 3 25000 26.30 36 E SC 75000 mortgage 4 6000 9.92 36 B CA 75000 rent 5 25000 9.43 60 B OH 254000 mortgage 6 6400 9.92 36 B IN 67000 mortgage Table 1.4: Variables and their descriptions for the loan50 data set. variable description loan_amount Amount of the loan received, in US dollars. interest_rate Interest rate on the loan, in an annual percentage. term The length of the loan, which is always set as a whole number of months. grade Loan grade, which takes a values A through G and represents the quality of the loan and its likelihood of being repaid. state US state where the borrower resides. total_income Borrower’s total income, including any second income, in US dollars. homeownership Indicates whether the person owns, owns but has a mortgage, or rents. The data in Table 1.3 represent a data frame (or data matrix), which is a convenient and common way to organize data, especially if collecting data in a spreadsheet. Each row of a data frame corresponds to a unique case (observational unit), and each column corresponds to a variable. When recording data, use a data frame unless you have a very good reason to use a different structure. This structure allows new cases to be added as rows or new variables as new columns. The grades for assignments, quizzes, and exams in a course are often recorded in a gradebook that takes the form of a data frame. How might you organize a course’s grade data using a data frame?3 We consider data for 3,142 counties in the United States, which include the name of each county, the state where it resides, its population in 2017, how its population changed from 2010 to 2017, poverty rate, and nine additional characteristics. How might these data be organized in a data frame?4 The data described in the Guided Practice above represent the county data set, which is shown as a data frame in Table 1.5. The variables as well as the variables in the data set that did not fit in Table 1.5 are described in Table 1.6 Table 1.5: Six observations and six variables from the county data set. name state pop2017 pop_change unemployment_rate median_edu Autauga County Alabama 55504 1.48 3.86 some_college Baldwin County Alabama 212628 9.19 3.99 some_college Barbour County Alabama 25270 -6.22 5.90 hs_diploma Bibb County Alabama 22668 0.73 4.39 hs_diploma Blount County Alabama 58013 0.68 4.02 hs_diploma Bullock County Alabama 10309 -2.28 4.93 hs_diploma Table 1.6: Variables and their descriptions for the county data set. variable description name Name of county. state Name of state. pop2000 Population in 2000. pop2010 Population in 2010. pop2017 Population in 2017. pop_change Population change from 2010 to 2017. poverty Percent of population in poverty in 2017. homeownership Homeownership rate, 2006-2010. multi_unit Percent of housing units in multi-unit structures, 2006-2010. unemployment_rate Unemployment rate in 2017. metro Whether the county contains a metropolitan area, taking one of the values yes or no. median_edu Median education level (2013-2017), taking one of the values below_hs, hs_diploma, some_college, or bachelors. per_capita_income Per capita (per person) income (2013-2017). median_hh_income Median household income. smoking_ban Describes whether the type of county-level smoking ban in place in 2010, taking one of the values none, partial, or comprehensive. The county data can be found in the openintro package. 1.2.2 Types of variables Examine the unemployment_rate, pop2017, state, and median_edu variables in the county data set. Each of these variables is inherently different from the other three, yet some share certain characteristics. First consider unemployment_rate, which is said to be a quantitative or numerical variable since it can take a wide range of numerical values, and it is sensible to add, subtract, or take averages with those values. On the other hand, we would not classify a variable reporting telephone area codes as quantitative since the average, sum, and difference of area codes doesn’t have any clear meaning. The pop2017 variable is also quantitative, although it seems to be a little different than unemployment_rate. This variable of the population count can only take whole non-negative numbers (0, 1, 2, …). For this reason, the population variable is said to be discrete since it can only take numerical values with jumps. On the other hand, the unemployment rate variable is said to be continuous. The variable state can take up to 51 values after accounting for Washington, DC: AL, AK, …, and WY. Because the responses themselves are categories, state is called a categorical variable, and the possible values are called the variable’s levels . Finally, consider the median_edu variable, which describes the median education level of county residents and takes values below_hs, hs_diploma, some_college, or bachelors in each county. This variable seems to be a hybrid: it is a categorical variable but the levels have a natural ordering. A variable with these properties is called an ordinal variable, while a regular categorical variable without this type of special ordering is called a nominal variable. To simplify analyses, any ordinal variable in this book will be treated as a nominal (unordered) categorical variable. Figure 1.2: Breakdown of variables into their respective types. Data were collected about students in a statistics course. Three variables were recorded for each student: number of siblings, student height, and whether the student had previously taken a statistics course. Classify each of the variables as continuous quantitative, discrete quantitative, or categorical. The number of siblings and student height represent quantitative variables. Because the number of siblings is a count, it is discrete. Height varies continuously, so it is a continuous quantitative variable. The last variable classifies students into two categories – those who have and those who have not taken a statistics course – which makes this variable categorical. An experiment is evaluating the effectiveness of a new drug in treating migraines. A group variable is used to indicate the experiment group for each patient: treatment or control. The num_migraines variable represents the number of migraines the patient experienced during a 3-month period. Classify each variable as either quantitative or categorical?5 1.2.3 Relationships between variables Many analyses are motivated by a researcher looking for a relationship between two or more variables. A social scientist may like to answer some of the following questions: Does a higher than average increase in county population tend to correspond to counties with higher or lower median household incomes? If homeownership is lower than the national average in one county, will the percent of multi-unit structures in that county tend to be above or below the national average? How useful a predictor is median education level for the median household income for US counties? To answer these questions, data must be collected, such as the county data set shown in Table 1.5. Examining summary statistics could provide insights for each of the three questions about counties. Additionally, graphs can be used to visually explore the data. Scatterplots are one type of graph used to study the relationship between two quantitative variables. Figure 1.3 displays the relationship between the variables homeownership and multi_unit, which is the percent of units in multi-unit structures (e.g., apartments, condos). Each point on the plot represents a single county (a single observational unit). For instance, the highlighted dot corresponds to County 413 in the county data set: Chattahoochee County, Georgia, which has 39.4% of units in multi-unit structures and a homeownership rate of 31.3%. The scatterplot suggests a relationship between the two variables: counties with a higher rate of multi-units tend to have lower homeownership rates. We might brainstorm as to why this relationship exists and investigate each idea to determine which are the most reasonable explanations. Figure 1.3: A scatterplot of homeownership versus the percent of units that are in multi-unit structures for US counties. The highlighted dot represents Chattahoochee County, Georgia, which has a multi-unit rate of 39.4% and a homeownership rate of 31.3%. The multi-unit and homeownership rates are said to be associated because the plot shows a discernible pattern. When two variables show some connection with one another, they are called associated variables. Associated variables can also be called dependent variables and vice-versa. Examine the variables in the loan50 data set, which are described in Table 1.4. Create two questions about possible relationships between variables in loan50 that are of interest to you.6 This example examines the relationship between the change in population from 2010 to 2017 and median household income for counties, which is visualized as a scatterplot in Figure 1.4. Are these variables associated? The larger the median household income for a county, the higher the population growth observed for the county. While this trend isn’t true for every county, the trend in the plot is evident. Since there is some relationship between the variables, they are associated. Figure 1.4: A scatterplot showing pop_change against median_hh_income. Owsley County of Kentucky, is highlighted, which lost 3.63% of its population from 2010 to 2017 and had median household income of $22,736. Because there is a downward trend in Figure 1.3 – counties with more units in multi-unit structures are associated with lower homeownership – these variables are said to be negatively associated. A positive association is shown in the relationship between the median_hh_income and pop_change variables in Figure 1.4, where counties with higher median household income tend to have higher rates of population growth. If two variables are not associated, then they are said to be independent. That is, two variables are independent if there is no evident relationship between the two. Associated or independent, not both. A pair of variables are either related in some way (associated) or not (independent). No pair of variables is both associated and independent. 1.2.4 Explanatory and response variables When we ask questions about the relationship between two variables, we sometimes also want to determine if the change in one variable causes a change in the other. Consider the following rephrasing of an earlier question bout the county data set: If there is an increase in the median household income in a county, does this drive an increase in its population? In this question, we are asking whether one variable affects another. If this is our underlying belief, then median household income is the explanatory variable variable and the population change is the response variable variable in the hypothesized relationship.7 Explanatory and response variables. When we suspect one variable might causally affect another, we label the first variable the explanatory variable and the second the response variable. explanatory variable \\(\\rightarrow\\) might affect \\(\\rightarrow\\) response variable For many pairs of variables, there is no hypothesized relationship, and these labels would not be applied to either variable in such cases. Bear in mind that the act of labeling the variables in this way does nothing to guarantee that a causal relationship exists. A formal evaluation to check whether one variable causes a change in another requires an experiment. 1.2.5 Introducing observational studies and experiments There are two primary types of data collection: observational studies and experiments. Researchers perform an observational study when they collect data in a way that does not directly interfere with how the data arise. For instance, researchers may collect information via surveys, review medical or company records, or follow a cohort of many similar individuals to form hypotheses about why certain diseases might develop. In each of these situations, researchers merely observe the data that arise. In general, observational studies can provide evidence of a naturally occurring association between variables, but they cannot by themselves show a causal connection. When researchers want to investigate the possibility of a causal connection, they conduct an experiment. Usually there will be both an explanatory and a response variable. For instance, we may suspect administering a drug will reduce mortality in heart attack patients over the following year. To check if there really is a causal connection between the explanatory variable and the response, researchers will collect a sample of individuals and split them into groups. The individuals in each group are assigned a treatment. When individuals are randomly assigned to a group, the experiment is called a randomized experiment. For example, each heart attack patient in the drug trial could be randomly assigned, perhaps by flipping a coin, into one of two groups: the first group receives a placebo (fake treatment) and the second group receives the drug. See the case study in Section 1.1 for another example of an experiment, though that study did not employ a placebo. Association \\(\\neq\\) Causation. In general, association does not imply causation, and causation can only be inferred from a randomized experiment. 1.3 Sampling principles and strategies The first step in conducting research is to identify topics or questions that are to be investigated. A clearly laid out research question is helpful in identifying what subjects or cases should be studied and what variables are important. It is also important to consider how data are collected so that they are reliable and help achieve the research goals. 1.3.1 Populations and samples Consider the following three research questions: What is the average mercury content in swordfish in the Atlantic Ocean? Over the last 5 years, what is the average time to complete a degree for Duke undergrads? Does a new drug reduce the number of deaths in patients with severe heart disease? Each research question refers to a target population. In the first question, the target population is all swordfish in the Atlantic ocean, and each fish represents a case. Often times, it is too expensive to collect data for every case in a population. Instead, a sample is taken. A sample represents a subset of the cases and is often a small fraction of the population. For instance, 60 swordfish (or some other number) in the population might be selected, and this sample data may be used to provide an estimate of the population average and answer the research question. For the second and third questions above, identify the target population and what represents an individual case.8 1.3.2 Anecdotal evidence Consider the following possible responses to the three research questions: A man on the news got mercury poisoning from eating swordfish, so the average mercury concentration in swordfish must be dangerously high. I met two students who took more than 7 years to graduate from Duke, so it must take longer to graduate at Duke than at many other colleges. My friend’s dad had a heart attack and died after they gave him a new heart disease drug, so the drug must not work. Each conclusion is based on data. However, there are two problems. First, the data only represent one or two cases. Second, and more importantly, it is unclear whether these cases are actually representative of the population. Data collected in this haphazard fashion are called anecdotal evidence. Anecdotal evidence. Be careful of data collected in a haphazard fashion. Such evidence may be true and verifiable, but it may only represent extraordinary cases. Figure 1.5: In February 2010, some media pundits cited one large snow storm as evidence against global warming. As comedian Jon Stewart pointed out, “It is one storm, in one region, of one country.” Anecdotal evidence typically is composed of unusual cases that we recall based on their striking characteristics. For instance, we are more likely to remember the two people we met who took 7 years to graduate than the six others who graduated in four years. Instead of looking at the most unusual cases, we should examine a sample of many cases that better represent the population. 1.3.3 Sampling from a population We might try to estimate the time to graduation for Duke undergraduates in the last 5 years by collecting a sample of students. All graduates in the last 5 years represent the population, and graduates who are selected for review are collectively called the sample. In general, we always seek to randomly select a sample from a population. The most basic type of random selection is equivalent to how raffles are conducted. For example, in selecting graduates, we could write each graduate’s name on a raffle ticket and draw 100 tickets. The selected names would represent a random sample of 100 graduates. We pick samples randomly to reduce the chance we introduce biases. Figure 1.6: In this graphic, five graduates are randomly selected from the population to be included in the sample. Suppose we ask a student who happens to be majoring in nutrition to select several graduates for the study. What kind of students do you think they might collect? Do you think their sample would be representative of all graduates? Perhaps they would pick a disproportionate number of graduates from health-related fields. Or perhaps their selection would be a good representation of the population. When selecting samples by hand, we run the risk of picking a biased sample, even if our bias is unintended. Figure 1.7: Asked to pick a sample of graduates, a nutrition major might inadvertently pick a disproportionate number of graduates from health-related majors. If someone was permitted to pick and choose exactly which graduates were included in the sample, it is entirely possible that the sample could be skewed to that person’s interests, which may be entirely unintentional. This introduces bias into a sample. Sampling randomly helps resolve this problem. The most basic random sample is called a simple random sample, and is equivalent to using a raffle to select cases. This means that each case in the population has an equal chance of being included and there is no implied connection between the cases in the sample. The act of taking a simple random sample helps minimize bias. However, bias can crop up in other ways. Even when people are picked at random, e.g. for surveys, caution must be exercised if the non-response rate is high. For instance, if only 30% of the people randomly sampled for a survey actually respond, then it is unclear whether the results are representative of the entire population. This non-response bias can skew results. Figure 1.8: Due to the possibility of non-response, surveys studies may only reach a certain group within the population. It is difficult, and often times impossible, to completely fix this problem. Another common downfall is a convenience sample , where individuals who are easily accessible are more likely to be included in the sample. For instance, if a political survey is done by stopping people walking in the Bronx, this will not represent all of New York City. It is often difficult to discern what sub-population a convenience sample represents. We can easily access ratings for products, sellers, and companies through websites. These ratings are based only on those people who go out of their way to provide a rating. If 50% of online reviews for a product are negative, do you think this means that 50% of buyers are dissatisfied with the product? Why or why not?9 1.3.4 Observational studies Data where no treatment has been explicitly applied (or explicitly withheld) is called observational data. For instance, the loan data and county data described in Section 1.2 are both examples of observational data. Making causal conclusions based on experiments is often reasonable. However, making the same causal conclusions based on observational data can be treacherous and is not recommended. Thus, observational studies are generally only sufficient to show associations or form hypotheses that can be later checked with experiments. Suppose an observational study tracked sunscreen use and skin cancer, and it was found that the more sunscreen someone used, the more likely the person was to have skin cancer. Does this mean sunscreen causes skin cancer?10 Some previous research tells us that using sunscreen actually reduces skin cancer risk, so maybe there is another variable that can explain this hypothetical association between sunscreen usage and skin cancer. One important piece of information that is absent is sun exposure. If someone is out in the sun all day, they are more likely to use sunscreen and more likely to get skin cancer. Exposure to the sun is unaccounted for in the simple investigation. Sun exposure is what is called a confounding variable11, which is a variable that is associated with both the explanatory and response variables. While one method to justify making causal conclusions from observational studies is to exhaust the search for confounding variables, there is no guarantee that all confounding variables can be examined or measured. Figure 1.3 shows a negative association between the homeownership rate and the percentage of multi-unit structures in a county. However, it is unreasonable to conclude that there is a causal relationship between the two variables. Suggest a variable that might explain the negative relationship.12 Observational studies come in two forms: prospective and retrospective studies. A prospective study identifies individuals and collects information as events unfold. For instance, medical researchers may identify and follow a group of patients over many years to assess the possible influences of behavior on cancer risk. One example of such a study is The Nurses’ Health Study. Started in 1976 and expanded in 1989, the Nurses’ Health Study has collected data on over 275,000 nurses and is still enrolling participants. This prospective study recruits registered nurses and then collects data from them using questionnaires. Retrospective studies collect data after events have taken place, e.g. researchers may review past events in medical records. Some data sets may contain both prospectively- and retrospectively-collected variables, such as medical studies which gather information on participants’ lives before they enter the study and subsequently collect data on participants throughout the study. 1.3.5 Four sampling methods Almost all statistical methods are based on the notion of implied randomness. If observational data are not collected in a random framework from a population, these statistical methods – the estimates and errors associated with the estimates – are not reliable. Here we consider four random sampling techniques: simple, stratified, cluster, and multistage sampling. Figures 1.9 and 1.10 provide graphical representations of these techniques. Figure 1.9: Examples of simple random and stratified sampling. In the top panel, simple random sampling was used to randomly select the 18 cases (denoted in red). In the bottom panel, stratified sampling was used: cases were grouped into strata, then simple random sampling was employed to randomly select 3 cases within each stratum. Simple random sampling is probably the most intuitive form of random sampling. Consider the salaries of Major League Baseball (MLB) players, where each player is a member of one of the league’s 30 teams. To take a simple random sample of 120 baseball players and their salaries, we could write the names of that season’s several hundreds of players onto slips of paper, drop the slips into a bucket, shake the bucket around until we are sure the names are all mixed up, then draw out slips until we have the sample of 120 players. In general, a sample is referred to as “simple random” if each case in the population has an equal chance of being included in the final sample and knowing that a case is included in a sample does not provide useful information about which other cases are included. Stratified sampling is a divide-and-conquer sampling strategy. The population is divided into groups called strata. The strata are chosen so that similar cases are grouped together, then a second sampling method, usually simple random sampling, is employed within each stratum. In the baseball salary example, each of the 30 teams could represent a strata, since some teams have a lot more money (up to 4 times as much!). Then we might randomly sample 4 players from each team for our sample of 120 players. Stratified sampling is especially useful when the cases in each stratum are very similar with respect to the outcome of interest. The downside is that analyzing data from a stratified sample is a more complex task than analyzing data from a simple random sample. The analysis methods introduced in this book would need to be extended to analyze data collected using stratified sampling. Why would it be good for cases within each stratum to be very similar? We might get a more stable estimate for the subpopulation in a stratum if the cases are very similar, leading to more precise estimates within each group. When we combine these estimates into a single estimate for the full population, that population estimate will tend to be more precise since each individual group estimate is itself more precise. In a cluster sample, we break up the population into many groups, called clusters. Then we sample a fixed number of clusters and include all observations from each of those clusters in the sample. A multistage sample is like a cluster sample, but rather than keeping all observations in each cluster, we would collect a random sample within each selected cluster. Figure 1.10: Examples of cluster and multistage sampling. In the top panel, cluster sampling was used: data were binned into nine clusters, three of these clusters were sampled, and all observations within these three cluster were included in the sample. In the bottom panel, multistage sampling was used, which differs from cluster sampling only in that we randomly select a subset of each cluster to be included in the sample rather than measuring every case in each sampled cluster. Sometimes cluster or multistage sampling can be more economical than the alternative sampling techniques. Also, unlike stratified sampling, these approaches are most helpful when there is a lot of case-to-case variability within a cluster but the clusters themselves don’t look very different from one another. For example, if neighborhoods represented clusters, then cluster or multistage sampling work best when the neighborhoods are very diverse. A~downside of these methods is that more advanced techniques are typically required to analyze the data, though the methods in this book can be extended to handle such data. Suppose we are interested in estimating the malaria rate in a densely tropical portion of rural Indonesia. We learn that there are 30 villages in that part of the Indonesian jungle, each more or less similar to the next, but the distances between the villages is substantial. Our goal is to test 150 individuals for malaria. What sampling method should be employed? A simple random sample would likely draw individuals from all 30 villages, which could make data collection extremely expensive. Stratified sampling would be a challenge since it is unclear how we would build strata of similar individuals. However, cluster sampling or multistage sampling seem like very good ideas. If we decided to use multistage sampling, we might randomly select half of the villages, then randomly select 10 people from each. This would probably reduce our data collection costs substantially in comparison to a simple random sample, and the cluster sample would still give us reliable information, even if we would need to analyze the data with slightly more advanced methods than we discuss in this book. 1.4 Experiments Studies where the researchers assign treatments to cases are called experiments. When this assignment includes randomization, e.g., using a coin flip to decide which treatment a patient receives, it is called a randomized experiment. Randomized experiments are fundamentally important when trying to show a causal connection between two variables. 1.4.1 Principles of experimental design Controlling. Researchers assign treatments to cases, and they do their best to control any other differences in the groups13. For example, when patients take a drug in pill form, some patients take the pill with only a sip of water while others may have it with an entire glass of water. To control for the effect of water consumption, a doctor may instruct every patient to drink a 12 ounce glass of water with the pill. Randomization. Researchers randomize patients into treatment groups to account for variables that cannot be controlled. For example, some patients may be more susceptible to a disease than others due to their dietary habits. Randomizing patients into the treatment or control group helps even out such differences, and it also prevents accidental bias from entering the study. Replication. The more cases researchers observe, the more accurately they can estimate the effect of the explanatory variable on the response. In a single study, we replicate by collecting a sufficiently large sample. Alternatively, a group of scientists may replicate an entire study to verify an earlier finding. Randomized experiments are generally built on four principles: Blocking. Researchers sometimes know or suspect that variables, other than the treatment, influence the response. Under these circumstances, they may first group individuals based on this variable into blocks and then randomize cases within each block to the treatment groups. This strategy is often referred to as blocking. For instance, if we are looking at the effect of a drug on heart attacks, we might first split patients in the study into low-risk and high-risk blocks, then randomly assign half the patients from each block to the control group and the other half to the treatment group, as shown in Figure 1.11. This strategy ensures each treatment group has an equal number of low-risk and high-risk patients. Figure 1.11: Blocking using a variable depicting patient risk. Patients are first divided into low-risk and high-risk blocks, then each block is evenly separated into the treatment groups using randomization. This strategy ensures an equal representation of patients in each treatment group from both the low-risk and high-risk categories. It is important to incorporate the first three experimental design principles into any study, and this book describes applicable methods for analyzing data from such experiments. Blocking is a slightly more advanced technique, and statistical methods in this book may be extended to analyze data collected using blocking. 1.4.2 Reducing bias in human experiments Randomized experiments have long been considered to be the gold standard for data collection, but they do not ensure an unbiased perspective into the cause and effect relationship in all cases. Human studies are perfect examples where bias can unintentionally arise. Here we reconsider a study where a new drug was used to treat heart attack patients. In particular, researchers wanted to know if the drug reduced deaths in patients. These researchers designed a randomized experiment because they wanted to draw causal conclusions about the drug’s effect. Study volunteers14 were randomly placed into two study groups. One group, the treatment group, received the drug. The other group, called the control group, did not receive any drug treatment. Put yourself in the place of a person in the study. If you are in the treatment group, you are given a fancy new drug that you anticipate will help you. On the other hand, a person in the other group doesn’t receive the drug and sits idly, hoping her participation doesn’t increase her risk of death. These perspectives suggest there are actually two effects in this study: the one of interest is the effectiveness of the drug, and the second is an emotional effect to (not) taking the drug, which is difficult to quantify. Researchers aren’t usually interested in the emotional effect, which might bias the study. To circumvent this problem, researchers do not want patients to know which group they are in. When researchers keep the patients uninformed about their treatment, the study is said to be blind. But there is one problem: if a patient doesn’t receive a treatment, they will know they’re in the control group. The solution to this problem is to give fake treatments to patients in the control group. A fake treatment is called a placebo, and an effective placebo is the key to making a study truly blind. A classic example of a placebo is a sugar pill that is made to look like the actual treatment pill. Often times, a placebo results in a slight but real improvement in patients. This effect has been dubbed the placebo effect. The patients are not the only ones who should be blinded: doctors and researchers can accidentally bias a study. When a doctor knows a patient has been given the real treatment, they might inadvertently give that patient more attention or care than a patient that they know is on the placebo. To guard against this bias, which again has been found to have a measurable effect in some instances, most modern studies employ a double-blind setup where doctors or researchers who interact with patients are, just like the patients, unaware of who is or is not receiving the treatment.15 Look back to the study in Section 1.1 where researchers were testing whether stents were effective at reducing strokes in at-risk patients. Is this an experiment? Was the study blinded? Was it double-blinded?16 For the study in Section 1.1, could the researchers have employed a placebo? If so, what would that placebo have looked like?17 You may have many questions about the ethics of sham surgeries to create a placebo. These questions may have even arisen in your mind when in the general experiment context, where a possibly helpful treatment was withheld from individuals in the control group; the main difference is that a sham surgery tends to create additional risk, while withholding a treatment only maintains a person’s risk. There are always multiple viewpoints of experiments and placebos, and rarely is it obvious which is ethically “correct”. For instance, is it ethical to use a sham surgery when it creates a risk to the patient? However, if we don’t use sham surgeries, we may promote the use of a costly treatment that has no real effect; if this happens, money and other resources will be diverted away from other treatments that are known to be helpful. Ultimately, this is a difficult situation where we cannot perfectly protect both the patients who have volunteered for the study and the patients who may benefit (or not) from the treatment in the future. 1.5 Data in R R is a powerful and open source software tool for working with data. Throughout this text, we provide some guidance on how to use R within the context of the statistical content that is being covered. As educators, we see the value of teaching with modern software to empower students to take optimal advantage of the concepts they are learning. However, we understand the limitations of some educational structures, and we know that not every classroom will be able to implement R alongside the statistical concepts. Generally, we will present the R techniques at the end of each chapter. There are times in the text when the concepts are not distinguishable from the software, and in those cases, we have have provided the R code within the main body of the chapter. We start with an introduction to R, focused on how data sets are structured in R and how the user can work with a data object in R. 1.5.1 Dataframes in R Throughout the text, we will work with many different data sets. Some data sets are pre-loaded into R, some get loaded through R packages, and some data sets will be created by the student. data sets can be viewed through the RStudio environment, but the data can also be investigated through the notebook features of an RMarkdown file. Consider the data that was described previously in this chapter. We can use the glimpse() function to see the variables included in the data set and their data type. Or, we could use the head() function to see the first few rows of the data set. data(email50) glimpse(email50) #&gt; Rows: 50 #&gt; Columns: 21 #&gt; $ spam &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,… #&gt; $ to_multiple &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,… #&gt; $ from &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… #&gt; $ cc &lt;int&gt; 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,… #&gt; $ sent_email &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,… #&gt; $ time &lt;dttm&gt; 2012-01-04 06:19:16, 2012-02-16 13:10:06, 2012-01-04 08… #&gt; $ image &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… #&gt; $ attach &lt;dbl&gt; 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0,… #&gt; $ dollar &lt;dbl&gt; 0, 0, 0, 0, 9, 0, 0, 0, 0, 23, 4, 0, 3, 2, 0, 0, 0, 0, 0… #&gt; $ winner &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, no, no, yes, no,… #&gt; $ inherit &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… #&gt; $ viagra &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… #&gt; $ password &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 8,… #&gt; $ num_char &lt;dbl&gt; 21.705, 7.011, 0.631, 2.454, 41.623, 0.057, 0.809, 5.229… #&gt; $ line_breaks &lt;int&gt; 551, 183, 28, 61, 1088, 5, 17, 88, 242, 578, 1167, 198, … #&gt; $ format &lt;dbl&gt; 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,… #&gt; $ re_subj &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,… #&gt; $ exclaim_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… #&gt; $ urgent_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… #&gt; $ exclaim_mess &lt;dbl&gt; 8, 1, 2, 1, 43, 0, 0, 2, 22, 3, 13, 1, 2, 2, 21, 10, 0, … #&gt; $ number &lt;fct&gt; small, big, none, small, small, small, small, small, sma… head(email50) #&gt; # A tibble: 6 x 21 #&gt; spam to_multiple from cc sent_email time image attach #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 0 1 0 1 2012-01-04 06:19:16 0 0 #&gt; 2 0 0 1 0 0 2012-02-16 13:10:06 0 0 #&gt; 3 1 0 1 4 0 2012-01-04 08:36:23 0 2 #&gt; 4 0 0 1 0 0 2012-01-04 10:49:52 0 0 #&gt; 5 0 0 1 0 0 2012-01-27 02:34:45 0 0 #&gt; 6 0 0 1 0 0 2012-01-17 10:31:57 0 0 #&gt; # … with 13 more variables: dollar &lt;dbl&gt;, winner &lt;fct&gt;, inherit &lt;dbl&gt;, #&gt; # viagra &lt;dbl&gt;, password &lt;dbl&gt;, num_char &lt;dbl&gt;, line_breaks &lt;int&gt;, #&gt; # format &lt;dbl&gt;, re_subj &lt;dbl&gt;, exclaim_subj &lt;dbl&gt;, urgent_subj &lt;dbl&gt;, #&gt; # exclaim_mess &lt;dbl&gt;, number &lt;fct&gt; Sometimes it is necessary to extract a column or a row from a data set. In R, the $ operator can be used to extract a column from a data set. For example, data$variable would extract the variable column from the data dataframe. When extracted, these columns can be thought of as vectors. With these vectors, if you desired to pull off a specific entry, you could use square brackets ([ ]), with the index (number) of the entry you wish to extract in the brackets. For example, data$variable[2] would extract the second entry (row) of the variable column. Because a dataframe can be (roughly) thought of as a set of many different vectors, you can extract rows and columns from a dataframe using familiar matrix notation (e.g. [row, column]. For example data[i,j] will extract the \\((i,j)^{th}\\) entry of data, data[i, ] will extract the \\(i^{th}\\) row, and data[ , j] will extract the \\(j^{th}\\) column. Notice, when extracting an entire row (or column), you do not need to specify the columns (or rows) you would like, which is why the second entry does not contain a number. email50$num_char #&gt; [1] 21.705 7.011 0.631 2.454 41.623 0.057 0.809 5.229 9.277 17.170 #&gt; [11] 64.401 10.368 42.793 0.451 29.233 9.794 2.139 0.130 4.945 11.533 #&gt; [21] 5.682 6.768 0.086 3.070 26.520 26.255 5.259 2.780 5.864 9.928 #&gt; [31] 25.209 6.563 24.599 25.757 0.409 11.223 3.778 1.493 10.613 0.493 #&gt; [41] 4.415 14.156 9.491 24.837 0.684 13.502 2.789 1.169 8.937 15.829 email50[47,3] #&gt; # A tibble: 1 x 1 #&gt; from #&gt; &lt;dbl&gt; #&gt; 1 1 Table 1.7: Data from the 47th row of the email data set. spam to_multiple from cc sent_email time image attach dollar winner inherit viagra password num_char line_breaks format re_subj exclaim_subj urgent_subj exclaim_mess number 0 1 1 2 0 2012-01-02 14:24:21 0 0 0 no 0 0 0 8.72 185 0 1 0 0 3 small 1.5.2 Tidy Structure of Data For plotting, analyses, model building, etc., the data should be structured according to certain principles. Hadley Wickham provides a thorough discussion and advice for cleaning up the data in Wickham and others (2014). Tidy Data: rows (cases/observational units) and columns (variables). The key is that every row is a case and every column is a variable. No exceptions. Creating tidy data is often not trivial. Within R (really within any type of computing language, Python, SQL, Java, etc.), it is important to understand how to build data using the patterns of the language. Some things to consider: object_name &lt;- anything is a way of assigning anything to the new object_name. object_name &lt;- function_name(data_table, arguments) is a way of using a function to create a new object. object_name &lt;- data_table %&gt;% function_name(arguments) uses chaining syntax as an extension of the ideas of functions. In chaining, the value on the left side of %&gt;% becomes the first argument to the function on the right side. object_name &lt;- data_table %&gt;% function_name(arguments) %&gt;% another_function_name(other_arguments) is extended chaining. %&gt;% is never at the front of the line, it is always connecting one idea with the continuation of that idea on the next line. * In R, all functions take arguments in round parentheses (as opposed to subsetting observations or variables from data objects which happen with square parentheses). Additionally, the spot to the left of %&gt;% is always a data table. * The pipe syntax should be read as then, %&gt;%. 1.5.3 Using the pipe to chain The pipe syntax (%&gt;%) takes a data frame (or data table) and sends it to the argument of a function. The mapping goes to the first available argument in the function. For example: x %&gt;% f(y) is the same as f(x, y) y %&gt;% f(x, ., z) is the same as f(x,y,z) Pipes are used commonly with functions in the dplyr package (see R examples in Chapter 2) and they allow us to sequentially build data wrangling operations. We’ll start with short pipes and throughout the course build up to longer pipes that perform multiple operations. Consider the data, High School and Beyond survey. Two hundred observations were randomly sampled from the High School and Beyond survey, a survey conducted on high school seniors by the National Center of Education Statistics. Of interest is the proportion of students at each of the two types of school, public and privaate. We use the table command to tabulate how many of each type of school are in the data set. Notice that the same result is produced by the $ command with table and the chaining syntax done with %&gt;%. data(hsb2) table(hsb2$schtyp) #&gt; #&gt; public private #&gt; 168 32 hsb2 %&gt;% select(schtyp) %&gt;% table() #&gt; . #&gt; public private #&gt; 168 32 What if we are interested only in public schools? First, we should take note of another piece of R syntax: the double equal sign. This is the logical test for “is equal to”. In other words, we first determine if school type is equal to public for each of the observations in the data set and filter for those where this is true. # Filter for public schools hsb2_public &lt;- hsb2 %&gt;% filter(schtyp == &quot;public&quot;) We can read this as: “take the hsb2 data frame and pipe it into the filter function. Filter the data for cases where school type is equal to public. Then, assign the resulting data frame to a new object called hsb2 underscore public.” Suppose we are not interested in the actual reading score of students, but instead whether their reading score is below average or at or above average. First, we need to calculate the average reading score with the mean function. This will give us the mean value, 52.23. However, in order to be able to refer back to this value later on, we might want to store it as an object that we can refer to by name. # Calculate average reading score and show the value mean(hsb2$read) #&gt; [1] 52.2 So instead of just printing the result, let’s save it as a new object called avg_read. # Calculate average reading score and store as avg_read avg_read &lt;- mean(hsb2$read) Before we more on, a quick tip: most often you’ll want to do both; see the value and also store it for later use. The approach we used here, running the mean function twice, is redundant. Instead, you can simply wrap your assignment code in parentheses so that R will not only assign the average value of reading test scores to avg read, but it will also print out its value. # Do both (avg_read &lt;- mean(hsb2$read)) #&gt; [1] 52.2 Next we need to determine whether each student is below or at or above average. For example, a reading score of 57 is above average, so is 68, but 44 is below. Obviously, going through each record like this would be tedious and error prone. Instead we can create this new variable with the mutate function from the dplyr package. We start with the data frame, hsb2, and pipe it into mutate, to create a new variable called read_cat (cat for categorical). Note that we are using a new variable name here in order to not overwrite the existing reading score variable. The new variable read_cat will be a column in the existing data frame hsb2. To indicate that the mutate function came from the dplyr package, we use the pacakge::function syntax. It is not usually necessary to provide the package name (unless there is ambiguity about where the function came from). hsb2 &lt;- hsb2 %&gt;% dplyr::mutate(read_cat = ifelse(read &lt; avg_read, &quot;below average&quot;, &quot;at or above average&quot;)) The decision criteria for this new variable is based on a TRUE/FALSE question: if the reading score of the student is below the average reading score, label “below average”, otherwise, label “at or above average”. This can be accomplished using the ifelse function in R. The first argument of the function is the logical test. The second argument is what to do if the result of the logical test is TRUE, in other words, if the student’s score is below the average score, and the last argument is what to do if the result is FALSE. The ifelse function can be used for more complicated discretization rules as well, by nesting many ifelse statements within each other. This is not necessary for this example, but it will come up later in the course. Add link to tutorial. 1.6 Chapter review 1.6.1 Terms We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as bolded text. anecdotal evidence control group non-response bias representative associated convenience sample non-response rate response variable barplot data observational data retrospective study bias data frame observational study sample blind dependent observational unit sample bias blocking discrete ordinal simple random sample case double-blind placebo simple random sampling categorical experiment placebo effect strata cluster explanatory variable population stratified sampling cluster sampling independent positive association summary statistic cohort level prospective study treatment group confounding variable multistage sample quantitative variable continuous negative association randomized experiment control nominal replicate References "],
["eda.html", "Chapter 2 Exploratory Data Analysis 2.1 Exploring quantitative data 2.2 Exploring categorical data", " Chapter 2 Exploratory Data Analysis This chapter focuses on the mechanics and construction of summary statistics and graphs. We use statistical software for generating the summaries and graphs presented in this chapter and book. However, since this might be your first exposure to these concepts, we take our time in this chapter to detail how to create them. Mastery of the content presented in this chapter will be crucial for understanding the methods and techniques introduced in rest of the book. 2.1 Exploring quantitative data In this section we will explore techniques for summarizing quantitative variables. For example, consider the loan_amount variable from the loan50 data set, which represents the loan size for all 50 loans in the data set. This variable is quantitative since we can sensibly discuss the numerical difference of the size of two loans. On the other hand, area codes and zip codes are not quantitative, but rather they are categorical variables. Throughout this section and the next, we will apply these methods using the loan50, county, and email50 data sets, which were introduced in Section 1.2. If you’d like to review the variables from either data set, see Tables 1.4 and 1.6. The loan50 and email50 data sets can be found in the openintro package. The county data can be found in the usdata package. 2.1.1 Scatterplots for paired data A scatterplot provides a case-by-case view of data for two quantitative variables. In Figure 1.3, a scatterplot was used to examine the homeownership rate against the fraction of housing units that were part of multi-unit properties (e.g. apartments) in the county data set. Another scatterplot is shown in Figure 2.1, comparing the total income of a borrower total_income and the amount they borrowed loan_amount for the loan50 data set. In any scatterplot, each point represents a single case. Since there are 50 cases in loan50, there are 50 points in Figure 2.1. When examining scatterplots, we describe four features: Form - If you were to trace the trend of the points, would the trend be linear or nonlinear? Direction - As values on the x-axis increase, do the y-values tend to increase (positive) or do they decrease (negative)? Strength - How closely to the points follow a trend? Unusual observations or outliers- Are there any unusual observations that do not seem to match the overall pattern of the scatterplot? Figure 2.1: A scatterplot of loan_amount versus total_income for the loan50 data set. Looking at Figure 2.1, we see that there are many borrowers with income below $100,000 on the left side of the graph, while there are a handful of borrowers with income above $250,000. The loan amounts vary from below $10,000 to around $40,000. The data seem to have a linear form, though the relationship between the two variables is quite weak. The direction is positive – as total income increases, the loan amount also tends to increase – and there may be a few unusual observations in the higher income range, though since the relationship is weak, it is hard to tell. Figure 2.2: A scatterplot of the median household income against the poverty rate for the county dataset. Data are from 2017. A statistical model has also been fit to the data and is shown as a dashed line. Figure 2.2 shows a plot of median household income against the poverty rate for r nrow(county) counties. What can be said about the relationship between these variables? The relationship is evidently nonlinear, as highlighted by the dashed line. This is different from previous scatterplots we have seen, which show relationships that do not show much, if any, curvature in the trend. The relationship is moderate to strong, the direction is negative, and there does not appear to be any unusual observations. What do scatterplots reveal about the data, and how are they useful?18 Describe two variables that would have a horseshoe-shaped association in a scatterplot (\\(\\cap\\) or \\(\\frown\\))19 2.1.2 Dot plots and the mean Sometimes we are interested in the distribution of a single variable. In these cases, a dot plot provides the most basic of displays. A dot plot is a one-variable scatterplot; an example using the interest rate of 50 loans is shown in Figure 2.3. Figure 2.3: A dot plot of interest_rate for the loan50 dataset. The rates have been rounded to the nearest percent in this plot, and the distribution’s mean is shown as a red triangle. The mean, often called the average is a common way to measure the center of a distribution of data. To compute the mean interest rate, we add up all the interest rates and divide by the number of observations. The sample mean is often labeled \\(\\bar{x}\\). The letter \\(x\\) is being used as a generic placeholder for the variable of interest and the bar over the \\(x\\) communicates we’re looking at the average interest rate, which for these 50 loans is 11.57%. It’s useful to think of the mean as the balancing point of the distribution, and it’s shown as a triangle in Figure 2.3. Mean. The sample mean can be calculated as the sum of the observed values divided by the number of observations: \\[ \\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n} \\] Examine the equation for the mean. What does \\(x_1\\) correspond to? And \\(x_2\\) Can you infer a general meaning to what \\(n_i\\) might represent?20 What was \\(n\\) in this sample of loans?21 The loan50 data set represents a sample from a larger population of loans made through Lending Club. We could compute a mean for this population in the same way as the sample mean. However, the population mean has a special label: \\(\\mu\\). The symbol \\(\\mu\\) is the Greek letter mu and represents the average of all observations in the population. Sometimes a subscript, such as \\(_x\\), is used to represent which variable the population mean refers to, e.g. \\(\\mu_x\\). Often times it is too expensive to measure the population mean precisely, so we often estimate \\(\\mu\\) using the sample mean, \\(\\bar{x}\\). The Greek letter \\(\\mu\\) is pronounced mu, listen to the pronunciation here. The average interest rate across all loans in the population can be estimated using the sample data. Based on the sample of 50 loans, what would be a reasonable estimate of \\(\\mu_x\\), the mean interest rate for all loans in the full data set? The sample mean, r {round(loan50_mean_intrest_rate, 2)}, provides a rough estimate of \\(\\mu_x\\). While it is not perfect, this is our single best guess point estimate of the average interest rate of all the loans in the population under study. In Chapter 5 and beyond, we will develop tools to characterize the accuracy of point estimates, like the sample mean. As you might have guessed, point estimates based on larger samples tend to be more accurate than those based on smaller samples. The mean is useful because it allows us to rescale or standardize a metric into something more easily interpretable and comparable. Suppose we would like to understand if a new drug is more effective at treating asthma attacks than the standard drug. A trial of 1500 adults is set up, where 500 receive the new drug, and 1000 receive a standard drug in the control group: New drug Standard drug Number of patients 500 1000 Total asthma attacks 200 300 Comparing the raw counts of 200 to 300 asthma attacks would make it appear that the new drug is better, but this is an artifact of the imbalanced group sizes. Instead, we should look at the average number of asthma attacks per patient in each group: New drug: \\(200 / 500 = 0.4\\) asthma attacks per patient Standard drug: \\(300 / 1000 = 0.3\\) asthma attacks per patient The standard drug has a lower average number of asthma attacks per patient than the average in the treatment group. Provide another examples where the mean is useful for making comparisons. Emilio opened a food truck last year where he sells burritos, and his business has stabilized over the last 3 months. Over that 3 month period, he has made $11,000 while working 625 hours. Emilio’s average hourly earnings provides a useful statistic for evaluating whether his venture is, at least from a financial perspective, worth it: \\[ \\frac{\\$11000}{625\\text{ hours}} = \\$17.60\\text{ per hour} \\] By knowing his average hourly wage, Emilio now has put his earnings into a standard unit that is easier to compare with many other jobs that he might consider. Suppose we want to compute the average income per person in the US. To do so, we might first think to take the mean of the per capita incomes across the 3,142 counties in the data set. What would be a better approach? The county data set is special in that each county actually represents many individual people. If we were to simply average across the income variable, we would be treating counties with 5,000 and 5,000,000 residents equally in the calculations. Instead, we should compute the total income for each county, add up all the counties’ totals, and then divide by the number of people in all the counties. If we completed these steps with the data, we would find that the per capita income for the US is $30,861. Had we computed the simple mean of per capita income across counties, the result would have been just $26,093! This example used what is called a weighted mean. For more information on this topic, check out the following online supplement regarding weighted means. 2.1.3 Histograms and shape Dot plots show the exact value for each observation. This is useful for small data sets, but they can become hard to read with larger samples. Rather than showing the value of each observation, we prefer to think of the value as belonging to a bin. For example, in the loan50 data set, we created a table of counts for the number of loans with interest rates between 5.0% and 7.5%, then the number of loans with rates between 7.5% and 10.0%, and so on. Observations that fall on the boundary of a bin (e.g. 10.00%) are allocated to the lower bin. This tabulation is shown in Table 2.1. These binned counts are plotted as bars in Figure 2.4 into what is called a histogram, which resembles a more heavily binned version of the stacked dot plot shown in Figure 2.3. Table 2.1: Counts for the binned interest_rate data. Interest rate 5% - 7.5% 7.5% - 10% 10% - 12.5% 12.5% - 15% 15% - 17.5% 17.5% - 20% 20% - 22.5% 22.5% - 25% 25% - 27.5% Count 11 15 8 4 5 4 1 1 1 Figure 2.4: A histogram of interest_rate. This distribution is strongly skewed to the right. Histograms provide a view of the data density. Higher bars represent where the data are relatively more common. For instance, there are many more loans with rates between 5% and 10% than loans with rates between 20% and 25% in the data set. The bars make it easy to see how the density of the data changes relative to the interest rate. Histograms are especially convenient for understanding the shape of the data distribution. Figure 2.4 suggests that most loans have rates under 15%, while only a handful of loans have rates above 20%. When data trail off to the right in this way and has a longer right tail, the shape is said to be right skewed22 Data sets with the reverse characteristic – a long, thinner tail to the left – are said to be left skewed. We also say that such a distribution has a long left tail. Data sets that show roughly equal trailing off in both directions are called symmetric. When data trail off in one direction, the distribution has a long tail. If a distribution has a long left tail, it is left skewed. If a distribution has a long right tail, it is right skewed. Besides the mean (since it was labeled), what can you see in the dot plot in Figure 2.3 that you cannot see in the histogram in Figure 2.4?23 In addition to looking at whether a distribution is skewed or symmetric, histograms can be used to identify modes. A mode is represented by a prominent peak in the distribution. There is only one prominent peak in the histogram of interest_rate. A definition of mode sometimes taught in math classes is the value with the most occurrences in the data set. However, for many real-world data sets, it is common to have no observations with the same value in a data set, making this definition impractical in data analysis. Figure 2.5 shows histograms that have one, two, or three prominent peaks. Such distributions are called unimodal, bimodal, and multimodal, respectively. Any distribution with more than 2~prominent peaks is called multimodal. Notice that there was one prominent peak in the unimodal distribution with a second less prominent peak that was not counted since it only differs from its neighboring bins by a few observations. Figure 2.5: Counting only prominent peaks, the distributions are (left to right) unimodal, bimodal, and multimodal. Note that the left plot is unimodal because we are counting prominent peaks, not just any peak. Figure 2.4 reveals only one prominent mode in the interest rate. Is the distribution unimodal, bimodal, or multimodal?24 Height measurements of young students and adult teachers at a K-3 elementary school were taken. How many modes would you expect in this height data set?25. Looking for modes isn’t about finding a clear and correct answer about the number of modes in a distribution, which is why prominent is not rigorously defined in this book. The most important part of this examination is to better understand your data. 2.1.4 Boxplots and the median A boxplot (or box-and-whisker plot) summarizes a data set using five statistics while also plotting unusual observations. The five statistics—minimum, first quartile, median, third quartile, maximum—together are called the five number summary. Figure 2.6 provides a vertical dot plot alongside a box plot of the variable from the data set. Figure 2.6: A vertical dot plot next to a labeled box plot for the number of characters in 50 emails. The median (6.89), splits the data into the bottom 50% and the top 50%, marked in the dot plot by horizontal dashes and open circles, respectively. The first step in building a box plot is drawing a dark line denoting the median, which splits the data in half. Figure 2.6 shows 50% of the data falling below the median (dashes) and other 50% falling above the median (open circles). There are 50 character counts in the data set (an even number) so the data are perfectly split into two groups of 25. We take the median in this case to be the average of the two observations closest to the 50th percentile: \\((6.768 + 7.012)/2 = 6.890\\). When there are an odd number of observations, there will be exactly one observation that splits the data into two halves, and in this case that observation is the median (no average needed). Median. If the data are ordered from smallest to largest, the sample median is the observation right in the middle. If there are an even number of observations, there will be two values in the middle, and the median is taken as their average. If we denote the sample size by \\(n\\), then if \\(n\\) is odd, the median is the \\([(n+1)/2]^{th}\\) smallest value in the data set, and if \\(n\\) is even, the median is the average of the \\((n/2)^{th}\\) and \\((n/2+1)^{th}\\) smallest values in the data set. The second step in building a box plot is drawing a rectangle to represent the area of the middle 50% of the data. The total length of the box, shown vertically in Figure 2.6, is called the interquartile range (IQR, for short)(interquartile range, IQR). It is a measure of variability in data. The more variable the data, the larger the IQR. The two boundaries of the box are called the first quartile (the 25th percentile, i.e., 25% of the data fall below this value) and the third quartile (the 75th percentile), and these are often labeled \\(Q_1\\) and \\(Q_3\\), respectively. (In case you’re wondering, the median is the 2nd quartile! These three quartiles break the data into four groups of equal size.) Interquartile range (IQR). The IQR is the length of the box in a box plot—the range of the middle 50% of the data. It is computed as \\[IQR = Q_3 - Q_1,\\] where \\(Q_1\\) and \\(Q_3\\) are the 25th and 75th percentiles. What percent of the data fall between \\(Q_1\\) and the median? What percent is between the median and \\(Q_3\\)?26 Extending out from the box, the whiskers attempt to capture the data outside of the box; however, their reach is never allowed to be more than \\(1.5\\times IQR\\).27 They capture everything within this reach. In Figure 2.6, the upper whisker does not extend to the last three points, which are beyond \\(Q_3 + 1.5\\times IQR\\), and so it extends only to the last point below this limit. The lower whisker stops at the lowest value, 33, since there is no additional data to reach; the lower whisker’s limit is not shown in the figure because the plot does not extend down to \\(Q_1 - 1.5\\times IQR\\). In a sense, the box is like the body of the box plot and the whiskers are like its arms trying to reach the rest of the data. The whiskers extend to actual data points—not the limits for outliers. That is, the values \\(Q_1 - 1.5\\times IQR\\) and \\(Q_3 + 1.5\\times IQR\\) should not be shown on the plot. Any observation that lies beyond the whiskers is labeled with a dot. The purpose of labeling these points—instead of just extending the whiskers to the minimum and maximum observed values—is to help identify any observations that appear to be unusually distant from the rest of the data. Unusually distant observations are called outliers. In this case, it would be reasonable to classify the emails with character counts of 41.623, 42.793, and 64.401 as outliers since they are numerically distant from most of the data. Outliers are extreme. An outlier is an observation that is extreme, relative to the rest of the data. Examination of data for possible outliers serves many useful purposes, including: Identifying strong skewness in the distribution Identifying data collection or entry errors. For instance, we re-examined the email purported to have 64.401 characters to ensure this value was accurate. Providing insight into interesting properties of the data. The observation 64.401, an outlier, was found to be an accurate observation. What would such an observation suggest about the nature of character counts in emails?28 Using Figure 2.6, estimate the following values for num_char in the email50 data set: \\(Q_1\\) \\(Q_3\\) \\(IQR\\).29 2.1.5 Robust statistics 2.1.6 Comparing distributions center, shape (symmetric, skewed, normal), spread, outliers side-by-side boxplots, stacked dotplots/histograms 2.1.7 Mapping data (special topic) 2.2 Exploring categorical data Like numerical data, categorical data can also be organized and analyzed. This section introduces tables and other basic tools for categorical data that are used throughout this book. The email data set contains information on 3,921 emails. In this section, we will examine whether the presence of numbers, small or large, in an email provides any useful value in classifying email as spam or not spam. 2.2.1 Contingency tables and conditional proportions One-way and Two-way/contingency tables Define frequencies/counts and relative frequencies/proportions association vs no association in conditional proportions Bayes theorem with tables and trees - conditional vs unconditional probabilities Table 2.2 summarizes two variables: Type (spam or not spam) and Number. Number is a categorical variable that describes whether an email contains no numbers, only small numbers (values under 1 million), or at least one big number (a value of 1 million or more). A table that summarizes data for two categorical variables in this way is called a contingency table or two-way table. Each value in the table represents the number of times, or frequency a particular combination of variable outcomes occurred. For example, the value 149 corresponds to the number of emails in the data set that are spam and had no number listed in the email. Row and column totals are also included. The row totals provide the total counts across each row (e.g., \\(149 + 168 + 50 = 367\\)), and column totals are total counts down each column. A table for a single variable is called a frequency table. Table 2.3 is a frequency table for the Number variable. If we replaced the counts with percentages or proportions, the table would be called a relative frequency table. Table 2.2: Contingency table of Type and Number variables. none small big Total spam 149 168 50 367 not spam 400 2659 495 3554 Total 549 2827 545 3921 Table 2.3: Frequency table of Number variable. none small big 549 2827 545 2.2.2 Bar plots and mosaic plots one cat bar plot two cat bar plot two cat mosaic plot association vs no association in bar plots 2.2.3 Why not pie charts? Answers may vary. Scatterplots are helpful in quickly spotting associations relating variables, whether those associations come in the form of simple trends or whether those relationships are more complex.↩︎ Consider the case where your vertical axis represents something “good” and your horizontal axis represents something that is only good in moderation. Health and water consumption fit this description: we require some water to survive, but consume too much and it become toxic and can kill a person.↩︎ \\(x_1\\) corresponds to the interest rate for the first loan in the sample, \\(x_2\\) to the second loan’s interest rate, and \\(x_i\\) corresponds to the interest rate for the \\(i^{th}\\) loan in the data set. For example, if \\(i = 4\\), then we’re examining \\(x_4\\), which refers to the fourth observation in the data set.↩︎ The sample size was \\(n = 50\\).↩︎ Other ways to describe data that are right skewed: skewed to the right, skewed to the high end, or skewed to the positive end.↩︎ The interest rates for individual loans.↩︎ Unimodal Remember that uni stands for 1 (think unicycles). Similarly, bi stands for 2 (think bicycles). We are hoping a multicycle will be invented to complete this analogy.↩︎ There might be two height groups visible in the data set: one of the students and one of the adults. That is, the data are probably bimodal.↩︎ Since \\(Q_1\\) and \\(Q_3\\) capture the middle 50% of the data, and the median splits the data in the middle, 25% of the data fall between \\(Q_1\\) and the median, and another 25% falls between the median and \\(Q_3\\).↩︎ While the choice of exactly 1.5 is arbitrary, it is the most commonly used value for box plots.↩︎ That occasionally there may be very long emails.↩︎ These visual estimates will vary a little from one person to the next: \\(Q_1\\approx\\) 3.000, \\(Q_3\\approx\\) 15.000, \\(IQR = Q_3 - Q_1 \\approx\\) 12.000. (The true values: \\(Q_1=\\) 2.536, \\(Q_3=\\) 15.411, \\(IQR=\\) 12.875.)↩︎ "],
["cor-reg.html", "Chapter 3 Correlation and Regression 3.1 Chapter 3 review", " Chapter 3 Correlation and Regression The content in this chapter is currently just placeholder. We will remove this banner once the chapter content has been updated and ready for review. review scatterplots correlation least squares regression line, fitted/predicted values residuals, SSE/SSR/SST, R-squared extrapolation outliers and influential points 3.1 Chapter 3 review 3.1.1 Terms We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as bolded text. 3.1.2 Chapter exercises "],
["mult-reg.html", "Chapter 4 Multiple Regression 4.1 Num vs. whatever - MLR 4.2 Parallel slopes 4.3 Hint at interaction, planes, and parallel planes but not quantify 4.4 Chapter 4 review", " Chapter 4 Multiple Regression The content in this chapter is currently just placeholder. We will remove this banner once the chapter content has been updated and ready for review. 4.1 Num vs. whatever - MLR Introduction to multiple regression 4.2 Parallel slopes 4.3 Hint at interaction, planes, and parallel planes but not quantify Visualization of higher-dimensional models (rgl demo) 4.4 Chapter 4 review 4.4.1 Terms We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as bolded text. "],
["inference-foundations.html", "Chapter 5 Foundations of inference 5.1 Randomization test 5.2 Bootstrap confidence interval 5.3 Mathematical model 5.4 Chapter 5 review", " Chapter 5 Foundations of inference The content in this chapter is currently just placeholder. We will remove this banner once the chapter content has been updated and ready for review. Statistical inference is primarily concerned with understanding and quantifying the uncertainty of parameter estimates. While the equations and details change depending on the setting, the foundations for inference are the same throughout all of statistics. We start with two case studies designed to motivate the process of making decisions about research claims. We formalize the process through the introducuction of the hypothesis testing framework, which allows us to formally evaluate claims about the population. Finally we expand on the familiar idea of using a sample proportion to estimate a population proportion. That is, we create what is called a confidence interval, which is a range of plausible values where we may find the true population value. Throughout the book so far, you have worked with data in a variety of contexts. You have learned how to summarize and visualize the data as well as how to model multiple variables at the same time. Sometimes the dataset at hand represents the entire research question. But more often than not, the data have been collected to answer a research question about a larger group of which the data are a (hopefully) representative subset. You may agree that there is almost always variability in data (one dataset will not be identical to a second dataset even if they are both collected from the same population using the same methods). However, quantifying the variability in the data is neither obvious nor easy to do (how different is one dataset from another?). Suppose your professor splits the students in class into two groups: students on the left and students on the right. If \\(\\hat{p}_{_L}\\) and \\(\\hat{p}_{_R}\\) represent the proportion of students who own an Apple product on the left and right, respectively, would you be surprised if \\(\\hat{p}_{_L}\\) did not exactly equal \\(\\hat{p}_{_R}\\)? While the proportions would probably be close to each other, it would be unusual for them to be exactly the same. We would probably observe a small difference due to chance. If we don’t think the side of the room a person sits on in class is related to whether the person owns an Apple product, what assumption are we making about the relationship between these two variables? (Reminder: for these Guided Practice questions, you can check your answer in the footnote.)30 Studying randomness of this form is a key focus of statistics. Throughout this chapter, and those that follow, we provide three different approaches for quantifying the variability inherent in data: randomization, bootstrapping, and mathematical models. Using the methods provided in this chapter, we will be able to draw conclusions beyond the dataset at hand to research questions about larger populations. 5.1 Randomization test The first type of variability we will explore comes from experiments where the explanatory variable (or treatment) is randomly assigned to the observational units. As you learned in Chapter 1, a randomized experiment is done to assess whether or not one variable (the explanatory variable) causes changes in a second variable (the response variable). Every dataset has some variability in it, so to decide whether the variability in the data is due to (1) the causal mechanism (the randomized explanatory variable in the experiment) or instead (2) natural variability inherent to the data, we set up a sham randomized experiment as a comparison. That is, we assume that each observational unit would have gotten the exact same response value regardless of the treatment level. By reassigning the treatments many many times, we can compare the actual experiment to the sham experiment. If the actual experiment has more extreme results than any of the sham experiments, we are led to believe that it is the explanatory variable which is causing the result and not inherent data variability. Using a few different case studies, let’s look more carefully at this idea of a randomization test. 5.1.1 Gender discrimination case study We consider a study investigating gender discrimination in the 1970s, which is set in the context of personnel decisions within a bank.31 The research question we hope to answer is, “Are females discriminated against in promotion decisions made by male managers?” Observed data The participants in this study were 48 male bank supervisors attending a management institute at the University of North Carolina in 1972. They were asked to assume the role of the personnel director of a bank and were given a personnel file to judge whether the person should be promoted to a branch manager position. The files given to the participants were identical, except that half of them indicated the candidate was male and the other half indicated the candidate was female. These files were randomly assigned to the subjects. Is this an observational study or an experiment? How does the type of study impact what can be inferred from the results?32 For each supervisor we recorded the gender associated with the assigned file and the promotion decision. Using the results of the study summarized in Table 5.1, we would like to evaluate if females are unfairly discriminated against in promotion decisions. In this study, a smaller proportion of females are promoted than males (0.583 versus 0.875), but it is unclear whether the difference provides convincing evidence that females are unfairly discriminated against. Table 5.1: Summary results for the gender discrimination study. decision promoted not promoted Total male 21 3 24 gender female 14 10 24 Total 35 13 48 The data are visualized in Figure 5.1. Note that the promoted decision is colored in red (promoted) and white(not promoted). Additionally, the observations are broken up into the male and female groups. Figure 5.1: The gender descriminiation study can be thought of as 48 red and black cards. Statisticians are sometimes called upon to evaluate the strength of evidence. When looking at the rates of promotion for males and females in this study, why might we be tempted to immediately conclude that females are being discriminated against? The large difference in promotion rates (58.3% for females versus 87.5% for males) suggest there might be discrimination against women in promotion decisions. However, we cannot yet be sure if the observed difference represents discrimination or is just from random chance. Generally there is a little bit of fluctuation in sample data, and we wouldn’t expect the sample proportions to be exactly equal, even if the truth was that the promotion decisions were independent of gender. The previous example is a reminder that the observed outcomes in the sample may not perfectly reflect the true relationships between variables in the underlying population. Table 5.1 shows there were 7 fewer promotions in the female group than in the male group, a difference in promotion rates of 29.2% \\(\\left( \\frac{21}{24} - \\frac{14}{24} = 0.292 \\right)\\). This observed difference is what we call a point estimate of the true effect. The point estimate of the difference is large, but the sample size for the study is small, making it unclear if this observed difference represents discrimination or whether it is simply due to chance. We label these two competing claims, \\(H_0\\) and \\(H_A\\): \\(H_0\\): Null hypothesis. The variables gender and decision are independent. They have no relationship, and the observed difference between the proportion of males and females who were promoted, 29.2%, was due to chance. \\(H_A\\): Alternative hypothesis. The variables gender and decision are not independent. The difference in promotion rates of 29.2% was not due to chance, and equally qualified females are less likely to be promoted than males. Hypothesis testing These hypotheses are part of what is called a hypothesis test. A hypothesis test is a statistical technique used to evaluate competing claims using data. Often times, the null hypothesis takes a stance of no difference or no effect. If the null hypothesis and the data notably disagree, then we will reject the null hypothesis in favor of the alternative hypothesis. Don’t worry if you aren’t a master of hypothesis testing at the end of this section. We’ll discuss these ideas and details many times in this chapter and those that follow. What would it mean if the null hypothesis, which says the variables gender and decision are unrelated, is true? It would mean each banker would decide whether to promote the candidate without regard to the gender indicated on the file. That is, the difference in the promotion percentages would be due to the way the files were randomly divided to the bankers, and the randomization just happened to give rise to a relatively large difference of 29.2%. Consider the alternative hypothesis: bankers were influenced by which gender was listed on the personnel file. If this was true, and especially if this influence was substantial, we would expect to see some difference in the promotion rates of male and female candidates. If this gender bias was against females, we would expect a smaller fraction of promotion recommendations for female personnel files relative to the male files. We will choose between these two competing claims by assessing if the data conflict so much with \\(H_0\\) that the null hypothesis cannot be deemed reasonable. If this is the case, and the data support \\(H_A\\), then we will reject the notion of independence and conclude that these data provide strong evidence of discrimination. Variability of the statistic Table 5.1 shows that 35 bank supervisors recommended promotion and 13 did not. Now, suppose the bankers’ decisions were independent of gender. Then, if we conducted the experiment again with a different random assignment of gender to the files, differences in promotion rates would be based only on random fluctuation. We can actually perform this randomization, which simulates what would have happened if the bankers’ decisions had been independent of gender but we had distributed the file genders differently.33 In this simulation, we thoroughly shuffle 48 personnel files, 35 labeled promoted and 13 labeled not promoted, and we deal these files into two stacks. Note that by keeping 35 promoted and 13 not promoted, we are assuming that 35 of the bank managers would have promoted the individual whose content is contained in the file (independent of gender). We will deal 24 files into the first stack, which will represent the 24 “female” files. The second stack will also have 24 files, and it will represent the 24 “male” files. Figure 5.2 highlights both the shuffle and the reallocation to the sham gender groups. Figure 5.2: The gender descriminiation data is shuffled and reallocated to the gender groups. Then, as we did with the original data, we tabulate the results and determine the fraction of male and female who were promoted. Since the randomization of files in this simulation is independent of the promotion decisions, any difference in the two promotion rates is entirely due to chance. Table 5.2 show the results of one such simulation. Table 5.2: Simulation results, where the difference in promotion rates between male and female is purely due to chance. decision promoted not promoted Total male 18 6 24 gender female 17 7 24 Total 35 13 48 What is the difference in promotion rates between the two simulated groups in Table 5.2 ? How does this compare to the observed difference 29.2% from the actual study?34 Figure 5.3 shows that the difference in promotion rates is much larger in the original data than it is in the simulated groups (0.292 &gt;&gt;&gt; 0.042). The quantity of interest throughout this case study has been the difference in promotion rates. We call the summary value the statistic of interest (or often the test statistic). When we encounter different data structures, the statistic is likely to change (e.g., we might calculate an average instead of a proportion), but we will always want to understand how the statistic varies from sample to sample. Figure 5.3: We summarize the randomized data to produce one estiamte of the difference in proportions given no gender discrimination. Observed statistic vs. null statistics We computed one possible difference under the null hypothesis in Guided Practice, which represents one difference due to chance. While in this first simulation, we physically dealt out files, it is much more efficient to perform this simulation using a computer. Repeating the simulation on a computer, we get another difference due to chance: -0.042. And another: 0.208. And so on until we repeat the simulation enough times that we have a good idea of what represents the distribution of differences from chance alone. Figure 5.4 shows a plot of the differences found from 100 simulations, where each dot represents a simulated difference between the proportions of male and female files recommended for promotion. Figure 5.4: A stacked dot plot of differences from 100 simulations produced under the null hypothesis, \\(H_0\\), where gender_simulated and decision are independent. Two of the 100 simulations had a difference of at least 29.2%, the difference observed in the study, and are shown as solid red dots. Note that the distribution of these simulated differences in proportions is centered around 0. Because we simulated differences in a way that made no distinction between men and women, this makes sense: we should expect differences from chance alone to fall around zero with some random fluctuation for each simulation. How often would you observe a difference of at least 29.2% (0.292) according to Figure 5.4? Often, sometimes, rarely, or never? It appears that a difference of at least 29.2% due to chance alone would only happen about 2% of the time according to Figure 5.4. Such a low probability indicates that observing such a large difference from chance is rare. The difference of 29.2% is a rare event if there really is no impact from listing gender in the candidates’ files, which provides us with two possible interpretations of the study results: \\(H_0\\): Null hypothesis. Gender has no effect on promotion decision, and we observed a difference that is so large that it would only happen rarely. \\(H_A\\): Alternative hypothesis. Gender has an effect on promotion decision, and what we observed was actually due to equally qualified women being discriminated against in promotion decisions, which explains the large difference of 29.2%. When we conduct formal studies, we reject a null position (the idea that the data are a result of chance only) if the data strongly conflict with that null position.35 In our analysis, we determined that there was only a \\(\\approx\\) 2% probability of obtaining a sample where \\(\\geq\\) 29.2% more males than females get promoted by chance alone, so we conclude that the data provide strong evidence of gender discrimination against women by the supervisors. In this case, we reject the null hypothesis in favor of the alternative. Statistical inference is the practice of making decisions and conclusions from data in the context of uncertainty. Errors do occur, just like rare events, and the data set at hand might lead us to the wrong conclusion. While a given data set may not always lead us to a correct conclusion, statistical inference gives us tools to control and evaluate how often these errors occur. Before getting into the nuances of hypothesis testing, let’s work through another case study. 5.1.2 Opportunity cost case study How rational and consistent is the behavior of the typical American college student? In this section, we’ll explore whether college student consumers always consider the following: money not spent now can be spent later. In particular, we are interested in whether reminding students about this well-known fact about money causes them to be a little thriftier. A skeptic might think that such a reminder would have no impact. We can summarize the two different perspectives using the null and alternative hypothesis framework. \\(H_0\\): Null hypothesis. Reminding students that they can save money for later purchases will not have any impact on students’ spending decisions. \\(H_A\\): Alternative hypothesis. Reminding students that they can save money for later purchases will reduce the chance they will continue with a purchase. In this section, we’ll explore an experiment conducted by researchers that investigates this very question for students at a university in the southwestern United States.36 Observed data One-hundred and fifty students were recruited for the study, and each was given the following statement: Imagine that you have been saving some extra money on the side to make some purchases, and on your most recent visit to the video store you come across a special sale on a new video. This video is one with your favorite actor or actress, and your favorite type of movie (such as a comedy, drama, thriller, etc.). This particular video that you are considering is one you have been thinking about buying for a long time. It is available for a special sale price of $14.99. What would you do in this situation? Please circle one of the options below. Half of the 150 students were randomized into a control group and were given the following two options: Buy this entertaining video. Not buy this entertaining video. The remaining 75 students were placed in the treatment group, and they saw a slightly modified option (B): Buy this entertaining video. Not buy this entertaining video. Keep the $14.99 for other purchases. Would the extra statement reminding students of an obvious fact impact the purchasing decision? Table 5.3 summarizes the study results. Table 5.3: Summary of student choices in the opportunity cost study. decision buy DVD not buy DVD Total control group 56 19 75 treatment group 41 34 75 Total 97 53 150 It might be a little easier to review the results using row proportions, specifically considering the proportion of participants in each group who said they would buy or not buy the DVD. These summaries are given in Table 5.4. Table 5.4: The data above are now summarized using row proportions. Row proportions are particularly useful here since we can view the proportion of buy and not buy decisions in each group. decision buy DVD not buy DVD Total control group 0.747 0.253 1.00 treatment group 0.547 0.453 1.00 Total 0.647 0.353 1.00 We will define a success in this study as a student who chooses not to buy the DVD.37 Then, the value of interest is the change in DVD purchase rates that results by reminding students that not spending money now means they can spend the money later. We can construct a point estimate for this difference as \\[\\begin{align*} \\hat{p}_{trmt} - \\hat{p}_{ctrl} = \\frac{34}{75} - \\frac{19}{75} = 0.453 - 0.253 = 0.200 \\end{align*}\\] The proportion of students who chose not to buy the DVD was 20% higher in the treatment group than the control group. However, is this result statistically significant? In other words, is a 20% difference between the two groups so prominent that it is unlikely to have occurred from chance alone? Variability of the statistic The primary goal in this data analysis is to understand what sort of differences we might see if the null hypothesis were true, i.e., the treatment had no effect on students. For this, we’ll use the same procedure we applied in Section 5.1.1: randomization. Let’s think about the data in the context of the hypotheses. If the null hypothesis (\\(H_0\\)) was true and the treatment had no impact on student decisions, then the observed difference between the two groups of 20% could be attributed entirely to chance. If, on the other hand, the alternative hypothesis (\\(H_A\\)) is true, then the difference indicates that reminding students about saving for later purchases actually impacts their buying decisions. Observed statistic vs. null statistics Just like with the gender discrimination study, we can perform a statistical analysis. Using the same randomization technique from the last section, let’s see what happens when we simulate the experiment under the scenario where there is no effect from the treatment. While we would in reality do this simulation on a computer, it might be useful to think about how we would go about carrying out the simulation without a computer. We start with 150 index cards and label each card to indicate the distribution of our response variable: decision. That is, 53 cards will be labeled “not buy DVD” to represent the 53 students who opted not to buy, and 97 will be labeled “buy DVD” for the other 97 students. Then we shuffle these cards thoroughly and divide them into two stacks of size 75, representing the simulated treatment and control groups. Any observed difference between the proportions of “not buy DVD” cards (what we earlier defined as success) can be attributed entirely to chance. If we are randomly assigning the cards into the simulated treatment and control groups, how many “not buy DVD” cards would we expect to end up with in each simulated group? What would be the expected difference between the proportions of “not buy DVD” cards in each group? Since the simulated groups are of equal size, we would expect \\(53 / 2 = 26.5\\), i.e., 26 or 27, “not buy DVD” cards in each simulated group, yielding a simulated point estimate of 0% . However, due to random fluctuations, we might actually observe a number a little above or below 26 and 27. The results of a single randomization from chance alone is shown in Table 5.5. From this table, we can compute a difference that occurred from chance alone: \\[\\begin{align*} \\hat{p}_{trmt, simulated} - \\hat{p}_{ctrl, simulated} = \\frac{24}{75} - \\frac{29}{75} = 0.32 - 0.387 = - 0.067 \\end{align*}\\] Table 5.5: Summary of student choices against their simulated groups. The group assignment had no connection to the student decisions, so any difference between the two groups is due to chance. decision buy DVD not buy DVD Total control group 46 29 75 treatment group 51 24 75 Total 97 53 150 Just one simulation will not be enough to get a sense of what sorts of differences would happen from chance alone. We’ll simulate another set of simulated groups and compute the new difference: 0.013. And again: 0.067. And again: -0.173. We’ll do this 1,000 times. The results are summarized in a dot plot in Figure 5.5, where each point represents a simulation. Since there are so many points, it is more convenient to summarize the results in a histogram such as the one in Figure 5.6, where the height of each histogram bar represents the fraction of observations in that group. Figure 5.5: A stacked dot plot of 1,000 chance differences produced under the null hypothesis, \\(H_0\\). Six of the 1,000 simulations had a difference of at least 20% , which was the difference observed in the study. Figure 5.6: A histogram of 1,000 chance differences produced under the null hypothesis, \\(H_0\\). Histograms like this one are a more convenient representation of data or results when there are a large number of observations. If there was no treatment effect, then we’d only observe a difference of at least +20% about 0.6% of the time, or about 1-in-150 times. That is really rare! Instead, we will conclude the data provide strong evidence there is a treatment effect: reminding students before a purchase that they could instead spend the money later on something else lowers the chance that they will continue with the purchase. Notice that we are able to make a causal statement for this study since the study is an experiment. 5.1.3 Hypothesis testing In the last two sections, we utilized a hypothesis test, which is a formal technique for evaluating two competing possibilities. In each scenario, we described a null hypothesis, which represented either a skeptical perspective or a perspective of no difference. We also laid out an alternative hypothesis, which represented a new perspective such as the possibility that there has been a change or that there is a treatment effect in an experiment. The alternative hypothesis is usually the reason the scientists set out to do the research in the first place. Null and alternative hypotheses. The null hypothesis (\\(H_0\\)) often represents either a skeptical perspective or a claim to be tested. The alternative hypothesis (\\(H_A\\)) represents an alternative claim under consideration and is often represented by a range of possible values for the value of interest. The hypothesis testing framework is a very general tool, and we often use it without a second thought. If a person makes a somewhat unbelievable claim, we are initially skeptical. However, if there is sufficient evidence that supports the claim, we set aside our skepticism. The hallmarks of hypothesis testing are also found in the US court system. The US court system A US court considers two possible claims about a defendant: they are either innocent or guilty. If we set these claims up in a hypothesis framework, which would be the null hypothesis and which the alternative? The jury considers whether the evidence is so convincing (strong) that there is no reasonable doubt regarding the person’s guilt. That is, the skeptical perspective (null hypothesis) is that the person is innocent until evidence is presented that convinces the jury that the person is guilty (alternative hypothesis). Jurors examine the evidence to see whether it convincingly shows a defendant is guilty. Notice that if a jury finds a defendant not guilty, this does not necessarily mean the jury is confident in the person’s innocence. They are simply not convinced of the alternative that the person is guilty. This is also the case with hypothesis testing: even if we fail to reject the null hypothesis, we typically do not accept the null hypothesis as truth. Failing to find strong evidence for the alternative hypothesis is not equivalent to providing evidence that the null hypothesis is true. p-value and statistical significance In Section 5.1.1 we encountered a study from the 1970’s that explored whether there was strong evidence that women were less likely to be promoted than men. The research question – are females discriminated against in promotion decisions? – was framed in the context of hypotheses: \\(H_0\\): Gender has no effect on promotion decisions. \\(H_A\\): Women are discriminated against in promotion decisions. The null hypothesis (\\(H_0\\)) was a perspective of no difference. The data on gender discrimination provided a point estimate of a 29.2% difference in recommended promotion rates between men and women. We determined that such a difference from chance alone would be rare: it would only happen about 2 in 100 times. When results like these are inconsistent with \\(H_0\\), we reject \\(H_0\\) in favor of \\(H_A\\). Here, we concluded there was discrimination against women. The 2-in-100 chance is what we call a p-value, which is a probability quantifying the strength of the evidence against the null hypothesis and in favor of the alternative. p-value. The p-value is the probability of observing data at least as favorable to the alternative hypothesis as our current dataset, if the null hypothesis were true. We typically use a summary statistic of the data, such as a difference in proportions, to help compute the p-value and evaluate the hypotheses. This summary value that is used to compute the p-value is often called the test statistic. In the gender discrimination study, the difference in discrimination rates was our test statistic. What was the test statistic in the opportunity cost study covered in Section 5.1.2? The test statistic in the opportunity cost study was the difference in the proportion of students who decided against the DVD purchase in the treatment and control groups. In each of these examples, the point estimate of the difference in proportions was used as the test statistic. When the p-value is small, i.e., less than a previously set threshold, we say the results are statistically significant. This means the data provide such strong evidence against \\(H_0\\) that we reject the null hypothesis in favor of the alternative hypothesis. The threshold, called the significance level and often represented by \\(\\alpha\\) (the Greek letter alpha), is typically set to \\(\\alpha = 0.05\\), but can vary depending on the field or the application. Using a significance level of \\(\\alpha = 0.05\\) in the discrimination study, we can say that the data provided statistically significant evidence against the null hypothesis. Statistical significance. We say that the data provide statistically significant evidence against the null hypothesis if the p-value is less than some reference value, often \\(\\alpha=0.05\\). In the opportunity cost study in Section 5.1.2, we analyzed an experiment where study participants had a 20% drop in likelihood of continuing with a DVD purchase if they were reminded that the money, if not spent on the DVD, could be used for other purchases in the future. We determined that such a large difference would only occur about 1-in-150 times if the reminder actually had no influence on student decision-making. What is the p-value in this study? Was the result statistically significant? The p-value was 0.006 (about 1/150). Since the p-value is less than 0.05, the data provide statistically significant evidence that US college students were actually influenced by the reminder. What’s so special about 0.05? We often use a threshold of 0.05 to determine whether a result is statistically significant. But why 0.05? Maybe we should use a bigger number, or maybe a smaller number. If you’re a little puzzled, that probably means you’re reading with a critical eye – good job! We’ve made a video to help clarify why 0.05: https://www.openintro.org/book/stat/why05/ Sometimes it’s also a good idea to deviate from the standard. We’ll discuss when to choose a threshold different than 0.05 in Section 6.2.1. 5.1.4 Randomization test summary Figure 5.7: An example of one simulation of the full randomization procedure. We repeat the steps hundreds or thousands of times. Frame the research question in terms of hypotheses. Hypothesis tests are appropriate for research questions that can be summarized in two competing hypotheses. The null hypothesis (\\(H_0\\)) usually represents a skeptical perspective or a perspective of no difference. The alternative hypothesis (\\(H_A\\)) usually represents a new view or a difference. Collect data with an observational study or experiment. If a research question can be formed into two hypotheses, we can collect data to run a hypothesis test. If the research question focuses on associations between variables but does not concern causation, we would run an observational study. If the research question seeks a causal connection between two or more variables, then an experiment should be used. Model the randomness as if the null hypothesis was true. In the examples above, the variability has been modeled as if the treatment (e.g., gender, opportunity, blood thinner) allocation was independent of the outcome of the study. The computer generated the null distribution from many different randomizations in order to quantify the null variability. Analyze the data. Choose an analysis technique appropriate for the data and identify the p-value. So far, we’ve only seen one analysis technique: randomization. Throughout the rest of this textbook, we’ll encounter several new methods suitable for many other contexts. Form a conclusion. Using the p-value from the analysis, determine whether the data provide statistically significant evidence against the null hypothesis. Also, be sure to write the conclusion in plain language so casual readers can understand the results. Table 5.6: Summary of Randomization Tests as an inferential statistical method. Randomization Test What does it do? Shuffles the explanatory variable to mimic the natural variability found in a randomized experiment. What is the random process described? randomized experiment Is there flexibility? Yes, can be used to describe random sampling in an observational model What is it best for? Hypothesis Testing (can be used for Confidence Intervals, but not covered in this text). What physical object represents the simulation process? shuffling cards 5.2 Bootstrap confidence interval Randomization is a statistical technique suitable for evaluating whether a difference in sample proportions is due to chance. Randomization tests are best suited for modeling experiments where the treatment (explanatory variable) has been randomly assigned to the observational units. In this section, we explore the situation where we focus on a single proportion, and we introduce a new simulation method, bootstrapping. Bootstrapping is best suited for modeling studies where the data have been generated through random sampling from a population. Sometime the mathematical theory for how an estimate varies is well-known; this is the case for the sample proportion as seen in Section 5.3. However, some statistics don’t have simple theory for how they vary, and bootstrapping provides a computational approach for providing interval estimates for almost any population parameter (we will revisit bootstrapping in Chapters 6, 7, and 8 so you’ll get plenty of practice as well as exposure to bootstrapping in many different data settings). If the goal is to produce a range of possible values for a population value, then in an ideal world, we would sample data from the population again and recompute the sample proportion. Then we could do it again. And again. And so on until we have a good sense of the variability of our original estimate. The ideal world where sampling data is free or extremely cheap is almost never the case, and taking repeated samples from a population is usually impossible. So, instead of using a “resample from the population” approach, bootstrapping uses a “resample from the sample” approach. 5.2.1 Medical consultant case study People providing an organ for donation sometimes seek the help of a special medical consultant. These consultants assist the patient in all aspects of the surgery, with the goal of reducing the possibility of complications during the medical procedure and recovery. Patients might choose a consultant based in part on the historical complication rate of the consultant’s clients. Observed data One consultant tried to attract patients by noting the average complication rate for liver donor surgeries in the US is about 10%, but her clients have had only 3 complications in the 62 liver donor surgeries she has facilitated. She claims this is strong evidence that her work meaningfully contributes to reducing complications (and therefore she should be hired!). We will let \\(p\\) represent the true complication rate for liver donors working with this consultant. (The “true” complication rate will be referred to as the parameter.) Estimate \\(p\\) using the data, and label this value \\(\\hat{p}\\). The sample proportion for the complication rate is 3 complications divided by the 62 surgeries the consultant has worked on: \\(\\hat{p} = 3/62 = 0.048\\). Is it possible to assess the consultant’s claim using the data? No. The claim is that there is a causal connection, but the data are observational, so we must be on the lookout for confounding variables. For example, maybe patients who can afford a medical consultant can afford better medical care, which can also lead to a lower complication rate. While it is not possible to assess the causal claim, it is still possible to understand the consultant’s true rate of complications. Parameter. A parameter is the “true” value of interest. We typically estimate the parameter using a point estimate from a sample of data. For example, we estimate the probability \\(p\\) of a complication for a client of the medical consultant by examining the past complications rates of her clients: \\[\\hat{p} = 3 / 62 = 0.048\\qquad\\text{is used to estimate}\\qquad p\\] Variability of the statistic In the medical consultant case study, the parameter is \\(p\\), the true probability of a complication for a client of the medical consultant. There is no reason to believe that \\(p\\) is exactly \\(\\hat{p} = 3/62\\), but there is also no reason to believe that \\(p\\) is particularly far from \\(\\hat{p} = 3/62\\). By sampling with replacement from the dataset (a process called bootstrapping), the variability of the possible \\(\\hat{p}\\) values can be approximated. need a new physical object! Most of the inferential procedures covered in this text are grounded in quantifying how one data set would differ from another when they are both taken from the same population. It doesn’t make sense to take repeated samples from the same population because if you have the means to take more samples, a larger sample size will benefit you more than the exact same sample twice. Instead, we measure how the samples behave under an estimate of the population. Figure 5.8 shows how the unknown original population can be estimated by using the sample to approximate the proportion of successes and failures (in our case, the proportion of complications and no complications for the medical consultant). Figure 5.8: first figure with the ? pop, then sample, then estimate of the pop. below i said 3 white (success) and 4 red (failure). easy to change the text below. By taking repeated samples from the estimated population, the variability from sample to sample can be observed. In Figure 5.9 the repeated bootstrap samples are obviously different both from each other and from the original population. Recall that the bootstrap samples were taken from the same (estimated) population, and so the differences are due entirely to natural variability in the sampling procedure. Figure 5.9: next fig, has the bootstrap samples By summarizing each of the bootstrap samples (here, using the sample proportion), we see, directly, the variability of the sample proportion, \\(\\hat{p}\\), from sample to sample. The distribution of \\(\\hat{p}_{bs}\\) for the example scenario is shown in Figure 5.10, and the bootstrap distribution for the medical consultant data is shown in Figure 5.13. Figure 5.10: WITH ADDED HISTOGRAM… boot samples, arrow, histogram of all of them It turns out that in practice, it is very difficult for computers to work with an infinite population (with the same proportional breakdown as in the sample). However, there is a physical and computational model which produces an equivalent bootstrap distribution of the sample proportion in a computationally efficient manner. Consider the observed data to be a bag of marbles 3 of which are success (white) and 4 of which are failures (red). By drawing the marbles out of the bag with replacement, we depict the same sampling process as was done with the infinitely large estimated population. Note in Figure 5.11 that when sampling the original observations, a particular data point may end up in the new sample one time (evidenced by a circle around the observation), two times (evidenced by two circles around the observation), or not at all (no circles around the observation). If we apply the bootstrap sampling process to the medical consultant example, we consider each client to be one of the marbles in the bag. There will be 59 white marbles (no complication) and 3 red marbles (complication). If we 62 choose marbles out of the bag (one at a time) and compute the proportion of simulated patients with complications, \\(\\hat{p}_{bs}\\), then this “bootstrap” proportion represents a single simulated proportion from the “resample from the sample” approach. Figure 5.11: sampling with replacement figure In a simulation of 62 patients, about how many would we expect to have had a complication?38 Figure 5.12 visualizes one simulation for the medical consultant. Out of the simulated cases, there were 5 with a complication and 57 without a complication: \\(\\hat{p}_{bs} = 5/62 = 0.081\\). Figure 5.12: how impossible would it be to create one simulation with 62 marbles? 3 red, 59 white? One simulation isn’t enough to get a sense of the variability from one bootstrap proportion to another bootstrap proportion, so we repeated the simulation 10,000 times using a computer. Figure 5.13 shows the distribution from the 10,000 bootstrap simulations. The bootstrapped proportions vary from about zero to 11.3%. The variability in the bootstrapped proportions leads us to believe that the true probability of complication (the parameter, \\(p\\)) is somewhere between 0 and 11.3%. Figure 5.13: The original medical consultant data is bootstrapped 10,000 times. Each simulation creates a sample from the original data where the probability of a complication is \\(\\hat{p} = 3/62\\). The bootstrap 2.5 percentile proportion is 0 and the 97.5 percentile is 0.113. The result is: we are confident that, in the population, the true probability of a complication is between 0% and 11.3%. The original claim was that the consultant’s true rate of complication was under the national rate of 10%. Does the interval estimate of 0 to 11.3% for the true probability of complication indicate that the surgical consultant has a lower rate of complications than the national average? Explain. No. Because the interval overlaps 10%, it might be that the consultant’s work is associated with a lower risk of complciations, or it might be that the consulant’s work is associated with a higher risk (i.e., greater than 10%) of complications! Additionally, as previously mentioned, because this is an observational study, even if an association can be measured, there is no evidence that the consultant’s work is the cause of the complication rate (being higher or lower). 5.2.2 Tappers and listeners case study Here’s a game you can try with your friends or family: pick a simple, well-known song, tap that tune on your desk, and see if the other person can guess the song. In this simple game, you are the tapper, and the other person is the listener. Observed data A Stanford University graduate student named Elizabeth Newton conducted an experiment using the tapper-listener game.39 In her study, she recruited 120 tappers and 120 listeners into the study. About 50% of the tappers expected that the listener would be able to guess the song. Newton wondered, is 50% a reasonable expectation? In Newton’s study, only 3 out of 120 listeners (\\(\\hat{p} = 0.025\\)) were able to guess the tune! That seems like quite a low number which leads the researcher to ask: what is the true proportion of people who can guess the tune? need a new physical object! Variability of the statistic To answer the question, we will again use a simulation. To simulate 120 games, this time we use a bag of 120 marbles 3 are red and 117 are white. Sampling from the bag 120 times (while not actually removing the marbles from the bag) produces one bootstrap sample. For example, we can start by simulating 5 tapper-listener pairs by sampling 5 marbles from the bag of 3 red and 117 white marbles. W W W R W Wrong Wrong Wrong Correct Wrong After selecting 120 marbles, we counted 2 red for \\(\\hat{p}_{bs} = 0.0167\\). As we did with the randomization technique, seeing what would happen with one simulation isn’t enough. In order to evaluate whether our originally observed proportion of 0.025 is unusual or not, we should generate more simulations. Here we’ve repeated the entire simulation ten times: \\[\\begin{align*} 0.0417 \\quad 0.0250 \\quad 0.0250 \\quad 0.0083 \\quad 0.0500 \\quad 0.0333 \\quad 0.0250 \\quad 0.000 \\quad 0.0083 \\quad 0.000 \\end{align*}\\] As before, we’ll run a total of 10,000 simulations using a computer. As seen in Figure 5.14, the range of 95% of the resampled \\(\\hat{p}_{bs}\\) is 0.000 to 0.0583. That is, we expect that between 0% and 5.83% of people are truly able to guess the tapper’s tune. Figure 5.14: The original listener-tapper data is bootstrapped 10,000 times. Each simulation creates a sample where the probability of being correct is \\(\\hat{p} = 3/120\\). The 2.5 percentile proportion is 0 and the 97.5 percentile is 0.0583. The result is that we are confident that, in the population, the true percent of people who can guess correctly is between 0% and 5.83%. Do the data provide statistically significant evidence against the claim that 50% of listeners can guess the tapper’s tune?40 5.2.3 Confidence intervals A point estimate provides a single plausible value for a parameter. However, a point estimate is rarely perfect; usually there is some error in the estimate. In addition to supplying a point estimate of a parameter, a next logical step would be to provide a plausible range of values for the parameter. Population parameter A plausible range of values for the population parameter is called a confidence interval. Using only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net. We can throw a spear where we saw a fish, but we will probably miss. On the other hand, if we toss a net in that area, we have a good chance of catching the fish. If we report a point estimate, we probably will not hit the exact population parameter. On the other hand, if we report a range of plausible values – a confidence interval – we have a good shot at capturing the parameter. If we want to be very certain we capture the population parameter, should we use a wider interval or a smaller interval?41 Bootstrap confidence interval As we saw above, a bootstrap sample is a sample of the original sample. In the case of the medical complications data, we proceed as follows: need a new physical object! Randomly sample one observation from the 62 patients (sample, without removing the marbles, 62 marbles from the bag). Randomly sample a second observation from the 62 patients. Because we sample with replacement (i.e., we don’t actually remove the marbles from the bag), there is a 1-in-62 chance that the second observation will be the same one sampled in the first step! … Randomly sample a 62nd observation from the 62 patients. Bootstrap sampling is often called sampling with replacement. A bootstrap sample behaves similarly to how an actual sample would behave, and we compute the point estimate of interest (here, compute \\(\\hat{p}_{bs}\\)). Due to theory that is beyond this text, we know that the bootstrap proportions \\(\\hat{p}_{bs}\\) vary around \\(\\hat{p}\\) in a similar way to how different sample (i.e., different datasets which would produce different \\(\\hat{p}\\) values) proportions vary around the true parameter \\(p\\). Therefore, an interval estimate for \\(p\\) can be produced using the \\(\\hat{p}_{bs}\\) values themselves. 95% Bootstrap confidence interval for a parameter \\(p\\). The 95% bootstrap confidence interval for the parameter \\(p\\) can be obtained directly using the ordered values \\(\\hat{p}_{bs}\\) values. Consider the sorted \\(\\hat{p}_{bs}\\) values, and let \\(\\hat{p}_{bs, 0.025}\\) be the 2.5% value and \\(\\hat{p}_{bs, 0.975}\\) be the 97.5% value. The 95% confidence interval is given by: (\\(\\hat{p}_{bs, 0.025}\\), \\(\\hat{p}_{bs, 0.975}\\)) In Section 6.1.1 we will discuss different percentages for the confidence interval (e.g., 90% confidence interval or 99% confidence interval). Section 6.1.1 also provides a longer discussion on what “95% confidence” actually means. 5.2.4 Bootstrap summary Bootstrap flow chart Figure 5.15: (not sure if this should include a histogram or not) the infinite pop compared to with replacement Table 5.7: Summary of bootstrapping as an inferential statistical method. Bootstrapping What does it do? Resamples (with replacement) from the observed data to mimic the sampling variability found by collecting data. What is the random process described? random sampling Is there flexibility? Yes, can be used to describe random allocation in an experiment What is it best for? Confidence Intervals (HT for one proportion covered in Chapter 6). What physical object represents the simulation process? pulling balls from a bag 5.3 Mathematical model 5.3.1 Central Limit Theorem We’ve encountered four case studies so far this chapter. While they differ in the settings, in their outcomes, and also in the technique we’ve used to analyze the data, they all have something in common: the general shape of the null distribution. Null distribution Figure 5.16 shows the null distributions in each of the four case studies where we ran 10,000 simulations. In the case of the opportunity cost study, which originally had just 1,000 simulations, we’ve included an additional 9,000 simulations. “The null distribution for each of the four case studies presented in Sections -.” Figure 5.16: The null distribution for each of the four case studies presented previously in this Chapter. Describe the shape of the distributions and note anything that you find interesting.42 As we observed in Chapter 1, it’s common for distributions to be skewed or contain outliers. However, the null distributions we’ve so far encountered have all looked somewhat similar and, for the most part, symmetric. They all resemble a bell-shaped curve. This is not a coincidence, but rather, is guaranteed by mathematical theory. Central Limit Theorem for proportions. If we look at a proportion (or difference in proportions) and the scenario satisfies certain conditions, then the sample proportion (or difference in proportions) will appear to follow a bell-shaped curve called the normal distribution. An example of a perfect normal distribution is shown in Figure 5.17. Imagine laying a normal curve over each of the four null distributions in Figure 5.16. While the mean (center) and standard deviation (width or spread) may change for each plot, the general shape remains roughly intact. Figure 5.17: A normal curve. Mathematical theory guarantees that if repeated samples are taken a sample proportion or a difference in sample proportions will follow something that resembles a normal distribution when certain conditions are met. (Note: we typically only take one sample, but the mathematical model lets us know what to expect if we had taken repeated samples.) These conditions fall into two categories: Observations in the sample are independent. Independence is guaranteed when we take a random sample from a population. It can also be guaranteed if we randomly divide individuals into treatment and control groups. The sample is large enough. The sample size cannot be too small. What qualifies as “small” differs from one context to the next, and we’ll provide suitable guidelines for proportions in Chapter 6. So far we’ve had no need for the normal distribution. We’ve been able to answer our questions somewhat easily using simulation techniques. However, soon this will change. Simulating data can be non-trivial. For example, some of the scenarios encountered in Chapters 3 and 4 would require complex simulations in order to make inferential conclusions. Instead, the normal distribution and other distributions like it offer a general framework for statistical inference that applies to a very large number of settings. Anticipating frequent use of the normal distribution Below we introduce three new settings where the normal distribution will be useful, and constructing suitable simulations can be difficult. The opportunity cost study determined that students are thriftier if they are reminded that saving money now means they can spend the money later. The study’s point estimate for the estimated impact was 20%, meaning 20% fewer students would move forward with a DVD purchase in the study scenario. However, as we’ve learned, point estimates aren’t perfect – they only provide an approximation of the truth. It would be useful if we could provide a range of plausible values for the impact, more formally known as a confidence interval. It is often difficult to construct a reliable confidence interval in many situations using simulations.43 However, doing so is reasonably straightforward using the normal distribution. Book prices were collected for 73 courses at UCLA in Spring 2010. Data were collected from both the UCLA Bookstore and Amazon. The differences in these prices are shown in Figure 5.18. The mean difference in the price of the books was $12.76, and we might wonder, does this provide strong evidence that the prices differ between the two book sellers? Here again we can apply the normal distribution, this time in the context of numerical data. We’ll explore this example and construct such a hypothesis test in Section 7.2. Figure 5.18: Histogram of the difference in price for each book sampled. These data are strongly skewed. Elmhurst College in Illinois released anonymized data for family income and financial support provided by the school for Elmhurst’s first-year students in 2011. Figure 5.19 shows a regression line fit to a scatterplot of a sample of the data. One question we will ask is, do the data show a real trend, or is the trend we observe reasonably explained by chance? In Chapter 3 we learned how to apply least squares regression to quantify the trend. In Chapter 8 we will focus on whether or not that trend can be explained by chance alone. For this case study, we could again use the normal distribution to help us answer this question. Figure 5.19: Gift aid and family income for a random sample of 50 first-year students from Elmhurst College, shown with a regression line. These examples highlight the value of the normal distribution approach. However, before we can apply the normal distribution to statistical inference, it is necessary to become familiar with the mechanics of the normal distribution. In Section 5.3.2 we discuss characteristics of the normal distribution, explore examples of data that follow a normal distribution, and learn a new plotting technique that is useful for evaluating whether a data set roughly follows the normal distribution. In Section 5.3.3, we apply the new knowledge in the context of hypothesis tests and confidence intervals. 5.3.2 Normal Distribution Among all the distributions we see in statistics, one is overwhelmingly the most common. The symmetric, unimodal, bell curve is ubiquitous throughout statistics. It is so common that people know it as a variety of names including the normal curve, normal model, or normal distribution.44 Under certain conditions, sample proportions, sample means, and sample differences can be modeled using the normal distribution. Additionally, some variables such as SAT scores and heights of US adult males closely follow the normal distribution. Normal distribution facts. Many summary statistics and variables are nearly normal, but none are exactly normal. Thus the normal distribution, while not perfect for any single problem, is very useful for a variety of problems. We will use it in data exploration and to solve important problems in statistics. In this section, we will discuss the normal distribution in the context of data to become familiar with normal distribution techniques. In the following sections and beyond, we’ll move our discussion to focus on applying the normal distribution and other related distributions to model point estimates for hypothesis tests and for constructing confidence intervals. Normal distribution model The normal distribution always describes a symmetric, unimodal, bell-shaped curve. However, normal curves can look different depending on the details of the model. Specifically, the normal model can be adjusted using two parameters: mean and standard deviation. As you can probably guess, changing the mean shifts the bell curve to the left or right, while changing the standard deviation stretches or constricts the curve. Figure 5.20 shows the normal distribution with mean \\(0\\) and standard deviation \\(1\\) (which is commonly referred to as the standard normal distribution) on top. A normal distributions with mean \\(19\\) and standard deviation \\(4\\) is shown on the bottom. Figure 5.21 shows the same two normal distributions on the same axis. Figure 5.20: Both curves represent the normal distribution, however, they differ in their center and spread. The normal distribution with mean 0 and standard deviation 1 is called the standard normal distribution. still can’t figure out references The normal models shown in Figure but plotted together and on the same scale. Figure 5.21: The two normal models shown above and now plotted together on the same scale. If a normal distribution has mean \\(\\mu\\) and standard deviation \\(\\sigma\\), we may write the distribution as \\(N(\\mu, \\sigma)\\). The two distributions in Figure 5.21 can be written as \\[\\begin{align*} N(\\mu=0,\\sigma=1)\\quad\\text{and}\\quad N(\\mu=19,\\sigma=4) \\end{align*}\\] Because the mean and standard deviation describe a normal distribution exactly, they are called the distribution’s parameters. Write down the short-hand for a normal distribution with (a) mean 5 and standard deviation 3, (b) mean -100 and standard deviation 10, and (c) mean 2 and standard deviation 9.45 Standardizing with Z scores Table 5.8 shows the mean and standard deviation for total scores on the SAT and ACT. The distribution of SAT and ACT scores are both nearly normal. Suppose Ann scored 1800 on her SAT and Tom scored 24 on his ACT. Who performed better?46 Table 5.8: Mean and standard deviation for the SAT and ACT. SAT ACT Mean 1500 21 SD 300 5 Figure 5.22: Ann’s and Tom’s scores shown with the distributions of SAT and ACT scores. The solution to the previous example relies on a standardization technique called a Z score, a method most commonly employed for nearly normal observations (but that may be used with any distribution). The Z score of an observation is defined as the number of standard deviations it falls above or below the mean. If the observation is one standard deviation above the mean, its Z score is 1. If it is 1.5 standard deviations below the mean, then its Z score is -1.5. If \\(x\\) is an observation from a distribution \\(N(\\mu, \\sigma)\\), we define the Z score mathematically as \\[\\begin{eqnarray*} Z = \\frac{x-\\mu}{\\sigma} \\end{eqnarray*}\\] Using \\(\\mu_{SAT}=1500\\), \\(\\sigma_{SAT}=300\\), and \\(x_{Ann}=1800\\), we find Ann’s Z score: \\[\\begin{eqnarray*} Z_{Ann} = \\frac{x_{Ann} - \\mu_{SAT}}{\\sigma_{SAT}} = \\frac{1800-1500}{300} = 1 \\end{eqnarray*}\\] The Z score. The Z score of an observation is the number of standard deviations it falls above or below the mean. We compute the Z score for an observation \\(x\\) that follows a distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) using \\[\\begin{eqnarray*} Z = \\frac{x-\\mu}{\\sigma} \\end{eqnarray*}\\] Use Tom’s ACT score, 24, along with the ACT mean and standard deviation to compute his Z score.47 Observations above the mean always have positive Z scores while those below the mean have negative Z scores. If an observation is equal to the mean (e.g., SAT score of 1500), then the Z score is \\(0\\). Let \\(X\\) represent a random variable from \\(N(\\mu=3, \\sigma=2)\\), and suppose we observe \\(x=5.19\\). (a) Find the Z score of \\(x\\). (b) Use the Z score to determine how many standard deviations above or below the mean \\(x\\) falls.48 Head lengths of brushtail possums follow a nearly normal distribution with mean 92.6 mm and standard deviation 3.6 mm. Compute the Z scores for possums with head lengths of 95.4 mm and 85.8 mm.49 We can use Z scores to roughly identify which observations are more unusual than others. One observation \\(x_1\\) is said to be more unusual than another observation \\(x_2\\) if the absolute value of its Z score is larger than the absolute value of the other observation’s Z score: \\(|Z_1| &gt; |Z_2|\\). This technique is especially insightful when a distribution is symmetric. Which of the two brushtail possum observations in the previous guided practice is more unusual?50 Normal probability calculations Ann from the SAT Guided Practice earned a score of 1800 on her SAT with a corresponding \\(Z=1\\). She would like to know what percentile she falls in among all SAT test-takers. Ann’s percentile is the percentage of people who earned a lower SAT score than Ann. We shade the area representing those individuals in Figure 5.23. The total area under the normal curve is always equal to 1, and the proportion of people who scored below Ann on the SAT is equal to the area shaded in Figure 5.23: 0.8413. In other words, Ann is in the \\(84^{th}\\) percentile of SAT takers. Figure 5.23: The normal model for SAT scores, shading the area of those individuals who scored below Ann. We can use the normal model to find percentiles or probabilities. A normal probability table, which lists Z scores and corresponding percentiles, can be used to identify a percentile based on the Z score (and vice versa). Statistical software can also be used. Normal probabilities are most commonly found using statistical software which we will show here using R. We use the software to identify the percentile corresponding to any particular Z score. For instance, the percentile of \\(Z=0.43\\) is 0.6664, or the \\(66.64^{th}\\) percentile. The pnorm() function is available in default R and will provide the percentile associated with any cutoff on a normal curve. The normTail() function is available in the openintro R package and will draw the associated curve if it is helpful. pnorm(0.43, m = 0, s = 1) #&gt; [1] 0.666 openintro::normTail(0.43, m = 0, s = 1) We can also find the Z score associated with a percentile. For example, to identify Z for the \\(80^{th}\\) percentile, we use qnorm() which identifies the quantile for a given percentage. The quantile represents the cutoff value. (To remember the function qnorm() as providing a cutoff, notice that both qnorm() and “cutoff” start with the sound “kuh”. To remember the pnorm() function as providing a probability from a given cutoff, notice that both pnorm() and probability start with the sound “puh”.) We determine the Z score for the \\(80^{th}\\) percentile using qnorm(): 0.84. qnorm(0.80, m = 0, s = 1) #&gt; [1] 0.842 openintro::normTail(0.84162, m = 0, s = 1) Determine the proportion of SAT test takers who scored better than Ann on the SAT.51 Normal probability examples Cumulative SAT scores are approximated well by a normal model, \\(N(\\mu=1500, \\sigma=300)\\). Shannon is a randomly selected SAT taker, and nothing is known about Shannon’s SAT aptitude. What is the probability that Shannon scores at least 1630 on her SATs? First, always draw and label a picture of the normal distribution. (Drawings need not be exact to be useful.) We are interested in the chance she scores above 1630, so we shade the upper tail. See the normal curve below. The picture shows the mean and the values at 2 standard deviations above and below the mean. The simplest way to find the shaded area under the curve makes use of the Z score of the cutoff value. With \\(\\mu=1500\\), \\(\\sigma=300\\), and the cutoff value \\(x=1630\\), the Z score is computed as \\[\\begin{eqnarray*} Z = \\frac{x - \\mu}{\\sigma} = \\frac{1630 - 1500}{300} = \\frac{130}{300} = 0.43 \\end{eqnarray*}\\] We use software to find the percentile of \\(Z=0.43\\), which yields 0.6664. However, the percentile describes those who had a Z score lower than 0.43. To find the area above \\(Z=0.43\\), we compute one minus the area of the lower tail, as seen below. The probability Shannon scores at least 1630 on the SAT is 0.3336. always draw a picture first, and find the Z score second. For any normal probability situation, always always always draw and label the normal curve and shade the area of interest first. The picture will provide an estimate of the probability. After drawing a figure to represent the situation, identify the Z score for the observation of interest. If the probability of Shannon scoring at least 1630 is 0.3336, then what is the probability she scores less than 1630? Draw the normal curve representing this exercise, shading the lower region instead of the upper one.52 the example and guided practice below both have figures in them. How do we put figures in footnotes? Edward earned a 1400 on his SAT. What is his percentile? First, a picture is needed. Edward’s percentile is the proportion of people who do not get as high as a 1400. These are the scores to the left of 1400. Identifying the mean \\(\\mu=1500\\), the standard deviation \\(\\sigma=300\\), and the cutoff for the tail area \\(x=1400\\) makes it easy to compute the Z score: \\[\\begin{eqnarray*} Z = \\frac{x - \\mu}{\\sigma} = \\frac{1400 - 1500}{300} = -0.33 \\end{eqnarray*}\\] Using the normal probability table, identify the row of \\(-0.3\\) and column of \\(0.03\\), which corresponds to the probability \\(0.3707\\). Edward is at the \\(37^{th}\\) percentile. Use the results of the previous example to compute the proportion of SAT takers who did better than Edward. Also draw a new picture.53 areas to the right. The normal probability table in most books gives the area to the left. If you would like the area to the right, first find the area to the left and then subtract this amount from one. Stuart earned an SAT score of 2100. Draw a picture for each part. (a) What is his percentile? (b) What percent of SAT takers did better than Stuart?54 Based on a sample of 100 men,55 the heights of male adults between the ages 20 and 62 in the US is nearly normal with mean 70.0’’ and standard deviation 3.3’’. Mike is 5’7’’ and Jim is 6’4’’. (a) What is Mike’s height percentile? (b) What is Jim’s height percentile? Also draw one picture for each part.56 The last several problems have focused on finding the probability or percentile for a particular observation. What if you would like to know the observation corresponding to a particular percentile? Erik’s height is at the \\(40^{th}\\) percentile. How tall is he? As always, first draw the picture (see below). In this case, the lower tail probability is known (0.40), which can be shaded on the diagram. We want to find the observation that corresponds to this value. As a first step in this direction, we determine the Z score associated with the \\(40^{th}\\) percentile. Because the percentile is below 50%, we know \\(Z\\) will be negative. Looking in the negative part of the normal probability table, we search for the probability inside the table closest to 0.4000. We find that 0.4000 falls in row \\(-0.2\\) and between columns \\(0.05\\) and \\(0.06\\). Since it falls closer to \\(0.05\\), we take this one: \\(Z=-0.25\\). Knowing \\(Z_{Erik}=-0.25\\) and the population parameters \\(\\mu=70\\) and \\(\\sigma=3.3\\) inches, the Z score formula can be set up to determine Erik’s unknown height, labeled \\(x_{Erik}\\): \\[\\begin{eqnarray*} -0.25 = Z_{Erik} = \\frac{x_{Erik} - \\mu}{\\sigma} = \\frac{x_{Erik} - 70}{3.3} \\end{eqnarray*}\\] Solving for \\(x_{Erik}\\) yields the height 69.18 inches. That is, Erik is about 5’9’’ (this is notation for 5-feet, 9-inches). qnorm(0.4, m = 0, s = 1) #&gt; [1] -0.253 What is the adult male height at the \\(82^{nd}\\) percentile? Again, we draw the figure first (see below). Next, we want to find the Z score at the \\(82^{nd}\\) percentile, which will be a positive value. Using qnorm(), the \\(82^{nd}\\) percentile corresponds to \\(Z=0.92\\). Finally, the height \\(x\\) is found using the Z score formula with the known mean \\(\\mu\\), standard deviation \\(\\sigma\\), and Z score \\(Z=0.92\\): \\[\\begin{eqnarray*} 0.92 = Z = \\frac{x-\\mu}{\\sigma} = \\frac{x - 70}{3.3} \\end{eqnarray*}\\] This yields 73.04 inches or about 6’1’’ as the height at the \\(82^{nd}\\) percentile. qnorm(0.82, m = 0, s = 1) #&gt; [1] 0.915 What is the \\(95^{th}\\) percentile for SAT scores? What is the \\(97.5^{th}\\) percentile of the male heights? As always with normal probability problems, first draw a picture.57 What is the probability that a randomly selected male adult is at least 6’2’’ (74 inches)? What is the probability that a male adult is shorter than 5’9’’ (69 inches)?58 What is the probability that a random adult male is between 5’9’’ and 6’2’’? These heights correspond to 69 inches and 74 inches. First, draw the figure. The area of interest is no longer an upper or lower tail. The total area under the curve is 1. If we find the area of the two tails that are not shaded (from the previous Guided Practice, these areas are \\(0.3821\\) and \\(0.1131\\)), then we can find the middle area: That is, the probability of being between 5’9’’ and 6’2’’ is 0.5048. What percent of SAT takers get between 1500 and 2000?59 What percent of adult males are between 5’5’’ and 5’7’’?60 68-95-99.7 rule Here, we present a useful general rule for the probability of falling within 1, 2, and 3 standard deviations of the mean in the normal distribution. The rule will be useful in a wide range of practical settings, especially when trying to make a quick estimate without a calculator or Z table. Figure 5.24: Probabilities for falling within 1, 2, and 3 standard deviations of the mean in a normal distribution. Use pnorm() (or a Z table) to confirm that about 68%, 95%, and 99.7% of observations fall within 1, 2, and 3, standard deviations of the mean in the normal distribution, respectively. For instance, first find the area that falls between \\(Z=-1\\) and \\(Z=1\\), which should have an area of about 0.68. Similarly there should be an area of about 0.95 between \\(Z=-2\\) and \\(Z=2\\).61 It is possible for a normal random variable to fall 4, 5, or even more standard deviations from the mean. However, these occurrences are very rare if the data are nearly normal. The probability of being further than 4 standard deviations from the mean is about 1-in-30,000. For 5 and 6 standard deviations, it is about 1-in-3.5 million and 1-in-1 billion, respectively. SAT scores closely follow the normal model with mean \\(\\mu = 1500\\) and standard deviation \\(\\sigma = 300\\). (a) About what percent of test takers score 900 to 2100? (b) What percent score between 1500 and 2100?62 5.3.3 Hypothesis testing case studies The approach for using the normal model in the context of inference is very similar to the practice of applying the model to individual observations that are nearly normal. We will replace null distributions we previously obtained using the randomization or simulation techniques and verify the results once again using the normal model. When the sample size is sufficiently large, the normal approximation generally provides us with the same conclusions as the simulation model. Standard error Point estimates vary from sample to sample, and we quantify this variability with what is called the standard error (SE). The standard error is equal to the standard deviation associated with the estimate. So, for example, if we used the standard deviation to quantify the variability of a point estimate from one sample to the next, this standard deviation would be called the standard error of the point estimate. The way we determine the standard error varies from one situation to the next. However, typically it is determined using a formula based on the Central Limit Theorem. Opportunity cost Observed data In Section 5.1.2 we were introduced to the opportunity cost study, which found that students became thriftier when they were reminded that not spending money now means the money can be spent on other things in the future. Let’s re-analyze the data in the context of the normal distribution and compare the results. Variability of the statistic Figure 5.25 summarizes the null distribution as determined using the randomization method. The best fitting normal distribution for the null distribution has a mean of 0. We can calculate the standard error of this distribution by borrowing a formula that we will become familiar with in Section 6.2, but for now let’s just take the value \\(SE = 0.078\\) as a given. Recall that the point estimate of the difference was 0.20, as shown in the plot. Next, we’ll use the normal distribution approach to compute the two-tailed p-value. Figure 5.25: Null distribution of differences with an overlaid normal curve for the opportunity cost study. 10,000 simulations were run for this figure. Observed statistic vs. null statistics As we learned in Section 5.3.2, it is helpful to draw and shade a picture of the normal distribution so we know precisely what we want to calculate. Here we want to find the area of the tail beyond 0.2, representing the p-value. Next, we can calculate the Z score using the observed difference, 0.20, and the two model parameters. The standard error, \\(SE = 0.078\\), is the equivalent of the model’s standard deviation. \\[\\begin{align*} Z = \\frac{\\text{observed difference} - 0}{SE} = \\frac{0.20 - 0}{0.078} = 2.56 \\end{align*}\\] We can either use statistical software or look up \\(Z = 2.56\\) in the normal probability table to determine the right tail area: 0.0052, which is about the same as what we got for the right tail using the randomization approach (0.006). Using this area as the p-value, we see that the p-value is less than 0.05, we conclude that the treatment did indeed impact students’ spending. Z score in a hypothesis test. In the context of a hypothesis test, the Z score for a point estimate is \\[\\begin{align*} Z = \\frac{\\text{point estimate} - \\text{null value}}{SE} \\end{align*}\\] The standard error in this case is the equivalent of the standard deviation of the point estimate, and the null value comes from the null hypothesis. We have confirmed that the randomization approach we used earlier and the normal distribution approach provide almost identical p-values and conclusions in the opportunity cost case study. Next, let’s turn our attention to the medical consultant case study. Medical consultant Observed data In Section 5.2.1 we learned about a medical consultant who reported that only 3 of her 62 clients who underwent a liver transplant had complications, which is less than the more common complication rate of 0.10. In that work, we did not model a null scenario, but we will discuss a simulation method for a one proportion null distribution in Section 6.1.1, such a distribution is provided in Figure 5.26. We have added the best-fitting normal curve to the figure, which has a mean of 0.10. Borrowing a formula that we’ll encounter in Chapter 6, the standard error of this distribution was also computed: \\(SE = 0.038\\). Variability of the statistic Before we begin, we want to point out a simple detail that is easy to overlook: the null distribution we generated from the simulation is slightly skewed, and the histogram is not particularly smooth. In fact, the normal distribution only sort-of fits this model. Figure 5.26: The null distribution for the sample proportion, created from 10,000 simulated studies, along with the best-fitting normal model. Observed statistic vs. null statistics As always, we’ll draw a picture before finding the normal probabilities. Below is a normal distribution centered at 0.10 with a standard error of 0.038. Next, we can calculate the Z score using the observed complication rate, \\(\\hat{p} = 0.048\\) along with the mean and standard deviation of the normal model. Here again, we use the standard error for the standard deviation. \\[\\begin{align*} Z = \\frac{\\hat{p} - p_0}{SE_{\\hat{p}}} = \\frac{0.048 - 0.10}{0.038} = -1.37 \\end{align*}\\] Identifying \\(Z = -1.37\\) using statistical software or in the normal probability table, we can determine that the left tail area is 0.0853 which is the estimated p-value for the hypothesis test. There is a small problem: the p-value of 0.0853 is slightly different from the simulation p-value or 0.1222 which will be calculated in Section 6.1.1. The discrepancy is explained by normal model’s poor representation of the null distribution in Figure 5.26. As noted earlier, the null distribution from the simulations is not very smooth, and the distribution itself is slightly skewed. That’s the bad news. The good news is that we can foresee these problems using some simple checks. We’ll learn about these checks in the following chapters. In Section 5.3.1 we noted that the two common requirements to apply the Central Limit Theorem are (1) the observations in the sample must be independent, and (2) the sample must be sufficiently large. The guidelines for this particular situation – which we will learn in Section 6.1 – would have alerted us that the normal model was a poor approximation. Conditions for applying the normal model The success story in this section was the application of the normal model in the context of the opportunity cost data. However, the biggest lesson comes from our failed attempt to use the normal approximation in the medical consultant case study. Statistical techniques are like a carpenter’s tools. When used responsibly, they can produce amazing and precise results. However, if the tools are applied irresponsibly or under inappropriate conditions, they will produce unreliable results. For this reason, with every statistical method that we introduce in future chapters, we will carefully outline conditions when the method can reasonably be used. These conditions should be checked in each application of the technique. 5.3.4 Confidence interval case study A point estimate is our best guess for the value of the parameter, so it makes sense to build the confidence interval around that value. The standard error, which is a measure of the uncertainty associated with the point estimate, provides a guide for how large we should make the confidence interval. The 68-95-99.7 rule tells us that, in general, 95% of observations are within 2 standard errors of the mean. Here, we use the value 1.96 to be slightly more precise. Constructing a 95% confidence interval. When the sampling distribution of a point estimate can reasonably be modeled as normal, the point estimate we observe will be within 1.96 standard errors of the true value of interest about 95% of the time. Thus, a 95% confidence interval for such a point estimate can be constructed: \\[\\begin{align} \\text{point estimate}\\ \\pm\\ 1.96 \\times SE \\label{95PercentConfidenceIntervalFormula} \\end{align}\\] We can be 95% confident this interval captures the true value. \\index{95% confidence interval} Compute the area between -1.96 and 1.96 for a normal distribution with mean 0 and standard deviation 1.63 The point estimate from the opportunity cost study was that 20% fewer students would buy a DVD if they were reminded that money not spent now could be spent later on something else. The point estimate from this study can reasonably be modeled with a normal distribution, and a proper standard error for this point estimate is \\(SE = 0.078\\). Construct a 95% confidence interval.64 Since the conditions for the normal approximation have already been verified, we can move forward with the construction of the 95% confidence interval: \\[\\begin{align*} \\text{point estimate}\\ \\pm\\ 1.96 \\times SE \\quad \\rightarrow \\quad 0.20\\ \\pm\\ 1.96 \\times 0.078 \\quad \\rightarrow \\quad (0.047, 0.353) \\end{align*}\\] We are 95% confident that the DVD purchase rate resulting from the treatment is between 4.7% and 35.3% lower than in the control group. Since this confidence interval does not contain 0, it is consistent with our earlier result where we rejected the notion of “no difference” using a hypothesis test. Stents Observed data Consider an experiment that examined whether implanting a stent in the brain of a patient at risk for a stroke helps reduce the risk of a stroke. The results from the first 30 days of this study, which included 451 patients, are summarized in Table 5.9. These results are surprising! The point estimate suggests that patients who received stents may have a higher risk of stroke: \\(p_{trmt} - p_{ctrl} = 0.090\\). Table 5.9: Descriptive statistics for 30-day results for the stent study. stroke no event Total treatment 33 191 224 control 13 214 227 Total 46 405 451 Variability of the statistic Consider the stent study and results. The conditions necessary to ensure the point estimate \\(p_{trmt} - p_{ctrl} = 0.090\\) is nearly normal have been verified for you, and the estimate’s standard error is \\(SE = 0.028\\). Construct a 95% confidence interval for the change in 30-day stroke rates from usage of the stent. The conditions for applying the normal model have already been verified, so we can proceed to the construction of the confidence interval: \\[\\begin{align*} \\text{point estimate}\\ \\pm\\ 1.96 \\times SE \\quad \\rightarrow \\quad 0.090\\ \\pm\\ 1.96 \\times 0.028 \\quad \\rightarrow \\quad (0.035, 0.145) \\end{align*}\\] We are 95% confident that implanting a stent in a stroke patient’s brain increased the risk of stroke within 30 days by a rate of 0.035 to 0.145. This confidence interval can also be used in a way analogous to a hypothesis test: since the interval does not contain 0, it means the data provide statistically significant evidence that the stent used in the study increases the risk of As with hypothesis tests, confidence intervals are imperfect. About 1-in-20 properly constructed 95% confidence intervals will fail to capture the parameter of interest, simply due to natural variability in the observed data. Figure 5.27 shows 25 confidence intervals for a proportion that were constructed from simulations where the true proportion was \\(p = 0.3\\). However, 1 of these 25 confidence intervals happened not to include the true value. The interval which does not capture \\(p=0.3\\) is not due to bad science. Instead, it is due to natural variability, and we should expect some of our intervals to miss the parameter of interest. Indeed, over a lifetime of creating 95% intervals, you should expect 5% of your reported intervals to miss the parameter of interest (unfortunately, you will not ever know which of your reported intervals captured the parameter and which missed the parameter). Figure 5.27: Twenty-five samples of size \\(n=300\\) were simulated when \\(p = 0.30\\). For each sample, a confidence interval was created to try to capture the true proportion \\(p\\). However, 1 of these 25 intervals did not capture \\(p = 0.30\\). In Figure 5.27, one interval does not contain the true proportion, \\(p = 0.3\\). Does this imply that there was a problem with the simulations run?65 Interpreting confidence intervals A careful eye might have observed the somewhat awkward language used to describe confidence intervals. Correct interpretation: We are XX% confident that the population parameter is between… Incorrect language might try to describe the confidence interval as capturing the population parameter with a certain probability. This is one of the most common errors: while it might be useful to think of it as a probability, the confidence level only quantifies how plausible it is that the parameter is in the interval. Another especially important consideration of confidence intervals is that they only try to capture the population parameter. Our intervals say nothing about the confidence of capturing individual observations, a proportion of the observations, or about capturing point estimates. Confidence intervals provide an interval estimate for and attempt to capture population parameters. 5.3.5 Mathematical model summary Math flow chart Table 5.10: Summary Mathematical Models as inferential statistical methods. Mathematical Model What does it do? Uses theory (primarily the Central Limit Theorem) to describe the hypothetical variability resulting from either repeated randomized experiments or random samples. What is the random process described? either / both Is there flexibility? Yes What is it best for? Quick analyses through, for example, calculating a Z score. What physical object represents the simulation process? NA 5.4 Chapter 5 review In Chapter 5, we have provided three different methods for statistical inference. We will continue to build on the methods throughout the text, and by the end, you should have an understanding of their similarities and differences. Meanwhile, it is important to note that the methods are designed to mimic variability with data, and we know that variability can come from different sources (e.g., random sampling vs. random allocation). In Table 5.11, we have summarized some of the ways the inferential procedures feature specific sources of variability. We hope that you refer back to the table often as you dive more deeply into the ideas in future chapters. Table 5.11: Summary and comparison of Randomization Tests, Bootstrapping, and Mathematical Models as inferential statistical methods. Randomization Test Bootstrapping Mathematical Model What does it do? Shuffles the explanatory variable to mimic the natural variability found in a randomized experiment. Resamples (with replacement) from the observed data to mimic the sampling variability found by collecting data. Uses theory (primarily the Central Limit Theorem) to describe the hypothetical variability resulting from either repeated randomized experiments or random samples. What is the random process described? randomized experiment random sampling either / both Is there flexibility? Yes, can be used to describe random sampling in an observational model Yes, can be used to describe random allocation in an experiment Yes What is it best for? Hypothesis Testing (can be used for Confidence Intervals, but not covered in this text). Confidence Intervals (HT for one proportion covered in Chapter 6). Quick analyses through, for example, calculating a Z score. What physical object represents the simulation process? shuffling cards pulling balls from a bag NA Figure 5.28: Analysis conclusions should be made carefully according to how the data were collected. Note that very few datasets come from the top left box because usually ethics require that randomly allocated treatments can only be given to volunteers. 5.4.1 Terms We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as bolded text. 95% confidence interval left skewed percentile standard error 95% confident normal curve permutation test standard normal distribution alternative hypothesis normal distribution point estimate statistic bootstrap sample normal model quantile-quantile plot statistically significant bootstrapping normal probability plot randomization success Central Limit Theorem normal probability table randomization test test statistic confidence interval null hypothesis right skewed Z score hypothesis test p-value sampling with replacement independent parameter simulation We would be assuming that these two variables are independent.↩︎ Rosen B and Jerdee T. 1974. “Influence of sex role stereotypes on personnel decisions.” Journal of Applied Psychology 59(1):9-14.↩︎ The study is an experiment, as subjects were randomly assigned a “male” file or a “female” file (remember, all the files were actually identical in content). Since this is an experiment, the results can be used to evaluate a causal relationship between gender of a candidate and the promotion decision.↩︎ The test procedure we employ in this section is sometimes referred to as a permutation test.↩︎ \\(18/24 - 17/24=0.042\\) or about 4.2% in favor of the men. This difference due to chance is much smaller than the difference observed in the actual groups.↩︎ This reasoning does not generally extend to anecdotal observations. Each of us observes incredibly rare events every day, events we could not possibly hope to predict. However, in the non-rigorous setting of anecdotal evidence, almost anything may appear to be a rare event, so the idea of looking for rare events in day-to-day activities is treacherous. For example, we might look at the lottery: there was only a 1 in 176 million chance that the Mega Millions numbers for the largest jackpot in history (March 30, 2012) would be (2, 4, 23, 38, 46) with a Mega ball of (23), but nonetheless those numbers came up! However, no matter what numbers had turned up, they would have had the same incredibly rare odds. That is, any set of numbers we could have observed would ultimately be incredibly rare. This type of situation is typical of our daily lives: each possible event in itself seems incredibly rare, but if we consider every alternative, those outcomes are also incredibly rare. We should be cautious not to misinterpret such anecdotal evidence.↩︎ Frederick S, Novemsky N, Wang J, Dhar R, Nowlis S. 2009. Opportunity Cost Neglect. Journal of Consumer Research 36: 553-561.↩︎ Success is often defined in a study as the outcome of interest, and a “success” may or may not actually be a positive outcome. For example, researchers working on a study on HIV prevalence might define a “success” in the statistical sense as a patient who is HIV+. A more complete discussion of the term success will be given in Chapter 6.↩︎ About 5% of the patients (6.2 on average) in the simulation will have a complication, though we will see a little variation from one simulation to the next.↩︎ This case study is described in Made to Stick by Chip and Dan Heath. Little known fact: the teaching principles behind many OpenIntro resources are based on Made to Stick.↩︎ Because 50% is not in the interval estimate for the true parameter, there is statistically significant evidence. Indeed, the data provide strong evidence that the chance a listener will guess the correct tune is less than 50%.↩︎ If we want to be more certain we will capture the fish, we might use a wider net. Likewise, we use a wider confidence interval if we want to be more certain that we capture the parameter.↩︎ In general, the distributions are reasonably symmetric. The case study for the medical consultant is the only distribution with any evident skew.↩︎ The percentile bootstrap method has been introduced in Section 5.2.3. We will revisit bootstrap intervals in Section 6.2.2.↩︎ It is also introduced as the Gaussian distribution after Frederic Gauss, the first person to formalize its mathematical expression.↩︎ (a) \\(N(\\mu=5,\\sigma=3)\\). (b) \\(N(\\mu=-100, \\sigma=10)\\). (c) \\(N(\\mu=2, \\sigma=9)\\).↩︎ We use the standard deviation as a guide. Ann is 1 standard deviation above average on the SAT: \\(1500 + 300=1800\\). Tom is 0.6 standard deviations above the mean on the ACT: \\(21+0.6\\times 5=24\\). In Figure 5.22, we can see that Ann tends to do better with respect to everyone else than Tom did, so her score was better.↩︎ \\(Z_{Tom} = \\frac{x_{Tom} - \\mu_{ACT}}{\\sigma_{ACT}} = \\frac{24 - 21}{5} = 0.6\\)↩︎ (a) Its Z score is given by \\(Z = \\frac{x-\\mu}{\\sigma} = \\frac{5.19 - 3}{2} = 2.19/2 = 1.095\\). (b) The observation \\(x\\) is 1.095 standard deviations above the mean. We know it must be above the mean since \\(Z\\) is positive.↩︎ For \\(x_1=95.4\\) mm: \\(Z_1 = \\frac{x_1 - \\mu}{\\sigma} = \\frac{95.4 - 92.6}{3.6} = 0.78\\). For \\(x_2=85.8\\) mm: \\(Z_2 = \\frac{85.8 - 92.6}{3.6} = -1.89\\).↩︎ Because the absolute value of Z score for the second observation is larger than that of the first, the second observation has a more unusual head length.↩︎ If 84% had lower scores than Ann, the number of people who had better scores must be 16%. (Generally ties are ignored when the normal model, or any other continuous distribution, is used.)↩︎ We found the probability to be 0.6664. A picture for this exercise is represented by the shaded area below “0.6664”.↩︎ If Edward did better than 37% of SAT takers, then about 63% must have done better than him. ↩︎ Numerical answers: (a) 0.9772. (b) 0.0228.↩︎ This sample was taken from the USDA Food Commodity Intake Database.↩︎ First put the heights into inches: 67 and 76 inches. Figures are shown below. (a) \\(Z_{Mike} = \\frac{67 - 70}{3.3} = -0.91\\ \\to\\ 0.1814\\). (b) \\(Z_{Jim} = \\frac{76 - 70}{3.3} = 1.82\\ \\to\\ 0.9656\\). \\↩︎ Remember: draw a picture first, then find the Z score. (We leave the pictures to you.) The Z score can be found by using the percentiles and the normal probability table. (a) We look for 0.95 in the probability portion (middle part) of the normal probability table, which leads us to row 1.6 and (about) column 0.05, i.e., \\(Z_{95}=1.65\\). Knowing \\(Z_{95}=1.65\\), \\(\\mu = 1500\\), and \\(\\sigma = 300\\), we setup the Z score formula: \\(1.65 = \\frac{x_{95} - 1500}{300}\\). We solve for \\(x_{95}\\): \\(x_{95} = 1995\\). (b) Similarly, we find \\(Z_{97.5} = 1.96\\), again setup the Z score formula for the heights, and calculate \\(x_{97.5} = 76.5\\).↩︎ Numerical answers: (a) 0.1131. (b) 0.3821.↩︎ This is an abbreviated solution. (Be sure to draw a figure!) First find the percent who get below 1500 and the percent that get above 2000: \\(Z_{1500} = 0.00 \\to 0.5000\\) (area below), \\(Z_{2000} = 1.67 \\to 0.0475\\) (area above). Final answer: \\(1.0000-0.5000 - 0.0475 = 0.4525\\).↩︎ 5’5’’ is 65 inches. 5’7’’ is 67 inches. Numerical solution: \\(1.000 - 0.0649 - 0.8183 = 0.1168\\), i.e., 11.68%.↩︎ First draw the pictures. To find the area between \\(Z=-1\\) and \\(Z=1\\), use pnorm() or the normal probability table to determine the areas below \\(Z=-1\\) and above \\(Z=1\\). Next verify the area between \\(Z=-1\\) and \\(Z=1\\) is about 0.68. Repeat this for \\(Z=-2\\) to \\(Z=2\\) and also for \\(Z=-3\\) to \\(Z=3\\).↩︎ (a) 900 and 2100 represent two standard deviations above and below the mean, which means about 95% of test takers will score between 900 and 2100. (b) Since the normal model is symmetric, then half of the test takers from part (a) (\\(\\frac{95\\%}{2} = 47.5\\%\\) of all test takers) will score 900 to 1500 while 47.5% score between 1500 and 2100.↩︎ We will leave it to you to draw a picture. The Z scores are \\(Z_{left} = -1.96\\) and \\(Z_{right} = 1.96\\). The area between these two Z scores is \\(0.9750 - 0.0250 = 0.9500\\). This is where “1.96” comes from in the 95% confidence interval formula.↩︎ We’ve used \\(SE = 0.078\\) from the last section. However, it would more generally be appropriate to recompute the \\(SE\\) slightly differently for this confidence interval using the technique introduced in Section 6.2. Don’t worry about this detail for now since the two resulting standard errors are, in this case, almost identical.↩︎ No. Just as some observations occur more than 1.96 standard deviations from the mean, some point estimates will be more than 1.96 standard errors from the parameter. A confidence interval only provides a plausible range of values for a parameter. While we might say other values are implausible based on the data, this does not mean they are impossible.↩︎ "],
["inference-cat.html", "Chapter 6 Inference for categorical data 6.1 One proportion 6.2 Difference of two proportions 6.3 Independence in two-way tables 6.4 Chapter 6 review", " Chapter 6 Inference for categorical data The content in this chapter is currently just placeholder. We will remove this banner once the chapter content has been updated and is ready for review. Focusing now on Statistical Inference for categorical data, we will revisit many of the foundational aspects of hypothesis testing from Chapter 5. The three data structures we detail are one binary variable, summarized using a single proportion; two binary variables, summarized using a difference of two proportions; and two categorical variables, summarized using a two-way table. When appropriate, each of the data structures will be analyzed using the three methods from Chapter 5: randomization test, bootstrapping, and mathematical models. As we build on the inferential ideas, we will visit new foundational concepts in statistical inference. For example, we will cover the conditions for when a normal model is appropriate; the two different error rates in hypothesis testing; and choosing the confidence level for a confidence interval. 6.1 One proportion We encountered inference methods for a single proportion in Chapter 5, exploring point estimates, confidence intervals, and hypothesis tests. In this section, we’ll do a review of these topics and also how to choose an appropriate sample size when collecting data for single proportion contexts. Note that there is only one variable being measured in a study which focuses on one proportion. For each observational unit, the single variable is measured as either a success or failure (e.g., “surgical complication” vs. “no surgical complication”). Because the nature of the research question at hand focuses on only a single variable, there is not a way to randomize the variable across a different (explanatory) variable. For this reason, we will not use randomization as an analysis tool when focusing on a single proportion. Instead, we will apply bootstrapping techniques to test a given hypothesis, and we will also revisit the associated mathematical models. 6.1.1 Bootstrap test for \\(H_0: p = p_0\\) The bootstrap simulation concept when \\(H_0\\) is true is similar to the ideas used in the case studies presented in Section 5.2 where we bootstrapped without an assumption about \\(H_0\\). Because we will be testing a hypothesized value of \\(p\\) (referred to as \\(p_0\\)), the bootstrap simulation for hypothesis testing has a fantastic advantage that it can be used for any sample size (a huge benefit for small samples, a nice alternative for large samples). We expand on the medical consultant example, see Section 5.2.1, where instead of finding an interval estimate for the true complication rate, we work to test a specific research claim. Observed data Recall the set-up for the example: People providing an organ for donation sometimes seek the help of a special “medical consultant”. These consultants assist the patient in all aspects of the surgery, with the goal of reducing the possibility of complications during the medical procedure and recovery. Patients might choose a consultant based in part on the historical complication rate of the consultant’s clients. One consultant tried to attract patients by noting the average complication rate for liver donor surgeries in the US is about 10%, but her clients have only had 3 complications in the 62 liver donor surgeries she has facilitated. She claims this is strong evidence that her work meaningfully contributes to reducing complications (and therefore she should be hired!). Using the data, is it possible to assess the consultant’s claim that her complication rate is less than 10%? No. The claim is that there is a causal connection, but the data are observational. Patients who hire this medical consultant may have lower complication rates for other reasons. While it is not possible to assess this causal claim, it is still possible to test for an association using these data. For this question we ask, could the low complication rate of \\(\\hat{p} = 0.048\\) be due to chance? Write out hypotheses in both plain and statistical language to test for the association between the consultant’s work and the true complication rate, \\(p\\), for the consultant’s clients.66 Because, as it turns out, the conditions of working with the normal distribution are not met (see Section 6.1.2), the uncertainty associated with the sample proportion should not be modeled using the normal distribution. However, we would still like to assess the hypotheses from the previous Guided Practice in absence of the normal framework. To do so, we need to evaluate the possibility of a sample value (\\(\\hat{p}\\)) as far below the null value, \\(p_0=0.10\\) as what was observed. The deviation of the sample value from the hypothesized parameter is usually quantified with a p-value. The p-value is computed based on the null distribution, which is the distribution of the test statistic if the null hypothesis is true. Supposing the null hypothesis is true, we can compute the p-value by identifying the chance of observing a test statistic that favors the alternative hypothesis at least as strongly as the observed test statistic. Here we will use a bootstrap simulation to measure the p-value. Variability of the statistic We want to identify the sampling distribution of the test statistic (\\(\\hat{p}\\)) if the null hypothesis was true. In other words, we want to see how the sample proportion changes due to chance alone. Then we plan to use this information to decide whether there is enough evidence to reject the null hypothesis. Under the null hypothesis, 10% of liver donors have complications during or after surgery. Suppose this rate was really no different for the consultant’s clients (for all the consultant’s clients, not just the 62 previously measured). If this was the case, we could simulate 62 clients to get a sample proportion for the complication rate from the null distribution. Simulating observations using a hypothesized null parameter value is often called a parametric bootstrap simulation. Similar to the process described in Section 5.2, each client can be simulated using a bag of marbles with 10% red marbles and 90% white marbles. Sampling a marble from the bag (with 10% red marbles) is one way of simulating whether a patient has a complication if the true complication rate is 10% for the data. If we select 62 marbles and then compute the proportion of patients with complications in the simulation, \\(\\hat{p}_{sim}\\), then the resulting sample proportion is exactly a sample from the null distribution. An undergraduate student was paid $2 to complete this simulation. There were 5 simulated cases with a complication and 57 simulated cases without a complication, i.e., \\(\\hat{p}_{sim} = 5/62 = 0.081\\). Is this one simulation enough to determine whether or not we should reject the null hypothesis? No. To assess the hypotheses, we need to see a distribution of many \\(\\hat{p}_{sim}\\), not just a single draw from this sampling distribution. Observed statistic vs. null statistics One simulation isn’t enough to get a sense of the null distribution; many simulation studies are needed. Roughly 10,000 seems sufficient. However, paying someone to simulate 10,000 studies by hand is a waste of time and money. Instead, simulations are typically programmed into a computer, which is much more efficient. Figure 6.1 shows the results of 10,000 simulated studies. The proportions that are equal to or less than \\(\\hat{p}=0.048\\) are shaded. The shaded areas represent sample proportions under the null distribution that provide at least as much evidence as \\(\\hat{p}\\) favoring the alternative hypothesis. There were 1222 simulated sample proportions with \\(\\hat{p}_{sim} \\leq 0.048\\). We use these to construct the null distribution’s left-tail area and find the p-value: \\[\\begin{align} \\text{left tail area }\\label{estOfPValueBasedOnSimulatedNullForSingleProportion} &amp;= \\frac{\\text{Number of observed simulations with }\\hat{p}_{sim}\\leq\\text{ 0.048}}{10000} \\end{align}\\] Of the 10,000 simulated \\(\\hat{p}_{sim}\\), 1222 were equal to or smaller than \\(\\hat{p}\\). Since the hypothesis test is one-sided, the estimated p-value is equal to this tail area: 0.1222. Figure 6.1: The null distribution for \\(\\hat{p}\\), created from 10,000 simulated studies. The left tail, representing the p-value for the hypothesis test, contains 12.22% of the simulations. Because the estimated p-value is 0.1222, which is larger than the significance level 0.05, we do not reject the null hypothesis. Explain what this means in plain language in the context of the problem.67 Does the conclusion in the previous Guided Practice imply there is no real association between the surgical consultant’s work and the risk of complications? Explain.68 Null distribution of \\(\\hat{p}\\) with bootstrap simulation Regardless of the statistial method chosen, the p-value is always derived by analyzing the null distribution of the test statistic. The normal model poorly approximates the null distribution for \\(\\hat{p}\\) when the success-failure condition is not satisfied. As a substitute, we can generate the null distribution using simulated sample proportions and use this distribution to compute the tail area, i.e., the p-value. In the previous Guided Practice, the p-value is estimated. It is not exact because the simulated null distribution itself is not exact, only a close approximation. An exact p-value can be generated using the binomial distribution, but that method will not be covered in this text. 6.1.2 Mathematical model Conditions In Section 5.3.2, we introduced the normal distribution and showed how it can be used as a mathematical model to describe the variability of a statistic. There are conditions under which a sample proportion \\(\\hat{p}\\) is well modeled using a normal distribution. When the sample observations are independent and the sample size is sufficiently large, the normal model will describe the variability quite well; when the observations violate the conditions, the normal model can be inaccurate Sampling distribution of \\(\\hat{p}\\) The sampling distribution for \\(\\hat{p}\\) based on a sample of size \\(n\\) from a population with a true proportion \\(p\\) is nearly normal when: The sample’s observations are independent, e.g., are from a simple random sample. We expected to see at least 10 successes and 10 failures in the sample, i.e., \\(np\\geq10\\) and \\(n(1-p)\\geq10\\). This is called the success-failure condition. When these conditions are met, then the sampling distribution of \\(\\hat{p}\\) is nearly normal with mean \\(p\\) and standard error of \\(\\hat{p}\\) as \\(SE = \\sqrt{\\frac{\\ p(1-p)\\ }{n}}\\). Typically we don’t know the true proportion \\(p\\), so we substitute some value to check conditions and estimate the standard error. For confidence intervals, the sample proportion \\(\\hat{p}\\) is used to check the success-failure condition and compute the standard error. For hypothesis tests, typically the null value – that is, the proportion claimed in the null hypothesis – is used in place of \\(p\\). The independence condition is a more nuanced requirement. When it isn’t met, it is important to understand how and why it isn’t met. For example, there exist no statistical methods available to truly correct the inherent biases of data from a convenience sample. On the other hand, if we took a cluster sample (see Section ??), the observations wouldn’t be independent, but suitable statistical methods are available for analyzing the data (but they are beyond the scope of even most second or third courses in statistic). In the examples based on large sample theory, we modeled \\(\\hat{p}\\) using the normal distribution. Why is this not appropriate for the case study on the medical consultant? The independence assumption may be reasonable if each of the surgeries is from a different surgical team. However, the success-failure condition is not satisfied. Under the null hypothesis, we would anticipate seeing \\(62\\times 0.10=6.2\\) complications, not the 10 required for the normal approximation. While this book is scoped to well-constrained statistical problems, do remember that this is just the first book in what is a large library of statistical methods that are suitable for a very wide range of data and contexts. Confidence interval for \\(p\\) A confidence interval provides a range of plausible values for the parameter \\(p\\), and when \\(\\hat{p}\\) can be modeled using a normal distribution, the confidence interval for \\(p\\) takes the form \\[\\begin{align*} \\hat{p} \\pm z^{\\star} \\times SE. \\end{align*}\\] We have seen \\(\\hat{p}\\) to be the sample proportion. The value \\(z^{\\star}\\) determines the confidence level (previously set to be 1.96) and will be discussed in detail in the examples following. The value of the standard error, \\(SE\\), depends heavily on the sample size. Standard Error of one proportion, \\(\\hat{p}\\) When the conditions are met so that the distribution fo \\(\\hat{p}\\) is nearly normal, the variability of a single proportion, \\(\\hat{p}\\) is well described by: \\[SE(\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}\\] Note that we almost never know the true value of \\(p\\). A more helpful formula to use is: \\[SE(\\hat{p}) \\approx \\sqrt{\\frac{(\\mbox{best guess of }p)(1 - \\mbox{best guess of }p)}{n}}\\] For hypothesis testing, we often use \\(p_0\\) as the best guess of \\(p\\). For confidence intervals, we typically use \\(\\hat{p}\\) as the best guess of \\(p\\). Consider taking many polls of registered voters (i.e., random samples) of size 300 asking them if they support legalized marijuana. It is suspected that about 2/3 of all voters support legalized marijuana. To understand how the sample proportion (\\(\\hat{p}\\)) would vary across the samples, calculate the standard error of \\(\\hat{p}\\).69 Variability of the statistic A simple random sample of 826 payday loan borrowers was surveyed to better understand their interests around regulation and costs. 70% of the responses supported new regulations on payday lenders. Is it reasonable to model the variability of \\(\\hat{p}\\) from sample to sample using a normal distribution? Estimate the standard error of \\(\\hat{p}\\). Construct a 95% confidence interval for \\(p\\), the proportion of payday borrowers who support increased regulation for payday lenders. The data are a random sample, so the observations are independent and representative of the population of interest. We also must check the success-failure condition, which we do using \\(\\hat{p}\\) in place of \\(p\\) when computing a confidence interval: \\[\\begin{align*} \\text{Support: } n p &amp; \\approx 826 \\times 0.70 = 578 &amp;\\text{Not: } n (1 - p) &amp; \\approx 826 \\times (1 - 0.70) = 248 \\end{align*}\\] Since both values are at least 10, we can use the normal distribution to model \\(\\hat{p}\\). Because \\(p\\) is unknown and the standard error is for a confidence interval, use \\(\\hat{p}\\) in place of \\(p\\) in the formula. \\(SE = \\sqrt{\\frac{p(1-p)}{n}} \\approx \\sqrt{\\frac{0.70 (1 - 0.70)} {826}} = 0.016\\). Using the point estimate 0.70, \\(z^{\\star} = 1.96\\) for a 95% confidence interval, and the standard error \\(SE = 0.016\\) from the pervious Guided Practice, the confidence interval is \\[\\begin{eqnarray*} \\text{point estimate} \\ \\pm\\ z^{\\star} \\times SE \\quad\\to\\quad 0.70 \\ \\pm\\ 1.96 \\times 0.016 \\quad\\to\\quad (0.669, 0.731) \\end{eqnarray*}\\] We are 95% confident that the true proportion of payday borrowers who supported regulation at the time of the poll was between 0.669 and 0.731. Constructing a confidence interval for a single proportion There are three steps to constructing a confidence interval for \\(p\\). Check independence and the success-failure condition using \\(\\hat{p}\\). If the conditions are met, the sampling distribution of \\(\\hat{p}\\) may be well-approximated by the normal model. Construct the standard error using \\(\\hat{p}\\) in place of \\(p\\) in the standard error formula. Apply the general confidence interval formula. For additional one-proportion confidence interval examples, see Section 5.2.3. Changing the confidence level Suppose we want to consider confidence intervals where the confidence level is somewhat higher than 95%: perhaps we would like a confidence level of 99%. Think back to the analogy about trying to catch a fish: if we want to be more sure that we will catch the fish, we should use a wider net. To create a 99% confidence level, we must also widen our 95% interval. On the other hand, if we want an interval with lower confidence, such as 90%, we could make our original 95% interval slightly slimmer. The 95% confidence interval structure provides guidance in how to make intervals with new confidence levels. Below is a general 95% confidence interval for a point estimate that comes from a nearly normal distribution: \\[\\begin{eqnarray} \\text{point estimate}\\ \\pm\\ 1.96\\times SE \\end{eqnarray}\\] There are three components to this interval: the point estimate, “1.96”, and the standard error. The choice of \\(1.96\\times SE\\) was based on capturing 95% of the data since the estimate is within 1.96 standard errors of the true value about 95% of the time. The choice of 1.96 corresponds to a 95% confidence level. If \\(X\\) is a normally distributed random variable, how often will \\(X\\) be within 2.58 standard deviations of the mean?70 Figure 6.2: The area between -\\(z^{\\star}\\) and \\(z^{\\star}\\) increases as \\(|z^{\\star}|\\) becomes larger. If the confidence level is 99%, we choose \\(z^{\\star}\\) such that 99% of the normal curve is between -\\(z^{\\star}\\) and \\(z^{\\star}\\), which corresponds to 0.5% in the lower tail and 0.5% in the upper tail: \\(z^{\\star}=2.58\\). To create a 99% confidence interval, change 1.96 in the 95% confidence interval formula to be \\(2.58\\). The previous Guided Practice highlights that 99% of the time a normal random variable will be within 2.58 standard deviations of its mean. This approach – using the Z scores in the normal model to compute confidence levels – is appropriate when the point estimate is associated with a normal distribution and we can properly compute the standard error. Thus, the formula for a 99% confidence interval is: \\[\\begin{eqnarray*} \\text{point estimate}\\ \\pm\\ 2.58\\times SE \\end{eqnarray*}\\] The normal approximation is crucial to the precision of the \\(z^\\star\\) confidence intervals (in contrast to the bootstrap confidence intervals). When the normal model is not a good fit, we will use alternative distributions that better characterize the sampling distribution or we will use bootstrapping procedures. Create a 99% confidence interval for the impact of the stent on the risk of stroke using the data from Section 1.1. The point estimate is 0.090, and the standard error is \\(SE = 0.028\\). It has been verified for you that the point estimate can reasonably be modeled by a normal distribution.71 Mathematical model confidence interval for any confidence level. If the point estimate follows the normal model with standard error \\(SE\\), then a confidence interval for the population parameter is \\[\\begin{eqnarray*} \\text{point estimate}\\ \\pm\\ z^{\\star} \\times SE \\end{eqnarray*}\\] where \\(z^{\\star}\\) corresponds to the confidence level selected. Figure 6.2 provides a picture of how to identify \\(z^{\\star}\\) based on a confidence level. We select \\(z^{\\star}\\) so that the area between -\\(z^{\\star}\\) and \\(z^{\\star}\\) in the normal model corresponds to the confidence level. Previously, we found that implanting a stent in the brain of a patient at risk for a stroke increased the risk of a stroke. The study estimated a 9% increase in the number of patients who had a stroke, and the standard error of this estimate was about \\(SE = 2.8%\\). Compute a 90% confidence interval for the effect.72 Hypothesis test for \\(H_0: p = p_0\\) One possible regulation for payday lenders is that they would be required to do a credit check and evaluate debt payments against the borrower’s finances. We would like to know: would borrowers support this form of regulation? Set up hypotheses to evaluate whether borrowers have a majority support for this type of regulation.73 To apply the normal distribution framework in the context of a hypothesis test for a proportion, the independence and success-failure conditions must be satisfied. In a hypothesis test, the success-failure condition is checked using the null proportion: we verify \\(np_0\\) and \\(n(1-p_0)\\) are at least 10, where \\(p_0\\) is the null value. Do payday loan borrowers support a regulation that would require lenders to pull their credit report and evaluate their debt payments? From a random sample of 826 borrowers, 51% said they would support such a regulation. Is it reasonable use a normal distribution to model \\(\\hat{p}\\) for a hypothesis test here?74 Using the hypotheses and data from the previous Guided Practices, evaluate whether the poll on lending regulations provides convincing evidence that a majority of payday loan borrowers support a new regulation that would require lenders to pull credit reports and evaluate debt payments. With hypotheses already set up and conditions checked, we can move onto calculations. The standard error in the context of a one-proportion hypothesis test is computed using the null value, \\(p_0\\): \\[\\begin{align*} SE = \\sqrt{\\frac{p_0 (1 - p_0)}{n}} = \\sqrt{\\frac{0.5 (1 - 0.5)}{826}} = 0.017 \\end{align*}\\] A picture of the normal model is shown below with the p-value represented by the shaded region. Based on the normal model, the test statistic can be computed as the Z-score of the point estimate: \\[\\begin{align*} Z = \\frac{\\text{point estimate} - \\text{null value}}{SE} = \\frac{0.51 - 0.50}{0.017} = 0.59 \\end{align*}\\] The single tail area which represents the p-value is 0.2776. Because the p-value is larger than 0.05, we do not reject \\(H_0\\). The poll does not provide convincing evidence that a majority of payday loan borrowers support regulations around credit checks and evaluation of debt payments. In Section 6.2.1 we discuss two-sided hypothesis tests of which the payday example may have been better structured. That is, we might have wanted to ask whether the borrows support or oppose the regulations (to study opinion in either direction away from the 50% benchmark). In that case, the p-value would have been doubled to 0.5552 (again, we would not reject \\(H_0\\)). In the two-sided hypothesis setting, the appropriate conclusion would be to claim that the poll does not provide convincing evidence that a majority of payday loan borrowers support or oppose regulations around credit checks and evaluation of debt payments. In both the one-sided or two-sided setting, the conclusion is somewhat unsatisfactory because there is no conclusion. That is, there is no resolution one way or the other about public opinion. We cannot claim that exactly 50% of people support the regulation, but we cannot claim a majority in either direction. Mathematical model hypothesis test for a proportion. Set up hypotheses and verify the conditions using the null value, \\(p_0\\), to ensure \\(\\hat{p}\\) is nearly normal under \\(H_0\\). If the conditions hold, construct the standard error, again using \\(p_0\\), and show the p-value in a drawing. Lastly, compute the p-value and evaluate the hypotheses. For additional one-proportion hypothesis test examples, see Section 5.1.3. Violating conditions We’ve spent a lot of time discussing conditions for when \\(\\hat{p}\\) can be reasonably modeled by a normal distribution. What happens when the success-failure condition fails? What about when the independence condition fails? In either case, the general ideas of confidence intervals and hypothesis tests remain the same, but the strategy or technique used to generate the interval or p-value change. When the success-failure condition isn’t met for a hypothesis test, we can simulate the null distribution of \\(\\hat{p}\\) using the null value, \\(p_0\\), as seen in Section 6.1.1. Unfortunately, methods for dealing with observations which are not independent are outside the scope of this book. 6.2 Difference of two proportions We now extend the methods from Section 6.1 to apply confidence intervals and hypothesis tests to differences in population proportions that come from two groups: \\(p_1 - p_2\\). In our investigations, we’ll identify a reasonable point estimate of \\(p_1 - p_2\\) based on the sample, and you may have already guessed its form: \\(\\hat{p}_1 - \\hat{p}_2\\). Then we’ll look at the inferential analysis in three different ways: using a randomization test, applying bootstrapping for interval estimates, and, if we verify that the point estimate can be modeled using a normal distribution, we compute the estimate’s standard error, and we apply the mathematical framework. 6.2.1 Randomization test for \\(H_0: p_1 - p_2 = 0\\) Observed data We consider a study on a new malaria vaccine called PfSPZ. In this study, volunteer patients were randomized into one of two experiment groups: 14 patients received an experimental vaccine or 6 patients received a placebo vaccine. Nineteen weeks later, all 20 patients were exposed to a drug-sensitive malaria virus strain; the motivation of using a drug-sensitive strain of virus here is for ethical considerations, allowing any infections to be treated effectively. The results are summarized in Table 6.1, where 9 of the 14 treatment patients remained free of signs of infection while all of the 6 patients in the control group patients showed some baseline signs of infection. Table 6.1: Summary results for the malaria vaccine experiment. outcome infection no infection Total vaccine 5 9 14 treatment placebo 6 0 6 Total 11 9 20 Is this an observational study or an experiment? What implications does the study type have on what can be inferred from the results?75 In this study, a smaller proportion of patients who received the vaccine showed signs of an infection (35.7% versus 100%). However, the sample is very small, and it is unclear whether the difference provides convincing evidence that the vaccine is effective. As we saw in Section 5.1, we can randomize the responses (infection or no infection) to the treatment conditions under the null hypothesis of independence and compute possible differences in proportions. The process by which we randomize observations to two groups is summarized and visualized in Figure 5.7. Variability of the statistic Figure 6.3 shows a stacked plot of the differences found from 100 randomization simulations (i.e., repeated iterations as described in Figure 5.7), where each dot represents a simulated difference between the infection rates (control rate minus treatment rate). Figure 6.3: A stacked dot plot of differences from 100 simulations produced under the independence model \\(H_0\\), where in these simulations infections are unaffected by the vaccine. Two of the 100 simulations had a difference of at least 64.3%, the difference observed in the study. Observed statistic vs null statistics Note that the distribution of these simulated differences is centered around 0. We simulated the differences assuming that the independence model was true, and under this condition, we expect the difference to be near zero with some random fluctuation, where near is pretty generous in this case since the sample sizes are so small in this study. How often would you observe a difference of at least 64.3% (0.643) according to Figure 6.3? Often, sometimes, rarely, or never? It appears that a difference of at least 64.3% due to chance alone would only happen about 2% of the time according to Figure 6.3. Such a low probability indicates a rare event. The difference of 64.3% being a rare event suggests two possible interpretations of the results of the study: \\(H_0\\) Independence model. The vaccine has no effect on infection rate, and we just happened to observe a difference that would only occur on a rare occasion. \\(H_A\\) Alternative model. The vaccine has an effect on infection rate, and the difference we observed was actually due to the vaccine being effective at combating malaria, which explains the large difference of 64.3%. Based on the simulations, we have two options: We conclude that the study results do not provide strong evidence against the independence model. That is, we do not have sufficiently strong evidence to conclude the vaccine had an effect in this clinical setting. We conclude the evidence is sufficiently strong to reject \\(H_0\\) and assert that the vaccine was useful. When we conduct formal studies, usually we reject the notion that we just happened to observe a rare event.76 In this case, we reject the independence model in favor of the alternative. That is, we are concluding the data provide strong evidence that the vaccine provides some protection against malaria in this clinical setting. Statistical inference, is built on evaluating whether such differences are due to chance. In statistical inference, data scientists evaluate which model is most reasonable given the data. Errors do occur, just like rare events, and we might choose the wrong model. While we do not always choose correctly, statistical inference gives us tools to control and evaluate how often these errors occur. Decision errors Hypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free. Similarly, data can point to the wrong conclusion. However, what distinguishes statistical hypothesis tests from a court system is that our framework allows us to quantify and control how often the data lead us to the incorrect conclusion. In a hypothesis test, there are two competing hypotheses: the null and the alternative. We make a statement about which one might be true, but we might choose incorrectly. There are four possible scenarios in a hypothesis test, which are summarized in Table 6.2. Table 6.2: Four different scenarios for hypothesis tests. Test conclusion \\(H_0\\) true good decision Type 1 Error Truth \\(H_A\\) true Type 2 Error good decision A Type 1 Error is rejecting the null hypothesis when \\(H_0\\) is actually true. Since we rejected the null hypothesis in the gender discrimination and opportunity cost studies, it is possible that we made a Type 1 Error in one or both of those studies. A Type 2 Error is failing to reject the null hypothesis when the alternative is actually true. In a US court, the defendant is either innocent (\\(H_0\\)) or guilty (\\(H_A\\)). What does a Type 1 Error represent in this context? What does a Type 2 Error represent? Table 6.2 may be useful. If the court makes a Type 1 Error, this means the defendant is innocent (\\(H_0\\) true) but wrongly convicted. A Type 2 Error means the court failed to reject \\(H_0\\) (i.e., failed to convict the person) when they were in fact guilty (\\(H_A\\) true). Consider the opportunity cost study where we concluded students were less likely to make a DVD purchase if they were reminded that money not spent now could be spent later. What would a Type 1 Error represent in this context?77 How could we reduce the Type 1 Error rate in US courts? What influence would this have on the Type 2 Error rate? To lower the Type 1 Error rate, we might raise our standard for conviction from “beyond a reasonable doubt” to “beyond a conceivable doubt” so fewer people would be wrongly convicted. However, this would also make it more difficult to convict the people who are actually guilty, so we would make more Type 2 Errors. How could we reduce the Type 2 Error rate in US courts? What influence would this have on the Type 1 Error rate?78 The example and guided practice above provide an important lesson: if we reduce how often we make one type of error, we generally make more of the other type. Choosing a significance level Choosing a significance level for a test is important in many contexts, and the traditional level is 0.05. However, it is sometimes helpful to adjust the significance level based on the application. We may select a level that is smaller or larger than 0.05 depending on the consequences of any conclusions reached from the test. If making a Type 1 Error is dangerous or especially costly, we should choose a small significance level (e.g., 0.01 or 0.001). If we want to be very cautious about rejecting the null hypothesis, we demand very strong evidence favoring the alternative \\(H_A\\) before we would reject \\(H_0\\). If a Type 2 Error is relatively more dangerous or much more costly than a Type 1 Error, then we should choose a higher significance level (e.g., 0.10). Here we want to be cautious about failing to reject \\(H_0\\) when the null is actually false. Significance levels should reflect consequences of errors. The significance level selected for a test should reflect the real-world consequences associated with making a Type 1 or Type 2 Error. Two-sided hypotheses In Section 5.1 we explored whether women were discriminated against and whether a simple trick could make students a little thriftier. In these two case studies, we’ve actually ignored some possibilities: What if men are actually discriminated against? What if the money trick actually makes students spend more? These possibilities weren’t considered in our original hypotheses or analyses. The disregard of the extra alternatives may have seemed natural since the data pointed in the directions in which we framed the problems. However, there are two dangers if we ignore possibilities that disagree with our data or that conflict with our world view: Framing an alternative hypothesis simply to match the direction that the data point will generally inflate the Type 1 Error rate. After all the work we’ve done (and will continue to do) to rigorously control the error rates in hypothesis tests, careless construction of the alternative hypotheses can disrupt that hard work. If we only use alternative hypotheses that agree with our worldview, then we’re going to be subjecting ourselves to confirmation bias, which means we are looking for data that supports our ideas. That’s not very scientific, and we can do better! The original hypotheses we’ve seen are called one-sided hypothesis tests because they only explored one direction of possibilities. Such hypotheses are appropriate when we are exclusively interested in the single direction, but usually we want to consider all possibilities. To do so, let’s learn about two-sided hypothesis tests in the context of a new study that examines the impact of using blood thinners on patients who have undergone CPR. Cardiopulmonary resuscitation (CPR) is a procedure used on individuals suffering a heart attack when other emergency resources are unavailable. This procedure is helpful in providing some blood circulation to keep a person alive, but CPR chest compressions can also cause internal injuries. Internal bleeding and other injuries that can result from CPR complicate additional treatment efforts. For instance, blood thinners may be used to help release a clot that is causing the heart attack once a patient arrives in the hospital. However, blood thinners negatively affect internal injuries. Here we consider an experiment with patients who underwent CPR for a heart attack and were subsequently admitted to a hospital.79 Each patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group). The outcome variable of interest was whether the patient survived for at least 24 hours. Form hypotheses for this study in plain and statistical language. Let \\(p_c\\) represent the true survival rate of people who do not receive a blood thinner (corresponding to the control group) and \\(p_t\\) represent the survival rate for people receiving a blood thinner (corresponding to the treatment group). We want to understand whether blood thinners are helpful or harmful. We’ll consider both of these possibilities using a two-sided hypothesis test. \\(H_0\\): Blood thinners do not have an overall survival effect, i.e., the survival proportions are the same in each group. \\(p_t - p_c = 0\\). \\(H_A\\): Blood thinners have an impact on survival, either positive or negative, but not zero. \\(p_t - p_c \\neq 0\\). Note that if we had done a one-sided hypothesis test, the resulting hypotheses would have been: \\(H_0\\): Blood thinners do not have a positive overall survival effect, i.e., the survival proportions for the blood thinner group is the same or lower than the control group. \\(p_t - p_c \\leq 0\\). \\(H_A\\): Blood thinners have a positive impact on survival. \\(p_t - p_c &gt; 0\\). There were 50 patients in the experiment who did not receive a blood thinner and 40 patients who did. The study results are shown in Table 6.3. Table 6.3: Results for the CPR study. Patients in the treatment group were given a blood thinner, and patients in the control group were not Survived Died Total Control 11 39 50 Treatment 14 26 40 Total 25 65 90 What is the observed survival rate in the control group? And in the treatment group? Also, provide a point estimate of the difference in survival proportions of the two groups: \\(\\hat{p}_t - \\hat{p}_c\\).80 According to the point estimate, for patients who have undergone CPR outside of the hospital, an additional 13% of these patients survive when they are treated with blood thinners. However, we wonder if this difference could be easily explainable by chance. As we did in our past two studies this chapter, we will simulate what type of differences we might see from chance alone under the null hypothesis. By randomly assigning “simulated treatment” and “simulated control” stickers to the patients’ files, we get a new grouping. If we repeat this simulation 10,000 times, we can build a null distribution of the differences shown in Figure 6.4. Figure 6.4: Null distribution of the point estimate for the difference in proportions, \\(\\hat{p}_t - \\hat{p}_c\\). The shaded right tail shows observations that are at least as large as the observed difference, 0.13. The right tail area is 0.131. (Note: it is only a coincidence that we also have \\(\\hat{p}_t - \\hat{p}_c=0.13\\).) However, contrary to how we calculated the p-value in previous studies, the p-value of this test is not 0.131! The p-value is defined as the chance we observe a result at least as favorable to the alternative hypothesis as the result (i.e., the difference) we observe. In this case, any differences less than or equal to -0.13 would also provide equally strong evidence favoring the alternative hypothesis as a difference of +0.13 did. A difference of -0.13 would correspond to 13% higher survival rate in the control group than the treatment group. In Figure 6.5 we’ve also shaded these differences in the left tail of the distribution. These two shaded tails provide a visual representation of the p-value for a two-sided test. Figure 6.5: Null distribution of the point estimate for the difference in proportions, \\(\\hat{p}_t - \\hat{p}_c\\). All values that are at least as extreme as +0.13 but in either direction away from 0 are shaded. For a two-sided test, take the single tail (in this case, 0.131) and double it to get the p-value: 0.262. Since this p-value is larger than 0.05, we do not reject the null hypothesis. That is, we do not find statistically significant evidence that the blood thinner has any influence on survival of patients who undergo CPR prior to arriving at the hospital. Default to a two-sided test. We want to be rigorous and keep an open mind when we analyze data and evidence. Use a one-sided hypothesis test only if you truly have interest in only one direction. Computing a p-value for a two-sided test. First compute the p-value for one tail of the distribution, then double that value to get the two-sided p-value. That’s it! Consider the situation of the medical consultant. Now that you know about one-sided and two-sided tests, which type of test do you think is more appropriate? The setting has been framed in the context of the consultant being helpful (which is what led us to a one-sided test originally), but what if the consultant actually performed worse than the average? Would we care? More than ever! Since it turns out that we care about a finding in either direction, we should run a two-sided test. The p-value for the two-sided test is double that of the one-sided test, here the simulated p-value would be 0.2444. Generally, to find a two-sided p-value we double the single tail area, which remains a reasonable approach even when the sampling distribution is asymmetric. However, the approach can result in p-values larger than 1 when the point estimate is very near the mean in the null distribution; in such cases, we write that the p-value is 1. Also, very large p-values computed in this way (e.g., 0.85), may also be slightly inflated. Typically, we do not worry too much about the precision of very large p-values because they lead to the same analysis conclusion, even if the value is slightly off. Controlling the Type 1 Error rate Now that we understand the difference between one-sided and two-sided tests, we must recognize when to use each type of test. Because of the result of increased error rates, it is never okay to change two-sided tests to one-sided tests after observing the data. We explore the consequences of ignoring this advice in the next example. Using \\(\\alpha=0.05\\), we show that freely switching from two-sided tests to one-sided tests will lead us to make twice as many Type 1 Errors as intended. Suppose we are interested in finding any difference from 0. We’ve created a smooth-looking null distribution representing differences due to chance in Figure 6.6. Suppose the sample difference was larger than 0. Then if we can flip to a one-sided test, we would use \\(H_A\\): difference \\(&gt; 0\\). Now if we obtain any observation in the upper 5% of the distribution, we would reject \\(H_0\\) since the p-value would just be a the single tail. Thus, if the null hypothesis is true, we incorrectly reject the null hypothesis about 5% of the time when the sample mean is above the null value, as shown in Figure 6.6. Suppose the sample difference was smaller than 0. Then if we change to a one-sided test, we would use \\(H_A\\): difference \\(&lt; 0\\). If the observed difference falls in the lower 5% of the figure, we would reject \\(H_0\\). That is, if the null hypothesis is true, then we would observe this situation about 5% of the time. By examining these two scenarios, we can determine that we will make a Type 1 Error \\(5\\%+5\\%=10\\%\\) of the time if we are allowed to swap to the “best” one-sided test for the data. This is twice the error rate we prescribed with our significance level: \\(\\alpha=0.05\\) (!). Figure 6.6: The shaded regions represent areas where we would reject \\(H_0\\) under the bad practices considered in when \\(\\alpha = 0.05\\). Hypothesis tests should be set up before seeing the data. After observing data, it is tempting to turn a two-sided test into a one-sided test. Avoid this temptation. Hypotheses should be set up before observing the data. 6.2.2 Bootstrap confidence interval for \\(p_1 - p_2\\) The key will be to use two different bags to simulate from the original data. Use the CPR data. After we find the CI (use percentile and SE methods), write the interval values down below in the math section that describes the generic confidence interval method. In Section 6.2.1, we worked with the randomization distribution to understand the distribution of \\(\\hat{p}_1 - \\hat{p}_2\\) when the null hypothesis \\(H_0: p_1 - p_2 = 0\\) is true. Now, through bootstrapping, we study the variability of \\(\\hat{p}_1 - \\hat{p}_2\\) without the null assumption. Observed data Reconsider the CPR data from Section 6.2.1 which is provided in Table 6.3. The experiment consisted of two treatments on patients who underwent CPR for a heart attack and were subsequently admitted to a hospital. Each patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group). The outcome variable of interest was whether the patient survived for at least 24 hours. Again, we use the difference in sample proportions as the observed statistic of interest. Here, the value of the statistic is: \\(\\hat{p}_t - \\hat{p}_c = 0.35 - 0.22 = 0.13\\). Variability of the statistic The bootstrap method applied to two samples is an extension of the method described in Section 5.2. Now, we have two samples, so each sample estimates the population from which they came. In the CPR setting, the treatment sample estimates the population of all individuals who have gotten (or will get) the treatment; the control sample estimate the population of all individuals who do not get the treatment and are controls. Figure 6.7 extends Figure 5.8 to show the bootstrapping process from two samples simultaneously. Figure 6.7: (probably populations only, no BS samples) two sample estimating two populations (two infinite popuations) The variability of the statistic (the difference in sample proportions) can be calculated by taking one treatment bootstrap sample and one control bootstrap sample and calculating the difference of the bootstrap survival proportions. One sample from each of the estimated populations has been taken with the sample proportions calculated for the treatment bootstrap sample and the control bootstrap sample. once the image is in, we need to describe above (and below) the values (proportions, differences) in the image explicitly. Figure 6.8: some way to connect the first BS sample on the left wiht the first BS sample on the right. As always, the variability of the difference in proportions can only be estimated by repeated simulations, in this case, repeated bootstrap samples. Figure 6.8 shows multiple bootstrap differences calculated for each of the repeated bootstrap samples. Figure 6.9: in this graph, some kind of connection between each of the two sides Do we also want to visualize “sampling with replacement” in the two sample case? Repeated bootstrap simulations lead to a bootstrap sampling distribution of the statistic of interest, here the difference in sample proportions. Figure 6.10 shows 1000 bootstrap differences in proportions for the CPR data. Figure 6.10: A histogram of differences in proportions from 1000 bootstrap simulations. Percentile vs. SE bootstrap confidence intervals Figure 6.10 provides an estimate for the variability of the difference in survival proportions from sample to sample, The values in the histogram can be used in two different ways to create a confidence interval for the parameter of interest: \\(p_1 - p_2\\). Percentile bootstrap interval As in Section 5.2, the bootstrap confidence interval can be calculated directly from the bootstrapped differences in Figure 6.10. The interval created from the percentiles of the distribution is called the percentile interval. Note that here we calculate the 90% confidence interval by finding the \\(5^{th}\\) and \\(95^{th}\\) percentile values from the bootstrapped differences. The bootstrap 5 percentile proportion is -0.155 and the 95 percentile is 0.167. The result is: we are 90% confident that, in the population, the true difference in probability of survival is between -0.155 and 0.167. The interval shows that we do not have much definitive evidence of the affect of blood thinners, one way or another. (#fig:CPR percentile interval)The CPR data is bootstrapped 1000 times. Each simulation creates a sample from the original data where the probability of survival in the treatment group is \\(\\hat{p}_{t} = 14/40\\) and the probability of survival in the control group is \\(\\hat{p}_{c} = 11/50\\). SE bootstrap interval Alternatively, we can use the variability in the bootstrapped differences to calculate a standard error of the difference. The resulting interval is called the SE interval. Section 6.2.3 details the mathematical model for the standard error of the difference in sample proportions, but the bootstrap distribution typically does an excellent job of estimating the variability. \\[SE(\\hat{p}_t - \\hat{p}_c) \\approx SD(\\hat{p}_{bs,t} - \\hat{p}_{bs,c}) = 0.0975\\] The calculation above was performed in R using the sd() function, but any statistical software will calculate the standard deviation of the differences, here, the exact quantity we hope to approximate. Because we don’t know the true distribution of \\(\\hat{p}_t - \\hat{p}_c\\), we will use a rough approximation to find a confidence interval for \\(p_t - p_c\\). A 95% confidence interval for \\(p_t - p_c\\) is given by: \\[\\begin{align} \\hat{p}_t - \\hat{p}_c &amp;\\pm&amp; 2 SE(\\hat{p}_t - \\hat{p}_c)\\\\ 14/40 - 11/50 &amp;\\pm&amp; 0.0975\\\\ &amp;&amp;(-0.065, 0.325) \\end{align}\\] We are 95% confident that the true value of \\(p_t - p_c\\) is between -0.065 and 0.325. Again, the wide confidence interval that overlaps zero indicates that the study provides very little evidence about the effectiveness of blood thinners. 6.2.3 Mathematical model Variability of \\(\\hat{p}_1 - \\hat{p}_2\\) Like with \\(\\hat{p}\\), the difference of two sample proportions \\(\\hat{p}_1 - \\hat{p}_2\\) can be modeled using a normal distribution when certain conditions are met. First, we require a broader independence condition, and secondly, the success-failure condition must be met by both groups. Conditions for the sampling distribution of \\(\\hat{p}_1 -\\hat{p}_2\\) to be normal. The difference \\(\\hat{p}_1 - \\hat{p}_2\\) can be modeled using a normal distribution when Independence, extended. The data are independent within and between the two groups. Generally this is satisfied if the data come from two independent random samples or if the data come from a randomized experiment. Success-failure condition. The success-failure condition holds for both groups, where we check successes and failures in each group separately. When these conditions are satisfied, the standard error of \\(\\hat{p}_1 - \\hat{p}_2\\) is \\[\\begin{eqnarray*} SE = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}} \\end{eqnarray*}\\] where \\(p_1\\) and \\(p_2\\) represent the population proportions, and \\(n_1\\) and \\(n_2\\) represent the sample sizes. Confidence interval for \\(p_1 - p_2\\) We can apply the generic confidence interval formula for a difference of two proportions, where we use \\(\\hat{p}_1 - \\hat{p}_2\\) as the point estimate and substitute the \\(SE\\) formula: \\[\\begin{align*} &amp;\\text{point estimate} \\ \\pm\\ z^{\\star} \\times SE &amp;&amp;\\to &amp;&amp;\\hat{p}_1 - \\hat{p}_2 \\ \\pm\\ z^{\\star} \\times \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}} \\end{align*}\\] Standard Error of the difference in two proportions, \\(\\hat{p}_1 -\\hat{p}_2\\). When the conditions are met so that the distribution fo \\(\\hat{p}\\) is nearly normal, the variability of the difference in proportions, \\(\\hat{p}_1 -\\hat{p}_2\\), is well described by: \\[\\begin{eqnarray*} SE(\\hat{p}_1 -\\hat{p}_2) = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}} \\end{eqnarray*}\\] We reconsider the experiment for patients who underwent cardiopulmonary resuscitation (CPR) for a heart attack and were subsequently admitted to a hospital. These patients were randomly divided into a treatment group where they received a blood thinner or the control group where they did not receive a blood thinner. The outcome variable of interest was whether the patients survived for at least 24 hours. The results are shown in Table 6.3. Check whether we can model the difference in sample proportions using the normal distribution. We first check for independence: since this is a randomized experiment, this condition is satisfied. Next, we check the success-failure condition for each group. We have at least 10 successes and 10 failures in each experiment arm (11, 14, 39, 26), so this condition is also satisfied. With both conditions satisfied, the difference in sample proportions can be reasonably modeled using a normal distribution for these data. Create and interpret a 90% confidence interval of the difference for the survival rates in the CPR study. We’ll use \\(p_t\\) for the survival rate in the treatment group and \\(p_c\\) for the control group: \\[\\begin{align*} \\hat{p}_{t} - \\hat{p}_{c} = \\frac{14}{40} - \\frac{11}{50} = 0.35 - 0.22 = 0.13 \\end{align*}\\] We use the standard error formula previously provided. As with the one-sample proportion case, we use the sample estimates of each proportion in the formula in the confidence interval context: \\[\\begin{align*} SE \\approx \\sqrt{\\frac{0.35 (1 - 0.35)}{40} + \\frac{0.22 (1 - 0.22)}{50}} = 0.095 \\end{align*}\\] For a 90% confidence interval, we use \\(z^{\\star} = 1.65\\): \\[\\begin{align*} \\text{point estimate} \\ \\pm\\ z^{\\star} \\times SE \\quad \\to \\quad 0.13 \\ \\pm\\ 1.65 \\times 0.095 \\quad \\to \\quad (-0.027, 0.287) \\end{align*}\\] We are 90% confident that blood thinners have a difference of -2.7% to +28.7% percentage point impact on survival rate for patients who are like those in the study. Because 0% is contained in the interval, we do not have enough information to say whether blood thinners help or harm heart attack patients who have been admitted after they have undergone CPR. not sure why the footnote in the guidedpractice below doesn’t show up? should be right after: “Also interpret the interval in the context of the study.” A 5-year experiment was conducted to evaluate the effectiveness of fish oils on reducing cardiovascular events, where each subject was randomized into one of two treatment groups. We’ll consider heart attack outcomes in the patients listed in Table 6.4. Create a 95% confidence interval for the effect of fish oils on heart attacks for patients who are well-represented by those in the study. Also interpret the interval in the context of the study.^[Because the patients were randomized, the subjects are independent, both within and between the two groups. The success-failure condition is also met for both groups as all counts are at least 10. This satisfies the conditions necessary to model the difference in proportions using a normal distribution. Compute the sample proportions (\\(\\hat{p}_{\\text{fish oil}} = 0.0112\\), \\(\\hat{p}_{\\text{placebo}} = 0.0155\\)), point estimate of the difference (\\(0.0112 - 0.0155 = -0.0043\\)), and standard error \\(SE = \\sqrt{\\frac{0.0112 \\times 0.9888}{12933} + \\frac{0.0155 \\times 0.9845}{12938}} = 0.00145\\). Next, plug the values into the general formula for a confidence interval, where we’ll use a 95% confidence level with \\(z^{\\star} = 1.96\\): \\[\\begin{align*} -0.0043 \\pm 1.96 \\times 0.00145 \\quad \\to \\quad (-0.0071, -0.0015) \\end{align*}\\] We are 95% confident that fish oils decreases heart attacks by 0.15 to 0.71 percentage points (off of a baseline of about 1.55%) over a 5-year period for subjects who are similar to those in the study. Because the interval is entirely below 0, and the treatment was randomly assigned the data provide strong evidence that fish oil supplements reduce heart attacks in patients like those in the study.] Table 6.4: Results for the study on n-3 fatty acid supplement and related health benefits. heart attack no event Total fish oil 145 12788 12933 placebo 200 12738 12938 Hypothesis test for \\(H_0: p_1 - p_2 = 0\\) A mammogram is an X-ray procedure used to check for breast cancer. Whether mammograms should be used is part of a controversial discussion, and it’s the topic of our next example where we learn about 2-proportion hypothesis tests when \\(H_0\\) is \\(p_1 - p_2 = 0\\) (or equivalently, \\(p_1 = p_2\\)). A 30-year study was conducted with nearly 90,000 female participants. During a 5-year screening period, each woman was randomized to one of two groups: in the first group, women received regular mammograms to screen for breast cancer, and in the second group, women received regular non-mammogram breast cancer exams. No intervention was made during the following 25 years of the study, and we’ll consider death resulting from breast cancer over the full 30-year period. Results from the study are summarized in Figure 6.5. If mammograms are much more effective than non-mammogram breast cancer exams, then we would expect to see additional deaths from breast cancer in the control group. On the other hand, if mammograms are not as effective as regular breast cancer exams, we would expect to see an increase in breast cancer deaths in the mammogram group. Table 6.5: Summary results for breast cancer study. Death from breast cancer? Yes No Mammogram 500 44,425 Control 505 44,405 Is this study an experiment or an observational study?81 Set up hypotheses to test whether there was a difference in breast cancer deaths in the mammogram and control groups.82 Using the previous example, we will check the conditions for using a normal distribution to analyze the results of the study. The details are very similar to that of confidence intervals. However, when the null hypothesis is that \\(p_1 - p_2 = 0\\), we use a special proportion called the pooled proportion to check the success-failure condition: \\[\\begin{align*} \\hat{p}_{\\textit{pool}} &amp;= \\frac {\\text{# of patients who died from breast cancer in the entire study}} {\\text{# of patients in the entire study}} \\\\ &amp;= \\frac{500 + 505}{500 + \\text{44,425} + 505 + \\text{44,405}} \\\\ &amp;= 0.0112 \\end{align*}\\] This proportion is an estimate of the breast cancer death rate across the entire study, and it’s our best estimate of the proportions \\(p_{mgm}\\) and \\(p_{ctrl}\\) if the null hypothesis is true that \\(p_{mgm} = p_{ctrl}\\). We will also use this pooled proportion when computing the standard error. Is it reasonable to model the difference in proportions using a normal distribution in this study? Because the patients are randomized, they can be treated as independent, both within and between groups. We also must check the success-failure condition for each group. Under the null hypothesis, the proportions \\(p_{mgm}\\) and \\(p_{ctrl}\\) are equal, so we check the success-failure condition with our best estimate of these values under \\(H_0\\), the pooled proportion from the two samples, \\(\\hat{p}_{\\textit{pool}} = 0.0112\\): \\[\\begin{align*} \\hat{p}_{\\textit{pool}} \\times n_{mgm} &amp;= 0.0112 \\times \\text{44,925} = 503 &amp; (1 - \\hat{p}_{\\textit{pool}}) \\times n_{mgm} &amp;= 0.9888 \\times \\text{44,925} = \\text{44,422} \\\\ \\hat{p}_{\\textit{pool}} \\times n_{ctrl} &amp;= 0.0112 \\times \\text{44,910} = 503 &amp; (1 - \\hat{p}_{\\textit{pool}}) \\times n_{ctrl} &amp;= 0.9888 \\times \\text{44,910} = \\text{44,407} \\end{align*}\\] The success-failure condition is satisfied since all values are at least 10. With both conditions satisfied, we can safely model the difference in proportions using a normal distribution. Use the pooled proportion when \\(H_0\\) is \\(p_1 - p_2 = 0\\). When the null hypothesis is that the proportions are equal, use the pooled proportion (\\(\\hat{p}_{\\textit{pooled}}\\)) to verify the success-failure condition and estimate the standard error: \\[\\begin{eqnarray*} \\hat{p}_{\\textit{pooled}} = \\frac{\\text{number of ``successes&quot;}} {\\text{number of cases}} = \\frac{\\hat{p}_1 n_1 + \\hat{p}_2 n_2}{n_1 + n_2} \\end{eqnarray*}\\] Here \\(\\hat{p}_1 n_1\\) represents the number of successes in sample 1 since \\[\\begin{eqnarray*} \\hat{p}_1 = \\frac{\\text{number of successes in sample 1}}{n_1} \\end{eqnarray*}\\] Similarly, \\(\\hat{p}_2 n_2\\) represents the number of successes in sample 2. In the previous example, the pooled proportion was used to check the success-failure condition83. In the next example, we see an additional place where the pooled proportion comes into play: the standard error calculation. Compute the point estimate of the difference in breast cancer death rates in the two groups, and use the pooled proportion \\(\\hat{p}_{\\textit{pool}} = 0.0112\\) to calculate the standard error. The point estimate of the difference in breast cancer death rates is \\[\\begin{align*} \\hat{p}_{mgm} - \\hat{p}_{ctrl} &amp;= \\frac{500}{500 + 44,425} - \\frac{505}{505 + 44,405} \\\\ &amp;= 0.01113 - 0.01125 \\\\ &amp;= -0.00012 \\end{align*}\\] The breast cancer death rate in the mammogram group was 0.012% less than in the control group. Next, the standard error is calculated , \\(\\hat{p}_{\\textit{pool}}\\): \\[\\begin{align*} SE = \\sqrt{ \\frac{\\hat{p}_{\\textit{pool}}(1-\\hat{p}_{\\textit{pool}})} {n_{mgm}} + \\frac{\\hat{p}_{\\textit{pool}}(1-\\hat{p}_{\\textit{pool}})} {n_{ctrl}} } = 0.00070 \\end{align*}\\] Using the point estimate \\(\\hat{p}_{mgm} - \\hat{p}_{ctrl} = -0.00012\\) and standard error \\(SE = 0.00070\\), calculate a p-value for the hypothesis test and write a conclusion. Just like in past tests, we first compute a test statistic and draw a picture: \\[\\begin{align*} Z = \\frac{\\text{point estimate} - \\text{null value}}{SE} = \\frac{-0.00012 - 0}{0.00070} = -0.17 \\end{align*}\\] The lower tail area is 0.4325, which we double to get the p-value: 0.8650. Because this p-value is larger than 0.05, we do not reject the null hypothesis. That is, the difference in breast cancer death rates is reasonably explained by chance, and we do not observe benefits or harm from mammograms relative to a regular breast exam. Can we conclude that mammograms have no benefits or harm? Here are a few considerations to keep in mind when reviewing the mammogram study as well as any other medical study: We do not accept the null hypothesis, which means we don’t have sufficient evidence to conclude that mammograms reduce or increase breast cancer deaths. If mammograms are helpful or harmful, the data suggest the effect isn’t very large. Are mammograms more or less expensive than a non-mammogram breast exam? If one option is much more expensive than the other and doesn’t offer clear benefits, then we should lean towards the less expensive option. The study’s authors also found that mammograms led to over-diagnosis of breast cancer, which means some breast cancers were found (or thought to be found) but that these cancers would not cause symptoms during patients’ lifetimes. That is, something else would kill the patient before breast cancer symptoms appeared. This means some patients may have been treated for breast cancer unnecessarily, and this treatment is another cost to consider. It is also important to recognize that over-diagnosis can cause unnecessary physical or emotional harm to patients. These considerations highlight the complexity around medical care and treatment recommendations. Experts and medical boards who study medical treatments use considerations like those above to provide their best recommendation based on the current evidence. 6.3 Independence in two-way tables Note that with two-way tables, there is not an obvious single parameter of interest. Instead, research questions usually focus on how the proportions of the response variable changes (or not) across the different levels of the explanatory variable. Because there is not a population parameter to estimate, bootstrapping to find the standard error of the estimate is not meaningful. As such, for two-way tables, we will focus on the randomization test and corresponding mathematical approximation (and not bootstrapping). 6.3.1 Randomization test of \\(H_0:\\) independence We all buy used products – cars, computers, textbooks, and so on – and we sometimes assume the sellers of those products will be forthright about any underlying problems with what they’re selling. This is not something we should take for granted. Researchers recruited 219 participants in a study where they would sell a used iPod84 that was known to have frozen twice in the past. The participants were incentivized to get as much money as they could for the iPod since they would receive a 5% cut of the sale on top of $10 for participating. The researchers wanted to understand what types of questions would elicit the seller to disclose the freezing issue. Unbeknownst to the participants who were the sellers in the study, the buyers were collaborating with the researchers to evaluate the influence of different questions on the likelihood of getting the sellers to disclose the past issues with the iPod. The scripted buyers started with “Okay, I guess I’m supposed to go first. So you’ve had the iPod for 2 years …” and ended with one of three questions: General: What can you tell me about it? Positive Assumption: It doesn’t have any problems, does it? Negative Assumption: What problems does it have? The question is the treatment given to the sellers, and the response is whether the question prompted them to disclose the freezing issue with the iPod. The results are shown in Table 6.6, and the data suggest that asking the, What problems does it have?, was the most effective at getting the seller to disclose the past freezing issues. However, you should also be asking yourself: could we see these results due to chance alone, or is this in fact evidence that some questions are more effective for getting at the truth? Table 6.6: Summary of the iPod study, where a question was posed to the study participant who acted. General Positive Assumptions Negative Assumptions Total Disclose Problem 2 23 36 61 Hide Problem 71 50 37 158 Total 73 73 73 219 The hypothesis test for the iPod experiment is really about assessing whether there is statistically significant evidence that there was a difference in the success rates that each question had on getting the participant to disclose the problem with the iPod. In other words, the goal is to check whether the buyer’s question was independent of whether the seller disclosed a problem. Expected counts in two-way tables While we would not expect the number of disclosures to be exactly the same across the three groups, the rate of disclosure seems substantially different across the three groups. In order to investigate whether the differences in rates is due to natural variability or due to a treatment effect (i.e., the question causing the differences), we need to compute estimated counts for each cell in a two-way table. From the experiment, we can compute the proportion of all sellers who disclosed the freezing problem as \\(61/219 = 0.2785\\). If there really is no difference among the questions and 27.85% of sellers were going to disclose the freezing problem no matter the question that was put to them, how many of the 73 people in the General group would we have expected to disclose the freezing problem? We would predict that \\(0.2785 \\times 73 = 20.33\\) sellers would disclose the problem. Obviously we observed fewer than this, though it is not yet clear if that is due to chance variation or whether that is because the questions vary in how effective they are at getting to the truth. If the questions were actually equally effective, meaning about 27.85% of respondents would disclose the freezing issue regardless of what question they were asked, about how many sellers would we expect to the freezing problem from the Positive Assumption group?85 We can compute the expected number of sellers who we would expect to disclose or hide the freezing issue for all groups, if the questions had no impact on what they disclosed, using the same strategies employed in the previous Example and Guided Practice to computed expected counts. These expected counts were used to construct Table 6.7, which is the same as Table 6.6, except now the expected counts have been added in parentheses. Table 6.7: The observed counts and the (expected counts). General Positive Assumptions Negative Assumptions Total Disclose Problem 2 (20.33) 23 (20.33) 36 (20.33) 61 Hide Problem 71 (52.67) 50 (52.67) 37 (52.67) 158 Total 73 73 73 219 The examples and exercises above provided some help in computing expected counts. In general, expected counts for a two-way table may be computed using the row totals, column totals, and the table total. For instance, if there was no difference between the groups, then about 27.85% of each column should be in the first row: \\[\\begin{align*} 0.2785\\times (\\text{column 1 total}) &amp;= 20.33 \\\\ 0.2785\\times (\\text{column 2 total}) &amp;= 20.33 \\\\ 0.2785\\times (\\text{column 3 total}) &amp;= 20.33 \\end{align*}\\] Looking back to how 0.2785 was computed – as the fraction of sellers who disclosed the freezing issue (\\(158/219\\)) – these three expected counts could have been computed as \\[\\begin{align*} \\left(\\frac{\\text{row 1 total}}{\\text{table total}}\\right) \\text{(column 1 total)} &amp;= 20.33 \\\\ \\left(\\frac{\\text{row 1 total}}{\\text{table total}}\\right) \\text{(column 2 total)} &amp;= 20.33 \\\\ \\left(\\frac{\\text{row 1 total}}{\\text{table total}}\\right) \\text{(column 3 total)} &amp;= 20.33 \\end{align*}\\] This leads us to a general formula for computing expected counts in a two-way table when we would like to test whether there is strong evidence of an association between the column variable and row variable. Computing expected counts in a two-way table. To identify the expected count for the \\(i^{th}\\) row and \\(j^{th}\\) column, compute \\[\\begin{align*} \\text{Expected Count}_{\\text{row }i,\\text{ col }j} = \\frac{(\\text{row $i$ total}) \\times (\\text{column $j$ total})}{\\text{table total}} \\end{align*}\\] The chi-square statistic Observed data The chi-square test statistic for a two-way table is found by comparing the observed and expected counts for each cell in the table. For each table count, compute: \\[\\begin{align*} &amp;\\text{General formula} &amp;&amp; \\frac{(\\text{observed count } - \\text{expected count})^2} {\\text{expected count}} \\\\ &amp;\\text{Row 1, Col 1} &amp;&amp; \\frac{(2 - 20.33)^2}{20.33} = 16.53 \\\\ &amp;\\text{Row 1, Col 2} &amp;&amp; \\frac{(23 - 20.33)^2}{20.33} = 0.35 \\\\ &amp; \\hspace{9mm}\\vdots &amp;&amp; \\hspace{13mm}\\vdots \\\\ &amp;\\text{Row 2, Col 3} &amp;&amp; \\frac{(37 - 52.67)^2}{52.67} = 4.66 \\end{align*}\\] Adding the computed value for each cell gives the chi-square test statistic \\(X^2\\): \\[\\begin{align*} X^2 = 16.53 + 0.35 + \\dots + 4.66 = 40.13 \\end{align*}\\] Randomization distribution of the chi-square statistic Variability of the statistic Is 40.13 a big number? That is, does it indicate that the observed and expected values are really different? Or is 40.13 a value of the statistic that we’d expect to see just due to natural variability? Previously, we applied the randomization test to the setting where the research question investigated a difference in proportions. The same idea of shuffling the data under the null hypothesis can be used in the setting of the two-way table. Assuming that the individuals would disclose or hide the problems regardless of the question they are given (i.e., that the null hypothesis is true), we can randomize the data by reassigning the 61 disclosed problems and 158 hidden problems to the three groups at random. Table 6.8 shows a possible randomization of the observed data under the condition that the null hypothesis is true (in contrast to the original observed data in Table 6.6). Table 6.8: Randomized allocation of the data to the question posed in the iPod study. General Positive Assumptions Negative Assumptions Total Disclose Problem 15 26 20 61 Hide Problem 58 47 53 158 Total 73 73 73 219 As before, the randomized data is used to find a single value for the test statistic (here a chi-squared statistic). The chi-square statistic for the randomized two-way table is found by comparing the observed and expected counts for each cell in the randomized table. For each cell, compute: \\[\\begin{align*} &amp;\\text{General formula} &amp;&amp; \\frac{(\\text{observed count } - \\text{expected count})^2} {\\text{expected count}} \\\\ &amp;\\text{Row 1, Col 1} &amp;&amp; \\frac{(15 - 20.33)^2}{20.33} = 1.399 \\\\ &amp;\\text{Row 1, Col 2} &amp;&amp; \\frac{(26 - 20.33)^2}{20.33} = 1.579 \\\\ &amp; \\hspace{9mm}\\vdots &amp;&amp; \\hspace{13mm}\\vdots \\\\ &amp;\\text{Row 2, Col 3} &amp;&amp; \\frac{(53 - 52.67)^2}{52.67} = 0.002 \\end{align*}\\] Adding the computed value for each cell gives the chi-square test statistic \\(X^2\\): \\[\\begin{align*} X^2 = 1.399 + 1.579 + \\dots + 0.002 = 4.136 \\end{align*}\\] Observed statistic vs null statistics As before, one randomization will not be sufficient for understanding if the observed data are particularly different from the expected chi-square statistics when \\(H_0\\) is true. To investigate whether 40.13 is large enough to indicate the the observed and expected counts are significantly different, we need to understand what values of the chi-squared statistic would happen just due to change. Figure 6.11 plots 1000 chi-squared statistics generated under the null hypothesis. We can see that the observed value is so far from the null statistics that the simulated p-value is zero. That is, the probability of seeing the observed statistic when the null hypothesis is true is virtually zero. In this case we can conclude that the decision of whether or not to disclose the iPod’s problem is changed by the question asked. (We use the causal language of “changed” because the study was an experiment.) Note that with a chi-squared test, we only know that the two variables (here: question and disclosure) are related (i.e., not independent). We are not able to claim which type of question causes which type of disclosure. Figure 6.11: A stacked dot plot of chi-square statisics from 1000 simulations produced under the null hypothesis, \\(H_0\\), where the question is independent of the disclosure. None of the 1000 simulations had a chi-square value of at least 40.13, the chi-square value observed in the study. Indeed, none of the simulated chi-squared statistics came anywhere close to the observed statistic! 6.3.2 Mathematical model The chi-square test of \\(H_0:\\) independence Variability of the statistic As it turns out, the chi-squared test statistic follows a chi-square distribution when the null hypothesis is true. For two way tables, the degrees of freedom is equal to: \\[\\begin{align*} df = \\text{(number of rows minus 1)}\\times \\text{(number of columns minus 1)} \\end{align*}\\] In our example, the degrees of freedom parameter is \\[\\begin{align*} df = (2-1)\\times (3-1) = 2 \\end{align*}\\] Observed statistic vs. null statistics To bring it back to the example, if the null hypothesis is true (i.e., the questions had no impact on the sellers in the experiment), then the test statistic \\(X^2 = 40.13\\) closely follows a chi-square distribution with 2 degrees of freedom. Using this information, we can compute the p-value for the test, which is depicted in Figure 6.12. Computing degrees of freedom for a two-way table. When applying the chi-square test to a two-way table, we use \\[\\begin{align*} df = (R-1)\\times (C-1) \\end{align*}\\] where \\(R\\) is the number of rows in the table and \\(C\\) is the number of columns. Figure 6.12: Visualization of the p-value for \\(X^2 = 40.13\\) when \\(df = 2\\). The software R can be used to find the p-value with the function pchisq(). Just like pnorm(), pchisq() always gives the area to the left of the cutoff value. Because, in this example, the p-value is represented by the area to the right of 40.13, we subtract the output of pchisq() from 1. 1 - pchisq(40.13, df = 2) #&gt; [1] 1.93e-09 Find the p-value and draw a conclusion about whether the question affects the sellers likelihood of reporting the freezing problem. Using a computer, we can compute a very precise value for the tail area above \\(X^2 = 40.13\\) for a chi-square distribution with 2 degrees of freedom: 0.000000002. Using a significance level of \\(\\alpha=0.05\\), the null hypothesis is rejected since the p-value is smaller. That is, the data provide convincing evidence that the question asked did affect a seller’s likelihood to tell the truth about problems with the iPod. Table 6.9 summarizes the results of an experiment evaluating three treatments for Type 2 Diabetes in patients aged 10-17 who were being treated with metformin. The three treatments considered were continued treatment with metformin (met), treatment with metformin combined with rosiglitazone (rosi), or a lifestyle intervention program. Each patient had a primary outcome, which was either lacked glycemic control (failure) or did not lack that control (success). What are appropriate hypotheses for this test? \\(H_0\\): There is no difference in the effectiveness of the three treatments. \\(H_A\\): There is some difference in effectiveness between the three treatments, e.g., perhaps the rosi treatment performed better than lifestyle. Table 6.9: Results for the Type 2 Diabetes study. Failure Success Total lifestyle 109 125 234 met 120 112 232 rosi 90 143 233 Total 319 380 699 A chi-square test for a two-way table may be used to test the hypotheses in the diabetes Example above. As a first step, compute the expected values for each of the six table cells.86 Note, when analyzing 2-by-2 contingency tables (that is, when both variables only have two possible options), one guideline is to use the two-proportion methods introduced in Section 6.2. 6.4 Chapter 6 review need to expand on the technical condition as the last row. also, is it helpful for the rest of the table to be repeated? Table 6.10: Summary and comparison of Randomization Tests, Bootstrapping, and Mathematical Models as inferential statistical methods. Randomization Test Bootstrapping Mathematical Model What does it do? Shuffles the explanatory variable to mimic the natural variability found in a randomized experiment. Resamples (with replacement) from the observed data to mimic the sampling variability found by collecting data. Uses theory (primarily the Central Limit Theorem) to describe the hypothetical variability resulting from either repeated randomized experiments or random samples. What is the random process described? randomized experiment random sampling either / both Is there flexibility? Yes, can be used to describe random sampling in an observational model Yes, can be used to describe random allocation in an experiment Yes What is it best for? Hypothesis Testing (can be used for Confidence Intervals, but not covered in this text). Confidence Intervals (HT for one proportion covered in Chapter 6). Quick analyses through, for example, calculating a Z score. What physical object represents the simulation process? shuffling cards pulling balls from a bag NA What are the technical conditions? independence independence, big n independence, big n 6.4.1 Terms We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as bolded text. categorical data one-sided hypothesis test pooled proportion success-failure condition confirmation bias parametric bootstrap SE interval two-sided hypothesis test margin of error percentile interval standard error for difference in proportions Type 1 Error null distribution point estimate standard error of single proportion Type 2 Error \\(H_0\\): There is no association between the consultant’s contributions and the clients’ complication rate. In statistical language, \\(p=0.10\\). \\(H_A\\): Patients who work with the consultant tend to have a complication rate lower than 10%, i.e., \\(p&lt;0.10\\).↩︎ There isn’t sufficiently strong evidence to support an association between the consultant’s work and fewer surgery complications.↩︎ No. It might be that the consultant’s work is associated with a reduction but that there isn’t enough data to convincingly show this connection.↩︎ Because the \\(p\\) is unknown but expected to be around 2/3, we will use 2/3 in place of \\(p\\) in the formula for the standard error. \\(SE = \\sqrt{\\frac{p(1-p)}{n}} \\approx \\sqrt{\\frac{2/3 (1 - 2/3)} {300}} = 0.027\\).↩︎ This is equivalent to asking how often the \\(Z\\) score will be larger than -2.58 but less than 2.58. (For a picture, see Figure 6.2.) To determine this probability, look up -2.58 and 2.58 in the normal probability table (0.0049 and 0.9951). Thus, there is a \\(0.9951-0.0049 \\approx 0.99\\) probability that the unobserved random variable \\(X\\) will be within 2.58 standard deviations of the mean.↩︎ Since the necessary conditions for applying the normal model have already been checked for us, we can go straight to the construction of the confidence interval: \\(\\text{point estimate}\\ \\pm\\ 2.58 \\times SE \\rightarrow (0.018, 0.162)\\). We are 99% confident that implanting a stent in the brain of a patient who is at risk of stroke increases the risk of stroke within 30 days by a rate of 0.018 to 0.162 (assuming the patients are representative of the population).↩︎ We must find \\(z^{\\star}\\) such that 90% of the distribution falls between -\\(z^{\\star}\\) and \\(z^{\\star}\\) in the standard normal model, \\(N(\\mu=0, \\sigma=1)\\). We can look up -\\(z^{\\star}\\) in the normal probability table by looking for a lower tail of 5% (the other 5% is in the upper tail), thus \\(z^{\\star}=1.65\\). The 90% confidence interval can then be computed as \\(\\text{point estimate}\\ \\pm\\ 1.65\\times SE \\to (4.4\\%, 13.6\\%)\\). (Note: the conditions for normality had earlier been confirmed for us.) That is, we are 90% confident that implanting a stent in a stroke patient’s brain increased the risk of stroke within 30 days by 4.4% to 13.6%.↩︎ \\(H_0\\): there is not support for the regulation; \\(H_0\\): \\(p \\leq 0.50\\). \\(H_A\\): the majority of borrowers support the regulation; \\(H_A\\): \\(p &gt; 0.50\\).↩︎ Independence holds since the poll is based on a random sample. The success-failure condition also holds, which is checked using the null value (\\(p_0 = 0.5\\)) from \\(H_0\\): \\(np_0 = 826 \\times 0.5 = 413\\), \\(n(1 - p_0) = 826 \\times 0.5 = 413\\). Recall that here, the best guess for \\(p\\) is \\(p_0\\) which comes from the null hypothesis (because we assume the null hypothesis is true when performing the testing procedure steps). \\(H_0\\): there is not support for the regulation; \\(H_0\\): \\(p \\leq 0.50\\). \\(H_A\\): the majority of borrowers support the regulation; \\(H_A\\): \\(p &gt; 0.50\\).↩︎ The study is an experiment, as patients were randomly assigned an experiment group. Since this is an experiment, the results can be used to evaluate a causal relationship between the malaria vaccine and whether patients showed signs of an infection.↩︎ This reasoning does not generally extend to anecdotal observations. Each of us observes incredibly rare events every day, events we could not possibly hope to predict. However, in the non-rigorous setting of anecdotal evidence, almost anything may appear to be a rare event, so the idea of looking for rare events in day-to-day activities is treacherous. For example, we might look at the lottery: there was only a 1 in 292 million chance that the Powerball numbers for the largest jackpot in history (January 13th, 2016) would be (04, 08, 19, 27, 34) with a Powerball of (10), but nonetheless those numbers came up! However, no matter what numbers had turned up, they would have had the same incredibly rare odds. That is, any set of numbers we could have observed would ultimately be incredibly rare. This type of situation is typical of our daily lives: each possible event in itself seems incredibly rare, but if we consider every alternative, those outcomes are also incredibly rare. We should be cautious not to misinterpret such anecdotal evidence.↩︎ Making a Type 1 Error in this context would mean that reminding students that money not spent now can be spent later does not affect their buying habits, despite the strong evidence (the data suggesting otherwise) found in the experiment. Notice that this does not necessarily mean something was wrong with the data or that we made a computational mistake. Sometimes data simply point us to the wrong conclusion, which is why scientific studies are often repeated to check initial findings.↩︎ To lower the Type 2 Error rate, we want to convict more guilty people. We could lower the standards for conviction from “beyond a reasonable doubt” to “beyond a little doubt”. Lowering the bar for guilt will also result in more wrongful convictions, raising the Type 1 Error rate.↩︎ B\\(\\ddot{\\text{o}}\\)ttiger et al. “Efficacy and safety of thrombolytic therapy after initially unsuccessful cardiopulmonary resuscitation: a prospective clinical trial.” The Lancet, 2001.↩︎ Observed control survival rate: \\(p_c = \\frac{11}{50} = 0.22\\). Treatment survival rate: \\(p_t = \\frac{14}{40} = 0.35\\). Observed difference: \\(\\hat{p}_t - \\hat{p}_c = 0.35 - 0.22 = 0.13\\).↩︎ This is an experiment. Patients were randomized to receive mammograms or a standard breast cancer exam. We will be able to make causal conclusions based on this study.↩︎ \\(H_0\\): the breast cancer death rate for patients screened using mammograms is the same as the breast cancer death rate for patients in the control, \\(p_{mgm} - p_{ctrl} = 0\\). \\(H_A\\): the breast cancer death rate for patients screened using mammograms is different than the breast cancer death rate for patients in the control, \\(p_{mgm} - p_{ctrl} \\neq 0\\).↩︎ For an example of a two-proportion hypothesis test that does not require the success-failure condition to be met, see Section 6.2.1.↩︎ For readers not as old as the authors, an iPod is basically an iPhone without any cellular service, assuming it was one of the later generations. Earlier generations were more basic.↩︎ We would expect \\((1 - 0.2785) \\times 73 = 52.67\\). It is okay that this result, like the result from Example , is a fraction.↩︎ The expected count for row one / column one is found by multiplying the row one total (234) and column one total (319), then dividing by the table total (699): \\(\\frac{234\\times 319}{699} = 106.8\\). Similarly for the second column and the first row: \\(\\frac{234\\times 380}{699} = 127.2\\). Row 2: 105.9 and 126.1. Row 3: 106.3 and 126.7.↩︎ "],
["inference-num.html", "Chapter 7 Inference for numerical data 7.1 One mean 7.2 Paired difference 7.3 Difference of two means 7.4 Comparing many means with ANOVA 7.5 Chapter 7 review", " Chapter 7 Inference for numerical data The content in this chapter is currently just placeholder. We will remove this banner once the chapter content has been updated and ready for review. Focusing now on Statistical Inference for numerical data, again, we will revisit and expand upon the foundational aspects of hypothesis testing from Chapter 5. The important data structure for this chapter is a numeric response variable (that is, the outcome is quantitative). The four data structures we detail are one numeric response variable, one numeric response variable which is a difference across a pair of observations, a numeric response variable broken down by a binary explanatory variable, and a numeric response variable broken down by an explanatory variable that has two or more levels. When appropriate, each of the data structures will be analyzed using the three methods from Chapter 5: randomization test, bootstrapping, and mathematical models. As we build on the inferential ideas, we will visit new foundational concepts in statistical inference. One key new idea rests in estimating how the sample mean (as opposed to the sample proportion) varies from sample to sample; the resulting value is referred to as the standard error of the mean. We will also introduce a new important mathematical model, the \\(t\\)-distribution (as the foundation for the \\(t\\)-test). In this chapter, we focus on the sample mean (instead of, for example, the sample median or the range of the observations) because of the well-studied mathematical model which describes the behavior of the sample mean. We will not cover mathematical models which describe other statistics, but the bootstrap and randomization techniques described below are immediately extendable to any function of the observed data. The sample mean will be calculated in one group, two paired groups, two independent groups, and many groups settings. The techniques described for each setting will vary slightly, but you will be well served to find the structural similarities across the different settings. 7.1 One mean Similar to how we can model the behavior of the sample proportion \\(\\hat{p}\\) using a normal distribution, the sample mean \\(\\bar{x}\\) can also be modeled using a normal distribution when certain conditions are met. However, we’ll soon learn that a new distribution, called the \\(t\\)-distribution, tends to be more useful when working with the sample mean. We’ll first learn about this new distribution, then we’ll use it to construct confidence intervals and conduct hypothesis tests for the mean. 7.1.1 Bootstrap confidence interval for \\(\\mu\\) As an employer who subsidizes housing for your employees, you need to know the average month rental price for a three bedroom flat in Edinburgh. In order to walk through the example more clearly, let’s say that you are only able to randomly sample five Edinburgh flats (if this were a real example, you would surely be able to take a much larger sample size, possibly even being able to measure the entire population!). Observed data Figure 7.1 presents the details of the random sample of observations where the monthly rent of five flats has been recorded. Figure 7.1: 5 flats The sample average monthly rent of £ 1648 is a first guess at the price of three bedroom flats. However, as a student of statistics, you understand that one sample mean based on a sample of five observations will not necessarily equal the true population average rent for all three bedroom flats in Edinburgh. Indeed, you can see that the observed rent prices vary with a standard deviation of 340.232, and surely the average monthly rent would be different if a different sample of size five had been taken from the population. Fortunately, as it did in previous chapters for the sample proportion, bootstrapping will approximate the variability of the sample mean from sample to sample. Variability of the statistic As with the inferential ideas covered in Chapter 5, the inferential analysis methods in this chapter are grounded in quantifying how one dataset differs from another when they are both taken from the same population. Again, it still doesn’t make sense to to take repeated samples from the same population (same reasoning: if you have the means to take more samples, a larger sample size will benefit you more than then exact same sample twice). Just like with proportions, we are going to use the observed data to Most of the inferential procedures covered in this text are grounded in quantifying how one data set would differ from another when they are both taken from the same population. It doesn’t make sense to take repeated samples from the same population because if you have the means to take more samples, a larger sample size will benefit you more than the exact same sample twice. Instead, we measure how the samples behave under an estimate of the population. Figure 7.2 shows how the unknown original population can be estimated by using the sample to approximate the distribution of need to fill in the example here Figure 7.2: first figure with the ? pop, then sample, then estimate of the pop. By taking repeated samples from the estimated population, the variability from sample to sample can be observed. In Figure 5.9 the repeated bootstrap samples are obviously different both from each other and from the original population. Recall that the bootstrap samples were taken from the same (estimated) population, and so the differences are due entirely to natural variability in the sampling procedure. Figure 7.3: next fig, has the bootstrap samples By summarizing each of the bootstrap samples (here, using the sample mean), we see, directly, the variability of the sample mean, \\(\\bar{x}\\), from sample to sample. The distribution of \\(\\hat{x}_{bs}\\) for the Edinburgh flats is shown in Figure 7.4. after the plot is made, describe the actual BS samples Figure 7.4: WITH ADDED HISTOGRAM… boot samples, arrow, histogram of all of them add the sampling with replacement part (????) Figure ?? summarizes one thousand bootstrap samples in a histogram of the bootstrap sample means. The bootstrapped average rent prices vary from £ 1250 to £ 1995 (with a small observed sample of size 5, a bootstrap resample can sometimes, although rarely, include only repeated measurements of the same observation). The bootstrap confidence interval is found by locating the middle 90% (for a 90% confidence interval) or a 95% (for a 95% confidence interval) of the bootstrapped statistics. Using Figure ??, find the 90% and 95% confidence intervals for the true mean monthly rental price of a three bedroom flat in Edinburgh. The SE of the bootstrapped means measures how variable the means are from resample to resample. The bootstrap SE is a good approximation to the SE of means as if we had taken repeated samples from the original population (which we agreed isn’t something we would do because of wasted resources). Logistically, we can find the standard deviation of the bootstrapped means using the same calculations from Chapter 2. That is, the bootstrapped means are the individual observations about which we measure the variability. It turns out that the standard deviation of the bootstrapped means from Figure ?? is £ 136.9. [Note: in R the calculation was done using the function sd().] The average of the observed prices, the best guess point estimate for \\(\\mu\\), is £ 1648. Find and interpret the confidence interval for \\(\\mu\\) (the true average rental price of flats in Edinbugh) using the Bootstrap SE inverval formula.^[Using the formula for the boostrap SE interval, we find the 95% confidence interval for \\(\\mu\\) is: \\(1648 \\pm 2 \\cdot 136.9 \\rightarrow\\) (£ 1374.2, £ 1921.8) We are 95% confident that the true average rent price for a three bedroom flat in Edinburgh is somewhere between £ 1374.2 and £ 1921.8.] Compare and contrast the two different 95% confidence intervals for \\(\\mu\\) created by finding the percentiles of the bootstrapped means and created by finding the SE of the bootstrapped means. Do you think the intervals should be identical? Percentile interval: (£ 1389.75, £ 1916) SE interval: (£ 1374.2, £ 1921.8) The intervals were created using different methods, so it is not surprising that they are not identical. However, we are pleased to see that the two methods provide very similar interval approximations. The technical details surrounding which data structures are best for percentile intervals and which are best for SE intervals is beyond the scope of this text. However, the larger the samples are, the better the interval estimates will be. Bootstrap confidence interval for \\(\\sigma\\) Suppose that the research question at hand seeks to understand how variable the rental price of the flats are in Edinburgh. That is, your interest is no longer in the average rental price of the flats but in the standard deviation of the rental prices of all three bedroom flats in Edinburgh, \\(\\sigma\\). You may have already realized that the sample standard deviation, \\(s\\), will work as a good point estimate for the parameter of interest: the population standard deviation, \\(\\sigma\\). The point estimate of the five observations is calculated to be \\(s =\\) £ 340.23. While \\(s =\\) £ 340.23 might be a good guess for \\(\\sigma\\), we prefer to have an interval Although there is a mathematical model which describes how \\(s\\) varies from sample to sample, the mathematical model will not be presented in this text. But even without the mathematical model, bootstrapping can be used to find a confidence interval for the parameter \\(\\sigma\\). Describe the bootstrap distribution for the standard deviation shown in Figure 7.5. The distribution is skewed left and centered near £ 340.23, which is the point estimate from the original data. Most observations in this distribution lie between £ 0 and £ 408.1. Using Figure 7.5, find and interpret a 90% confidence interval for the population standard deviation for three bedroom flat prices in Edinburgh.87 Figure 7.5: The original Edinburgh data is bootstrapped 1,000 times. The histogram provides a sense for the variability of the standard deviation of the rent values from sample to sample. Bootstrapping is not a solution to small sample sizes! The example presented above is done for a sample with only five observations. As with analysis techniques that build on mathematical models, bootstrapping works best when a large random sample has been taken from the population. Bootstrapping is a method for capturing the variability of a statistic when the mathematical model is unknown (it is not a method for navigating small samples). As you might guess, the larger the random sample, the more accurately that sample will represent the population of interest. 7.1.2 Mathematical model As with the sample proportion, the variability of the sample mean is well described by the mathematical theory given by the Central Limit Theorem. However, because of missing information about the inherent variability in the population, a \\(t\\)-distribution is used in place of the standard normal when performing hypothesis test or confidence interval analyses. 7.1.2.1 A mathematical distribution of \\(\\bar{x}\\) The sample mean tends to follow a normal distribution centered at the population mean, \\(\\mu\\), when certain conditions are met. Additionally, we can compute a standard error for the sample mean using the population standard deviation \\(\\sigma\\) and the sample size \\(n\\). Central Limit Theorem for the sample mean When we collect a sufficiently large sample of \\(n\\) independent observations from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the sampling distribution of \\(\\bar{x}\\) will be nearly normal with \\[\\begin{align*} &amp;\\text{Mean}=\\mu &amp;&amp;\\text{Standard Error }(SE) = \\frac{\\sigma}{\\sqrt{n}} \\end{align*}\\] Before diving into confidence intervals and hypothesis tests using \\(\\bar{x}\\), we first need to cover two topics: When we modeled \\(\\hat{p}\\) using the normal distribution, certain conditions had to be satisfied. The conditions for working with \\(\\bar{x}\\) are a little more complex, and below, we will discuss how to check conditions for inference using a mathematical model. The standard error is dependent on the population standard deviation, \\(\\sigma\\). However, we rarely know \\(\\sigma\\), and instead we must estimate it. Because this estimation is itself imperfect, we use a new distribution called the \\(t\\)-distribution to fix this problem, which we discuss in Evaluating the two conditions required for modeling \\(\\bar{x}\\) Two conditions are required to apply the Central Limit Theorem for a sample mean \\(\\bar{x}\\): * Independence. The sample observations must be independent, The most common way to satisfy this condition is when the sample is a simple random sample from the population. If the data come from a random process, analogous to rolling a die, this would also satisfy the independence condition. * Normality. When a sample is small, we also require that the sample observations come from a normally distributed population. We can relax this condition more and more for larger and larger sample sizes. This condition is obviously vague, making it difficult to evaluate, so next we introduce a couple rules of thumb to make checking this condition easier. General rule: how to perform the normality check There is no perfect way to check the normality condition, so instead we use two general rules: \\(\\mathbf{n &lt; 30}\\): If the sample size \\(n\\) is less than 30 and there are no clear outliers in the data, then we typically assume the data come from a nearly normal distribution to satisfy the condition. \\(\\mathbf{n \\geq 30}\\): If the sample size \\(n\\) is at least 30 and there are no particularly extreme outliers, then we typically assume the sampling distribution of \\(\\bar{x}\\) is nearly normal, even if the underlying distribution of individual observations is not. In this first course in statistics, you aren’t expected to develop perfect judgement on the normality condition. However, you are expected to be able to handle clear cut cases based on the rules of thumb.88 Consider the following two plots that come from simple random samples from different populations. Their sample sizes are \\(n_1 = 15\\) and \\(n_2 = 50\\). Are the independence and normality conditions met in each case? Each samples is from a simple random sample of its respective population, so the independence condition is satisfied. Let’s next check the normality condition for each using the rule of thumb. The first sample has fewer than 30 observations, so we are watching for any clear outliers. None are present; while there is a small gap in the histogram on the right, this gap is small and 20% of the observations in this small sample are represented in that far right bar of the histogram, so we can hardly call these clear outliers. With no clear outliers, the normality condition is reasonably met. The second sample has a sample size greater than 30 and includes an outlier that appears to be roughly 5 times further from the center of the distribution than the next furthest observation. This is an example of a particularly extreme outlier, so the normality condition would not be satisfied. In practice, it’s typical to also do a mental check to evaluate whether we have reason to believe the underlying population would have moderate skew (if \\(n &lt; 30\\)) or have particularly extreme outliers (\\(n \\geq 30\\)) beyond what we observe in the data. For example, consider the number of followers for each individual account on Twitter, and then imagine this distribution. The large majority of accounts have built up a couple thousand followers or fewer, while a relatively tiny fraction have amassed tens of millions of followers, meaning the distribution is extremely skewed. When we know the data come from such an extremely skewed distribution, it takes some effort to understand what sample size is large enough for the normality condition to be satisfied. Introducing the \\(t\\)-distribution In practice, we cannot directly calculate the standard error for \\(\\bar{x}\\) since we do not know the population standard deviation, \\(\\sigma\\). We encountered a similar issue when computing the standard error for a sample proportion, which relied on the population proportion, \\(p\\). Our solution in the proportion context was to use sample value in place of the population value when computing the standard error. We’ll employ a similar strategy for computing the standard error of \\(\\bar{x}\\), using the sample standard deviation \\(s\\) in place of \\(\\sigma\\): \\[\\begin{align*} SE = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{s}{\\sqrt{n}} \\end{align*}\\] This strategy tends to work well when we have a lot of data and can estimate \\(\\sigma\\) using \\(s\\) accurately. However, the estimate is less precise with smaller samples, and this leads to problems when using the normal distribution to model \\(\\bar{x}\\). We’ll find it useful to use a new distribution for inference calculations called the \\(t\\)-distribution. A \\(t\\)-distribution, shown as a solid line in Figure 7.6, has a bell shape. However, its tails are thicker than the normal distribution’s, meaning observations are more likely to fall beyond two standard deviations from the mean than under the normal distribution. The extra thick tails of the \\(t\\)-distribution are exactly the correction needed to resolve the problem of using \\(s\\) in place of \\(\\sigma\\) in the \\(SE\\) calculation. Figure 7.6: Comparison of a \\(t\\)-distribution and a normal distribution. The \\(t\\)-distribution is always centered at zero and has a single parameter: degrees of freedom. The degrees of freedom {degrees of freedom (\\(df\\))!\\(t\\)-distribution} describes the precise form of the bell-shaped \\(t\\)-distribution. Several \\(t\\)-distributions are shown in Figure 7.7 in comparison to the normal distribution. In general, we’ll use a \\(t\\)-distribution with \\(df = n - 1\\) to model the sample mean when the sample size is \\(n\\). That is, when we have more observations, the degrees of freedom will be larger and the \\(t\\)-distribution will look more like the standard normal distribution; when the degrees of freedom is about 30 or more, the \\(t\\)-distribution is nearly indistinguishable from the normal distribution. Figure 7.7: The larger the degrees of freedom, the more closely the \\(t\\)-distribution resembles the standard normal distribution. Degrees of freedom: df The degrees of freedom describes the shape of the \\(t\\)-distribution. The larger the degrees of freedom, the more closely the distribution approximates the normal model. When modeling \\(\\bar{x}\\) using the \\(t\\)-distribution, use \\(df = n - 1\\). The \\(t\\)-distribution allows us greater flexibility than the normal distribution when analyzing numerical data. In practice, it’s common to use statistical software, such as R, Python, or SAS for these analyses. In R, the function used for calculating probabilities under a \\(t\\)-distribution is pt() (which should seem similar to previous R functions, pnorm() and pchisq()). Don’t forget that with the \\(t\\)-distribution, the degrees of freedom must always be specified! No matter the approach you choose, apply your method using the examples below to confirm your working understanding of the \\(t\\)-distribution. What proportion of the \\(t\\)-distribution with 18 degrees of freedom falls below -2.10? Just like a normal probability problem, we first draw the picture in Figure 7.8 and shade the area below -2.10. Using statistical software, we can obtain a precise value: 0.0250. # using pt() to find probability under the $t$-distribution pt(-2.10, df = 18) #&gt; [1] 0.025 Figure 7.8: The \\(t\\)-distribution with 18 degrees of freedom. The area below -2.10 has been shaded. A \\(t\\)-distribution with 20 degrees of freedom is shown in the top panel of Figure 7.9. Estimate the proportion of the distribution falling above 1.65. With a normal distribution, this would correspond to about 0.05, so we should expect the \\(t\\)-distribution to give us a value in this neighborhood. Using statistical software: 0.0573. Figure 7.9: Top: The \\(t\\)-distribution with 20 degrees of freedom, with the area above 1.65 shaded. Bottom: The \\(t\\)-distribution with 2 degrees of freedom, with the area further than 3 units from 0 shaded. A \\(t\\)-distribution with 2 degrees of freedom is shown in the bottom panel of Figure 7.9. Estimate the proportion of the distribution falling more than 3 units from the mean (above or below). With so few degrees of freedom, the \\(t\\)-distribution will give a more notably different value than the normal distribution. Under a normal distribution, the area would be about 0.003 using the 68-95-99.7 rule. For a \\(t\\)-distribution with \\(df = 2\\), the area in both tails beyond 3 units totals 0.0955. This area is dramatically different than what we obtain from the normal distribution. What proportion of the \\(t\\)-distribution with 19 degrees of freedom falls above -1.79 units? Use your preferred method for finding tail areas.89 One sample \\(t\\)-confidence intervals Let’s get our first taste of applying the \\(t\\)-distribution in the context of an example about the mercury content of dolphin muscle. Elevated mercury concentrations are an important problem for both dolphins and other animals, like humans, who occasionally eat them. Figure 7.10: A Risso’s dolphin. Photo by Mike Baird, www.bairdphotos.com Observed data We will identify a confidence interval for the average mercury content in dolphin muscle using a sample of 19 Risso’s dolphins from the Taiji area in Japan. The data are summarized in Table 7.1. The minimum and maximum observed values can be used to evaluate whether or not there are clear outliers. Table 7.1: Summary of mercury content in the muscle of 19 Risso’s dolphins from the Taiji area. Measurements are in micrograms of mercury per wet gram of muscle (\\(\\mu\\)g/wet g). \\(n\\) \\(\\bar{x}\\) s minimum maximum 19 4.4 2.3 1.7 9.2 Are the independence and normality conditions satisfied for this data set? The observations are a simple random sample, therefore independence is reasonable. The summary statistics in Table 7.1 do not suggest any clear outliers, with all observations are within 2.5 standard deviations of the mean. Based on this evidence, the normality condition seems reasonable. In the normal model, we used \\(z^{\\star}\\) and the standard error to determine the width of a confidence interval. We revise the confidence interval formula slightly when using the \\(t\\)-distribution: \\[\\begin{align*} &amp;\\text{point estimate} \\ \\pm\\ t^{\\star}_{df} \\times SE &amp;&amp;\\to &amp;&amp;\\bar{x} \\ \\pm\\ t^{\\star}_{df} \\times \\frac{s}{\\sqrt{n}} \\end{align*}\\] Using the summary statistics in Table 7.1, compute the standard error for the average mercury content in the \\(n = 19\\) dolphins. We plug in \\(s\\) and \\(n\\) into the formula: \\(SE = s / \\sqrt{n} = 2.3 / \\sqrt{19} = 0.528\\). The value \\(t^{\\star}_{df}\\) is a cutoff we obtain based on the confidence level and the \\(t\\)-distribution with \\(df\\) degrees of freedom. That cutoff is found in the same way as with a normal distribution: we find \\(t^{\\star}_{df}\\) such that the fraction of the \\(t\\)-distribution with \\(df\\) degrees of freedom within a distance \\(t^{\\star}_{df}\\) of 0 matches the confidence level of interest. When \\(n = 19\\), what is the appropriate degrees of freedom? Find \\(t^{\\star}_{df}\\) for this degrees of freedom and the confidence level of 95% The degrees of freedom is easy to calculate: \\(df = n - 1 = 18\\). Using statistical software, we find the cutoff where the upper tail is equal to 2.5%: \\(t^{\\star}_{18} = 2.10\\). The area below -2.10 will also be equal to 2.5%. That is, 95% of the \\(t\\)-distribution with \\(df = 18\\) lies within 2.10 units of 0. Degrees of freedom for a single sample. If the sample has \\(n\\) observations and we are examining a single mean, then we use the \\(t\\)-distribution with \\(df=n-1\\) degrees of freedom. %In our current example, we should use the \\(t\\)-distribution %with \\(df=19-1=18\\) degrees of freedom. %We can generally identify \\(t_{18}^{\\star}\\) %using statistical software. %Alternatively, we could use the \\(t\\)-table in %Appendix . %Generally the value of \\(t^{\\star}_{df}\\) is slightly larger %than what we would get under the normal model with \\(z^{\\star}\\). Compute and interpret the 95% confidence interval for the average mercury content in Risso’s dolphins. We can construct the confidence interval as \\[\\begin{align*} \\bar{x} \\ \\pm\\ t^{\\star}_{18} \\times SE \\quad \\to \\quad 4.4 \\ \\pm\\ 2.10 \\times 0.528 \\quad \\to \\quad (3.29, 5.51) \\end{align*}\\] We are 95% confident the average mercury content of muscles in Risso’s dolphins is between 3.29 and 5.51 \\(\\mu\\)g/wet gram, which is considered extremely high. Finding a \\(t\\)-confidence interval for the mean, \\(\\mu\\). Based on a sample of \\(n\\) independent and nearly normal observations, a confidence interval for the population mean is \\[\\begin{align*} &amp;\\text{point estimate} \\ \\pm\\ t^{\\star}_{df} \\times SE &amp;&amp;\\to &amp;&amp;\\bar{x} \\ \\pm\\ t^{\\star}_{df} \\times \\frac{s}{\\sqrt{n}} \\end{align*}\\] where \\(\\bar{x}\\) is the sample mean, \\(t^{\\star}_{df}\\) corresponds to the confidence level and degrees of freedom \\(df\\), and \\(SE\\) is the standard error as estimated by the sample. The FDA’s webpage provides some data on mercury content of fish. Based on a sample of 15 croaker white fish (Pacific), a sample mean and standard deviation were computed as 0.287 and 0.069 ppm (parts per million), respectively. The 15 observations ranged from 0.18 to 0.41 ppm. We will assume these observations are independent. Based on the summary statistics of the data, do you have any objections to the normality condition of the individual observations?90 Estimate the standard error of \\(\\bar{x} = 0.287\\) ppm using the data summaries in the previous Guided Practice. If we are to use the \\(t\\)-distribution to create a 90% confidence interval for the actual mean of the mercury content, identify the degrees of freedom and \\(t^{\\star}_{df}\\). The standard error: \\(SE = \\frac{0.069}{\\sqrt{15}} = 0.0178\\). Degrees of freedom: \\(df = n - 1 = 14\\). Since the goal is a 90% confidence interval, we choose \\(t_{14}^{\\star}\\) so that the two-tail area is 0.1: \\(t^{\\star}_{14} = 1.76\\). Using the information and results of the previous Guided Practice and Example, compute a 90% confidence interval for the average mercury content of croaker white fish (Pacific).91 The 90% confidence interval from the previous Guided Practice is 0.256 ppm to 0.318 ppm. Can we say that 90% of croaker white fish (Pacific) have mercury levels between 0.256 and 0.318 ppm?92 One sample \\(t\\)-tests Now that we’ve used the \\(t\\)-distribution for making a confidence intervals for a mean, let’s speed on through to hypothesis tests for the mean. Is the typical US runner getting faster or slower over time? We consider this question in the context of the Cherry Blossom Race, which is a 10-mile race in Washington, DC each spring. The average time for all runners who finished the Cherry Blossom Race in 2006 was 93.29 minutes (93 minutes and about 17 seconds). We want to determine using data from 100 participants in the 2017 Cherry Blossom Race whether runners in this race are getting faster or slower, versus the other possibility that there has been no change. What are appropriate hypotheses for this context?93 The data come from a simple random sample of all participants, so the observations are independent. However, should we be worried about the normality condition? See Figure 7.11 for a histogram of the differences and evaluate if we can move forward.94 Figure 7.11: A histogram of time for the sample Cherry Blossom Race data. When completing a hypothesis test for the one-sample mean, the process is nearly identical to completing a hypothesis test for a single proportion. First, we find the Z score using the observed value, null value, and standard error; however, we call it a T score since we use a \\(t\\)-distribution for calculating the tail area. Then we finding the p-value using the same ideas we used previously: find the one-tail area under the sampling distribution, and double it. With both the independence and normality conditions satisfied, we can proceed with a hypothesis test using the \\(t\\)-distribution. The sample mean and sample standard deviation of the sample of 100 runners from the 2017 Cherry Blossom Race are 97.32 and 16.98 minutes, respectively. Recall that the sample size is 100 and the average run time in 2006 was 93.29 minutes. Find the test statistic and p-value. What is your conclusion? To find the test statistic (T score), we first must determine the standard error: \\[\\begin{align*} SE = 16.98 / \\sqrt{100} = 1.70 \\end{align*}\\] Now we can compute the using the sample mean (97.32), null value (98.29), and \\(SE\\): \\[\\begin{align*} T = \\frac{97.32 - 93.29}{1.70} = 2.37 \\end{align*}\\] For \\(df = 100 - 1 = 99\\), we can determine using statistical software (or a \\(t\\)-table, see below) that the one-tail area is 0.01, which we double to get the p-value: 0.02. Because the p-value is smaller than 0.05, we reject the null hypothesis. That is, the data provide strong evidence that the average run time for the Cherry Blossom Run in 2017 is different than the 2006 average. Since the observed value is above the null value and we have rejected the null hypothesis, we would conclude that runners in the race were slower on average in 2017 than in 2006. # using pt() to find the p-value 1 - pt(2.37, df = 99) #&gt; [1] 0.00986 When using a \\(t\\)-distribution, we use a T score (same as Z score). To help us remember to use the \\(t\\)-distribution, we use a \\(T\\) to represent the test statistic, and we often call this a T score. The Z score and T score are computed in the exact same way and are conceptually identical: each represents how many standard errors the observed value is from the null value. 7.2 Paired difference 7.2.1 case study 7.2.2 Randomization test for \\(H_0: \\mu_d = 0\\) for randomization, idea of coin flipping Observed data Variability of the statistic Observed statistic vs. null statistics 7.2.3 Bootstrap confidence interval for \\(\\mu_d\\) for bootstrap and mathematical model there is not much to do here except go through a full example. tie the ideas back to the one-sample problem. Observed data Variability of the statistic 7.2.4 Mathematical model for bootstrap and mathematical model there is not much to do here except go through a full example. tie the ideas back to the one-sample problem. Observed data Variability of the statistic Observed statistic vs. null statistics 7.3 Difference of two means 7.3.1 case study 7.3.2 Randomization test for \\(H_0: \\mu_1 - \\mu_2 = 0\\) need to talk about the way to randomize is almost identical to chapter 5 &amp; 6. a new plot will probably help (but again, very similar to 5.7) Observed data Variability of the statistic Observed statistic vs. null statistics 7.3.3 Bootstrap confidence interval for \\(\\mu_1 - \\mu_2\\) tie back to idea in chapter 6 for two proportion CI. Observed data Variability of the statistic 7.3.4 Mathematical model t-test. mention that there are lots of nuances outside the scope of this book. Observed data Variability of the statistic Observed statistic vs. null statistics 7.4 Comparing many means with ANOVA Sometimes we want to compare means across many groups. We might initially think to do pairwise comparisons. For example, if there were three groups, we might be tempted to compare the first mean with the second, then with the third, and then finally compare the second and third means for a total of three comparisons. However, this strategy can be treacherous. If we have many groups and do many comparisons, it is likely that we will eventually find a difference just by chance, even if there is no difference in the populations. Instead, we should apply a holistic test to check whether there is evidence that at least one pair groups are in fact different, and this is where ANOVA saves the day. In this section, we will learn a new method called analysis of variance (ANOVA) and a new test statistic called \\(F\\) (which we will introduce in our discussion of mathematical models). ANOVA uses a single hypothesis test to check whether the means across many groups are equal: \\(H_0\\): The mean outcome is the same across all groups. In statistical notation, \\(\\mu_1 = \\mu_2 = \\cdots = \\mu_k\\) where \\(\\mu_i\\) represents the mean of the outcome for observations in category \\(i\\). \\(H_A\\): At least one mean is different. Generally we must check three conditions on the data before performing ANOVA: * the observations are independent within and across groups, * the data within each group are nearly normal, and * the variability across the groups is about equal. When these three conditions are met, we may perform an ANOVA to determine whether the data provide strong evidence against the null hypothesis that all the \\(\\mu_i\\) are equal. College departments commonly run multiple lectures of the same introductory course each semester because of high demand. Consider a statistics department that runs three lectures of an introductory statistics course. We might like to determine whether there are statistically significant differences in first exam scores in these three classes (\\(A\\), \\(B\\), and \\(C\\)). Describe appropriate hypotheses to determine whether there are any differences between the three classes. The hypotheses may be written in the following form: * \\(H_0\\): The average score is identical in all lectures. Any observed difference is due to chance. Notationally, we write \\(\\mu_A=\\mu_B=\\mu_C\\). * \\(H_A\\): The average score varies by class. We would reject the null hypothesis in favor of the alternative hypothesis if there were larger differences among the class averages than what we might expect from chance alone. Strong evidence favoring the alternative hypothesis in ANOVA is described by unusually large differences among the group means. We will soon learn that assessing the variability of the group means relative to the variability among individual observations within each group is key to ANOVA’s success. Examine Figure 7.12. Compare groups I, II, and III. Can you visually determine if the differences in the group centers is due to chance or not? Now compare groups IV, V, and VI. Do these differences appear to be due to chance? Any real difference in the means of groups I, II, and III is difficult to discern, because the data within each group are very volatile relative to any differences in the average outcome. On the other hand, it appears there are differences in the centers of groups IV, V, and VI. For instance, group V appears to have a higher mean than that of the other two groups. Investigating groups IV, V, and VI, we see the differences in the groups’ centers are noticeable because those differences are large relative to the variability in the individual observations within each group. Figure 7.12: Side-by-side dot plot for the outcomes for six groups. Batting case study We would like to discern whether there are real differences between the batting performance of baseball players according to their position: outfielder (OF), infielder (IF), %designated hitter (DH), and catcher (C). We will use a data set called mlb_players_18, which includes batting records of 429 Major League Baseball (MLB) players from the 2018 season who had at least 100 at bats. Six of the 429 cases represented in mlb_players_18 are shown in Figure 7.2, and descriptions for each variable are provided in Figure 7.3. The measure we will use for the player batting performance (the outcome variable) is on-base percentage (OBP). The on-base percentage roughly represents the fraction of the time a player successfully gets on base or hits a home run. Table 7.2: Six cases from the mlb_players_18 data matrix. name team position games AB R H doubles triples HR RBI walks strike_outs stolen_bases caught_stealing_base AVG OBP SLG OPS Abreu, J CWS 1B 128 499 68 132 36 1 22 78 37 109 2 0 0.265 0.325 0.473 0.798 Acuna Jr., R ATL LF 111 433 78 127 26 4 26 64 45 123 16 5 0.293 0.366 0.552 0.917 Adam, J KC P 1 0 0 0 0 0 0 0 0 0 0 0 0.000 0.000 0.000 0.000 Adames, W TB SS 85 288 43 80 7 0 10 34 31 95 6 5 0.278 0.348 0.406 0.754 Adams, A WSH P 2 0 0 0 0 0 0 0 0 0 0 0 0.000 0.000 0.000 0.000 Adams, C NYY P 1 0 0 0 0 0 0 0 0 0 0 0 0.000 0.000 0.000 0.000 Table 7.3: Variables and their descriptions for the mlb_players_18 data set. variable description name Player name team The abbreviated name of the player’s team position The player’s primary field position (OF, IF, C) AB Number of opportunities at bat H Number of hits HR Number of home runs RBI Number of runs batted in AVG Batting average, which is equal to H/AB OBP On-base percentage, which is roughly equal to the fraction of times a player gets on base or hits a home run The null hypothesis under consideration is the following: \\(\\mu_{OF} = \\mu_{IF} = %\\mu_{DH} = \\mu_{C}\\). Write the null and corresponding alternative hypotheses in plain language.95 The player positions have been divided into three groups: outfield (OF), infield (IF), and catcher (C). What would be an appropriate point estimate of the on-base percentage by outfielders, \\(\\mu_{OF}\\)? A good estimate of the on-base percentage by outfielders would be the sample average of OBP for just those players whose position is outfield: \\(\\bar{x}_{OF} = 0.320\\). 7.4.1 Randomization test for \\(H_0: \\mu_1 = \\mu_2 = \\ldots = \\mu_k\\) Table 7.4 provides summary statistics for each group. A side-by-side box plot for the on-base percentage is shown in Figure 7.13. Notice that the variability appears to be approximately constant across groups; nearly constant variance across groups is an important assumption that must be satisfied before we consider the ANOVA approach. Table 7.4: Summary statistics of on-base percentage, split by player position. OF IF C Sample size (\\(n_i\\)) 160.000 205.000 64.000 Sample mean (\\(\\bar{x}_i\\)) 0.320 0.318 0.302 Sample SD (\\(s_i\\)) 0.043 0.038 0.038 Figure 7.13: Side-by-side box plot of the on-base percentage for 429 players across four groups. There is one prominent outlier visible in the infield group, but with 154 observations in the infield group, this outlier is not a concern. The largest difference between the sample means is between the catcher and the outfielder positions. Consider again the original hypotheses: \\(H_0\\): \\(\\mu_{OF} = \\mu_{IF} = \\mu_{C}\\) \\(H_A\\): The average on-base percentage (\\(\\mu_i\\)) varies across some (or all) groups. Why might it be inappropriate to run the test by simply estimating whether the difference of \\(\\mu_{C}\\) and \\(\\mu_{OF}\\) is statistically significant at a 0.05 significance level? — The primary issue here is that we are inspecting the data before picking the groups that will be compared. It is inappropriate to examine all data by eye (informal testing) and only afterwards decide which parts to formally test. This is called data snooping or data fishing. Naturally, we would pick the groups with the large differences for the formal test, and this would leading to an inflation in the Type 1 Error rate. To understand this better, let’s consider a slightly different problem. Suppose we are to measure the aptitude for students in 20 classes in a large elementary school at the beginning of the year. In this school, all students are randomly assigned to classrooms, so any differences we observe between the classes at the start of the year are completely due to chance. However, with so many groups, we will probably observe a few groups that look rather different from each other. If we select only these classes that look so different and then perform a formal test, we will probably make the wrong conclusion that the assignment wasn’t random. While we might only formally test differences for a few pairs of classes, we informally evaluated the other classes by eye before choosing the most extreme cases for a comparison. For additional information on the ideas expressed above, we recommend reading about the prosecutor’s fallacy.96 Observed data In the next section we will learn how to use the \\(F\\) statistic to test whether observed differences in sample means could have happened just by chance even if there was no difference in the respective population means. The method of analysis of variance in this context focuses on answering one question: is the variability in the sample means so large that it seems unlikely to be from chance alone? This question is different from earlier testing procedures since we will simultaneously consider many groups, and evaluate whether their sample means differ more than we would expect from natural variation. We call this variability the mean square between groups (\\(MSG\\)), and it has an associated degrees of freedom, \\(df_{G} = k - 1\\) when there are \\(k\\) groups. The \\(MSG\\) can be thought of as a scaled variance formula for means. If the null hypothesis is true, any variation in the sample means is due to chance and shouldn’t be too large. Details of \\(MSG\\) calculations are provided in the footnote.97 However, we typically use software for these computations. The mean square between the groups is, on its own, quite useless in a hypothesis test. We need a benchmark value for how much variability should be expected among the sample means if the null hypothesis is true. To this end, we compute a pooled variance estimate, often abbreviated as the mean square error (\\(MSE\\)), which has an associated degrees of freedom value \\(df_E = n - k\\). It is helpful to think of \\(MSE\\) as a measure of the variability within the groups. Details of the computations of the \\(MSE\\) and a link to an extra online section for ANOVA calculations are provided in the footnote^[Let \\(\\bar{x}\\) represent the mean of outcomes across all groups. Then the sum of squares total (\\(SST\\)) is computed as \\[\\begin{align*} SST = \\sum_{i=1}^{n} \\left(x_{i} - \\bar{x}\\right)^2 \\end{align*}\\] where the sum is over all observations in the data set. Then we compute the sum of squared errors (\\(SSE\\)) in one of two equivalent ways: \\[\\begin{align*} SSE &amp;= SST - SSG \\\\ &amp;= (n_1-1)s_1^2 + (n_2-1)s_2^2 + \\cdots + (n_k-1)s_k^2 \\end{align*}\\] where \\(s_i^2\\) is the sample variance (square of the standard deviation) of the residuals in group \\(i\\). Then the \\(MSE\\) is the standardized form of \\(SSE\\): \\(MSE = \\frac{1}{df_{E}}SSE\\). See additional details on ANOVA calculations for interested readers. Variability of the statistic When the null hypothesis is true, any differences among the sample means are only due to chance, and the \\(MSG\\) and \\(MSE\\) should be about equal. As a test statistic for ANOVA, we examine the fraction of \\(MSG\\) and \\(MSE\\): \\[\\begin{align*} F = \\frac{MSG}{MSE} \\end{align*}\\] The \\(MSG\\) represents a measure of the between-group variability, and \\(MSE\\) measures the variability within each of the groups. For the baseball data, \\(MSG = 0.00803\\) and \\(MSE=0.00158\\). Identify the degrees of freedom associated with MSG and MSE and verify the \\(F\\) statistic is approximately 5.077.98 Observed statistic vs. null statistic We can use the \\(F\\) statistic to evaluate the hypotheses in what is called an F-test. A p-value can be computed from the \\(F\\) statistic using an \\(F\\) distribution, which has two associated parameters: \\(df_{1}\\) and \\(df_{2}\\). For the \\(F\\) statistic in ANOVA, \\(df_{1} = df_{G}\\) and \\(df_{2} = df_{E}\\). An \\(F\\) distribution with 2 and 426 degrees of freedom, corresponding to the \\(F\\) statistic for the baseball hypothesis test, is shown in Figure 7.14. Figure 7.14: An \\(F\\) distribution with \\(df_1=3\\) and \\(df_2=323\\). need a simulation method here where the data gets randomized, the F statistic is calculted, and the p-value is obtained from the histogram. 7.4.2 Mathematical model The ANOVA F-test Variability of the statistic The larger the observed variability in the sample means (\\(MSG\\)) relative to the within-group observations (\\(MSE\\)), the larger \\(F\\) will be and the stronger the evidence against the null hypothesis. Because larger values of \\(F\\) represent stronger evidence against the null hypothesis, we use the upper tail of the distribution to compute a p-value. The F statistic and the F-test. Analysis of variance (ANOVA) is used to test whether the mean outcome differs across 2 or more groups. ANOVA uses a test statistic \\(F\\), which represents a standardized ratio of variability in the sample means relative to the variability within the groups. If \\(H_0\\) is true and the model conditions are satisfied, the statistic \\(F\\) follows an \\(F\\) distribution with parameters \\(df_{1} = k - 1\\) and \\(df_{2} = n - k\\). The upper tail of the \\(F\\) distribution is used to represent the p-value. Observed statistic vs. null statistics The p-value corresponding to the shaded area in Figure 7.14 is equal to about 0.0066. Does this provide strong evidence against the null hypothesis? The p-value is smaller than 0.05, indicating the evidence is strong enough to reject the null hypothesis at a significance level of 0.05. That is, the data provide strong evidence that the average on-base percentage varies by player’s primary field position. Note that the small p-value indicates that there is a significant difference between the average batting averages of the different positions. However, the ANOVA test does not provide a mechanism for knowing which group is driving the significant differences. The follow-up questions surrounding individual group comparisons is called a problem of multiple comparisons and is outside the scope of this text. We encourage you to learn more about multiple comparisons, however, so that additional comparisons after a significant ANOVA test does not lead to undue false positive conclusions. Reading an ANOVA table from software The calculations required to perform an ANOVA by hand are tedious and prone to human error. For these reasons, it is common to use statistical software to calculate the \\(F\\) statistic and p-value. An ANOVA can be summarized in a table very similar to that of a regression summary, which we saw in Chapters 3 and 4. Table 7.5 shows an ANOVA summary to test whether the mean of on-base percentage varies by player positions in the MLB. Many of these values should look familiar; in particular, the \\(F\\)-test statistic and p-value can be retrieved from the last two columns. Table 7.5: ANOVA summary for testing whether the average on-base percentage differs across player positions. Df Sum Sq Mean Sq F value Pr(&gt;F) position 2 0.0161 0.0080 5.08 0.0066 Residuals 426 0.6740 0.0016 a \\(s_{pooled} = 0.040\\) on \\(df = 423\\) Conditions for an ANOVA analysis There are three conditions we must check for an ANOVA analysis: all observations must be independent, the data in each group must be nearly normal, and the variance within each group must be approximately equal. Independence. If the data are a simple random sample, this condition is satisfied. For processes and experiments, carefully consider whether the data may be independent (e.g. no pairing). For example, in the MLB data, the data were not sampled. However, there are not obvious reasons why independence would not hold for most or all observations. Approximately normal. As with one- and two-sample testing for means, the normality assumption is especially important when the sample size is quite small when it is ironically difficult to check for non-normality. A histogram of the observations from each group is shown in Figure 7.15. Since each of the groups we’re considering have relatively large sample sizes, what we’re looking for are major outliers. None are apparent, so this conditions is reasonably met. Figure 7.15: Histograms of OBP for each field position. Constant variance. The last assumption is that the variance in the groups is about equal from one group to the next. This assumption can be checked by examining a side-by-side box plot of the outcomes across the groups, as in Figure 7.13. In this case, the variability is similar in the four groups but not identical. We see in Table 7.4 that the standard deviation doesn’t vary much from one group to the next. Diagnostics for an ANOVA analysis. Independence is always important to an ANOVA analysis. The normality condition is very important when the sample sizes for each group are relatively small. The constant variance condition is especially important when the sample sizes differ between groups. 7.5 Chapter 7 review need to expand on the technical condition as the last row. also, is it helpful for the rest of the table to be repeated? Table 7.6: Summary and comparison of Randomization Tests, Bootstrapping, and Mathematical Models as inferential statistical methods. Randomization Test Bootstrapping Mathematical Model What does it do? Shuffles the explanatory variable to mimic the natural variability found in a randomized experiment. Resamples (with replacement) from the observed data to mimic the sampling variability found by collecting data. Uses theory (primarily the Central Limit Theorem) to describe the hypothetical variability resulting from either repeated randomized experiments or random samples. What is the random process described? randomized experiment random sampling either / both Is there flexibility? Yes, can be used to describe random sampling in an observational model Yes, can be used to describe random allocation in an experiment Yes What is it best for? Hypothesis Testing (can be used for Confidence Intervals, but not covered in this text). Confidence Intervals (HT for one proportion covered in Chapter 6). Quick analyses through, for example, calculating a Z score. What physical object represents the simulation process? shuffling cards pulling balls from a bag NA What are the technical conditions? independence independence, big n independence, big n 7.5.1 Terms We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as bolded text. By looking at the percentile values in Figure 7.5, the middle 90% of the bootstrap standard deviations are given by the 5 percentile (£ 153.9) and 95 percentile (£ 385.6). That is, we are 90% confident that the true standard deviation of rent prices is between £ 153.9 and £ 385.6.↩︎ More nuanced guidelines would consider further relaxing the particularly extreme outlier check when the sample size is very large. However, we’ll leave further discussion here to a future course.↩︎ We want to find the shaded area above -1.79 (we leave the picture to you). The lower tail area has an area of 0.0447, so the upper area would have an area of \\(1 - 0.0447 = 0.9553\\).↩︎ The sample size is under 30, so we check for obvious outliers: since all observations are within 2 standard deviations of the mean, there are no such clear outliers.↩︎ \\(\\bar{x} \\ \\pm\\ t^{\\star}_{14} \\times SE \\ \\to\\ 0.287 \\ \\pm\\ 1.76 \\times 0.0178 \\ \\to\\ (0.256, 0.318)\\). We are 90% confident that the average mercury content of croaker white fish (Pacific) is between 0.256 and 0.318 ppm.↩︎ No, a confidence interval only provides a range of plausible values for a population parameter, in this case the population mean. It does not describe what we might observe for individual observations.↩︎ \\(H_0\\): The average 10-mile run time was the same for 2006 and 2017. \\(\\mu = 93.29\\) minutes. \\(H_A\\): The average 10-mile run time for 2017 was than that of 2006. \\(\\mu \\neq 93.29\\) minutes.↩︎ With a sample of 100, we should only be concerned if there is are particularly extreme outliers. The histogram of the data doesn’t show any outliers of concern (and arguably, no outliers at all).↩︎ \\(H_0\\): The average on-base percentage is equal across the four positions. \\(H_A\\): The average on-base percentage varies across some (or all) groups.↩︎ See, for example, [textbook-prosecutors_fallacy](andrewgelman.com/2007/05/18/the_prosecutors.↩︎ Let \\(\\bar{x}\\) represent the mean of outcomes across all groups. Then the mean square between groups is computed as \\[\\begin{align*} MSG = \\frac{1}{df_{G}}SSG = \\frac{1}{k-1}\\sum_{i=1}^{k} n_{i} \\left(\\bar{x}_{i} - \\bar{x}\\right)^2 \\end{align*}\\] where \\(SSG\\) is called the sum of squares between groups and \\(n_{i}\\) is the sample size of group \\(i\\).↩︎ There are \\(k = 3\\) groups, so \\(df_{G} = k - 1 = 2\\). There are \\(n = n_1 + n_2 + n_3 = 429\\) total observations, so \\(df_{E} = n - k = 426\\). Then the \\(F\\) statistic is computed as the ratio of \\(MSG\\) and \\(MSE\\): \\(F = \\frac{MSG}{MSE} = \\frac{0.00803}{0.00158} = 5.082 \\approx 5.077\\). (\\(F = 5.077\\) was computed by using values for \\(MSG\\) and \\(MSE\\) that were not rounded.)↩︎ "],
["inference-reg.html", "Chapter 8 Inference for regression 8.1 Chapter 8 review", " Chapter 8 Inference for regression Randomization for slope/correlation t-distribution for regression coefficients - test/CI 8.1 Chapter 8 review 8.1.1 Terms We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as bolded text. "],
["probability.html", "Chapter 9 Appendix: Probability", " Chapter 9 Appendix: Probability The content in this chapter is currently just placeholder. We will remove this banner once the chapter content has been updated and ready for review. (Keep same content as before, minus the bit of probability that got moved to categorical EDA) "],
["references.html", "References", " References Chimowitz, Marc I, Michael J Lynn, Colin P Derdeyn, Tanya N Turan, David Fiorella, Bethany F Lane, L Scott Janis, et al. 2011. “Stenting Versus Aggressive Medical Therapy for Intracranial Arterial Stenosis.” New England Journal of Medicine 365 (11): 993–1003. http://www.nejm.org/doi/full/10.1056/NEJMoa1105335. Wickham, Hadley, and others. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. "]
]

[
["index.html", "Montana State Introductory Statistics with R Welcome Textbook overview Statistical computing Acknowledgements", " Montana State Introductory Statistics with R Nicole Carnegie, Stacey Hancock, Elijah Meyer, Jade Schmidt, Melinda Yager 2020-10-12 Welcome We hope readers will take away three ideas from this book in addition to forming a foundation of statistical thinking and methods. Statistics is an applied field with a wide range of practical applications. You don’t have to be a math guru to learn from interesting, real data. Data are messy, and statistical tools are imperfect. However, when you understand the strengths and weaknesses of these tools, you can use them to learn interesting things about the world. Textbook overview This textbook accompanies the curriculum for STAT 216: Introduction to Statistics at Montana State University. The syllabus and other course information can be found on the course webpage. Detailed learning outcomes for the course can be found here. Introduction to data. Data structures, variables, and basic data collection techniques. Exploratory data analysis. Data visualization and summarization for one and two variables, with a taste of probability. Correlation and regression. Visualizing, describing, and quantifying relationships between two quantitative variables. Multiple regression. Descriptive summaries for quantifying the relationship between many variables. Foundations for inference. Case studies are used to introduce the ideas of statistical inference with randomization and simulations. Inference for categorical data. Inference for one or two proportions using simulation and randomization techniques as well as the normal distribution. Inference for numerical data. Inference for one or two means using simulation and randomization techniques as well as the \\(t\\)-distribution. Inference for regression. Inference for a regression slope or correlation using simulation and randomization techniques as well as the \\(t\\)-distribution. Case studies. A series of case studies assigned weekly in this course. Statistical computing STAT 216 and this textbook use R and RStudio for statistical computing. In particular, we use the tidyverse collection of packages designed for doing data science. STAT 216 also has its own R package called catstats, which contains all of the functions for running simulation-based inference in this course. Getting RStudio Students have four options for accessing this free software: Download to your own laptop. (Note R and RStudio will not run on iPad, notebooks, or Chromebooks. If you have one of these devices, see the cloud-based option below.) Download and install R. Download and install RStudio Desktop. Install the catstats package. Use RStudio through the RStudio Cloud. This resource allows you to use RStudio through a web browser. It is free for use, but it does limit you to a certain number of project hours per month. Use RStudio through an MSU virtual machine. Use RStudio in an MSU on-campus computer lab. View this tutorial video on installing R and RStudio if you would like additional installation instructions. Installing catstats To use the R functions in the catstats package, you need to first install the remotes package, and then install catstats from Github. In the RStudio console, run the following commands: install.packages(&quot;remotes&quot;) remotes::install_github(&quot;greenwood-stat/catstats&quot;) If during the installation, it gives you an option to update the more recent versions of packages, type 1 (to choose to install All), then type Yes if it asks if you want to install. You only need to run the installation commands once, but you will need to load the catstats package each time you restart RStudio using the following command: library(catstats) Note that the catstats package will install all of the packages needed to run code in this textbook, so you will not need to load other packages (e.g., openintro) once you load catstats into your R session. Acknowledgements This resource is largely a derivative of the 1st and 2nd editions of the OpenIntro textbook Introductory Statistics with Randomization and Simulation, without which this effort would not have been possible. The authors would also like to thank the Montana State University Library, who generously funded this project. "],
["about-the-authors.html", "About the Authors Montana State University Authors OpenIntro Authors", " About the Authors Montana State University Authors Nicole Carnegie Associate Professor of Statistics nicole.carnegie@montana.edu Stacey Hancock Assistant Professor of Statistics stacey.hancock@montana.edu Elijah Meyer PhD Graduate Student elijah.meyer@montana.edu Jade Schmidt Student Success Coordinator for Statistics jade.schmidt2@montana.edu Melinda Yager Assistant Coordinator for Statistics melinda.yager@montana.edu OpenIntro Authors Mine Çetinkaya-Rundel mine@openintro.org University of Edinburgh, Duke University, RStudio Johanna Hardin jo@openintro.org Pomona College David Diez david@openintro.org Google/YouTube Christopher D Barr Yale School of Management "],
["copyright.html", "Copyright", " Copyright Copyright © 2020. This textbook is available under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported license (CC BY-NC-SA): http://creativecommons.org/licenses/by-nc-sa/3.0/ This textbook was derived from the 1st and 2nd editions of the OpenIntro Introductory Statistics with Randomization and Simulation textbook. Visit the following link for further copyright information: http://www.openintro.org/perm/stat2nd_v1.txt "],
["intro-to-data.html", "Chapter 1 Introduction to data 1.1 Case study: using stents to prevent strokes 1.2 Data basics 1.3 Sampling principles and strategies 1.4 Observational studies 1.5 Experiments 1.6 Scope of inference 1.7 Data in R 1.8 Chapter 1 review", " Chapter 1 Introduction to data Scientists seek to answer questions using rigorous methods and careful observations. These observations—collected from the likes of field notes, surveys, and experiments—form the backbone of a statistical investigation and are called data. Statistics is the study of how best to collect, analyze, and draw conclusions from data, and in this first chapter, we focus on both the properties of data and on the collection of data. Though we were calculating probabilities in the 16th century, and the first US Census was directed by Thomas Jefferson in 17901, the discipline of statistics as we know it came about in the 1800s. Up until the 21st century, the statistical investigation process looked something like this (adapted from Tintle et al. (2016)): Ask a research question. Design a study and collect data. Summarize and visualize the data. Use statistical analysis methods to draw inferences from the data. Communicate the results and answer the research question. Revisit and look forward. With the rise of data science, however, we may not start with a research question, and instead start with a data set2. In this case, the statistical investigation process looks more like the data exploration cycle found in Figure 1.1 taken from Wickham and Grolemund (2017). Figure 1.1: Wickham and Grolemund’s data exploration cycle (2017). In either case, the ideas, concepts, and methods presented in this book will provide you with the tools to work through the statistical investigation process, whether starting with a research question or starting with data. 1.1 Case study: using stents to prevent strokes In this section, we introduce a classic challenge in statistics: evaluating the efficacy of a medical treatment. Terms in this section, and indeed much of this chapter, will all be revisited later in the text. The plan for now is simply to get a sense of the role statistics can play in practice. Here, we will consider an experiment that studies effectiveness of stents in treating patients at risk of stroke (Chimowitz et al. 2011). Stents are small mesh tubes that are placed inside narrow or weak arteries to assist in patient recovery after cardiac events and reduce the risk of an additional heart attack or death. Many doctors have hoped that there would be similar benefits for patients at risk of stroke. We start by writing the principal question the researchers hope to answer: Does the use of stents reduce the risk of stroke? The researchers who asked this question conducted an experiment with 451 at-risk patients. Each volunteer patient was randomly assigned to one of two groups: Treatment group. Patients in the treatment group received a stent and medical management. The medical management included medications, management of risk factors, and help in lifestyle modification. Control group. Patients in the control group received the same medical management as the treatment group, but they did not receive stents. Researchers randomly assigned 224 patients to the treatment group and 227 to the control group. In this study, the control group provides a reference point against which we can measure the medical impact of stents in the treatment group. Researchers studied the effect of stents at two time points: 30 days after enrollment and 365 days after enrollment. The data collected on 5 of these patients are summarized in Table 1.1. Patient outcomes are recorded as stroke or no event, representing whether or not the patient had a stroke during that time period. The stent30 and stent365 data sets from this study can be found in the openintro package. Table 1.1: Results for five patients from the stent study. group 30 days 365 days patient treatment no event no event 1 treatment stroke stroke 2 treatment no event no event 3 treatment no event no event 4 control no event no event 5 Considering data from each of the 451 patients individually would be a long, cumbersome path towards answering the original research question. Instead, performing a statistical data analysis allows us to consider all of the data at once. Table 1.2 summarizes the raw data in a more helpful way. In this table, we can quickly see what happened over the entire study. For instance, to identify the number of patients in the treatment group who had a stroke within 30 days after the treatment, we look in the leftmost column (30 days), at the intersection of treatment and stroke: 33. To identify the number of control patients who did not have a stroke after 365 days after receiving treatment, we look at the rightmost column (365 days), at the intersection of control and no event: 199. Table 1.2: Descriptive statistics for the stent study. 30 days 365 days stroke no event stroke no event treatment 33 191 45 179 control 13 214 28 199 Total 46 405 73 378 The data summarized in this table can also be visualized with a barplot, seen in Figure 1.2: Figure 1.2: Segmented barplot of outcomes in stent study by group and time. Of the 224 patients in the treatment group, 45 had a stroke by the end of the first year. Using these two numbers, compute the proportion of patients in the treatment group who had a stroke by the end of their first year. (Please note: answers to all Guided Practice exercises are provided using footnotes.)3 We can compute summary statistics from the table to give us a better idea of how the impact of the stent treatment differed between the two groups. A summary statistic is a single number summarizing a large amount of data. For instance, the primary results of the study after 1 year could be described by two summary statistics: the proportion of people who had a stroke in the treatment and control groups. Proportion who had a stroke in the treatment (stent) group: \\(45/224 = 0.20 = 20\\%\\). Proportion who had a stroke in the control group: \\(28/227 = 0.12 = 12\\%\\). These two summary statistics are useful in looking for differences in the groups, and we are in for a surprise: an additional 8% of patients in the treatment group had a stroke! This is important for two reasons. First, it is contrary to what doctors expected, which was that stents would reduce the rate of strokes. Second, it leads to a statistical question: do the data show a “real” difference between the groups? This second question is subtle, and is the basis of what we call statistical inference. Suppose you flip a coin 100 times. While the chance a coin lands heads in any given coin flip is 50%, we probably won’t observe exactly 50 heads. This type of fluctuation is part of almost any type of data generating process. It is possible that the 8% difference in the stent study is due to this natural variation. However, the larger the difference we observe (for a particular sample size), the less believable it is that the difference is due to chance. So what we are really asking is the following: is the difference so large that we should reject the notion that it was due to chance? While we don’t yet have our statistical tools to fully address this question on our own, we can comprehend the conclusions of the published analysis: there was compelling evidence of harm by stents in this study of stroke patients. Be careful. Do not generalize the results of this study to all patients and all stents. This study looked at patients with very specific characteristics who volunteered to be a part of this study and who may not be representative of all stroke patients. In addition, there are many types of stents and this study only considered the self-expanding Wingspan stent (Boston Scientific). However, this study does leave us with an important lesson: we should keep our eyes open for surprises. 1.2 Data basics Effective presentation and description of data is a first step in most analyses. This section introduces one structure for organizing data as well as some terminology that will be used throughout this book. 1.2.1 Observations, variables, and data frames Here, we will consider loans offered through the Lending Club, a peer-to-peer lending company. Such data could be used to explore characteristics of people receiving loans from the platform, such as job titles, annual income, or home ownership. Table 1.3 displays six rows of a data set for 50 randomly sampled loans. These observations will be referred to as the loan50 data set. The loan50 data can be found in the openintro package. Each row in the table represents a single loan. The formal name for a row is a case or observational unit. Since there are 50 observational units in our data set, the sample size, denoted by \\(n\\), is 50 (\\(n = 50\\)). The columns represent characteristics of each loan, where each column is referred to as a variable. A variable is something that can be measured on an individual observational unit. Be careful not to confuse summary statistics—calculated from a group of observational units—with variables. For example, the first row represents a loan of $7,500 with an interest rate of 7.34%, where the borrower is based in Maryland (MD) and has an income of $70,000. What is the grade of the first loan in Table 1.3? And what is the home ownership status of the borrower for that first loan? Reminder: for these Guided Practice questions, you can check your answer in the footnote.4 In practice, it is especially important to ask clarifying questions to ensure important aspects of the data are understood. For instance, it is always important to be sure we know what each variable means and its units of measurement. Descriptions of the variables in the loan50 data set are given in Table 1.4. Table 1.3: Six rows from the loan50 data set. loan_amount interest_rate term grade state total_income homeownership 1 22000 10.90 60 B NJ 59000 rent 2 6000 9.92 36 B CA 60000 rent 3 25000 26.30 36 E SC 75000 mortgage 4 6000 9.92 36 B CA 75000 rent 5 25000 9.43 60 B OH 254000 mortgage 6 6400 9.92 36 B IN 67000 mortgage Table 1.4: Variables and their descriptions for the loan50 data set. variable description loan_amount Amount of the loan received, in US dollars. interest_rate Interest rate on the loan, in an annual percentage. term The length of the loan, which is always set as a whole number of months. grade Loan grade, which takes on values A through G and represents the quality of the loan and its likelihood of being repaid. state US state where the borrower resides. total_income Borrower’s total income, including any second income, in US dollars. homeownership Indicates whether the person owns, owns but has a mortgage, or rents. The data in Table 1.3 represent a data frame (or data matrix), which is a convenient and common way to organize data, especially if collecting data in a spreadsheet. Each row of a data frame corresponds to a unique case (observational unit), and each column corresponds to a variable. When recording data, use a data frame unless you have a very good reason to use a different structure. This structure allows new cases to be added as rows or new variables as new columns. The grades for assignments, quizzes, and exams in a course are often recorded in a gradebook that takes the form of a data frame. How might you organize a course’s grade data using a data frame?5 We consider data for 3,142 counties in the United States, which include the name of each county, the state where it resides, its population in 2017, how its population changed from 2010 to 2017, poverty rate, and nine additional characteristics. How might these data be organized in a data frame?6 The data described in the Guided Practice above represent the county data set, which is shown as a data frame in Table 1.5. The variables as well as the variables in the data set that did not fit in Table 1.5 are described in Table 1.6 Table 1.5: Six observations and six variables from the county data set. name state pop2017 pop_change unemployment_rate median_edu Autauga County Alabama 55504 1.48 3.86 some_college Baldwin County Alabama 212628 9.19 3.99 some_college Barbour County Alabama 25270 -6.22 5.90 hs_diploma Bibb County Alabama 22668 0.73 4.39 hs_diploma Blount County Alabama 58013 0.68 4.02 hs_diploma Bullock County Alabama 10309 -2.28 4.93 hs_diploma Table 1.6: Variables and their descriptions for the county data set. variable description name Name of county. state Name of state. pop2000 Population in 2000. pop2010 Population in 2010. pop2017 Population in 2017. pop_change Population change from 2010 to 2017. poverty Percent of population in poverty in 2017. homeownership Homeownership rate, 2006-2010. multi_unit Percent of housing units in multi-unit structures, 2006-2010. unemployment_rate Unemployment rate in 2017. metro Whether the county contains a metropolitan area, taking one of the values yes or no. median_edu Median education level (2013-2017), taking one of the values below_hs, hs_diploma, some_college, or bachelors. per_capita_income Per capita (per person) income (2013-2017). median_hh_income Median household income. smoking_ban Describes whether the type of county-level smoking ban in place in 2010, taking one of the values none, partial, or comprehensive. The county data can be found in the openintro package. 1.2.2 Types of variables Examine the unemployment_rate, pop2017, state, metro, and median_edu variables in the county data set. Each of these variables is inherently different from the others, yet some share certain characteristics. First consider unemployment_rate, which is said to be a quantitative or numerical variable since it can take a wide range of numerical values, and it is sensible to add, subtract, or take averages with those values. On the other hand, we would not classify a variable reporting telephone area codes as quantitative since the average, sum, and difference of area codes doesn’t have any clear meaning. The pop2017 variable is also quantitative, although it seems to be a little different than unemployment_rate. This variable of the population count can only take whole non-negative numbers (0, 1, 2, …). For this reason, the population variable is said to be discrete since it can only take numerical values with jumps. On the other hand, the unemployment rate variable is said to be continuous. The variable state can take up to 51 values after accounting for Washington, DC: AL, AK, …, and WY. Because the responses themselves are categories, state is called a categorical variable, and the possible values are called the variable’s levels . The variable metro is also categorical, but it only has two levels (yes or no). A categorical variable with only two levels is called a binary variable. When working with a generic binary variable, we often call the two possible levels “success” and “failure.” Finally, consider the median_edu variable, which describes the median education level of county residents and takes values below_hs, hs_diploma, some_college, or bachelors in each county. This variable seems to be a hybrid: it is a categorical variable but the levels have a natural ordering. A variable with these properties is called an ordinal variable, while a regular categorical variable without this type of special ordering is called a nominal variable. To simplify analyses, any ordinal variable in this book will be treated as a nominal (unordered) categorical variable. Figure 1.3: Breakdown of variables into their respective types. Data were collected about students in a statistics course. Three variables were recorded for each student: number of siblings, student height, and whether the student had previously taken a statistics course. Classify each of the variables as continuous quantitative, discrete quantitative, or categorical. The number of siblings and student height represent quantitative variables. Because the number of siblings is a count, it is discrete. Height varies continuously, so it is a continuous quantitative variable. The last variable classifies students into two categories—those who have and those who have not taken a statistics course—which makes this variable categorical. An experiment is evaluating the effectiveness of a new drug in treating migraines. A group variable is used to indicate the experiment group for each patient: treatment or control. The num_migraines variable represents the number of migraines the patient experienced during a 3-month period. Classify each variable as either quantitative or categorical?7 1.2.3 Relationships between variables Many analyses are motivated by a researcher looking for a relationship between two or more variables. A social scientist may like to answer some of the following questions: Does a higher than average increase in county population tend to correspond to counties with higher or lower median household incomes? If homeownership is lower than the national average in one county, will the percent of multi-unit structures in that county tend to be above or below the national average? How useful a predictor is median education level for the median household income for US counties? To answer these questions, data must be collected, such as the county data set shown in Table 1.5. Examining summary statistics could provide insights for each of the three questions about counties. Additionally, graphs can be used to visually explore the data. Scatterplots are one type of graph used to study the relationship between two quantitative variables. Figure 1.4 displays the relationship between the variables homeownership and multi_unit, which is the percent of units in multi-unit structures (e.g., apartments, condos). Each point on the plot represents a single county (a single observational unit). For instance, the highlighted dot corresponds to County 413 in the county data set: Chattahoochee County, Georgia, which has 39.4% of units in multi-unit structures and a homeownership rate of 31.3%. The scatterplot suggests a relationship between the two variables: counties with a higher rate of multi-units tend to have lower homeownership rates. We might brainstorm as to why this relationship exists and investigate each idea to determine which are the most reasonable explanations. Figure 1.4: A scatterplot of homeownership versus the percent of units that are in multi-unit structures for US counties. The highlighted dot represents Chattahoochee County, Georgia, which has a multi-unit rate of 39.4% and a homeownership rate of 31.3%. The multi-unit and homeownership rates are said to be associated because the plot shows a discernible pattern. When two variables show some connection with one another, they are called associated variables. Associated variables can also be called dependent variables and vice-versa. Examine the variables in the loan50 data set, which are described in Table 1.4. Create two questions about possible relationships between variables in loan50 that are of interest to you.8 This example examines the relationship between the change in population from 2010 to 2017 and median household income for counties, which is visualized as a scatterplot in Figure 1.5. Are these variables associated? The larger the median household income for a county, the higher the population growth observed for the county. While this trend isn’t true for every county, the trend in the plot is evident. Since there is some relationship between the variables, they are associated. Figure 1.5: A scatterplot showing pop_change against median_hh_income. Owsley County of Kentucky, is highlighted, which lost 3.63% of its population from 2010 to 2017 and had median household income of $22,736. Because there is a downward trend in Figure 1.4—counties with more units in multi-unit structures are associated with lower homeownership—these variables are said to be negatively associated. A positive association is shown in the relationship between the median_hh_income and pop_change variables in Figure 1.5, where counties with higher median household income tend to have higher rates of population growth. If two variables are not associated, then they are said to be independent. That is, two variables are independent if there is no evident relationship between the two. Associated or independent, not both. A pair of variables are either related in some way (associated) or not (independent). No pair of variables is both associated and independent. 1.2.4 Explanatory and response variables When we ask questions about the relationship between two variables, we sometimes also want to determine if the change in one variable causes a change in the other. Consider the following rephrasing of an earlier question about the county data set: If there is an increase in the median household income in a county, does this drive an increase in its population? In this question, we are asking whether one variable affects another. If this is our underlying belief, then median household income is the explanatory variable variable and the population change is the response variable variable in the hypothesized relationship.9 Explanatory and response variables. When we suspect one variable might causally affect another, we label the first variable the explanatory variable and the second the response variable. explanatory variable \\(\\rightarrow\\) might affect \\(\\rightarrow\\) response variable For many pairs of variables, there is no hypothesized relationship, and these labels would not be applied to either variable in such cases. Bear in mind that the act of labeling the variables in this way does nothing to guarantee that a causal relationship exists. A formal evaluation to check whether one variable causes a change in another requires an experiment. 1.2.5 Introducing observational studies and experiments There are two primary types of data collection: observational studies and experiments. We already encountered an experiment in the case study in Section 1.1, and an observational study with the Lending Club data in this section. Researchers perform an observational study when they collect data in a way that does not directly interfere with how the data arise. For instance, researchers may collect information via surveys, review medical or company records, or follow a cohort of many similar individuals to form hypotheses about why certain diseases might develop. In each of these situations, researchers merely observe the data that arise. In general, observational studies can provide evidence of a naturally occurring association between variables, but they cannot by themselves show a causal connection. When researchers want to investigate the possibility of a causal connection, they conduct an experiment. Usually there will be both an explanatory and a response variable. For instance, we may suspect administering a drug will reduce mortality in heart attack patients over the following year. To check if there really is a causal connection between the explanatory variable and the response, researchers will collect a sample of individuals and split them into groups. The individuals in each group are assigned a treatment. When individuals are randomly assigned to a group, the experiment is called a randomized experiment. For example, each heart attack patient in the drug trial could be randomly assigned, perhaps by flipping a coin, into one of two groups: the first group receives a placebo (fake treatment) and the second group receives the drug. Note that the case study in Section 1.1 did not use a placebo. Association \\(\\neq\\) Causation. In general, association does not imply causation, and causation can only be inferred from a randomized experiment. 1.3 Sampling principles and strategies The first step in conducting research is to identify topics or questions that are to be investigated. A clearly laid out research question is helpful in identifying what subjects or cases should be studied and what variables are important. It is also important to consider how data are collected so that they are reliable and help achieve the research goals. 1.3.1 Populations and samples Consider the following three research questions: What is the average mercury content in swordfish in the Atlantic Ocean? Over the last 5 years, what is the average time to complete a degree for Duke undergrads? Does a new drug reduce the risk deaths in patients with severe heart disease? Each research question refers to a target population. In the first question, the target population is all swordfish in the Atlantic ocean, and each fish represents a case. Often times, it is too expensive to collect data for every case in a population. Instead, a sample is taken. A sample represents a subset of the cases and is often a small fraction of the population. For instance, 60 swordfish (or some other number) in the population might be selected, and this sample data may be used to provide an estimate of the population average and answer the research question. For the second and third questions above, identify the target population and what represents an individual case.10 1.3.2 Anecdotal evidence Consider the following possible responses to the three research questions: A man on the news got mercury poisoning from eating swordfish, so the average mercury concentration in swordfish must be dangerously high. I met two students who took more than 7 years to graduate from Duke, so it must take longer to graduate at Duke than at many other colleges. My friend’s dad had a heart attack and died after they gave him a new heart disease drug, so the drug must not work. Each conclusion is based on data. However, there are two problems. First, the data only represent one or two cases. Second, and more importantly, it is unclear whether these cases are actually representative of the population. Data collected in this haphazard fashion are called anecdotal evidence. Anecdotal evidence. Be careful of data collected in a haphazard fashion. Such evidence may be true and verifiable, but it may only represent extraordinary cases. Figure 1.6: In February 2010, some media pundits cited one large snow storm as evidence against global warming. As comedian Jon Stewart pointed out, “It is one storm, in one region, of one country.” Anecdotal evidence typically is composed of unusual cases that we recall based on their striking characteristics. For instance, we are more likely to remember the two people we met who took 7 years to graduate than the six others who graduated in four years. Instead of looking at the most unusual cases, we should examine a sample of many cases that better represent the population. 1.3.3 Sampling from a population We might try to estimate the time to graduation for Duke undergraduates in the last 5 years by collecting a sample of students. All graduates in the last 5 years represent the population, and graduates who are selected for review are collectively called the sample. In general, we always seek to randomly select a sample from a population. The most basic type of random selection is equivalent to how raffles are conducted–where each raffle ticket has an equal chance of being selected. For example, in selecting graduates, we could write each graduate’s name on a raffle ticket and draw 100 tickets. The selected names would represent a random sample of 100 graduates. We pick samples randomly to reduce the chance we introduce biases. Figure 1.7: In this graphic, five graduates are randomly selected from the population (all graduates in the last 5 years) to be included in the sample. Suppose we ask a student who happens to be majoring in nutrition to select several graduates for the study. What kind of students do you think they might collect? Do you think their sample would be representative of all graduates? Perhaps they would pick a disproportionate number of graduates from health-related fields. Or perhaps their selection would be a good representation of the population. When selecting samples by hand, we run the risk of picking a biased sample, even if our bias is unintended. Figure 1.8: Asked to pick a sample of graduates, a nutrition major might inadvertently pick a disproportionate number of graduates from health-related majors. If someone was permitted to pick and choose exactly which graduates were included in the sample, it is entirely possible that the sample could be skewed to that person’s interests, which may be entirely unintentional. This introduces bias into a sampling method. There are three common types of sampling bias we will discuss: Selection bias: the method in how your sample is selected tends to produce samples that either over-represent or under-represent certain portions of the population. Non-response bias: individuals selected for the sample are unwilling or cannot respond. Response bias: individuals selected for the sample respond in a way that does not accurately represent the truth—due to question wording, lack of anonymity, or other issues. A common downfall in survey studies is a convenience sample, where individuals who are easily accessible are more likely to be included in the sample. For instance, if a political survey is done by stopping people walking in the Bronx, this will not represent all of New York City. It is often difficult to discern what sub-population a convenience sample represents. Is a convenience sample an example of selection bias, non-response bias, or response bias?11 Sampling randomly helps resolve selection bias. The most basic random sample is called a simple random sample, and is equivalent to using a raffle to select cases. This means that each case in the population has an equal chance of being included and there is no implied connection between the cases in the sample. Even when people are picked at random, however, caution must be exercised if the non-response rate is high, or if response bias is present. For instance, if only 30% of the people randomly sampled for a survey actually respond, then it is unclear whether the results are representative of the entire population. This non-response bias can produce results in the sample that do not accurately reflect the entire population. Figure 1.9: Due to the possibility of non-response, survey studies may only reach a certain group within the population. It is difficult, and often times impossible, to completely fix this problem. Asking the uninformed. Popular late night host Jimmy Kimmel has a segment on his show called “Lie Witness News,” where Kimmel’s staff take to the streets to ask pedestrians about recent stories in the news. However, these recent stories are not really stories at all—they’re fake. Without fail, those asked always express an opinion, unflinchingly. Why? People do not like to appear as if they don’t know what they’re talking about, so we make up answers. For an entertaining display of this fascinating psychological example of response bias, watch the Coachella 2013 episode of Lie Witness News. We can easily access ratings for products, sellers, and companies through websites. These ratings are based only on those people who go out of their way to provide a rating. If 50% of online reviews for a product are negative, do you think this means that 50% of buyers are dissatisfied with the product? Why or why not?12 1.3.4 Four sampling methods (special topic) Almost all statistical methods are based on the notion of implied randomness. If observational data are not collected in a random framework from a population, these statistical methods—the estimates and errors associated with the estimates—are not reliable. Here we consider four random sampling techniques: simple, stratified, cluster, and multistage sampling. Figures 1.10 and 1.11 provide graphical representations of these techniques. Figure 1.10: Examples of simple random and stratified sampling. In the top panel, simple random sampling was used to randomly select the 18 cases (denoted in red). In the bottom panel, stratified sampling was used: cases were grouped into strata, then simple random sampling was employed to randomly select 3 cases within each stratum. Simple random sampling is probably the most intuitive form of random sampling. Consider the salaries of Major League Baseball (MLB) players, where each player is a member of one of the league’s 30 teams. To take a simple random sample of 120 baseball players and their salaries, we could write the names of that season’s several hundreds of players onto slips of paper, drop the slips into a bucket, shake the bucket around until we are sure the names are all mixed up, then draw out slips until we have the sample of 120 players. In general, a sample is referred to as “simple random” if each case in the population has an equal chance of being included in the final sample and knowing that a case is included in a sample does not provide useful information about which other cases are included. Stratified sampling is a divide-and-conquer sampling strategy. The population is divided into groups called strata. The strata are chosen so that similar cases are grouped together, then a second sampling method, usually simple random sampling, is employed within each stratum. In the baseball salary example, each of the 30 teams could represent a strata, since some teams have a lot more money (up to 4 times as much!). Then we might randomly sample 4 players from each team for our sample of 120 players. Stratified sampling is especially useful when the cases in each stratum are very similar with respect to the outcome of interest. The downside is that analyzing data from a stratified sample is a more complex task than analyzing data from a simple random sample. The analysis methods introduced in this book would need to be extended to analyze data collected using stratified sampling. Why would it be good for cases within each stratum to be very similar? We might get a more stable estimate for the subpopulation in a stratum if the cases are very similar, leading to more precise estimates within each group. When we combine these estimates into a single estimate for the full population, that population estimate will tend to be more precise since each individual group estimate is itself more precise. In a cluster sample, we break up the population into many groups, called clusters. Then we sample a fixed number of clusters and include all observations from each of those clusters in the sample. A multistage sample is like a cluster sample, but rather than keeping all observations in each cluster, we would collect a random sample within each selected cluster. Figure 1.11: Examples of cluster and multistage sampling. In the top panel, cluster sampling was used: data were binned into nine clusters, three of these clusters were sampled, and all observations within these three cluster were included in the sample. In the bottom panel, multistage sampling was used, which differs from cluster sampling only in that we randomly select a subset of each cluster to be included in the sample rather than measuring every case in each sampled cluster. Sometimes cluster or multistage sampling can be more economical than the alternative sampling techniques. Also, unlike stratified sampling, these approaches are most helpful when there is a lot of case-to-case variability within a cluster but the clusters themselves don’t look very different from one another. For example, if neighborhoods represented clusters, then cluster or multistage sampling work best when the neighborhoods are very diverse. A downside of these methods is that more advanced techniques are typically required to analyze the data, though the methods in this book can be extended to handle such data. Suppose we are interested in estimating the malaria rate in a densely tropical portion of rural Indonesia. We learn that there are 30 villages in that part of the Indonesian jungle, each more or less similar to the next, but the distances between the villages is substantial. Our goal is to test 150 individuals for malaria. What sampling method should be employed? A simple random sample would likely draw individuals from all 30 villages, which could make data collection extremely expensive. Stratified sampling would be a challenge since it is unclear how we would build strata of similar individuals. However, cluster sampling or multistage sampling seem like very good ideas. If we decided to use multistage sampling, we might randomly select half of the villages, then randomly select 10 people from each. This would probably reduce our data collection costs substantially in comparison to a simple random sample, and the cluster sample would still give us reliable information, even if we would need to analyze the data with slightly more advanced methods than we discuss in this book. 1.4 Observational studies Data where no treatment has been explicitly applied (or explicitly withheld) is called observational data. For instance, the loan data and county data described in Section 1.2 are both examples of observational data. Observational studies are generally only sufficient to show associations or form hypotheses that can be later checked with experiments. Making causal conclusions based on experiments is often reasonable. However, making the same causal conclusions based on observational data can be treacherous and is not recommended. Indeed, making causal conclusions based on observational data is arguably the most common mistake in our news headlines and social media posts! Suppose an observational study tracked sunscreen use and skin cancer, and it was found that the more sunscreen someone used, the more likely the person was to have skin cancer. Does this mean sunscreen causes skin cancer?13 Some previous research tells us that using sunscreen actually reduces skin cancer risk, so maybe there is another variable that can explain this hypothetical association between sunscreen usage and skin cancer. One important piece of information that is absent is sun exposure. If someone is out in the sun all day, they are more likely to use sunscreen and more likely to get skin cancer. Exposure to the sun is unaccounted for in the simple investigation. Sun exposure is what is called a confounding variable14, which is a variable that is associated with both the explanatory and response variables. While one method to justify making causal conclusions from observational studies is to exhaust the search for confounding variables, there is no guarantee that all confounding variables can be examined or measured. A confounding variable is a variable that is both associated with the explanatory variable, and associated with the response variable. When both these conditions are met, if we observe an association between the explanatory variable and the response variable in the data, we cannot be sure if this association is due to the explanatory variable or the confounding variable—the explanatory and confounding variables are “confounded.” Figure 1.4 shows a negative association between the homeownership rate and the percentage of multi-unit structures in a county. However, it is unreasonable to conclude that there is a causal relationship between the two variables. Suggest a variable that might explain the negative relationship.15 Houndstongue (a noxious weed) is found in abundance on private and public lands that have been grazed by cattle. Houndstongue is rarely found on lands that have been grazed by mountain goats. One investigator concluded that houndstongue infestations could be reduced by importing mountain goats to the infested areas. What is wrong with this conclusion?16 Observational studies come in two forms: prospective and retrospective studies. A prospective study identifies individuals and collects information as events unfold. For instance, medical researchers may identify and follow a group of patients over many years to assess the possible influences of behavior on cancer risk. One example of such a study is the Nurses’ Health Study. Started in 1976 and expanded in 1989, the Nurses’ Health Study has collected data on over 275,000 nurses and is still enrolling participants. This prospective study recruits registered nurses and then collects data from them using questionnaires. Retrospective studies collect data after events have taken place, e.g. researchers may review past events in medical records. Some data sets may contain both prospectively- and retrospectively-collected variables, such as medical studies which gather information on participants’ lives before they enter the study and subsequently collect data on participants throughout the study. 1.5 Experiments Studies where the researchers assign treatments to cases are called experiments. When this assignment includes randomization, e.g., using a coin flip to decide which treatment a patient receives, it is called a randomized experiment. Randomized experiments are fundamentally important when trying to show a causal connection between two variables. 1.5.1 Principles of experimental design Randomized experiments are generally built on four principles: Controlling. Researchers assign treatments to cases, and they do their best to control any other differences in the groups17. For example, when patients take a drug in pill form, some patients take the pill with only a sip of water while others may have it with an entire glass of water. To control for the effect of water consumption, a doctor may instruct every patient to drink a 12 ounce glass of water with the pill. Randomization. Researchers randomize patients into treatment groups to account for variables that cannot be controlled. For example, some patients may be more susceptible to a disease than others due to their dietary habits. Randomizing patients into the treatment or control group helps even out such differences, and it also prevents accidental bias from entering the study. Replication. The more cases researchers observe, the more accurately they can estimate the effect of the explanatory variable on the response. In a single study, we replicate by collecting a sufficiently large sample. Alternatively, a group of scientists may replicate an entire study to verify an earlier finding. Blocking. Researchers sometimes know or suspect that variables, other than the treatment, influence the response. Under these circumstances, they may first group individuals based on this variable into blocks and then randomize cases within each block to the treatment groups. This strategy is often referred to as blocking. For instance, if we are looking at the effect of a drug on heart attacks, we might first split patients in the study into low-risk and high-risk blocks, then randomly assign half the patients from each block to the control group and the other half to the treatment group, as shown in Figure 1.12. This strategy ensures each treatment group has an equal number of low-risk and high-risk patients. Figure 1.12: Blocking using a variable depicting patient risk. Patients are first divided into low-risk and high-risk blocks, then each block is evenly separated into the treatment groups using randomization. This strategy ensures an equal representation of patients in each treatment group from both the low-risk and high-risk categories. It is important to incorporate the first three experimental design principles into any study, and this book describes applicable methods for analyzing data from such experiments. Blocking is a slightly more advanced technique, and statistical methods in this book may be extended to analyze data collected using blocking. 1.5.2 Reducing bias in human experiments Randomized experiments have long been considered to be the gold standard for data collection, but they do not ensure an unbiased perspective into the cause and effect relationship in all cases. Human studies are perfect examples where bias can unintentionally arise. Here we reconsider a study where a new drug was used to treat heart attack patients. In particular, researchers wanted to know if the drug reduced deaths in patients. These researchers designed a randomized experiment because they wanted to draw causal conclusions about the drug’s effect. Study volunteers18 were randomly placed into two study groups. One group, the treatment group, received the drug. The other group, called the control group, did not receive any drug treatment. Put yourself in the place of a person in the study. If you are in the treatment group, you are given a fancy new drug that you anticipate will help you. On the other hand, a person in the other group doesn’t receive the drug and sits idly, hoping her participation doesn’t increase her risk of death. These perspectives suggest there are actually two effects in this study: the one of interest is the effectiveness of the drug, and the second is an emotional effect to (not) taking the drug, which is difficult to quantify. Researchers aren’t usually interested in the emotional effect, which might bias the study. To circumvent this problem, researchers do not want patients to know which group they are in. When researchers keep the patients uninformed about their treatment, the study is said to be blind. But there is one problem: if a patient doesn’t receive a treatment, they will know they’re in the control group. The solution to this problem is to give fake treatments to patients in the control group. A fake treatment is called a placebo, and an effective placebo is the key to making a study truly blind. A classic example of a placebo is a sugar pill that is made to look like the actual treatment pill. Often times, a placebo results in a slight but real improvement in patients. This effect has been dubbed the placebo effect. The patients are not the only ones who should be blinded: doctors and researchers can accidentally bias a study. When a doctor knows a patient has been given the real treatment, they might inadvertently give that patient more attention or care than a patient that they know is on the placebo. To guard against this bias, which again has been found to have a measurable effect in some instances, most modern studies employ a double-blind setup where doctors or researchers who interact with patients are, just like the patients, unaware of who is or is not receiving the treatment.19 Look back to the study in Section 1.1 where researchers were testing whether stents were effective at reducing strokes in at-risk patients. Is this an experiment? Was the study blinded? Was it double-blinded?20 For the study in Section 1.1, could the researchers have employed a placebo? If so, what would that placebo have looked like?21 You may have many questions about the ethics of sham surgeries to create a placebo. These questions may have even arisen in your mind when in the general experiment context, where a possibly helpful treatment was withheld from individuals in the control group; the main difference is that a sham surgery tends to create additional risk, while withholding a treatment only maintains a person’s risk. There are always multiple viewpoints of experiments and placebos, and rarely is it obvious which is ethically “correct”. For instance, is it ethical to use a sham surgery when it creates a risk to the patient? However, if we don’t use sham surgeries, we may promote the use of a costly treatment that has no real effect; if this happens, money and other resources will be diverted away from other treatments that are known to be helpful. Ultimately, this is a difficult situation where we cannot perfectly protect both the patients who have volunteered for the study and the patients who may benefit (or not) from the treatment in the future. 1.6 Scope of inference When statisticians refer to the scope of inference of a study, we are asking two questions: Generalizability: To which population can we generalize these results? Causation: Do these results provide evidence for a causal relationship? The answer to the first question is determined by the sampling method—if we selected our sample randomly, and there are no other sources of sampling bias, then we can reasonably generalize to the population from which the sample was taken. The answer to the second question is determined by the type of study—if the study is a randomized experiment, then it can investigate whether changes in the explanatory variable caused changes in the response variable; in an observational study, one can only investigate associations between the variables. We summarize how to determine a study’s scope of inference in Figure 1.13. Figure 1.13: Determining scope of inference of a study. 1.7 Data in R R is a powerful and open source software tool for working with data. Throughout this text, we provide some guidance on how to use R within the context of the statistical content that is being covered. As educators, we see the value of teaching with modern software to empower students to take optimal advantage of the concepts they are learning. However, we understand the limitations of some educational structures, and we know that not every classroom will be able to implement R alongside the statistical concepts. Generally, we will present the R techniques at the end of each chapter. There are times in the text when the concepts are not distinguishable from the software, and in those cases, we have have provided the R code within the main body of the chapter. We start with an introduction to R, focused on how data sets are structured in R and how the user can work with a data object in R. 1.7.1 Dataframes in R Throughout the text, we will work with many different data sets. Some data sets are pre-loaded into R, some get loaded through R packages, and some data sets will be created by the student. Data sets can be viewed through the RStudio environment, but the data can also be investigated through the notebook features of an RMarkdown file. Consider the data that was described previously in this chapter. We can use the glimpse() function to see the variables included in the data set and their data type. Or, we could use the head() function to see the first few rows of the data set. data(email50) glimpse(email50) #&gt; Rows: 50 #&gt; Columns: 21 #&gt; $ spam &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,… #&gt; $ to_multiple &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,… #&gt; $ from &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… #&gt; $ cc &lt;int&gt; 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,… #&gt; $ sent_email &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,… #&gt; $ time &lt;dttm&gt; 2012-01-04 06:19:16, 2012-02-16 13:10:06, 2012-01-04 08… #&gt; $ image &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… #&gt; $ attach &lt;dbl&gt; 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0,… #&gt; $ dollar &lt;dbl&gt; 0, 0, 0, 0, 9, 0, 0, 0, 0, 23, 4, 0, 3, 2, 0, 0, 0, 0, 0… #&gt; $ winner &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, no, no, yes, no,… #&gt; $ inherit &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… #&gt; $ viagra &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… #&gt; $ password &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 8,… #&gt; $ num_char &lt;dbl&gt; 21.705, 7.011, 0.631, 2.454, 41.623, 0.057, 0.809, 5.229… #&gt; $ line_breaks &lt;int&gt; 551, 183, 28, 61, 1088, 5, 17, 88, 242, 578, 1167, 198, … #&gt; $ format &lt;dbl&gt; 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,… #&gt; $ re_subj &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,… #&gt; $ exclaim_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… #&gt; $ urgent_subj &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… #&gt; $ exclaim_mess &lt;dbl&gt; 8, 1, 2, 1, 43, 0, 0, 2, 22, 3, 13, 1, 2, 2, 21, 10, 0, … #&gt; $ number &lt;fct&gt; small, big, none, small, small, small, small, small, sma… head(email50) #&gt; # A tibble: 6 x 21 #&gt; spam to_multiple from cc sent_email time image attach #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 0 1 0 1 2012-01-04 06:19:16 0 0 #&gt; 2 0 0 1 0 0 2012-02-16 13:10:06 0 0 #&gt; 3 1 0 1 4 0 2012-01-04 08:36:23 0 2 #&gt; 4 0 0 1 0 0 2012-01-04 10:49:52 0 0 #&gt; 5 0 0 1 0 0 2012-01-27 02:34:45 0 0 #&gt; 6 0 0 1 0 0 2012-01-17 10:31:57 0 0 #&gt; # … with 13 more variables: dollar &lt;dbl&gt;, winner &lt;fct&gt;, inherit &lt;dbl&gt;, #&gt; # viagra &lt;dbl&gt;, password &lt;dbl&gt;, num_char &lt;dbl&gt;, line_breaks &lt;int&gt;, #&gt; # format &lt;dbl&gt;, re_subj &lt;dbl&gt;, exclaim_subj &lt;dbl&gt;, urgent_subj &lt;dbl&gt;, #&gt; # exclaim_mess &lt;dbl&gt;, number &lt;fct&gt; Sometimes it is necessary to extract a column or a row from a data set. In R, the $ operator can be used to extract a column from a data set. For example, data$variable would extract the variable column from the data dataframe. When extracted, these columns can be thought of as vectors. With these vectors, if you desired to pull off a specific entry, you could use square brackets ([ ]), with the index (number) of the entry you wish to extract in the brackets. For example, data$variable[2] would extract the second entry (row) of the variable column. Because a dataframe can be (roughly) thought of as a set of many different vectors, you can extract rows and columns from a dataframe using familiar matrix notation (e.g. [row, column]. For example data[i,j] will extract the \\((i,j)^{th}\\) entry of data, data[i, ] will extract the \\(i^{th}\\) row, and data[ , j] will extract the \\(j^{th}\\) column. Notice, when extracting an entire row (or column), you do not need to specify the columns (or rows) you would like, which is why the second entry does not contain a number. email50$num_char #&gt; [1] 21.705 7.011 0.631 2.454 41.623 0.057 0.809 5.229 9.277 17.170 #&gt; [11] 64.401 10.368 42.793 0.451 29.233 9.794 2.139 0.130 4.945 11.533 #&gt; [21] 5.682 6.768 0.086 3.070 26.520 26.255 5.259 2.780 5.864 9.928 #&gt; [31] 25.209 6.563 24.599 25.757 0.409 11.223 3.778 1.493 10.613 0.493 #&gt; [41] 4.415 14.156 9.491 24.837 0.684 13.502 2.789 1.169 8.937 15.829 email50[47,3] #&gt; # A tibble: 1 x 1 #&gt; from #&gt; &lt;dbl&gt; #&gt; 1 1 Table 1.7: Data from the 47th row of the email data set. spam to_multiple from cc sent_email time image attach dollar winner inherit viagra password num_char line_breaks format re_subj exclaim_subj urgent_subj exclaim_mess number 0 1 1 2 0 2012-01-02 14:24:21 0 0 0 no 0 0 0 8.72 185 0 1 0 0 3 small 1.7.2 Tidy structure of data For plotting, analyses, model building, etc., the data should be structured according to certain principles. Hadley Wickham provides a thorough discussion and advice for cleaning up the data in Wickham and others (2014). Tidy data: rows (cases/observational units) and columns (variables). The key is that every row is a case and every column is a variable. No exceptions. Creating tidy data is often not trivial. Within R (really within any type of computing language, Python, SQL, Java, etc.), it is important to understand how to build data using the patterns of the language. Some things to consider: object_name &lt;- anything is a way of assigning anything to the new object_name. object_name &lt;- function_name(data_table, arguments) is a way of using a function to create a new object. object_name &lt;- data_table %&gt;% function_name(arguments) uses chaining syntax as an extension of the ideas of functions. In chaining, the value on the left side of %&gt;% becomes the first argument to the function on the right side. object_name &lt;- data_table %&gt;% function_name(arguments) %&gt;% another_function_name(other_arguments) is extended chaining. %&gt;% is never at the front of the line, it is always connecting one idea with the continuation of that idea on the next line. In R, all functions take arguments in round parentheses (as opposed to subsetting observations or variables from data objects which happen with square parentheses). Additionally, the spot to the left of %&gt;% is always a data table. The pipe syntax should be read as then, %&gt;%. 1.7.3 Using the pipe to chain The pipe syntax (%&gt;%) takes a data frame (or data table) and sends it to the argument of a function. The mapping goes to the first available argument in the function. For example: x %&gt;% f(y) is the same as f(x, y) y %&gt;% f(x, ., z) is the same as f(x,y,z) Pipes are used commonly with functions in the dplyr package (see R examples in Chapter 2) and they allow us to sequentially build data wrangling operations. We’ll start with short pipes and throughout the course build up to longer pipes that perform multiple operations. Consider the data, High School and Beyond survey. Two hundred observations were randomly sampled from the High School and Beyond survey, a survey conducted on high school seniors by the National Center of Education Statistics. Of interest is the proportion of students at each of the two types of school, public and privaate. We use the table command to tabulate how many of each type of school are in the data set. Notice that the same result is produced by the $ command with table and the chaining syntax done with %&gt;%. data(hsb2) table(hsb2$schtyp) #&gt; #&gt; public private #&gt; 168 32 hsb2 %&gt;% select(schtyp) %&gt;% table() #&gt; . #&gt; public private #&gt; 168 32 What if we are interested only in public schools? First, we should take note of another piece of R syntax: the double equal sign. This is the logical test for “is equal to”. In other words, we first determine if school type is equal to public for each of the observations in the data set and filter for those where this is true. # Filter for public schools hsb2_public &lt;- hsb2 %&gt;% filter(schtyp == &quot;public&quot;) We can read this as: “take the hsb2 data frame and pipe it into the filter function. Filter the data for cases where school type is equal to public. Then, assign the resulting data frame to a new object called hsb2 underscore public.” Suppose we are not interested in the actual reading score of students, but instead whether their reading score is below average or at or above average. First, we need to calculate the average reading score with the mean function. This will give us the mean value, 52.23. However, in order to be able to refer back to this value later on, we might want to store it as an object that we can refer to by name. # Calculate average reading score and show the value mean(hsb2$read) #&gt; [1] 52.2 So instead of just printing the result, let’s save it as a new object called avg_read. # Calculate average reading score and store as avg_read avg_read &lt;- mean(hsb2$read) Before we more on, a quick tip: most often you’ll want to do both; see the value and also store it for later use. The approach we used here, running the mean function twice, is redundant. Instead, you can simply wrap your assignment code in parentheses so that R will not only assign the average value of reading test scores to avg read, but it will also print out its value. # Do both (avg_read &lt;- mean(hsb2$read)) #&gt; [1] 52.2 Next we need to determine whether each student is below or at or above average. For example, a reading score of 57 is above average, so is 68, but 44 is below. Obviously, going through each record like this would be tedious and error prone. Instead we can create this new variable with the mutate function from the dplyr package. We start with the data frame, hsb2, and pipe it into mutate, to create a new variable called read_cat (cat for categorical). Note that we are using a new variable name here in order to not overwrite the existing reading score variable. The new variable read_cat will be a column in the existing data frame hsb2. To indicate that the mutate function came from the dplyr package, we use the pacakge::function syntax. It is not usually necessary to provide the package name (unless there is ambiguity about where the function came from). hsb2 &lt;- hsb2 %&gt;% dplyr::mutate(read_cat = ifelse(read &lt; avg_read, &quot;below average&quot;, &quot;at or above average&quot;)) The decision criteria for this new variable is based on a TRUE/FALSE question: if the reading score of the student is below the average reading score, label “below average”, otherwise, label “at or above average”. This can be accomplished using the ifelse function in R. The first argument of the function is the logical test. The second argument is what to do if the result of the logical test is TRUE, in other words, if the student’s score is below the average score, and the last argument is what to do if the result is FALSE. The ifelse function can be used for more complicated discretization rules as well, by nesting many ifelse statements within each other. This is not necessary for this example, but it will come up later in the course. 1.7.4 Interactive R tutorials Navigate the concepts you’ve learned in this chapter in R using the following self-paced tutorials. All you need is your browser to get started! Tutorial 1: Getting Started with Data Tutorial 1 - Lesson 1: Language of data Tutorial 1 - Lesson 2: Types of studies Tutorial 1 - Lesson 3: Sampling strategies and Experimental design Tutorial 1 - Lesson 4: Case study You can also access the full list of tutorials supporting this book here. 1.7.5 R labs Further apply the concepts you’ve learned in this chapter in R with computational labs that walk you through a data analysis case study. Intro to R - Birth rates Full list of labs supporting OpenIntro::Introduction to Modern Statistics 1.8 Chapter 1 review 1.8.1 Terms We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as bolded text. anecdotal evidence convenience sample observational data retrospective study associated data observational study sample barplot data frame observational unit sample bias binary dependent ordinal sample size blind discrete placebo sampling bias blocking double-blind placebo effect selection bias case experiment population simple random sample categorical explanatory variable positive association simple random sampling cluster independent prospective study statistical investigation process cluster sampling level quantitative strata cohort multistage sample randomized experiment stratified sampling confounding variable negative association replicate summary statistic continuous nominal representative treatment group control non-response bias response bias variable control group non-response rate response variable References "],
["eda.html", "Chapter 2 Exploratory data analysis 2.1 Exploring categorical data 2.2 Probability with tables 2.3 Exploring quantitative data 2.4 R: Exploratory data analysis 2.5 Chapter 2 review", " Chapter 2 Exploratory data analysis This chapter focuses on the mechanics and construction of summary statistics and graphs. We use statistical software for generating the summaries and graphs presented in this chapter and book. However, since this might be your first exposure to these concepts, we take our time in this chapter to detail how to create them. Mastery of the content presented in this chapter will be crucial for understanding the methods and techniques introduced in rest of the book. 2.1 Exploring categorical data In this section, we will introduce tables and other basic tools for organizing and analyzing categorical data that are used throughout this book. Table 2.1 displays the first six rows of the email data set containing information on 3,921 emails sent to David Diez’s Gmail account (one of the authors of the OpenIntro textbooks). In this section we will examine whether the presence of numbers, small or large, in an email provides any useful value in classifying email as spam or not spam. Descriptions of all five email variables are given in Table 2.2. The email data can be found in the openintro package.22 Table 2.1: Six rows from the email data set. type num_char line_breaks format number 1 not spam 11.37 202 HTML big 2 not spam 10.50 202 HTML small 3 not spam 7.77 192 HTML small 4 not spam 13.26 255 HTML small 5 not spam 1.23 29 not HTML none 6 not spam 1.09 25 not HTML none Table 2.2: Variables and their descriptions for the email data set. variable description type Whether the email was spam or not spam. num_char The number of characters in the email, in thousands. line_breaks The number of line breaks in the email (does not count text wrapping). format Whether the email was written using HTML (e.g., may have included bolding or active links) or not. number Factor variable saying whether there was no number, a small number (under 1 million), or a big number. 2.1.1 Contingency tables and conditional proportions A summary table for a single categorical variable that reports the number of observations (frequency) in each category is called a frequency table. Table 2.3 is a frequency table for the number variable. If we replaced the counts with percentages or proportions (relative frequencies), the table would be called a relative frequency table. Table 2.3: Frequency table of Number variable. none small big 549 2827 545 Table 2.4 summarizes two variables: type (spam or not spam) and number. A table that summarizes data for two categorical variables in this way is called a contingency table or two-way table. Each value in the table represents the number of times, or frequency a particular combination of variable outcomes occurred. For example, the value 149 corresponds to the number of emails in the data set that are spam and had no number listed in the email. Row and column totals are also included. The row totals provide the total counts across each row (e.g., \\(149 + 168 + 50 = 367\\)), and column totals are total counts down each column. In this textbook, we generally take the convention of putting the categories of the explanatory variable as the columns and the categories of the response variable as the rows (if there exists and explanatory-response relationship between the two variables). Table 2.4: Contingency table of number (cols) and type (rows) variables. number none small big Total spam 400 2659 495 3554 type not spam 149 168 50 367 Total 549 2827 545 3921 We would like to examine whether the presence of numbers, small or large, in an email provides any useful value in classifying email as spam or not spam—that is, is there an association between the variables number and type? To determine if a relationship exists between whether an email is spam or not, and whether the email has no numbers, a small number, or a big number, why isn’t it helpful to compare the number of spam emails across the number categories?23 The proportion of emails that were classified as spam in the data set is \\(3554/3921 = 0.906\\), or about 91%. Let’s compare this unconditional proportion to the conditional proportions of spam within each number category: \\(400/549 = 73\\)% of emails with no numbers are spam; \\(2659/2827 = 94\\)% of emails with small numbers are spam; and \\(495/545 = 91\\)% of emails with big numbers are spam. Since these three conditional proportions differ, we say the variables number and type are associated in this data set. Note that some differ from the overall, or unconditional, proportion of spam emails in the data set—91%. Association between two categorical variables. An unconditional proportion is a proportion measured out of the total sample size. A conditional proportion is a proportion measured out of a subgroup in the sample. If the conditional proportions of a particular outcome (e.g., spam email) within levels of a categorical variable (e.g., whether no number, a small number, or a big number appears in the email) differ across levels, we say those two variables are associated. We can also determine if two categorical variables are associated by checking if any of the conditional proportions of the outcome within categories differ from the overall, or unconditional proportion. Row and column proportions Conditional proportions that condition on a row category are called row proportions; conditional proportions that condition on a column category are called column proportions. Table 2.5 shows the row proportions for Table 2.4. The row proportions are computed as the counts divided by their row totals. The value 149 at the intersection of type and none is replaced by \\(149/367=0.406\\), i.e., 149 divided by its row total, 367. So what does 0.406 represent? It corresponds to the conditional proportion of spam emails in the sample that do not have any numbers. Table 2.5: A contingency table with row proportions for the type and number variables. none small big spam 0.406 0.458 0.136 not spam 0.113 0.748 0.139 A contingency table of the column proportions is computed in a similar way, where each column proportion is computed as the count divided by the corresponding column total. Table 2.6 shows such a table, and here the value 0.271 indicates that 27.1% of emails with no numbers were spam. This rate of spam is much higher than emails with only small numbers (5.9%) or big numbers (9.2%). Because these spam rates vary between the three levels of number (none, small, big), this provides evidence that the spam and number variables are associated in this data set. Table 2.6: A contingency table with column proportions for the type and number variables. none small big spam 0.271 0.059 0.092 not spam 0.729 0.941 0.908 Table 2.7: A contingency table for type and format. not HTML HTML Total spam 209 158 367 not spam 986 2568 3554 Total 1195 2726 3921 The previous Example points out that row and column proportions are not equivalent. Before settling on one form for a table, it is important to consider each to ensure that the most useful table is constructed. Sample proportions and population proportions In the field of statistics, summary measures that summarize a sample of data are called statistics. Numbers that summarize an entire population are called parameters. You can remember this distinction by looking at the first letter of each term: Statistics summarize Samples. Parameters summarize Populations. Proportions calculated from a sample of data are denoted by \\(\\hat{p}\\). In our example, we were interested in the proportion of spam emails in our data set, so we could denote this by \\(\\hat{p} = 0.91\\). If there are different groups we want to summarize with a proportion, we can add subscripts: \\(\\hat{p}_{none} = 0.73\\), \\(\\hat{p}_{small} = 0.94\\), and \\(\\hat{p}_{big} = 0.91\\). Each of these values is a statistic since it is computed from a sample of data. These 3921 emails were a sample from a larger group of emails—all emails that are sent to David Diez, either in the past or in the future. This larger group of emails is the population. There is some unknown value for the proportion of all emails in the population that would be classified as spam, which we denote by \\(\\pi\\). Similarly, there are unknown values for the proportion of all emails with no numbers in the population that would be classified as spam, denoted by \\(\\pi_{none}\\). Each of these unknown values are called parameters. We typically use Roman letters to symbolize statistics (e.g., \\(\\bar{x}\\), \\(\\hat{p}\\)), and Greek letters to symbolize parameters (e.g., \\(\\mu\\), \\(\\pi\\)). Since we rarely can measure the entire population, and thus rarely know the actual parameter values, we like to say, “We don’t know Greek, and we don’t know parameters!” 2.1.2 Bar plots and mosaic plots A bar plot is a common way to display a single categorical variable. The left panel of Figure 2.1 shows a bar plot for the number variable. In the right panel, the counts are converted into proportions (e.g., \\(549/3921=0.140\\) for none). Figure 2.1: Two bar plots of number. The left panel shows the counts, and the right panel shows the proportions in each group. Bar plots are also used to display the relationship between two categorical variables. When the bars are stacked such that each bar totals 100% and is segmented by another categorical variable, it is called a segmented bar plot. A segmented bar plot is a graphical display of contingency table information. For example, segmented bar plots representing Table 2.6 is shown in Figure 2.2, where we have first created a non-standardized segmented bar plot using the number variable and then separated each group by the levels of type. The standardized segmented bar plot using the column proportions of Table 2.6 is a helpful visualization of the fraction of spam emails in each level of number. Figure 2.2: (a) Segmented bar plot for numbers found in emails, where the counts have been further broken down by type. (b) Segmented bar plot using column proportions of each type within each number category. Since the proportion of spam changes across the groups in Figure 2.2 (seen in plot (b)), we can conclude the variables are dependent, which is something we were also able to discern using the column proportions in Table 2.6. Because both the none and big groups have relatively few observations compared to the small group, the association is more difficult to see in plot (a) of Figure 2.2. In some other cases, a segmented bar plot that is not standardized will be more useful in communicating important information. Before settling on a particular segmented bar plot, create standardized and non-standardized forms and decide which is more effective at communicating features of the data. Mosaic plots A mosaic plot is a graphical display of contingency table information that is similar to a bar plot for one variable or a segmented bar plot when using two variables. Figure 2.3 plot (a) shows a mosaic plot for the number variable. Each column represents a level of number, and the column widths correspond to the proportion of emails of each number type. For instance, there are fewer emails with no numbers than emails with only small numbers, so the no number email column is slimmer. In general, mosaic plots use box areas to represent the number of observations. Figure 2.3: (a) Mosaic plot for numbers found in emails. (b) Mosaic plot where the number counts have been further broken down by type. This one-variable mosaic plot is further divided into pieces in Figure 2.3 plot (b) using the type variable. Each column is split proportionally according to the fraction of emails that were spam in each number category. For example, the second column, representing emails with only small numbers, was divided into emails that were spam (lower) and not spam (upper). As another example, the bottom of the third column represents spam emails that had big numbers, and the upper part of the third column represents regular emails that had big numbers. We can again use this plot to see that the type and number variables are associated since some columns are divided in different vertical locations than others, which was the same technique used for checking an association in the standardized version of the segmented bar plot. 2.1.3 Why not pie charts? While pie charts are well known, they are not typically as useful as other charts in a data analysis. A pie chart is shown in Figure 2.4 alongside a bar plot. It is generally more difficult to compare group sizes in a pie chart (comparing angles) than in a bar plot (comparing heights), especially when categories have nearly identical counts or proportions. In the case of the none and big categories, the difference is so slight you may be unable to distinguish any difference in group sizes for either plot! Figure 2.4: A pie chart and bar plot of number for the email data set. This is the only pie chart you will see in this book! Pie charts are nearly useless when trying to compare two categorical variables, as is shown in Figure 2.5. Figure 2.5: Try comparing the distributions of colors across pie charts A, B, and C—it’s impossible!24 If you’re still not convinced that you shouldn’t use pie charts, read “The Issue with Pie Chart” on the “from Data to Viz” blog, and “The Worst Chart in the World” article on Business Insider. 2.1.4 Simpson’s paradox Race and capital punishment A 1991 study by Radelet and Pierce examined whether race was associated with whether the death penalty was invoked in homicide cases25. Table 2.8 and Figure 2.6 summarize data on 674 defendants in indictments involving cases with multiple murders in Florida from 1976 through 1987. Table 2.8: Contingency table of homicide cases in Florida from 1976 through 1987. Defendant’s race Caucasian African American Total Death penalty 53 15 68 Sentence No death penalty 430 176 606 Total 483 191 674 Figure 2.6: Segmented bar plot comparing the proportion of defendants who received the death penalty between Caucasians and African Americans. Is the race of the defendant associated with the sentence of the trial?26 Overall, a lower percentage of African American defendants received the death penalty than Caucasian defendants (8% compared to 11%). Given studies have shown racial bias in sentencing, this may be surprising. Let’s look at the data more closely. Since these are observational data, confounding variables are most likely present. Recall, a confounding variable is one that is associated with both the response variable (sentence) and the explanatory variable (race of the defendant). What confounding variables could be present?27 If we subset the data by the race of the victim, we see a different picture. Table 2.9 and Figure 2.7 summarize the same data, but separately for Caucasian and African American homicide victims. Table 2.9: Contingency table of homicide cases in Florida from 1976 through 1987; sentences classified by defendant’s race and victim’s race. Death Penalty? Victim’s race Defendant’s race Yes No Percent Yes Caucasian Caucasian 53 414 11.3% African American 11 37 22.9% African American Caucasian 0 16 0.0% African American 4 139 2.8% Total Caucasian 53 430 11.0% African American 15 176 7.9% Figure 2.7: Segmented bar plots comparing the proportion of Caucasian and African American defendants who received the death penalty; separate plots for Caucasian victims and African American victims. If we compare Figures 2.6 and 2.7, we see that the direction of the association between the race of the defendant and the sentence is reversed if we subgroup by the race of the victim. Overall, a larger proportion of Caucasians were sentenced to the death penalty than African Americans. However, when we only compare cases with the same victim’s race, a larger proportion of African Americans were sentenced to the death penalty than Caucasians! How did this happen? The answer has to do with the race of the victim being a confounding variable. Figure 2.8 shows two segmented barplots examining the relationship between the race of the victim and the sentence (the response variable), and the relationship between the race of the victim and the race of the defendant (the explanatory variable). We see that the race of the victim is associated with both the response and the explanatory variables: defendants are more likely to involve a victim of the same race, and cases with African American victims are less likely to result in the death penalty. Figure 2.8: The race of the victim is associated both with the sentence (death penalty or no death penalty) and with the race of the defendant. Defendants are more likely to involve a victim of the same race, and cases with African American victims are less likely to result in the death penalty. Thus, the extremely low chance of a homicide case resulting in the death penalty for African Americans combined with the fact that most cases with African American defendants also had an African American victim results in an overall lower rate of death penalty sentences for African American defendants than for Caucasian defendants. The overall results in Figure 2.6 and the results in each subgroup of Figure 2.7 are both valid—they are not the result of any “bad statistics”—but they suggest opposite conclusions. Data such as these, where an observed effect reverses itself when you examine the variables within subgroups, exhibit Simpson’s Paradox. Simpson’s Paradox. When the association between an explanatory variable and a response variable reverses itself when we examine the association within different levels of a confounding variable, we say that these data exhibit Simpson’s Paradox. 2.2 Probability with tables 2.2.1 Defining probability A random process is one in which the outcome is unpredictable. We encounter random processes every day: will it rain today? how many minutes will pass until receiving your next text message? will the Seahawks win the Super Bowl? Though the outcome of one particular random process is unpredictable, if we observe the process many many times, the pattern of outcomes, or its probability distribution, can often be modeled mathematically. Though there are several philosophical definitions of probability, we will use the “frequentist” definition of probability—a long-run relative frequency. Probability. The probability of an event is the long-run proportion of times the event would occur if the random process were repeated indefinitely (under identical conditions). Consider the simple example of flipping a fair coin once. What is the probability the coin lands on heads. From its physical properties, we assume the probability of heads is 0.5, but let’s use simulation to examine the probability. Figure 2.9 shows the long-run proportion of times a simulated coin flip lands on heads on the y-axis, and the number of tosses on the x-axis. Notice how the long-run proportion starts converging to 0.5 as the number of tosses increases. Figure 2.9: One simulation of flipping a fair coin, tracking the long-run proportion of times the coin lands on heads. 2.2.2 Finding probabilities with tables We can solve many real-life probability problems without using any equations by creating a hypothetical two-way table of the scenario. This tool is best demonstrated by an example. As a student at Montana State University, suppose your first class on Mondays is in Wilson Hall at 8:00am and you commute to school. You have a Bobcat parking permit. From past experience, you know that there is a 20% chance of finding an open parking spot in Lot 6 by Animal Bioscience. Otherwise, you have to park in Lot 18 by graduate housing. If you find a spot in Lot 6, you only have a 5% chance of being late to class. However, if you have to park in Lot 18, you have a 15% chance of being late to class. What is the probability that you will be late to class this Monday? There are two random variables in this scenario: whether you park in Lot 6 or Lot 18, and whether or not you are late to class. Since we know probability is a long-run relative frequency, let’s imagine 1000 hypothetical Mondays, and fill in a contingency table with the frequencies we’d expect in each cell. Late to class Not late to class Total Lot 6 10 190 200 Lot 18 120 680 800 Total 130 870 1000 Now we can find the probability of being late to class by reading it off the table: 130/1000 = 0.13. How did we create the table in the last Example? Let’s work through it step-by-step. Identify the unconditional probabilities given in the problem: 20% chance of parking in Lot 6, which means an 80% chance of parking in Lot 18. Take 20% and 80% of 1000 to fill in the row totals: Late to class Not late to class Total Lot 6 1000 \\(\\times\\) 0.20 = 200 Lot 18 1000 \\(\\times\\) 0.80 = 800 Total 1000 Identify the conditional probabilities given in the problem: if you park in Lot 6, the probability of being late to class is 5%; if you park in Log 18, the probability of being late to class is 15%. Fill in the corresponding cells in the table by taking 5% of the times you parked in Lot 6, and 15% of the times you parked in Lot 18: Late to class Not late to class Total Lot 6 200 \\(\\times\\) 0.05 = 10 200 Lot 18 800 \\(\\times\\) 0.15 = 120 800 Total 1000 Use subtraction to fill in the remaining cells for the column “Not late to class.” Use addition to find the column totals. Using the hypothetical two-way table given in the last Example, find the following probabilities: What is the probability you are not late to class? What is the probability that you park in Lot 6 and you are not late to class? Given that you were late to class, what is the probability you parked in Lot 18?28 Carefully read how each of the probabilities is described in the Guided Practice—note the subtle difference between “the probability of being late to class, given that you parked in Lot 18” (\\(120/800 = 0.15\\)) and “the probability of parking in Lot 18, given that you were late to class” (\\(120/130 = 0.923\\)). When we are given extra information, this is called a conditional probability, and the denominator in the probability calculation is a row total (e.g., 800) or column total (e.g., 130) rather than the overall total in the hypothetical two-way table. In the previous Guided Practice, which of the probabilities are conditional probabilities? which are unconditional?29 2.2.3 Probability notation For ease of translating probability problems into calculations, let’s define some notation. We will denote “events” (e.g., being late to class) by upper case letters near the beginning of the alphabet, e.g., \\(A\\), \\(B\\), \\(C\\). The probability of an event \\(A\\) will be denoted by \\(P(A)\\), so \\(P(A)\\) is a number between 0 and 1. The event that \\(A\\) does not happen is called the complement of \\(A\\) and is denoted by \\(P(A^C)\\). Sometimes we have additional information that we would like to condition on, and we denote the conditional probability of \\(A\\) given \\(B\\) by \\(P(A | B)\\)—the probability that \\(A\\) happens given that \\(B\\) has already happened. In our coin flip example, we could let \\(A\\) be the event that the coin lands on heads. Then we can denote the probability that the coin lands on heads by \\(P(A) = 0.5\\). We could flip the coin twice and let \\(H_1\\) be the event that the first flip lands on heads, and \\(H_2\\) be the event that the second flip lands on heads. Since the coin does not remember its last flip, if the first flip lands on heads, the second flip still has a 50% chance of landing on heads. That is, \\(P(H_2 | H_1) = 0.5\\). 2.2.4 Diagnostic testing Medical diagnostic tests for diseases spend years in development. Through clinical trials, developers of the diagnostic test are able to determine two important properties of the test: The sensitivity of a diagnostic test is the probability the test yields a positive result, given the individual has the disease. In other words, what proportion of the diseased population would test positive? The specificity of the diagnostic test is the probability the test yields a negative result, given the individual does not have the disease. That is, what proportion of the non-diseased population would test negative? A good diagnostic test has very high (near 100%) sensitivity and specificity. However, even for a near-perfect test, the probability that you have the disease given you test positive could still be quite low. To investigate this counter-intuitive result, we need another definition: We will call the proportion of the population that has the disease—the probability of contracting the disease—the prevalence (incidence) of a disease. Let \\(D\\) be the event that an individual has the disease and \\(T\\) be the event that an individual tests positive. How would you express each of the following quantities using probability notation? sensitivity specificity prevalence30 Note that sensitivity and specificity are conditional probabilities, while prevalence is an unconditional probability. While the above probabilities are useful information, if you test positive on a diagnostic test, none of these quantities is the probability you really want to know: the conditional probability of having the disease, given you tested positive, \\(P(D | T)\\). The case of Baby Jeff The following case study was presented by Slawson and Shaughnessy (2002). A poster in a hospital’s newborn nursery announced that all male newborns would be screened for muscular dystrophy using a heel stick blood test for creatinine phosphokinase (CPK). The test characteristics of the screening tests were nearly perfect: a sensitivity of 100% and a specificity of 99.98%. The prevalence of muscular dystrophy in male newborns ranges from 1 in 3,500 to 1 in 15,000. Baby Jeff had an abnormal CPK test. The parents of the baby wanted to know, “What is the chance that our son has muscular dystrophy?” Doctors informed the parents that though not 100% likely, it was highly probable. First, take a minute and predict this probability – what do you think? 80% chance? 99% chance? Let’s investigate using a two-way table of a hypothetical population of 100,000 male newborns. For our calculations, let’s use a prevalence of 1 in 10,000. Then out of 100,000 hypothetical male newborns, we would expect 1 in 10,000 to have muscular dystrophy, or 10: \\((1/10000)\\times 100000 = 10\\). The sensitivity of the test is perfect, so all 10 of the male newborns with muscular dystrophy will test positive. Of the \\(100000-10 = 99,990\\) male newborns that do not have muscular dystrophy, 99.98% will test negative: \\((0.9998)\\times 99990 = 99,970\\) infants. That leaves \\(99990 - 99970 = 20\\) male newborns that test positive even though they do not have muscular dystrophy. This allows us to fill in the counts in our hypothetical two-way table: Tests positive Tests negative Total Has muscular dystrophy 10 0 10 Does not have muscular dystrophy 20 99,970 99,990 Total 30 99,970 100,000 Now we can read off the desired probability from the table: of the 30 male newborns we’d expect to test positive, only 10 of them actually have muscular dystrophy. This means the chance Baby Jeff has muscular dystrophy is only about 33%! How would this probability change if the prevalence were 1/3500? 1/15000? Try it.31 Why did this counter-intuitive result occur? With such high sensitivity and specificity, why does this test perform so poorly? The answer has to do with the prevalence of the disease. For a rare disease, the very small proportion that test positive out of the very large group of people without the disease will overwhelm the very large proportion that test positive out of the very small group of people with the disease. The number of false positives can be much higher than the number of true positives. 2.3 Exploring quantitative data In this section we will explore techniques for summarizing quantitative variables. For example, consider the loan_amount variable from the loan50 data set, which represents the loan size for all 50 loans in the data set. This variable is quantitative since we can sensibly discuss the numerical difference of the size of two loans. On the other hand, area codes and zip codes are not quantitative, but rather they are categorical variables. Throughout this section and the next, we will apply these methods using the loan50, county, and email50 data sets, which were introduced in Section 1.2. If you’d like to review the variables from either data set, see Tables 1.4 and 1.6. The loan50 and email50 data sets can be found in the openintro package. The county data can be found in the usdata package. 2.3.1 Scatterplots for paired data A scatterplot provides a case-by-case view of data for two quantitative variables. In Figure 1.4, a scatterplot was used to examine the homeownership rate against the fraction of housing units that were part of multi-unit properties (e.g. apartments) in the county data set. Another scatterplot is shown in Figure 2.10, comparing the total income of a borrower total_income and the amount they borrowed loan_amount for the loan50 data set. In any scatterplot, each point represents a single case. Since there are 50 cases in loan50, there are 50 points in Figure 2.10. When examining scatterplots, we describe four features: Form - If you were to trace the trend of the points, would the trend be linear or nonlinear? Direction - As values on the x-axis increase, do the y-values tend to increase (positive direction) or do they decrease (negative direction)? Strength - How closely do the points follow a trend? Unusual observations or outliers- Are there any unusual observations that do not seem to match the overall pattern of the scatterplot? Figure 2.10: A scatterplot of loan_amount versus total_income for the loan50 data set. Looking at Figure 2.10, we see that there are many borrowers with income below $100,000 on the left side of the graph, while there are a handful of borrowers with income above $250,000. The loan amounts vary from below $10,000 to around $40,000. The data seem to have a linear form, though the relationship between the two variables is quite weak. The direction is positive—as total income increases, the loan amount also tends to increase—and there may be a few unusual observations in the higher income range, though since the relationship is weak, it is hard to tell. Figure 2.11: A scatterplot of the median household income against the poverty rate for the county data set. Data are from 2017. A statistical model has also been fit to the data and is shown as a dashed line. Figure 2.11 shows a plot of median household income against the poverty rate for 3,142 counties. What can be said about the relationship between these variables? The relationship is evidently nonlinear, as highlighted by the dashed line. This is different from previous scatterplots we have seen, which show relationships that do not show much, if any, curvature in the trend. The relationship is moderate to strong, the direction is negative, and there does not appear to be any unusual observations. What do scatterplots reveal about the data, and how are they useful?32 Describe two variables that would have a horseshoe-shaped association in a scatterplot (\\(\\cap\\) or \\(\\frown\\))33 2.3.2 Dot plots and the mean Sometimes we are interested in the distribution of a single variable. In these cases, a dot plot provides the most basic of displays. A dot plot is a one-variable scatterplot; an example using the interest rate of 50 loans is shown in Figure 2.12. Figure 2.12: A dot plot of interest_rate for the loan50 data set. The rates have been rounded to the nearest percent in this plot, and the distribution’s mean is shown as a red triangle. The distribution of a variable is a description of the possible values it takes and how frequently each value occurs. The mean, often called the average, is a common way to measure the center of a distribution of data. To compute the mean interest rate of the 50 loans above, we add up all the interest rates and divide by the number of observations. The sample mean is often labeled \\(\\bar{x}\\). The letter \\(x\\) is being used as a generic placeholder for the variable and the bar over the \\(x\\) communicates we’re looking at the average of that variable. In our example \\(x\\) would represent interest rate, and \\(\\bar{x}\\) = 11.57%. It is useful to think of the mean as the balancing point of the distribution34, and it’s shown as a triangle in Figure 2.12. Mean. The sample mean can be calculated as the sum of the observed values divided by the number of observations: \\[ \\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n} \\] Examine the equation for the mean. What does \\(x_1\\) correspond to? And \\(x_2\\) Can you infer a general meaning to what \\(x_i\\) might represent?35 What was \\(n\\) in this sample of loans?36 The loan50 data set represents a sample from a larger population of loans made through Lending Club. We could compute a mean for this population in the same way as the sample mean. However, the population mean has a special label: \\(\\mu\\). The symbol \\(\\mu\\) is the Greek letter mu and represents the average of all observations in the population. Sometimes a subscript, such as \\(_x\\), is used to represent which variable the population mean refers to, e.g., \\(\\mu_x\\). Often times it is too expensive or time consuming to measure the population mean precisely, so we often estimate \\(\\mu\\) using the sample mean, \\(\\bar{x}\\). The Greek letter \\(\\mu\\) is pronounced mu, listen to the pronunciation here. The average interest rate across all loans in the population can be estimated using the sample data. Based on the sample of 50 loans, what would be a reasonable estimate of \\(\\mu_x\\), the mean interest rate for all loans in the full data set? The sample mean, 11.57%, provides a rough estimate of \\(\\mu_x\\). While it is not perfect, this statistic our single best guess point estimate of the average interest rate of all the loans in the population under study, the parameter. In Chapter 5 and beyond, we will develop tools to characterize the accuracy of point estimates, like the sample mean. As you might have guessed, point estimates based on larger samples tend to be more accurate than those based on smaller samples. The mean is useful for making comparisons across different samples that may have different sample sizes because it allows us to rescale or standardize a metric into something more easily interpretable and comparable. Suppose we would like to understand if a new drug is more effective at treating asthma attacks than the standard drug. A trial of 1500 adults is set up, where 500 receive the new drug, and 1000 receive a standard drug in the control group: New drug Standard drug Number of patients 500 1000 Total asthma attacks 200 300 Comparing the raw counts of 200 to 300 asthma attacks would make it appear that the new drug is better, but this is an artifact of the imbalanced group sizes. Instead, we should look at the average number of asthma attacks per patient in each group: New drug: \\(200 / 500 = 0.4\\) asthma attacks per patient Standard drug: \\(300 / 1000 = 0.3\\) asthma attacks per patient The standard drug has a lower average number of asthma attacks per patient than the average in the treatment group. Emilio opened a food truck last year where he sells burritos, and his business has stabilized over the last 4 months. Over that 4 month period, he has made $11,000 while working 625 hours. Emilio’s competition, Francis, has made $13,000 over the last 4 months while working 800 hours. Francis brags to Emilio that her business is more profitable. Is Francis’ claim warranted? Emilio’s average hourly earnings provide a useful statistic for evaluating how much his venture is, at least from a financial perspective, worth: \\[ \\frac{\\$11000}{625\\text{ hours}} = \\$17.60\\text{ per hour} \\] By knowing his average hourly wage, Emilio now has put his earnings into a standard unit that is easier to compare with many other jobs that he might consider. In comparison, Francis’ average hourly wage was \\[ \\frac{\\$13000}{800\\text{ hours}} = \\$16.25\\text{ per hour} \\] Thus, while Francis’ total earnings were larger than Emilio’s, when standardizing by hour, Francis shouldn’t brag. Suppose we want to compute the average income per person in the US. To do so, we might first think to take the mean of the per capita incomes across the 3,142 counties in the data set. What would be a better approach? The county data set is special in that each county actually represents many individual people. If we were to simply average across the income variable, we would be treating counties with 5,000 and 5,000,000 residents equally in the calculations. Instead, we should compute the total income for each county, add up all the counties’ totals, and then divide by the number of people in all the counties. If we completed these steps with the data, we would find that the per capita income for the US is $30,861. Had we computed the simple mean of per capita income across counties, the result would have been just $26,093! This example used what is called a weighted mean. For more information on this topic, check out the following online supplement regarding weighted means. 2.3.3 Histograms and shape Dot plots show the exact value for each observation. This is useful for small data sets, but they can become hard to read with larger samples. Rather than showing the value of each observation, we prefer to think of the value as belonging to a bin. For example, in the loan50 data set, we created a table of counts for the number of loans with interest rates between 5.0% and 7.5%, then the number of loans with rates between 7.5% and 10.0%, and so on. Observations that fall on the boundary of a bin (e.g., 10.00%) are allocated to the lower bin. This tabulation is shown in Table 2.10. These binned counts are plotted as bars in Figure 2.13 into what is called a histogram, which resembles a more heavily binned version of the stacked dot plot shown in Figure 2.12. Table 2.10: Counts for the binned interest_rate data. Interest rate 5% - 7.5% 7.5% - 10% 10% - 12.5% 12.5% - 15% 15% - 17.5% 17.5% - 20% 20% - 22.5% 22.5% - 25% 25% - 27.5% n 11 15 8 4 5 4 1 1 1 Figure 2.13: A histogram of interest_rate. This distribution is strongly skewed to the right. Histograms provide a view of the data density. Higher bars represent where the data are relatively more common, or “dense.” For instance, there are many more loans with rates between 5% and 10% than loans with rates between 20% and 25% in the data set. The bars make it easy to see how the density of the data changes relative to the interest rate. Histograms are especially convenient for understanding the shape of the data distribution. Figure 2.13 suggests that most loans have rates under 15%, while only a handful of loans have rates above 20%. When data trail off to the right in this way and has a longer right tail, the shape is said to be right skewed37 Data sets with the reverse characteristic—a long, thinner tail to the left—are said to be left skewed. We also say that such a distribution has a long left tail. Data sets that show roughly equal trailing off in both directions are called symmetric. When data trail off in one direction, the distribution has a long tail. If a distribution has a long left tail, it is left skewed or negatively skewed. If a distribution has a long right tail, it is right skewed or positively skewed. Besides the mean (since it was labeled), what can you see in the dot plot in Figure 2.12 that you cannot see in the histogram in Figure 2.13?38 In addition to looking at whether a distribution is skewed or symmetric, histograms can be used to identify modes. A mode is represented by a prominent peak in the distribution. There is only one prominent peak in the histogram of interest_rate. A definition of mode sometimes taught in math classes is the value with the most occurrences in the data set. However, for many real-world data sets, it is common to have no observations with the same value in a data set, making this definition impractical in data analysis. Figure 2.14 shows histograms that have one, two, or three prominent peaks. Such distributions are called unimodal, bimodal, and multimodal, respectively. Any distribution with more than two prominent peaks is called multimodal. Notice that there was one prominent peak in the unimodal distribution with a second less prominent peak that was not counted since it only differs from its neighboring bins by a few observations. Figure 2.14: Counting only prominent peaks, the distributions are (left to right) unimodal, bimodal, and multimodal. Note that the left plot is unimodal because we are counting prominent peaks, not just any peak. Figure 2.13 reveals only one prominent mode in the interest rate. Is the distribution unimodal, bimodal, or multimodal?39 Height measurements of young students and adult teachers at a K-3 elementary school were taken. How many modes would you expect in this height data set?40. Looking for modes isn’t about finding a clear and correct answer about the number of modes in a distribution, which is why prominent is not rigorously defined in this book. The most important part of this examination is to better understand your data. Another type of plot that is helpful for exploring the shape of a distribution is a smoothed histogram, called a density plot. A density plot will scale the \\(y\\)-axis so that the total area under the density curve is equal to one. This allows us to get a sense of what proportion of the data lie in a certain interval, rather than the frequency of data in the interval. We can change the scale of a histogram to plot proportions rather than frequencies, then overlay a density curve on this rescaled histogram, as seen in Figure 2.15. Figure 2.15: A density plot of interest_rate overlayed on a histogram using density scale. 2.3.4 Variance and standard deviation The mean was introduced as a method to describe the center of a data set, and variability in the data is also important. Here, we introduce two measures of variability: the variance and the standard deviation. Both of these are very useful in data analysis, even though their formulas are a bit tedious to calculate by hand. The standard deviation is the easier of the two to comprehend, and it roughly describes how far away the typical observation is from the mean. We call the distance of an observation from its mean its deviation. Below are the deviations for the \\(1^{st}\\), \\(2^{nd}\\), \\(3^{rd}\\), and \\(50^{th}\\) observations in the interest_rate variable: \\[ x_1 - \\bar{x} = 10.9 - 11.57 = -0.67 \\] \\[ x_2 - \\bar{x} = 9.92 - 11.57 = -1.65 \\] \\[ x_3 - \\bar{x} = 26.3 - 11.57 = 14.73 \\] \\[ \\vdots \\] \\[ x_{50} - \\bar{x} = 6.08 - 11.57 = -5.49 \\] If we square these deviations and then take an average, the result is equal to the sample variance, denoted by \\(s^2\\): \\[ s^2 = \\frac{(-0.67)^2 + (-1.65)^2 + (14.73)^2 + \\cdots + (-5.49)^2}{50 - 1} = \\frac{0.45 + 2.72 + \\cdots + 30.14}{49} = 25.52 \\] We divide by \\(n - 1\\), rather than dividing by \\(n\\), when computing a sample’s variance; there’s some mathematical nuance here, but the end result is that doing this makes this statistic slightly more reliable and useful. Notice that squaring the deviations does two things. First, it makes large values relatively much larger. Second, it gets rid of any negative signs. The standard deviation is defined as the square root of the variance: \\[ s = \\sqrt{25.52} = 5.05 \\] While often omitted, a subscript of \\(_x\\) may be added to the variance and standard deviation, i.e., \\(s_x^2\\) and \\(s_x\\), if it is useful as a reminder that these are the variance and standard deviation of the observations represented by \\(x_1\\), \\(x_2\\), …, \\(x_n\\). Variance and standard deviation. The sample variance is the (near) average squared distance from the mean: \\[ s^2 = \\frac{((x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\cdots + (x_n - \\bar{x})^2)}{n-1} \\] The sample standard deviation is the square root of the variance: \\(s = \\sqrt{s^2}\\). The standard deviation is useful when considering how far the data are distributed from the mean. The standard deviation represents the typical deviation of observations from the mean. If the distribution is bell-shaped, about 70% of the data will be within one standard deviation of the mean and about 95% will be within two standard deviations. However, these percentages do not necessarily hold for other shaped distributions! Like the mean, the population values for variance and standard deviation have special symbols: \\(\\sigma^2\\) for the variance and \\(\\sigma\\) for the standard deviation. The Greek letter \\(\\sigma\\) is pronounced sigma, listen to the pronunciation here. Figure 2.16: For the interest_rate variable, 34 of the 50 loans (68%) had interest rates within 1 standard deviation of the mean, and 48 of the 50 loans (96%) had rates within 2 standard deviations. Usually about 70% of the data are within 1 standard deviation of the mean and 95% within 2 standard deviations, though this is far from a hard rule. Figure 2.17: Three very different population distributions with the same mean (0) and standard deviation (1). A good description of the shape of a distribution should include modality and whether the distribution is symmetric or skewed to one side. Using Figure 2.17 as an example, explain why such a description is important.41 Describe the distribution of the interest_rate variable using the histogram in Figure 2.13. The description should incorporate the center, variability, and shape of the distribution, and it should also be placed in context. Also note any especially unusual cases. The distribution of interest rates is unimodal and skewed to the high end. Many of the rates fall near the mean at 11.57%, and most fall within one standard deviation (5.05%) of the mean. There are a few exceptionally large interest rates in the sample that are above 20%. In practice, the variance and standard deviation are sometimes used as a means to an end, where the “end” is being able to accurately estimate the uncertainty associated with a sample statistic. For example, in Chapter 6 the standard deviation is used in calculations that help us understand how much a sample mean varies from one sample to the next. 2.3.5 Box plots, quartiles, and the median A box plot (or box-and-whisker plot) summarizes a data set using five statistics while also identifying unusual observations. The five statistics—minimum, first quartile, median, third quartile, maximum—together are called the five number summary. Figure 2.18 provides a dot plot alongside a box plot of the interest_rate variable from the loan50 data set. Figure 2.18: Plot A shows a dot plot and Plot B shows a box plot of the distribution of interest rates from the loan50 dataset. The dark line inside the box represents the median, which splits the data in half: 50% of the data fall below this value and 50% fall above it. Since in the loan50 dataset there are 50 observations (an even number), the median is defined as the average of the two observations closest to the \\(50^{th}\\) percentile. Table @ref(tab:loan50_int_rate_sorted) shows all interest rates, arranged in ascending order. We can see that the \\(25^{th}\\) and the \\(26^{th}\\) values are both 9.93, which corresponds to the dark line in the box plot in Figure 2.18. (#tab:loan50_int_rate_sorted)Interest rates from the loan50 dataset, arranged in ascending order. 1 2 3 4 5 6 7 8 9 10 1 5.31 5.31 5.32 6.08 6.08 6.08 6.71 6.71 7.34 7.35 10 7.35 7.96 7.96 7.96 7.97 9.43 9.43 9.44 9.44 9.44 20 9.92 9.92 9.92 9.92 9.93 9.93 10.42 10.42 10.90 10.90 30 10.91 10.91 10.91 11.98 12.62 12.62 12.62 14.08 15.04 16.02 40 17.09 17.09 17.09 18.06 18.45 19.42 20.00 21.45 24.85 26.30 When there are an odd number of observations, there will be exactly one observation that splits the data into two halves, and in such a case that observation is the median (no average needed). Median: the number in the middle. If the data are ordered from smallest to largest, the median is the observation right in the middle. If there are an even number of observations, there will be two values in the middle, and the median is taken as their average. Mathematically, if we denote the sample size by \\(n\\), then if \\(n\\) is odd, the median is the \\([(n+1)/2]^{th}\\) smallest value in the data set, and if \\(n\\) is even, the median is the average of the \\((n/2)^{th}\\) and \\((n/2+1)^{th}\\) smallest values in the data set. The second step in building a box plot is drawing a rectangle to represent the middle 50% of the data. The length of the the box is called the interquartile range, or IQR for short. It, like the standard deviation, is a measure of variability in data. The more variable the data, the larger the standard deviation and IQR tend to be. The two boundaries of the box are called the first quartile (the \\(25^{th}\\) percentile, i.e., 25% of the data fall below this value) and the third quartile (the \\(75^{th}\\) percentile, i.e., 75% of the data fall below this value) , and these are often labeled \\(Q_1\\) and \\(Q_3\\), respectively42 Interquartile range (IQR). The IQR interquartile range is the length of the box in a box plot. It is computed as \\[ IQR = Q_3 - Q_1, \\] where \\(Q_1\\) and \\(Q_3\\) are the \\(25^{th}\\) and \\(75^{th}\\) percentiles, respectively. What percent of the data fall between \\(Q_1\\) and the median? What percent is between the median and \\(Q_3\\)?43 Extending out from the box, the whiskers attempt to capture the data outside of the box. The whiskers of a box plot reach to the minimum and the maximum values in the data, unless there are points that are considered unusually high or unusually low, which are identified as potential outliers by the box plot. These are labeled with a dot on the box plot. The purpose of labeling these points—instead of extending the whiskers to the minimum and maximum observed values—is to help identify any observations that appear to be unusually distant from the rest of the data. There are a variety of formulas for determining whether a particular data point is considered an outlier, and different statistical software use different formulas. A commonly used formula is that any value that is beyond \\(1.5\\times IQR\\)[While the choice of exactly 1.5 is arbitrary, it is the most commonly used value for box plots.] away from the box is considered an outlier. In a sense, the box is like the body of the box plot and the whiskers are like its arms trying to reach the rest of the data, up to the outliers. In Figure 2.18, the upper whisker does not extend to the last two points, 24.85% and 26.3%, which are above \\(Q_3 + 1.5\\times IQR\\), and so it extends only to the last point below this limit. The lower whisker stops at the minimum value in the data set, 5.31%, since there are no outliers on the lower end of the distribution. The whiskers extend to actual data points—not the limits for outliers. That is, the values \\(Q_1 - 1.5\\times IQR\\) and \\(Q_3 + 1.5\\times IQR\\) should not be shown on the plot. Outliers are extreme. An outlier is an observation that appears extreme relative to the rest of the data. Examining data for outliers serves many useful purposes, including identifying strong skew in the distribution, identifying possible data collection or data entry errors, and providing insight into interesting properties of the data. Using the box plot in Figure 2.18, estimate the values of the \\(Q_1\\), \\(Q_3\\), and IQR for interest_rate in the loan50 data set.44 2.3.6 Describing and comparing quantitative distributions As a review, when describing a scatterplot—the association between two quantitative variables, we look for the four features: Form Direction Strength Outliers When asked to describe or compare univariate (single variable) quantitative distributions, we look for four features: Center Variability Shape Outliers We can compare quantitative distributions by using side-by-side box plots, or stacked histograms or dot plots. Recall that the loan50 data set represents a sample from a larger loan data set called loans. This larger data set contains information on 10,000 loans made through Lending Club. Figure 2.19 examines the relationship between homeownership, which for the loans data can take a value of rent, mortgage (owns but has a mortgage), or own, and interest_rate. Note that homeownership is a categorical variable and interest_rate is a quantitative variable. Figure 2.19: Side-by-side box plots of loan interest rates by homeownership category. We see immediately that some features are easier to discern in box plots, while others in histograms. Shape is shown more clearly in histograms, while center (as measured by the median) is easy to compare across groups in the side-by-side box plots. Using Figure 2.19 write a few sentences comparing the distributions of loan amount across the different homeownership categories. The median loan amount is higher for those with a mortgage (around $16,000) than for those who own or rent (around $12,000-$13,000). However, variability in loan amounts is similar across homeownership categories, with an IQR of around $15,000 and loans ranging from a few hundred dollars to $40,000. We see from the histograms that the distribution of loan amounts is skewed right for all three homeownership categories, which means the mean loan amount will be higher than the median loan amount. There are no apparent outliers in the mortgage category, but both the rent and own categories have outliers at $40,000. Besides center, variability, shape, and outliers, another interesting feature in these distributions is the result of rounding. Loan amounts in the data set are often rounded to the nearest 100, so we see spikes on these values in the histogram—something that is not evident in the box plots. 2.3.7 Robust statistics How are the sample statistics of the interest_rate data set affected by the observation, 26.3%? What would have happened if this loan had instead been only 15%? What would happen to these summary statistics if the observation at 26.3% had been even larger, say 35%? These scenarios are plotted alongside the original data in Figure 2.20, and sample statistics are computed under each scenario in Table 2.11. Figure 2.20: Dot plots of the original interest rate data and two modified data sets. Table 2.11: A comparison of how the median, IQR, mean, and standard deviation change as the value of an extereme observation from the original interest data changes. Robust Not robust Scenario Median IQR Mean SD Original data 9.93 5.75 11.6 5.05 Move 26.3% to 15% 9.93 5.75 11.3 4.61 Move 26.3% to 35% 9.93 5.75 11.7 5.68 Which is more affected by extreme observations, the mean or median? Is the standard deviation or IQR more affected by extreme observations?45 The median and IQR are called robust statistics because extreme observations have little effect on their values—moving the most extreme value generally has little influence on these statistics. On the other hand, the mean and standard deviation are more heavily influenced by changes in extreme observations, which can be important in some situations. Additionally, the mean tends to get pulled in the direction of a distribution’s skewness, while the skewness has little affect on the median. The median and IQR did not change under the three scenarios in Table 2.11. Why might this be the case? The median and IQR are only sensitive to numbers near \\(Q_1\\), the median, and \\(Q_3\\). Since values in these regions are stable in the three data sets, the median and IQR estimates are also stable. The distribution of loan amounts in the loan50 data set is right skewed, with a few large loans lingering out into the right tail. If you were wanting to understand the typical loan size, should you be more interested in the mean or median?46 2.3.8 Transforming data (special topic) When data are very strongly skewed, we sometimes transform them so they are easier to model. A transformation is a rescaling of the data using a function. Figure 2.21: Plot A: A histogram of the populations of all US counties. Plot B: A histogram of log\\(_{10}\\)-transformed county populations. For this plot, the x-value corresponds to the power of 10, e.g. 4 on the x-axis corresponds to \\(10^4 =\\) 10,000. Data are from 2017. Consider the histogram of county populations shown in the left of Figure 2.21, which shows extreme skew. What is not so useful about this plot? Nearly all of the data fall into the left-most bin, and the extreme skew obscures many of the potentially interesting details in the data. There are some standard transformations that may be useful for strongly right skewed data where much of the data is positive but clustered near zero. For instance, a plot of the logarithm (base 10) of county populations results in the new histogram in Figure 2.21. This data is symmetric, and any potential outliers appear much less extreme than in the original data set. By reigning in the outliers and extreme skew, transformations like this often make it easier to build statistical models against the data. Transformations can also be applied to one or both variables in a scatterplot. A scatterplot of the population change from 2010 to 2017 against the population in 2010 is shown in Figure 2.22. In this first scatterplot, it’s hard to decipher any interesting patterns because the population variable is so strongly skewed. However, if we apply a log\\(_{10}\\) transformation to the population variable, as shown in Figure 2.22, a positive association between the variables is revealed. In fact, we may be interested in fitting a trend line to the data when we explore methods around fitting regression lines in Chapter 3. Figure 2.22: Plot A: Scatterplot of population change against the population before the change. Plot B: A~scatterplot of the same data but where the population size has been log-transformed. Transformations other than the logarithm can be useful, too. For instance, the square root (\\(\\sqrt{\\text{original observation}}\\)) and inverse (\\(\\frac{1}{\\text{original observation}}\\)) are commonly used by data scientists. Common goals in transforming data are to see the data structure differently, reduce skew, assist in modeling, or straighten a nonlinear relationship in a scatterplot. 2.3.9 Mapping data (special topic) The county data set offers many numerical variables that we could plot using dot plots, scatterplots, or box plots, but these miss the true nature of the data. Rather, when we encounter geographic data, we should create an intensity map, where colors are used to show higher and lower values of a variable. Figures 2.23 and 2.24 show intensity maps for poverty rate in percent (poverty), unemployment rate (unemployment_rate), homeownership rate in percent (homeownership), and median household income (median_hh_income). The color key indicates which colors correspond to which values. The intensity maps are not generally very helpful for getting precise values in any given county, but they are very helpful for seeing geographic trends and generating interesting research questions or hypotheses. What interesting features are evident in the poverty and unemployment rate intensity maps? Poverty rates are evidently higher in a few locations. Notably, the deep south shows higher poverty rates, as does much of Arizona and New Mexico. High poverty rates are evident in the Mississippi flood plains a little north of New Orleans and also in a large section of Kentucky. The unemployment rate follows similar trends, and we can see correspondence between the two variables. In fact, it makes sense for higher rates of unemployment to be closely related to poverty rates. One observation that stands out when comparing the two maps: the poverty rate is much higher than the unemployment rate, meaning while many people may be working, they are not making enough to break out of poverty. What interesting features are evident in the median household income intensity map in Figure 2.24?47 Figure 2.23: Plot A: Intensity map of poverty rate (percent). Plot B: Intensity map of the unemployment rate (percent). Figure 2.24: Plot A: Intensity map of homeownership rate (percent). Plot B: Intensity map of median household income ($1000s). 2.4 R: Exploratory data analysis 2.4.1 Interactive R tutorials Navigate the concepts you’ve learned in this chapter in R using the following self-paced tutorials. All you need is your browser to get started! Tutorial 2: Summarizing and visualizing data Tutorial 2 - Lesson 1: Visualizing categorical data Tutorial 2 - Lesson 2: Visualizing numerical data Tutorial 2 - Lesson 3: Summarizing with statistics Tutorial 2 - Lesson 4: Case study You can also access the full list of tutorials supporting this book here. 2.4.2 R labs Further apply the concepts you’ve learned in this chapter in R with computational labs that walk you through a data analysis case study. Intro to data - Flight delays Full list of labs supporting OpenIntro::Introduction to Modern Statistics 2.5 Chapter 2 review We’re encountered a variety of univariate (one variable) and bivariate (two variable) summary statistics and data visualization methods in this chapter. 2.5.1 Data visualization summary Figure 2.25 presents a decision tree for deciding which type of plot is most appropriate for a given number and types of variables. In the next chapter, we’ll further explore how to model the association between two quantitative variables, called regression. In Chapter 4, we’ll look at exploratory data analysis methods for more than two variables. Figure 2.25: Decision tree for determining an appropriate plot given a number of variables and their types. 2.5.2 Summary measures Though some of these summary measures will be covered in later chapters, Table 2.5.2 provides a comprehensive summary of these measures according to the type(s) of variable(s) which they summarize. Table 2.12: Summary measures for different types of variables covered in this textbook and Sections where they appear. A binary variable is a categorical variable with only two categories. Sections Response Variable Explanatory Variable Summary Measure(s) 2.1, 5.3 Binary (None) One proportion 2.1, 5.4 Binary Binary Difference in proportions, relative risk 2.3, 6.1 Quantitative (None) One mean 2.3, 6.2 Quantitative Binary (Paired) Mean difference 2.3, 6.3 Quantitative Binary (Independent) Difference in means 2.3, 7.1 Quantitative Quantitative Correlation, R-squared, regression slope 2.5.3 Notation summary In the field of statistics, summary measures that summarize a sample of data are called statistics. Numbers that summarize an entire population parameters. You can remember this distinction by looking at the first letter of each term: Statistics summarize Samples. Parameters summarize Populations. We typically use Roman letters to symbolize statistics (e.g., \\(\\bar{x}\\), \\(\\hat{p}\\)), and Greek letters to symbolize parameters (e.g., \\(\\mu\\), \\(\\pi\\)). Summary Measure Statistic Parameter Sample size \\(n\\) \\(-\\) Proportion \\(\\hat{p}\\) \\(\\pi\\) Mean \\(\\bar{x}\\) \\(\\mu\\) Correlation \\(r\\) \\(\\rho\\) Regression line slope \\(b_1\\) \\(\\beta_1\\) Regression line \\(y\\)-intercept \\(b_0\\) \\(\\beta_0\\) 2.5.4 Terms We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as bolded text. average first quartile outliers statistic bar plot form parameter strength bimodal frequency pie chart symmetric box plot histogram point estimate tail column proportions intensity map relative frequency third quartile column totals interquartile range right skewed transformation contingency table IQR robust statistics two-way table data density left skewed row proportions unimodal density plot mean row totals variability deviation median scatterplot variance direction mosaic plot segmented bar plot weighted mean distribution multimodal Simpson’s Paradox whiskers dot plot outlier standard deviation References "],
["cor-reg.html", "Chapter 3 Correlation and regression 3.1 Fitting a line, residuals, and correlation 3.2 Least squares regression 3.3 Outliers in linear regression 3.4 R: Correlation and regression 3.5 Chapter review", " Chapter 3 Correlation and regression Linear regression is a very powerful statistical technique. Many people have some familiarity with regression just from reading the news, where straight lines are overlaid on scatterplots. Linear models can be used for prediction or to evaluate whether there is a linear relationship between two numerical variables. 3.1 Fitting a line, residuals, and correlation It’s helpful to think deeply about the line fitting process. In this section, we define the form of a linear model, explore criteria for what makes a good fit, and introduce a new statistic called correlation. 3.1.1 Fitting a line to data Figure 3.1 shows two variables whose relationship can be modeled perfectly with a straight line. The equation for the line is \\(y = 5 + 64.96 x\\). Consider what a perfect linear relationship means: we know the exact value of \\(y\\) just by knowing the value of \\(x\\). This is unrealistic in almost any natural process. For example, if we took family income (\\(x\\)), this value would provide some useful information about how much financial support a college may offer a prospective student (\\(y\\)). However, the prediction would be far from perfect, since other factors play a role in financial support beyond a family’s finances. Figure 3.1: Requests from twelve separate buyers were simultaneously placed with a trading company to purchase Target Corporation stock (ticker TGT, December 28th, 2018), and the total cost of the shares were reported. Because the cost is computed using a linear formula, the linear fit is perfect. Linear regression is the statistical method for fitting a line to data where the relationship between two variables, \\(x\\) and \\(y\\), can be modeled by a straight line with some error: \\[ y = \\beta_0 + \\beta_1x + \\varepsilon\\] The values \\(\\beta_0\\) and \\(\\beta_1\\) represent the model’s parameters (\\(\\beta\\) is the Greek letter beta), and the error is represented by \\(\\varepsilon\\) (the Greek letter epsilon). The parameters are estimated using data, and we write their point estimates as \\(b_0\\) and \\(b_1\\). When we use \\(x\\) to predict \\(y\\), we usually call \\(x\\) the explanatory or predictor variable, and we call \\(y\\) the response. We also often drop the \\(\\epsilon\\) term when writing down the model since our main focus is often on the prediction of the average outcome. If the \\(\\epsilon\\) term is dropped, then we put a “hat” on \\(y\\) (\\(\\hat{y}\\)) to signal that the model yields a prediction for \\(y\\), and not the actual value. It is rare for all of the data to fall perfectly on a straight line. Instead, it’s more common for data to appear as a cloud of points, such as those examples shown in Figure 3.2. In each case, the data fall around a straight line, even if none of the observations fall exactly on the line. The first plot shows a relatively strong downward linear trend, where the remaining variability in the data around the line is minor relative to the strength of the relationship between \\(x\\) and \\(y\\). The second plot shows an upward trend that, while evident, is not as strong as the first. The last plot shows a very weak downward trend in the data, so slight we can hardly notice it. In each of these examples, we will have some uncertainty regarding our estimates of the model parameters, \\(\\beta_0\\) and \\(\\beta_1\\). For instance, we might wonder, should we move the line up or down a little, or should we tilt it more or less? As we move forward in this chapter, we will learn about criteria for line-fitting, and we will also learn about the uncertainty associated with estimates of model parameters. Figure 3.2: Three data sets where a linear model may be useful even though the data do not all fall exactly on the line. There are also cases where fitting a straight line to the data, even if there is a clear relationship between the variables, is not helpful. One such case is shown in Figure 3.3 where there is a very clear relationship between the variables even though the trend is not linear. We discuss nonlinear trends in this chapter and the next, but details of fitting nonlinear models are saved for a later course. Figure 3.3: The best fitting line for these data is flat, which is not useful in this nonlinear case. These data are from a physics experiment. 3.1.2 Using linear regression to predict possum head lengths Brushtail possums are a marsupial that lives in Australia, and a photo of one is shown in Figure 3.4. Researchers captured 104 of these animals and took body measurements before releasing the animals back into the wild. We consider two of these measurements: the total length of each possum, from head to tail, and the length of each possum’s head. Figure 3.4: The common brushtail possum of Australia. Photo by Greg Schecter, flic.kr/p/9BAFbR, CC BY 2.0 license. The possum data can be found in the openintro package. Figure 3.5 shows a scatterplot for the head length (mm) and total length (cm) of the possums. Each point represents a single possum from the data. The head and total length variables are associated: possums with an above average total length also tend to have above average head lengths. While the relationship is not perfectly linear, it could be helpful to partially explain the connection between these variables with a straight line. Figure 3.5: A scatterplot showing head length against total length for 104 brushtail possums. A point representing a possum with head length 86.7 mm and total length 84 cm is highlighted. We want to describe the relationship between the head length and total length variables in the possum data set using a line. In this example, we will use the total length as the predictor variable, \\(x\\), to predict a possum’s head length, \\(y\\). We could fit the linear relationship using technology (criteria to be discussed in Section 3.2), as in Figure 3.6. Figure 3.6: A reasonable linear model was fit to represent the relationship between head length and total length. The equation for this line is \\[\\hat{y} = 43+0.57x.\\] A “hat” on \\(y\\) is used to signify that this is an estimate. We can use this line to discuss properties of possums. For instance, the equation predicts a possum with a total length of 80 cm will have a head length of \\[\\hat{y} = 43 + 0.57 \\times 80 = 88.6 \\text{ mm}.\\] The estimate may be viewed as an average: the equation predicts that possums with a total length of 80 cm will have an average head length of 88.6 mm. Absent further information about an 80 cm possum, the prediction for head length that uses the average is a reasonable estimate. There may be other variables that could help us predict the head length of a possum besides its length. Perhaps the relationship would be a little different for male possums than female possums, or perhaps it would differ for possums from one region of Australia versus another region. Plot A in Figure 3.7 shows the relationship between total length and head length of brushtail possums, taking into consideration their sex. Male possums (represented by blue triangles) seem to be larger in terms of total length and head length than female possums (represented by red circles). Plot B in Figure 3.7 shows the same relationship, taking into consideration their age. It’s harder to tell if age changes the relationship between total length and head length for these possums. Figure 3.7: Relationship between total length and head lentgh of brushtail possums, taking into consideration their sex (Plot A) or age (Plot B). In Chapter 4, we’ll learn about how we can include more than one predictor in our model. Before we get there, we first need to better understand how to best build a simple linear model with one predictor. 3.1.3 Residuals Residuals are the leftover variation in the data after accounting for the model fit: \\[\\text{Data} = \\text{Fit} + \\text{Residual}\\] Each observation will have a residual, and three of the residuals for the linear model we fit for the data are shown in Figure 3.8. If an observation is above the regression line, then its residual, the vertical distance from the observation to the line, is positive. Observations below the line have negative residuals. One goal in picking the right linear model is for these residuals to be as small as possible. Figure 3.8 is almost a replica of Figure 3.6, with three points from the data highlighted. The observation marked by a red circle has a small, negative residual of about -1; the observation marked by a green diamond has a large residual of about +7; and the observation marked by a yellow triangle has a moderate residual of about -4. The size of a residual is usually discussed in terms of its absolute value. For example, the residual for the observation marked by a yellow triangle is larger than that of the observation marked by a red circle because \\(|-4|\\) is larger than \\(|-1|\\). Figure 3.8: A reasonable linear model was fit to represent the relationship between head length and total length, with three points and their residuals highlighted. Residual: Difference between observed and expected. The residual of the \\(i^{th}\\) observation \\((x_i, y_i)\\) is the difference of the observed response (\\(y_i\\)) and the response we would predict based on the model fit (\\(\\hat{y}_i\\)): \\[e_i = y_i - \\hat{y}_i\\] We typically identify \\(\\hat{y}_i\\) by plugging \\(x_i\\) into the model. The linear fit shown in Figure 3.8 is given as \\(\\hat{y} = 43+0.57x\\). Based on this line, formally compute the residual of the observation \\((76.0, 85.1)\\). This observation is marked by a red circle in Figure 3.8. Check it against the earlier visual estimate, \\(-1\\). We first compute the predicted value of the observation marked by a red circle based on the model: \\[\\hat{y} = 43+0.57x = 43+0.57\\times 76.0 = 86.3mm\\] Next, we compute the difference of the actual head length and the predicted head length: \\[e = y - \\hat{y} = 85.1 - 86.3 = -1.2 mm\\] The model’s error is \\(e = -1.2\\) mm, which is very close to the visual estimate of \\(-1\\) mm. The negative residual indicates that the linear model overpredicted head length for this particular possum. If a model underestimates an observation, will the residual be positive or negative? What about if it overestimates the observation?48 Compute the residuals for the observation marked by a green diamond, \\((85.0, 98.6)\\), and the observation marked by a yellow triangle, \\((95.5, 94.0)\\), in the figure using the linear relationship \\(\\hat{y} = 43 + 0.57x\\).49 Residuals are helpful in evaluating how well a linear model fits a data set. We often display them in a residual plot such as the one shown in Figure 3.9 for the regression line in Figure 3.8. The residuals are plotted at their fitted values on the \\(x\\)-axis but with the vertical coordinate as the residual. For instance, the point \\((85.0, 98.6)\\) (marked by the green diamond) had a predicted value of 91.45 mm and had a residual of 7.15 mm, so in the residual plot it is placed at \\((91.45, 7.15)\\). Creating a residual plot is sort of like tipping the scatterplot over so the regression line is horizontal. Figure 3.9: Residual plot for the model predicting head length from total length for brushtail possums. One purpose of residual plots is to identify characteristics or patterns still apparent in data after fitting a model. Figure 3.10 shows three scatterplots with linear models in the first row and residual plots in the second row. Can you identify any patterns remaining in the residuals? In the first data set (first column), the residuals show no obvious patterns. The residuals appear to be scattered randomly around the dashed line that represents 0. The second data set shows a pattern in the residuals. There is some curvature in the scatterplot, which is more obvious in the residual plot. We should not use a straight line to model these data. Instead, a more advanced technique should be used. The last plot shows very little upwards trend, and the residuals also show no obvious patterns. It is reasonable to try to fit a linear model to the data. However, it is unclear whether the slope parameter is statistically discernible from zero. The point estimate of the slope parameter, labeled \\(b_1\\), is not zero, but we might wonder if this could just be due to chance. We will address this sort of scenario in Chapter 7. Figure 3.10: Sample data with their best fitting lines (top row) and their corresponding residual plots (bottom row). 3.1.4 Describing linear relationships with correlation We’ve seen plots with strong linear relationships and others with very weak linear relationships. It would be useful if we could quantify the strength of these linear relationships with a statistic. Correlation: strength and direction of a linear relationship. Correlation which always takes values between -1 and 1, is a summary statistic that describes the strength (by its magnitude) and direction (by its sign) of the linear relationship between two variables. We denote the correlation by \\(R\\) or \\(r\\). We can compute the correlation using a formula, just as we did with the sample mean and standard deviation. This formula is rather complex50, and like with other statistics, we generally perform the calculations on a computer or calculator. Figure 3.11 shows eight plots and their corresponding correlations. Only when the relationship is perfectly linear is the correlation either -1 or 1. If the relationship is strong and positive, the correlation will be near +1. If it is strong and negative, it will be near -1. If there is no apparent linear relationship between the variables, then the correlation will be near zero. Figure 3.11: Sample scatterplots and their correlations. The first row shows variables with a positive relationshiop, represented by the trend up and to the right. The second row shows variables with a negative trend, where a large value in one variable is associated with a low value in the other. The correlation is intended to quantify the strength and direction of a linear trend. Nonlinear trends, even when strong, sometimes produce correlations that do not reflect the strength of the relationship; see three such examples in Figure 3.12. Figure 3.12: Sample scatterplots and their correlations. In each case, there is a strong relationship between the variables, However, because the relationship is nonlinear, the correlation is relatively weak. No straight line is a good fit for the data sets represented in Figure 3.12. Try drawing nonlinear curves on each plot. Once you create a curve for each, describe what is important in your fit.51 3.2 Least squares regression Fitting linear models by eye is open to criticism since it is based on an individual’s preference. In this section, we use least squares regression as a more rigorous approach. 3.2.1 Gift aid for freshman at Elmhurst College This section considers family income and gift aid data from a random sample of fifty students in the freshman class of Elmhurst College in Illinois. Gift aid is financial aid that does not need to be paid back, as opposed to a loan. A scatterplot of the data is shown in Figure 3.13 along with two linear fits. The lines follow a negative trend in the data; students who have higher family incomes tended to have lower gift aid from the university. Figure 3.13: Gift aid and family income for a random sample of 50 freshman students from Elmhurst College, shown with the least squares line (solid line) and line fit by minimizing the sum of the residual magnitudes (dashed line). Is the correlation positive or negative in Figure 3.13?52 3.2.2 An objective measure for finding the best line We begin by thinking about what we mean by “best”. Mathematically, we want a line that has small residuals. The first option that may come to mind is to minimize the sum of the residual magnitudes: \\[|e_1| + |e_2| + \\dots + |e_n|\\] which we could accomplish with a computer program. The resulting dashed line shown in Figure 3.13 demonstrates this fit can be quite reasonable. However, a more common practice is to choose the line that minimizes the sum of the squared residuals: \\[e_{1}^2 + e_{2}^2 + \\dots + e_{n}^2\\] The line that minimizes this least squares criterion is represented as the solid line in Figure 3.13. This is commonly called the least squares line. The following are three possible reasons to choose this option instead of trying to minimize the sum of residual magnitudes without any squaring: It is the most commonly used method. Computing the least squares line is widely supported in statistical software. In many applications, a residual twice as large as another residual is more than twice as bad. For example, being off by 4 is usually more than twice as bad as being off by 2. Squaring the residuals accounts for this discrepancy. The first two reasons are largely for tradition and convenience; the last reason explains why the least squares criterion is typically most helpful.53 3.2.3 Finding and interpreting the least squares line For the Elmhurst data, we could write the equation of our linear regression model as \\[aid = \\beta_0 + \\beta_{1}\\times \\textit{family_income} + \\epsilon.\\] Here the model equation is set up to predict gift aid based on a student’s family income, which would be useful to students considering Elmhurst. The two unknown values \\(\\beta_0\\) and \\(\\beta_1\\) are the parameters of the linear regression model. The least squares regression line, computed based on the observed data, provides estimates of the parameters \\(\\beta_0\\) and \\(\\beta_1\\): \\[\\widehat{aid} = b_0 + b_{1}\\times \\textit{family_income}.\\] In practice, this estimation is done using a computer in the same way that other estimates, like a sample mean, can be estimated using a computer or calculator. The dataset where these data are stored is called elmhurst. The first 5 rows of this dataset are given in Table 3.1. Table 3.1: First five rows of the elmhurst dataset. family_income gift_aid price_paid 92.92 21.7 14.28 0.25 27.5 8.53 53.09 27.8 14.25 50.20 27.2 8.78 137.61 18.0 24.00 We can see that family income is recorded in a variable called family_income and gift aid from university is recorded in a variable called gift_aid. For now, we won’t worry about the price_paid variable. We should also note that these data are from the 2011-2012 academic year, and all monetary amounts are given in $1,000s, i.e., the family income of the first student in the data shown in Table 3.1 is $92,900 and they received a gift aid of $21,700. (The data source states that all numbers have been rounded to the nearest whole dollar.) Using these data, we can estimate the linear regression line by fitting a linear model to the data with the lm() function in R. lm(gift_aid ~ family_income, data = elmhurst) #&gt; #&gt; Call: #&gt; lm(formula = gift_aid ~ family_income, data = elmhurst) #&gt; #&gt; Coefficients: #&gt; (Intercept) family_income #&gt; 24.3193 -0.0431 The model output tells us that the intercept is approximately 24.319 and the slope is approximately -0.043. But what do these values mean? Interpreting parameters in a regression model is often one of the most important steps in the analysis. The intercept and slope estimates for the Elmhurst data are \\(b_0\\) = 24.319 and \\(b_1\\) = -0.043. What do these numbers really mean? Interpreting the slope parameter is helpful in almost any application. For each additional $1,000 of family income, we would expect a student to receive a net difference of 1,000 \\(\\times\\) (-0.0431) = -$43.10 in aid on average, i.e., $43.10 less. Note that a higher family income corresponds to less aid because the coefficient of family income is negative in the model. We must be cautious in this interpretation: while there is a real association, we cannot interpret a causal connection between the variables because these data are observational. That is, increasing a student’s family income may not cause the student’s aid to drop. (It would be reasonable to contact the college and ask if the relationship is causal, i.e., if Elmhurst College’s aid decisions are partially based on students’ family income.) A more appropriate interpretation would then be: An additional $1,000 of family income is associated with an estimated decrease of $43.10 in aid on average. The estimated intercept \\(b_0\\) = 24.319 describes the average aid if a student’s family had no income. The meaning of the intercept is relevant to this application since the family income for some students at Elmhurst is $0. In other applications, the intercept may have little or no practical value if there are no observations where \\(x\\) is near zero. Interpreting parameters estimated by least squares. The slope describes the estimated difference in the \\(y\\) variable if the explanatory variable \\(x\\) for a case happened to be one unit larger. The intercept describes the average outcome of \\(y\\) if \\(x=0\\) and the linear model is valid all the way to \\(x=0\\), which in many applications is not the case. Suppose a high school senior is considering Elmhurst College. Can they simply use the linear equation that we have estimated to calculate her financial aid from the university? She may use it as an estimate, though some qualifiers on this approach are important. First, the data all come from one freshman class, and the way aid is determined by the university may change from year to year. Second, the equation will provide an imperfect estimate. While the linear equation is good at capturing the trend in the data, no individual student’s aid will be perfectly predicted. Statistical software is usually used to compute the least squares line and the typical output generated as a result of fitting regression models looks like the one shown in Table 3.2. For now we will focus on the first column of the output, which lists \\({b}_0\\) and \\({b}_1\\). In Chapter 7 we will dive deeper into the remaining columns which give us information on how accurate and precise these values of intercept and slope that are calculated from a sample of 50 students are in estimating the population parameters of intercept and slope for all students. Table 3.2: Summary of least squares fit for the Elmhurst data. term estimate std.error statistic p.value (Intercept) 24.319 1.291 18.83 0 family_income -0.043 0.011 -3.98 0 If you would like to learn more about using R to fit linear models, see Section 3.4.1 for the interactive R tutorials. 3.2.3.1 Calculating the least squares regression line using summary statistics (special topic) An alternative way of calculating the values of intercept and slope of a least squares line is manual calculations using formulas. While this method is not commonly used by practicing statisticians and data scientists, it is useful to work through the first time you’re learning about the least squares line and modeling in general. Calculating these values by hand leverages two properties of the least squares line: The slope of the least squares line can be estimated by \\[b_1 = \\frac{s_y}{s_x} R \\] where \\(R\\) is the correlation between the two variables, and \\(s_x\\) and \\(s_y\\) are the sample standard deviations of the explanatory variable and response, respectively. If \\(\\bar{x}\\) is the sample mean of the explanatory variable and \\(\\bar{y}\\) is the sample mean of the vertical variable, then the point \\((\\bar{x}, \\bar{y})\\) is on the least squares line. Table 3.3 shows the sample means for the family income and gift aid as $101,780 and $19,940, respectively. We could plot the point \\((102, 19.9)\\) on Figure 3.13 to verify it falls on the least squares line (the solid line). Table 3.3: Summary statistics for family income and gift aid. Family income, \\(x\\) Gift aid, \\(y\\) mean sd mean sd R 102 63.2 19.9 5.46 -0.499 Next, we formally find the point estimates \\(b_0\\) and \\(b_1\\) of the parameters \\(\\beta_0\\) and \\(\\beta_1\\). Using the summary statistics in Table 3.3, compute the slope for the regression line of gift aid against family income. Compute the slope using the summary statistics from Table 3.3: \\[b_1 = \\frac{s_y}{s_x} r = \\frac{5.46}{63.2}(-0.499) = -0.0431\\] You might recall the form of a line from math class, which we can use to find the model fit, including the estimate of \\(b_0\\). Given the slope of a line and a point on the line, \\((x_0, y_0)\\), the equation for the line can be written as \\[y - y_0 = slope\\times (x - x_0)\\] Identifying the least squares line from summary statistics. To identify the least squares line from summary statistics: Estimate the slope parameter, \\(b_1 = (s_y / s_x) R\\). Noting that the point \\((\\bar{x}, \\bar{y})\\) is on the least squares line, use \\(x_0 = \\bar{x}\\) and \\(y_0 = \\bar{y}\\) with the point-slope equation: \\(y - \\bar{y} = b_1 (x - \\bar{x})\\). Simplify the equation, which would reveal that \\(b_0 = \\bar{y} - b_1 \\bar{x}\\). Using the point (102, 19.9) from the sample means and the slope estimate \\(b_1 = -0.0431\\), find the least-squares line for predicting aid based on family income. Apply the point-slope equation using \\((102, 19.9)\\) and the slope \\(b_1 = -0.0431\\): \\[\\begin{aligned} y - y_0 &amp;= b_1 (x - x_0) \\\\ y - 19.9 &amp;= -0.0431(x - 102) \\end{aligned}\\] Expanding the right side and then adding 19.9 to each side, the equation simplifies: \\[\\begin{aligned} \\widehat{aid} = 24.3 - 0.0431 \\times \\textit{family_income} \\end{aligned}\\] Here we have replaced \\(y\\) with \\(\\widehat{aid}\\) and \\(x\\) with family_income to put the equation in context. The final equation should always include a “hat” on the variable being predicted, whether it is a generic “\\(y\\)” or a named variable like “\\(aid\\)”. 3.2.4 Extrapolation is treacherous When those blizzards hit the East Coast this winter, it proved to my satisfaction that global warming was a fraud. That snow was freezing cold. But in an alarming trend, temperatures this spring have risen. Consider this: On February \\(6^{th}\\) it was 10 degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So clearly folks the climate debate rages on.54 Stephen Colbert April 6th, 2010 Linear models can be used to approximate the relationship between two variables. However, these models have real limitations. Linear regression is simply a modeling framework. The truth is almost always much more complex than our simple line. For example, we do not know how the data outside of our limited window will behave. Use the model \\(\\widehat{aid} = 24.3 - 0.0431 \\times \\textit{family_income}\\) to estimate the aid of another freshman student whose family had income of $1 million. We want to calculate the aid for a family with $1 million income. Note that in our model, this will be represented as 1,000 since the data are in $1,000s. \\[24.3 - 0.0431 \\times 1000 = -18.8 \\] The model predicts this student will have -$18,800 in aid (!). However, Elmhurst College does not offer negative aid where they select some students to pay extra on top of tuition to attend. Applying a model estimate to values outside of the realm of the original data is called extrapolation. Generally, a linear model is only an approximation of the real relationship between two variables. If we extrapolate, we are making an unreliable bet that the approximate linear relationship will be valid in places where it has not been analyzed. 3.2.5 Describing the strength of a fit We evaluated the strength of the linear relationship between two variables earlier using the correlation, \\(R\\). However, it is more common to explain the strength of a linear fit using \\(R^2\\), called R-squared. If provided with a linear model, we might like to describe how closely the data cluster around the linear fit. The \\(R^2\\) of a linear model describes the amount of variation in the response that is explained by the least squares line. For example, consider the Elmhurst data, shown in Figure 3.13. The variance of the response variable, aid received, is about \\(s_{aid}^2 \\approx 29.8\\) million. However, if we apply our least squares line, then this model reduces our uncertainty in predicting aid using a student’s family income. The variability in the residuals describes how much variation remains after using the model: \\(s_{_{RES}}^2 \\approx 22.4\\) million. In short, there was a reduction of \\[\\frac{s_{aid}^2 - s_{_{RES}}^2}{s_{aid}^2} = \\frac{29800 - 22400}{29800} = \\frac{7500}{29800} \\approx 0.25\\] or about 25% in the data’s variation by using information about family income for predicting aid using a linear model. This corresponds exactly to the R-squared value: \\[R = -0.499 \\rightarrow R^2 = 0.25\\] The squared correlation coefficient, \\(R^2\\), is also called the coefficient of determination. Coefficient of determination: proportion of variability in the response explained by the model. Since \\(R\\) is always between -1 and 1, \\(R^2\\) will always be between 0 and 1. This statistic is called the coefficient of determination and measures the proportion of variation in the response variable, \\(y\\), that can be explained by the linear model with predictor \\(x\\). Examine the scatterplot of head length (mm) versus total length (cm) of possums in Figure 3.6. The correlation between these two variables is \\(R = 0.69\\). Find and interpret the coefficient of determination. To find \\(R^2\\), we square the correlation: \\(R^2 = (0.69)^2 = 0.48\\). This tells us that about 48% of variation in possum head length can be explained by total length. This is visualized in Figure 3.14. Figure 3.14: For these 104 possums, the range of head lengths is about 103 \\(-\\) 83 = 20 mm. However, among possums of the same total length (e.g., 85 cm), the range in head lengths is reduced to about 10 mm, or about a 50% reduction, which matches \\(R^2 = 0.48\\), or 48%. If a linear model has a very strong negative relationship with a correlation of -0.97, how much of the variation in the response is explained by the explanatory variable?55 More generally, \\(R^2\\) can be calculated as a ratio of a measure of variability around the line divided by a measure of total variability. Sums of squares to measure variability in \\(y\\). We can measure the variability in the \\(y\\) values by how far they tend to fall from their mean, \\(\\bar{y}\\). We define this value as the total sum of squares, \\[ SST = (y_1 - \\bar{y})^2 + (y_2 - \\bar{y})^2 + \\cdots + (y_n - \\bar{y})^2. \\] Left-over variability in the \\(y\\) values if we know \\(x\\) can be measured by the sum of squared errors, or sum of squared residuals56, \\[ SSE = (y_1 - \\hat{y}_1)^2 + (y_2 - \\hat{y}_2)^2 + \\cdots + (y_n - \\hat{y}_n)^2 = e_{1}^2 + e_{2}^2 + \\dots + e_{n}^2 \\] The coefficient of determination can then be calculated as \\[ R^2 = \\frac{SST - SSE}{SST} = 1 - \\frac{SSE}{SST} \\] Among 104 possums, the total variability in head length (mm) is \\(SST = 1315.2\\)57. The sum of squared residuals is \\(SSE = 687.0\\). Find \\(R^2\\). Since we know \\(SSE\\) and \\(SST\\), we can calculate \\(R^2\\) as \\[ R^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{687.0}{1315.2} = 0.48, \\] the same value we found when we squared the correlation: \\(R^2 = (0.69)^2 = 0.48\\). 3.2.6 Categorical predictors with two levels (special topic) Categorical variables are also useful in predicting outcomes. Here we consider a categorical predictor with two levels (recall that a level is the same as a category). We’ll consider Ebay auctions for a video game, Mario Kart for the Nintendo Wii, where both the total price of the auction and the condition of the game were recorded. Here we want to predict total price based on game condition, which takes values used and new. The mariokart data can be found in the openintro package. A plot of the auction data is shown in Figure 3.15. Note that the original dataset contains some Mario Kart games being sold at prices above $100 but for this analysis we have limited our focus to the 141 Mario Kart games that are sold below $100. Figure 3.15: Total auction prices for the video game Mario Kart, divided into used (\\(x = 0\\)) and new (\\(x = 1\\)) condition games. The least squares regression line is also shown. To incorporate the game condition variable into a regression equation, we must convert the categories into a numerical form. We will do so using an indicator variable called condnew, which takes value 1 when the game is new and 0 when the game is used. Using this indicator variable, the linear model may be written as \\[\\widehat{price} = \\beta_0 + \\beta_1 \\times condnew\\] The parameter estimates are given in Table 3.4. Table 3.4: Least squares ression summary for the final auction price against the condition of the game. term estimate std.error statistic p.value (Intercept) 42.9 0.814 52.67 0 condnew 10.9 1.258 8.66 0 Using values from Table 3.4, the model equation can be summarized as \\[\\widehat{price} = 42.871 + 10.90 \\times condnew\\] Interpret the two parameters estimated in the model for the price of Mario Kart in eBay auctions. The intercept is the estimated price when condnew takes value 0, i.e. when the game is in used condition. That is, the average selling price of a used version of the game is $42.87. The slope indicates that, on average, new games sell for about $10.90 more than used games. Interpreting model estimates for categorical predictors. The estimated intercept is the value of the response variable for the first category (i.e. the category corresponding to an indicator value of 0). The estimated slope is the average change in the response variable between the two categories. We’ll elaborate further on this topic in Chapter 4, where we examine the influence of many predictor variables simultaneously using multiple regression. 3.3 Outliers in linear regression In this section, we identify criteria for determining which outliers are important and influential. Outliers in regression are observations that fall far from the cloud of points. These points are especially important because they can have a strong influence on the least squares line. 3.3.1 Types of outliers There are three plots shown in Figure 3.16 along with the least squares line and residual plots. For each scatterplot and residual plot pair, identify the outliers and note how they influence the least squares line. Recall that an outlier is any point that doesn’t appear to belong with the vast majority of the other points. A: There is one outlier far from the other points, though it only appears to slightly influence the line. B: There is one outlier on the right, though it is quite close to the least squares line, which suggests it wasn’t very influential. C: There is one point far away from the cloud, and this outlier appears to pull the least squares line up on the right; examine how the line around the primary cloud doesn’t appear to fit very well. Figure 3.16: Three plots, each with a least squares line and residual plot. All data sets have at least one outlier. There are three plots shown in Figure 3.17 along with the least squares line and residual plots. As you did in the previous exercise, for each scatterplot and residual plot pair, identify the outliers and note how they influence the least squares line. Recall that an outlier is any point that doesn’t appear to belong with the vast majority of the other points. D: There is a primary cloud and then a small secondary cloud of four outliers. The secondary cloud appears to be influencing the line somewhat strongly, making the least square line fit poorly almost everywhere. There might be an interesting explanation for the dual clouds, which is something that could be investigated. E: There is no obvious trend in the main cloud of points and the outlier on the right appears to largely control the slope of the least squares line. F: There is one outlier far from the cloud. However, it falls quite close to the least squares line and does not appear to be very influential. Figure 3.17: Three plots, each with a least squares line and residual plot. All data sets have at least one outlier. Examine the residual plots in Figures 3.16 and 3.17. You will probably find that there is some trend in the main clouds of Plots C, D, and E. In these cases, the outliers influenced the slope of the least squares lines. In Plot E, data with no clear trend were assigned a line with a large trend simply due to one outlier (!). Leverage. Points that fall horizontally away from the center of the cloud tend to pull harder on the line, so we call them points with high leverage. Points that fall horizontally far from the line are points of high leverage; these points can strongly influence the slope of the least squares line. If one of these high leverage points does appear to actually invoke its influence on the slope of the line – as in Plots C, D, and E of Figures 3.16 and 3.17 – then we call it an influential point. Influential point. A point is influential if, had we fitted the line without it, the influential point would have been unusually far from the least squares line. Influential points tend to pull the slope of the line up or down from what we would have seen had we fit the regression line without it. It is tempting to remove outliers. Don’t do this without a very good reason. Models that ignore exceptional (and interesting) cases often perform poorly. For instance, if a financial firm ignored the largest market swings – the “outliers” – they would soon go bankrupt by making poorly thought-out investments. 3.4 R: Correlation and regression 3.4.1 Interactive R tutorials Navigate the concepts you’ve learned in this chapter in R using the following self-paced tutorials. All you need is your browser to get started! Tutorial 3: Introduction to linear models Tutorial 3 - Lesson 1: Visualizing two variables Tutorial 2 - Lesson 2: Correlation Tutorial 2 - Lesson 3: Simple linear regression Tutorial 2 - Lesson 4: Interpreting regression models Tutorial 2 - Lesson 5: Model fit You can also access the full list of tutorials supporting this book here. 3.4.2 R labs Further apply the concepts you’ve learned in this chapter in R with computational labs that walk you through a data analysis case study. Introduction to linear regression - Human Freedom Index Full list of labs supporting OpenIntro::Introduction to Modern Statistics 3.5 Chapter review 3.5.1 Terms We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as bolded text. coefficient of determination indicator variable predictor total sum of squares correlation influential point R-squared extrapolation least squares criterion residuals high leverage least squares line sum of squared error If a model underestimates an observation, then the model estimate is below the actual. The residual, which is the actual observation value minus the model estimate, must then be positive. The opposite is true when the model overestimates the observation: the residual is negative.↩︎ Green diamond: \\(\\hat{y} = 43+0.57x = 43+0.57\\times 85.0 = 91.45 \\rightarrow e = y - \\hat{y} = 98.6-91.45=7.15\\). This is close to the earlier estimate of 7. Yellow triangle: \\(\\hat{y} = 43+0.57x = 97.44 \\rightarrow e = -3.44\\). This is also close to the estimate of -4.↩︎ Formally, we can compute the correlation for observations \\((x_1, y_1)\\), \\((x_2, y_2)\\), ..., \\((x_n, y_n)\\) using the formula \\[R = \\frac{1}{n-1} \\sum_{i=1}^{n} \\frac{x_i-\\bar{x}}{s_x}\\frac{y_i-\\bar{y}}{s_y}\\] where \\(\\bar{x}\\), \\(\\bar{y}\\), \\(s_x\\), and \\(s_y\\) are the sample means and standard deviations for each variable.↩︎ We’ll leave it to you to draw the lines. In general, the lines you draw should be close to most points and reflect overall trends in the data.↩︎ Larger family incomes are associated with lower amounts of aid, so the correlation will be negative. Using a computer, the correlation can be computed: -0.499.↩︎ There are applications where the sum of residual magnitudes may be more useful, and there are plenty of other criteria we might consider. However, this book only applies the least squares criterion.↩︎ http://www.cc.com/video-clips/l4nkoq↩︎ About \\(R^2 = (-0.97)^2 = 0.94\\) or 94% of the variation is explained by the linear model.↩︎ The difference \\(SST - SSE\\) is called the regression sum of squares, \\(SSR\\), and can also be calculated as \\(SSR = (\\hat{y}_1 - \\bar{y})^2 + (\\hat{y}_2 - \\bar{y})^2 + \\cdots + (\\hat{y}_n - \\bar{y})^2\\). \\(SSR\\) represents the variation in \\(y\\) that was accounted for in our model.↩︎ \\(SST\\) can be calculated by finding the sample variance, \\(s^2\\) and multiplying by \\(n-1\\).↩︎ "],
["mult-reg.html", "Chapter 4 Multivariable models 4.1 Gapminder world 4.2 Simpson’s Paradox, revisited 4.3 Multiple regression (special topic) 4.4 Chapter 4 review", " Chapter 4 Multivariable models The principles of simple linear regression lay the foundation for more sophisticated regression models used in a wide range of challenging settings. In this chapter, we explore the idea of “multivariable thinking” – investigating how multiple variables interact with a response variable and with each other – through a few examples. Multiple regression, which introduces the possibility of more than one predictor in a linear model, and logistic regression, a technique for predicting categorical outcomes with two levels, are presented as special topics not covered in this course. 4.1 Gapminder world Gapminder is a “fact tank” that uses publicly available world data to produce data visualizations and teaching resources on global development. We will use an excerpt of their data to explore relationships among world health metrics across countries and regions between the years 1952 and 2007. The gapminder data can be found in the gapminder package. First, let’s look at the relationship between Gross Domestic Product (GDP) per capita (a measure of the wealth of a country) and Life Expectancy (in years) in the year 2007 in Figure 4.1. Figure 4.1: Scatterplot displaying the relationship Life Expectancy and GDP per capita in the year 2007. Note that GDP per capita is plotted on the log scale. What does each dot represent?58 As one might expect, there is a general positive trend between GDP and life expectancy. But does this trend hold across all regions? Let’s explore in Figure 4.2. Figure 4.2: Scatterplot displaying the relationship Life Expectancy and GDP per capita by region in the year 2007. Note that GDP per capita is plotted on the log scale. Regression lines for each continent have been added. Does the relationship between GDP per capita and life expectancy differ across regions of the world? Yes. Looking at Figure 4.2, the five regression lines have differing slopes, telling us that the estimated change in life expectancy for a given increase in GDP per capita differs across countries. In the Americas and Oceania, life expectancy seems to rise faster with GDP per capita than the other three regions. In this case, we say that GDP per capita interacts with continent in its relationship with life expectancy. Interaction between two explanatory variables. If the relationship between an explanatory variable \\(x\\) and response variable \\(y\\) changes for different levels of another variable \\(z\\), then we say that \\(x\\) and \\(z\\) interact in their relationship with \\(y\\). If \\(x\\) and \\(y\\) are quantitative, and \\(z\\) is categorical, as in Figure 4.2 – where \\(x\\) = GDP per capita, \\(y\\) = life expectancy, and \\(z\\) = continent – then if the different regression lines for each level of \\(z\\) have parallel slopes, we say that \\(x\\) and \\(z\\) do not interact. If the slopes are not parallel, then interaction exists between \\(x\\) and \\(z\\). So far, we’ve explored relationships between three variables, how could we visualize relationships between five variables?59 Let’s add another variable to our plot – population. An aesthetic is a visual property of the objects in your plot. Each variable is mapped to an aesthetic. Some possible aesthetics and whether they should be used for quantitative or categorical variables are listed in Table 4.1. Table 4.1: Examples of aesthetics and types of variables mapped to these aesthetics. Aesthetic Variable Position on the x-axis as a number line Quantitative Position on the y-axis as a number line Quantitative Position on the x-axis as categories Categorical Position on the y-axis as categories Categorical Size Quantitative Color Categorical Shape Categorical In Figure 4.3, quantitative variables GDP per capita, life expectancy, and population are mapped to aesthetics: position on the \\(x\\)-axis, position on the \\(y\\)-axis, and population, respectively. The categorical variable Region is mapped to color. Explore individual countries by hovering over the points. Figure 4.3: Scatterplot displaying the relationship between four variables in the year 2007: GDP per capita (x-axis), Life Expectancy (y-axis), Population (size), and Region (color).] How does this pattern compare to what was happening in 1952 (see Figure 4.4)? Figure 4.4: Scatterplot displaying the relationship between four variables in the year 1952: GDP per capita (x-axis), Life Expectancy (y-axis), Population (size), and Region (color). We can visualize relationships among four variables in the plots above (the three quantitative variables on the x- and y-axes and size, and the categorical variable as color). We could even add a fifth variable using another aesthetic, like using shape to represent the most popular religion in each country. How could we visualize what happens across time? Hans Rosling has the answer with dynamic visualization. Click on the image below to watch. 4.2 Simpson’s Paradox, revisited Simpson’s Paradox was introduced in Section 2.1.4 through an example on race and capital punishment. In that example, all three variables of interest were categorical. In this section, we present another example of this paradox using three quantitative variables. In 1993, respected political essayist George Will, wrote the following criticism of spending on public education in the United States. “The 10 states with the lowest per pupil spending included four – North Dakota, South Dakota, Tennessee, Utah – among the 10 states with the top SAT scores. Only one of the 10 states with the highest per pupil expenditures – Wisconsin – was among the 10 states with the highest SAT scores. New Jersey has the highest per pupil expenditures, an astonishing $10,561, which teachers’ unions elsewhere try to use as a negotiating benchmark. New Jersey’s rank regarding SAT scores? Thirty-ninth… The fact that the quality of schools… [fails to correlate] with education appropriations will have no effect on the teacher unions’ insistence that money is the crucial variable.” — George F. Will, September 12, 1993, “Meaningless Money Factor,” The Washington Post, C7. George Will based his claim state expenditures, average SAT scores, and other education-based variables. These data are in the data set SAT60, the first six rows of which are displayed in Table 4.2. Variables for this data set are described in Table 4.3 Table 4.2: Six rows from the SAT data set. State expend ratio salary frac verbal math sat 1 Alabama 4.41 17.2 31.1 8 491 538 1029 2 Alaska 8.96 17.6 48.0 47 445 489 934 3 Arizona 4.78 19.3 32.2 27 448 496 944 4 Arkansas 4.46 17.1 28.9 6 482 523 1005 5 California 4.99 24.0 41.1 45 417 485 902 6 Colorado 5.44 18.4 34.6 29 462 518 980 Table 4.3: Variables and their descriptions for the SAT data set. Variable Description State Name of state expend Expenditure per pupil in average daily attendance in public elementary and secondary schools, 1994-95 (in thousands of dollars) ratio Average pupil/teacher ratio in public elementary and secondary schools, Fall 1994 salary Estimated average annual salary of teachers in public elementary and secondary schools, 1994-95 (in thousands of dollars) frac Percentage of all eligible students taking the SAT, 1994-95 verbal Average verbal SAT score, 1994-95 math Average math SAT score, 1994-95 sat Average total score on the SAT, 1994-95 Mr. Will claims that expenditure per pupil has a negative correlation with average SAT scores across states. Is this true? Indeed, the correlation between expend and sat is equal to \\(r\\) = -0.381, and the relationship between the two variables is shown in Figure 4.5. Hover over each point to view data on a particular State. Figure 4.5: Expenditure per pupil in average daily attendance in public elementary and secondary schools ($1000) verses average SAT score for the 50 states plus the District of Columbia over school year 1994-1995. This may seem surprising, but remember – these are observational data. We cannot conclude, as George Will does, that decreasing expenditures will increase SAT scores. In fact, there is one clear confounding variable in these data: percentage of all eligible students taking the SAT. What confounding variables may be present in this study? How could we determine whether a variable is confounding the relationship between school expenditures and SAT scores? In some states at the time these data were collected, it was more common to take the ACT than the SAT. For students in these states, if they wanted to go to a state school, they need only take the ACT. However, if they wanted to attend college in another state, they might take the SAT. Thus, the percent of students taking the SAT in a state, frac, could be a confounding variable. In order for frac to be confounding, it needs to be associated with our explanatory variable, expend, as well as with the response variable, sat. One could look at scatterplots and correlation between frac and expend, and between frac and sat, to determine if frac is confounding the relationship between expend and sat. Scatterplots of expend versus frac and sat versus frac are displayed in Figure 4.6. The correlation between expend and frac is 0.593, and the correlation between sat and frac is -0.887. Figure 4.6: Expenditure per pupil in average daily attendance in public elementary and secondary schools ($1000) and average SAT score plotted against percent of students taking the SAT for the 50 states plus the District of Columbia over school year 1994-1995. Now that we’ve determined that frac is a confounding variable, let’s examine if and how it modifies the relationship between expend and sat. Since it is hard to visualize three quantitative variables – 3-D scatterplots are difficult to visualize – let’s bin the variable frac into three groups. States with fewer than 15% of eligible students taking the SAT will be classified as a low percentage. States with between 15% - 55% of eligible students taking the SAT will be classified as medium Those states with more than 55% of eligible students taking the SAT will be called high. Next, we fit separate regression lines for each group. This model is shown in Figure 4.7. Figure 4.7: Average SAT score plotted against school expenditures per pupil, categorized by a Low (\\(&lt;\\) 15%), Medium (15-55%), or High (\\(&gt;\\) 55%) percent of students taking the SAT. Figure 4.7 demonstrates that the overall negative correlation between SAT scores and expenditures disappears, and even turns slightly positive, when we examine this relationship within states with similar fractions of students taking the SAT. Why do these data exhibit Simpson’s Paradox?61 4.3 Multiple regression (special topic) The principles of simple linear regression lay the foundation for more sophisticated regression models used in a wide range of challenging settings. In this section, we explore multiple regression, which introduces the possibility of more than one predictor in a linear model. Multiple regression extends simple two-variable regression to the case that still has one response but many predictors (denoted \\(x_1\\), \\(x_2\\), \\(x_3\\), ...). The method is motivated by scenarios where many variables may be simultaneously connected to an output. We will consider data about loans from the peer-to-peer lender, Lending Club, which is a data set we first encountered in Chapter 1. The loan data includes terms of the loan as well as information about the borrower. The outcome variable we would like to better understand is the interest rate assigned to the loan. For instance, all other characteristics held constant, does it matter how much debt someone already has? Does it matter if their income has been verified? Multiple regression will help us answer these and other questions. The data set includes results from 10,000 loans, and we’ll be looking at a subset of the available variables, some of which will be new from those we saw in earlier chapters. The first six observations in the data set are shown in Table 4.4, and descriptions for each variable are shown in Table 4.5. Notice that the past bankruptcy variable (bankruptcy) is an indicator variable, where it takes the value 1 if the borrower had a past bankruptcy in their record and 0 if not. Using an indicator variable in place of a category name allows for these variables to be directly used in regression. Two of the other variables are categorical (verified_income and issue_month), each of which can take one of a few different non-numerical values; we’ll discuss how these are handled in the model in Section 4.3.1. The data can be found in the openintro package: loans_full_schema. Based on the data in this dataset we have created to new variables: credit_util which is calculated as the total credit utilized divided by the total credit limit and bankruptcy which turns the number of bankruptcies to an indicator variable (0 for no bankrupties and 1 for at least 1 bankruptcies). We will refer to this modified dataset as loans. Table 4.4: First six rows from the loans_full_schema data set. interest_rate verified_income debt_to_income credit_util bankruptcy term issue_month credit_checks 14.07 Verified 18.01 0.548 0 60 Mar-2018 6 12.61 Not Verified 5.04 0.150 1 36 Feb-2018 1 17.09 Source Verified 21.15 0.661 0 36 Feb-2018 4 6.72 Not Verified 10.16 0.197 0 36 Jan-2018 0 14.07 Verified 57.96 0.755 0 36 Mar-2018 7 6.72 Not Verified 6.46 0.093 0 36 Jan-2018 6 Table 4.5: Variables and their descriptions for the loans data set. variable description interest_rate Interest rate on the loan, in an annual percentage. verified_income Categorical variable describing whether the borrower’s income source and amount have been verified, with levels Verified, Source Verified, and Not Verified. debt_to_income Debt-to-income ratio, which is the percentage of total debt of the borrower divided by their total income. credit_util Of all the credit available to the borrower, what fraction are they utilizing. For example, the credit utilization on a credit card would be the card’s balance divided by the card’s credit limit. bankruptcy An indicator variable for whether the borrower has a past bankruptcy in their record. This variable takes a value of 1 if the answer is yes and 0 if the answer is no. term The length of the loan, in months. issue_month The month and year the loan was issued, which for these loans is always during the first quarter of 2018. credit_checks Number of credit checks in the last 12 months. For example, when filing an application for a credit card, it is common for the company receiving the application to run a credit check. 4.3.1 Indicator and categorical predictors Let’s start by fitting a linear regression model for interest rate with a single predictor indicating whether or not a person has a bankruptcy in their record: \\[\\widehat{\\texttt{interest_rate}} = 12.33 + 0.74 \\times bankruptcy\\] Results of this model are shown in Table 4.6. Table 4.6: Summary of a linear model for predicting interest rate based on whether the borrower has a bankruptcy in their record. Degrees of freedom for this model is 9998. term estimate std.error statistic p.value (Intercept) 12.338 0.053 231.49 &lt;0.0001 bankruptcy 0.737 0.153 4.82 &lt;0.0001 Interpret the coefficient for the past bankruptcy variable in the model. Is this coefficient significantly different from 0? The variable takes one of two values: 1 when the borrower has a bankruptcy in their history and 0 otherwise. A slope of 0.74 means that the model predicts a 0.74% higher interest rate for those borrowers with a bankruptcy in their record. (See Section 3.2.6 for a review of the interpretation for two-level categorical predictor variables.) Examining the regression output in Table 4.6, we can see that the p-value for is very close to zero, indicating there is strong evidence the coefficient is different from zero when using this simple one-predictor model. Suppose we had fit a model using a 3-level categorical variable, such as verified_income. The output from software is shown in Table 4.7. This regression output provides multiple rows for the variable. Each row represents the relative difference for each level of verified_income. However, we are missing one of the levels: Not Verified. The missing level is called the reference level and it represents the default level that other levels are measured against. Table 4.7: Summary of a linear model for predicting interest rate based on whether the borrower’s income source and amount has been verified. This predictor has three levels, which results in 2 rows in the regression output. term estimate std.error statistic p.value (Intercept) 11.10 0.081 137.2 &lt;0.0001 verified_incomeSource Verified 1.42 0.111 12.8 &lt;0.0001 verified_incomeVerified 3.25 0.130 25.1 &lt;0.0001 How would we write an equation for this regression model? The equation for the regression model may be written as a model with two predictors: \\[\\widehat{\\texttt{interest_rate}} = 11.10 + 1.42 \\times \\text{verified_income}_{\\text{Source Verified}} + 3.25 \\times \\text{verified_income}_{\\text{Verified}}\\] We use the notation \\(\\text{variable}_{\\text{level}}\\) to represent indicator variables for when the categorical variable takes a particular value. For example, \\(\\text{verified_income}_{\\text{Source Verified}}\\) would take a value of 1 if was for a loan, and it would take a value of 0 otherwise. Likewise, \\(\\text{verified_income}_{\\text{Verified}}\\) would take a value of 1 if took a value of verified and 0 if it took any other value. The notation \\(\\text{variable}_{\\text{level}}\\) may feel a bit confusing. Let’s figure out how to use the equation for each level of the verified_income variable. Using the model for predicting interest rate from income verification type, compute the average interest rate for borrowers whose income source and amount are both unverified. When verified_income takes a value of Not Verified, then both indicator functions in the equation for the linear model are set to 0: \\[\\widehat{\\texttt{interest_rate}} = 11.10 + 1.42 \\times 0 + 3.25 \\times 0 = 11.10\\] The average interest rate for these borrowers is 11.1%. Because the level does not have its own coefficient and it is the reference value, the indicators for the other levels for this variable all drop out. Using the model for predicting interest rate from income verification type, compute the average interest rate for borrowers whose income source and amount are both unverified. When verified_income takes a value of Source Verified, then the corresponding variable takes a value of 1 while the other (\\(\\text{verified_income}_{\\text{Verified}}\\)) is 0: \\[\\widehat{\\texttt{interest_rate}} = = 11.10 + 1.42 \\times 1 + 3.25 \\times 0 = 12.52\\] The average interest rate for these borrowers is 12.52%. Compute the average interest rate for borrowers whose income source and amount are both verified.62 Predictors with several categories. When fitting a regression model with a categorical variable that has \\(k\\) levels where \\(k &gt; 2\\), software will provide a coefficient for \\(k - 1\\) of those levels. For the last level that does not receive a coefficient, this is the , and the coefficients listed for the other levels are all considered relative to this reference level. Interpret the coefficients in the model.63 The higher interest rate for borrowers who have verified their income source or amount is surprising. Intuitively, we’d think that a loan would look less risky if the borrower’s income has been verified. However, note that the situation may be more complex, and there may be confounding variables that we didn’t account for. For example, perhaps lender require borrowers with poor credit to verify their income. That is, verifying income in our data set might be a signal of some concerns about the borrower rather than a reassurance that the borrower will pay back the loan. For this reason, the borrower could be deemed higher risk, resulting in a higher interest rate. (What other confounding variables might explain this counter-intuitive relationship suggested by the model?) How much larger of an interest rate would we expect for a borrower who has verified their income source and amount vs a borrower whose income source has only been verified?64 4.3.2 Many predictors in a model The world is complex, and it can be helpful to consider many factors at once in statistical modeling. For example, we might like to use the full context of borrower to predict the interest rate they receive rather than using a single variable. This is the strategy used in multiple regression. While we remain cautious about making any causal interpretations using multiple regression on observational data, such models are a common first step in gaining insights or providing some evidence of a causal connection. We want to construct a model that accounts for not only for any past bankruptcy or whether the borrower had their income source or amount verified, but simultaneously accounts for all the variables in the loans data set: verified_income, debt_to_income, credit_util, bankruptcy, term, issue_month, and credit_checks. \\[\\begin{align*} \\widehat{\\texttt{interest_rate}} &amp;= \\beta_0 + \\beta_1\\times \\texttt{verified_income}_{\\texttt{Source Verified}} + \\beta_2\\times \\texttt{verified_income}_{\\texttt{Verified}} \\\\ &amp;\\qquad\\ + \\beta_3\\times \\texttt{debt_to_income} \\\\ &amp;\\qquad\\ + \\beta_4 \\times \\texttt{credit_util} \\\\ &amp;\\qquad\\ + \\beta_5 \\times \\texttt{bankruptcy} \\\\ &amp;\\qquad\\ + \\beta_6 \\times \\texttt{term} \\\\ &amp;\\qquad\\ + \\beta_7 \\times \\texttt{issue_month}_{\\texttt{Jan-2018}} + \\beta_8 \\times \\texttt{issue_month}_{\\texttt{Mar-2018}} \\\\ &amp;\\qquad\\ + \\beta_9 \\times \\texttt{credit_checks} \\end{align*}\\] This equation represents a holistic approach for modeling all of the variables simultaneously. Notice that there are two coefficients for verified_income and also two coefficients for issue_month, since both are 3-level categorical variables. We estimate the parameters \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\cdots\\), \\(\\beta_9\\) in the same way as we did in the case of a single predictor. We select \\(b_0\\), \\(b_1\\), \\(b_2\\), \\(\\cdots\\), \\(b_9\\) that minimize the sum of the squared residuals: \\[SSE = e_1^2 + e_2^2 + \\dots + e_{10000}^2 = \\sum_{i=1}^{10000} e_i^2 = \\sum_{i=1}^{10000} \\left(y_i - \\hat{y}_i\\right)^2\\] where \\(y_i\\) and \\(\\hat{y}_i\\) represent the observed interest rates and their estimated values according to the model, respectively. 10,000 residuals are calculated, one for each observation. We typically use a computer to minimize the sum of squares and compute point estimates, as shown in the sample output in Table 4.8. Using this output, we identify the point estimates \\(b_i\\) of each \\(\\beta_i\\), just as we did in the one-predictor case. Table 4.8: Output for the regression model, where interest rate is the outcome and the variables listed are the predictors. Degrees of freedom for this model is 9990. term estimate std.error statistic p.value (Intercept) 1.894 0.210 9.008 &lt;0.0001 verified_incomeSource Verified 0.997 0.099 10.056 &lt;0.0001 verified_incomeVerified 2.563 0.117 21.873 &lt;0.0001 debt_to_income 0.022 0.003 7.434 &lt;0.0001 credit_util 4.897 0.162 30.249 &lt;0.0001 bankruptcy 0.391 0.132 2.957 0.0031 term 0.153 0.004 38.889 &lt;0.0001 issue_monthJan-2018 0.046 0.108 0.421 0.6736 issue_monthMar-2018 -0.042 0.107 -0.391 0.696 credit_checks 0.228 0.018 12.516 &lt;0.0001 Multiple regression model. A multiple regression model is a linear model with many predictors. In general, we write the model as \\[\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k\\] when there are \\(k\\) predictors. We always estimate the \\(\\beta_i\\) parameters using statistical software. Write out the regression model using the point estimates from Table 4.8. How many predictors are there in this model? The fitted model for the interest rate is given by: \\[\\begin{align*} \\widehat{\\texttt{interest_rate}} &amp;= 1.925 + 0.975 \\times \\texttt{verified_income}_{\\texttt{Source Verified}} \\\\ &amp;\\qquad\\ + 2.537 \\times \\texttt{verified_income}_{\\texttt{Verified}} \\\\ &amp;\\qquad\\ + 0.021 \\times \\texttt{debt_to_income} \\\\ &amp;\\qquad\\ + 4.896 \\times \\texttt{credit_util} \\\\ &amp;\\qquad\\ + 0.386 \\times \\texttt{bankruptcy} \\\\ &amp;\\qquad\\ + 0.154 \\times \\texttt{term} \\\\ &amp;\\qquad\\ + 0.028 \\times \\texttt{issue_month}_{\\texttt{Jan-2018}} \\\\ &amp;\\qquad\\ - 0.040 \\times \\texttt{issue_month}_{\\texttt{Mar-2018}} \\\\ &amp;\\qquad\\ + 0.228 \\times \\texttt{credit_checks} \\end{align*}\\] If we count up the number of predictor coefficients, we get the effective number of predictors in the model: \\(k = 9\\). Notice that the categorical predictor counts as two, once for the two levels shown in the model. In general, a categorical predictor with \\(p\\) different levels will be represented by \\(p - 1\\) terms in a multiple regression model. What does \\(\\beta_4\\), the coefficient of variable , represent? What is the point estimate of \\(\\beta_4\\)?65 Compute the residual of the first observation in Table 4.4 on page using the full model.66 We estimated a coefficient for in Section 4.3.1 of \\(b_4 = 0.74\\) with a standard error of \\(SE_{b_1} = 0.15\\) when using simple linear regression. Why is there a difference between that estimate and the estimated coefficient of 0.39 in the multiple regression setting? If we examined the data carefully, we would see that some predictors are correlated. For instance, when we estimated the connection of the outcome interest_rate and predictor bankruptcy using simple linear regression, we were unable to control for other variables like whether the borrower had her income verified, the borrower’s debt-to-income ratio, and other variables. That original model was constructed in a vacuum and did not consider the full context. When we include all of the variables, underlying and unintentional bias that was missed by these other variables is reduced or eliminated. Of course, bias can still exist from other confounding variables. The previous example describes a common issue in multiple regression: correlation among predictor variables. We say the two predictor variables are (pronounced as co-linear) when they are correlated, and this collinearity complicates model estimation. While it is impossible to prevent collinearity from arising in observational data, experiments are usually designed to prevent predictors from being collinear. The estimated value of the intercept is 1.925, and one might be tempted to make some interpretation of this coefficient, such as, it is the model’s predicted price when each of the variables take value zero: income source is not verified, the borrower has no debt (debt-to-income and credit utilization are zero), and so on. Is this reasonable? Is there any value gained by making this interpretation?67 4.3.3 Interactive R tutorials Explore the additional topics of multiple regression and logistic regression in R using the following self-paced tutorials. All you need is your browser to get started! Tutorial 4: Multiple and logistic regression Tutorial 4 - Lesson 1: Parallel slopes Tutorial 4 - Lesson 2: Evaluating and extending parallel slopes model Tutorial 4 - Lesson 3: Multiple regression Tutorial 4 - Lesson 4: Logistic regression Tutorial 4 - Lesson 5: Case study - Italian restaurants in NYC You can also access the full list of tutorials supporting this book here. 4.3.4 R labs Further apply the concepts you’ve learned in this chapter in R with computational labs that walk you through a data analysis case study. Multiple linear regression - Grading the professor Full list of labs supporting OpenIntro::Introduction to Modern Statistics 4.4 Chapter 4 review 4.4.1 Terms We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as bolded text. aesthetic interaction parallel slopes confounding variable multiple regression reference level Each observational unit is a single country.↩︎ Each variable can be mapped to some “aesthetic” of the plot. Aesthetics include position on the x-axis, position on the y-axis, size, color, and shape. Since position and size are quantitative, they should be used for quantitative variables. Categorical variables should be mapped to color or shape, though we could also map them to position on the x-axis or y-axis if the axis lists categories rather than a number line.↩︎ These data can be downloaded from (https://math.montana.edu/courses/s216/data/sat.csv)[https://math.montana.edu/courses/s216/data/sat.csv].↩︎ The direction of a relationship of interest (SAT scores versus expenditures) was reversed when accounting for a third variable (percent taking the SAT).↩︎ When verified_income takes a value of Verified, then the corresponding variable takes a value of 1 while the other is 0: \\[11.10 + 1.42 \\times 0 + 3.25 \\times 1 = 14.35\\] The average interest rate for these borrowers is 14.35%.↩︎ Each of the coefficients gives the incremental interest rate for the corresponding level relative to the Not Verified level, which is the reference level. For example, for a borrower whose income source and amount have been verified, the model predicts that they will have a 3.25% higher interest rate than a borrower who has not had their income source or amount verified.↩︎ Relative to the Not Verified category, the Verified category has an interest rate of 3.25% higher, while the Source Verified category is only 1.42% higher. Thus, Verified borrowers will tend to get an interest rate about \\(3.25% - 1.42% = 1.83%\\) higher than Source Verified borrowers.↩︎ \\(\\beta_4\\) represents the change in interest rate we would expect if someone’s credit utilization was 0 and went to 1, all other factors held even. The point estimate is \\(b_4 = 4.90%\\).↩︎ To compute the residual, we first need the predicted value, which we compute by plugging values into the equation from earlier. For example, \\(\\texttt{verified_income}_{\\texttt{Source Verified}}\\) takes a value of 0, \\(\\texttt{verified_income}_{\\texttt{Verified}}\\) takes a value of 1 (since the borrower’s income source and amount were verified), was 18.01, and so on. This leads to a prediction of \\(\\widehat{\\texttt{interest_rate}}_1 = 18.09\\). The observed interest rate was 14.07%, which leads to a residual of \\(e_1 = 14.07 - 18.09 = -4.02\\).↩︎ Many of the variables do take a value 0 for at least one data point, and for those variables, it is reasonable. However, one variable never takes a value of zero: , which describes the length of the loan, in months. If is set to zero, then the loan must be paid back immediately; the borrower must give the money back as soon as she receives it, which means it is not a real loan. Ultimately, the interpretation of the intercept in this setting is not insightful.↩︎ "],
["inference-cat.html", "Chapter 5 Inference for categorical data 5.1 Foundations of inference 5.2 The normal distribution 5.3 One proportion 5.4 Difference of two proportions 5.5 Errors, Power, and Practical Importance 5.6 Summary of Z-procedures 5.7 R: Inference for categorical data 5.8 Chapter 5 review", " Chapter 5 Inference for categorical data Statistical inference is primarily concerned with understanding and quantifying the uncertainty of parameter estimates—that is, how variable is a sample statistic from sample to sample? While the equations and details change depending on the setting, the foundations for inference are the same throughout all of statistics. We will begin this chapter with a discussion of the foundations of inference, and introduce the two primary vehicles of inference: the hypothesis test and confidence interval. The rest of this chapter focuses on statistical inference for categorical data. The two data structures we detail are: one binary variable, summarized using a single proportion, and two binary variables, summarized using a difference (or ratio) of two proportions. We will also introduce a new important mathematical model, the normal distribution (as the foundation for the \\(z\\)-test). Throughout the book so far, you have worked with data in a variety of contexts. You have learned how to summarize and visualize the data as well as how to visualize multiple variables at the same time. Sometimes the data set at hand represents the entire research question. But more often than not, the data have been collected to answer a research question about a larger group of which the data are a (hopefully) representative subset. You may agree that there is almost always variability in data (one data set will not be identical to a second data set even if they are both collected from the same population using the same methods). However, quantifying the variability in the data is neither obvious nor easy to do (how different is one data set from another?). Suppose your professor splits the students in class into two groups: students on the left and students on the right. If \\(\\hat{p}_{_L}\\) and \\(\\hat{p}_{_R}\\) represent the proportion of students who own an Apple product on the left and right, respectively, would you be surprised if \\(\\hat{p}_{_L}\\) did not exactly equal \\(\\hat{p}_{_R}\\)? While the proportions would probably be close to each other, it would be unusual for them to be exactly the same. We would probably observe a small difference due to chance. If we don’t think the side of the room a person sits on in class is related to whether the person owns an Apple product, what assumption are we making about the relationship between these two variables? (Reminder: for these Guided Practice questions, you can check your answer in the footnote.)68 Studying randomness of this form is a key focus of statistics. Throughout this chapter, and those that follow, we provide two different approaches for quantifying the variability inherent in data: simulation-based methods and theory-based methods (mathematical models). Using the methods provided in this and future chapters, we will be able to draw conclusions beyond the data set at hand to research questions about larger populations. 5.1 Foundations of inference Given results seen in a sample, the process of determining what we can infer to the population based on sample results is called statistical inference. Statistical inferential methods enable us to understand and quantify the uncertainty of our sample results. Statistical inference helps us answer two questions about the population: How strong is the evidence of an effect? How large is the effect? The first question is answered through a hypothesis test, while the second is addressed with a confidence interval. Statistical inference is the practice of making decisions and conclusions from data in the context of uncertainty. Errors do occur, just like rare events, and the data set at hand might lead us to the wrong conclusion. While a given data set may not always lead us to a correct conclusion, statistical inference gives us tools to control and evaluate how often these errors occur. 5.1.1 Motivating example: Martian alphabet How well can humans distinguish one “Martian” letter from another? The Figure 5.1 displays two Martian letters—one is Kiki and the another is Bumba. Which do you think is Kiki and which do you think is Bumba?69 Figure 5.1: Two Martian letters: Bumba and Kiki. Do you think the letter Bumba is on the left or the right?70 This same image and question were presented to an introductory statistics class of 38 students. In that class, 34 students correctly identified Bumba as the Martian letter on the left. Assuming we can’t read Martian, is this result surprising? One of two possibilities occurred: We can’t read Martian, and these results just occurred by chance. We can read Martian, and these results reflect this ability. To decide between these two possibilities, we could calculate the probability of observing such results in a randomly selected sample of 38 students, under the assumption that students were just guessing. If this probability is very low, we’d have reason to reject the first possibility in favor of the second. We can calculate this probability using one of two methods: Simulation-based method: simulate lots of samples (Classes) of 38 students under the assumption that students are just guessing, then calculate the proportion of these simulated samples where we saw 34 or more students guessing correctly, or Theory-based method: develop a mathematical model for the sample proportion in this scenario and use the model to calculate the probability. How could you use a coin or cards to simulate the guesses of one sample of 38 students who cannot read Martian?71 For this situation—since “just guessing” means you have a 50% chance of guessing correctly—we could simulate a sample of 38 students’ guesses by flipping a coin 38 times and counting the number of times it lands on heads. Using a computer to repeat this process 1,000 times, we create the dot plot in Figure 5.2. Figure 5.2: A dot plot of 1,000 sample proportions; each calculated by flipping a coin 38 times and calculating the proportion of times the coin landed on heads. None of the 1,000 simulations had sample proportion of at least 89%, which was the proportion observed in the study. None of our simulated samples produce 34 of 38 correct guesses! That is, if students were just guessing, it is nearly impossible to observe 34 or more correct guesses in a sample of 38 students. Given this low probability, the more plausible possibility is 2. We can read Martian, and these results reflect this ability. We’ve just completed our first hypothesis test! Now, obviously no one can read Martian, so a more realistic possibility is that humans tend to choose Bumba on the left more often than the right—there is a greater than 50% chance of choosing Bumba as the letter on the left. Even though we may think we’re guessing just by chance, we have a preference for Bumba on the left. It turns out that the explanation for this preference is called synesthesia, a tendency for humans to correlate sharp sounding noises (e.g., Kiki) with sharp looking images.72 But wait—we’re not done! We have evidence that humans tend to prefer Bumba on the left, but by how much? To answer this, we need a confidence interval—an interval of plausible values for the true probability humans will select Bumba as the left letter. The width of this interval is determined by how variable sample proportions are from sample to sample. It turns out, there is a mathematical model for this variability that we will explore later in this chapter. For now, let’s take the standard deviation from our simulated sample proportions as an estimate for this variability: 0.08. Since the simulated distribution of proportions is bell-shaped, we know about 95% of sample proportions should fall within two standard deviations of the true proportion, so we can add and subtract this margin of error to our sample proportion to calculate an approximate 95% confidence interval73: \\[ \\frac{34}{38} \\pm 2\\times 0.08 = 0.89 \\pm 0.16 = (0.73, 1) \\] Thus, based on this data, we are 95% confident that the probability a human guesses Bumba on the left is somewhere between 73% and 100%. 5.1.2 Variability in a statistic There are two approaches to modeling how a statistic, such as a sample proportion, may vary from sample to sample. In the Martian alphabet example, we used a simulation-based approach to model this variability, using the standard deviation of the simulated distribution of sample proportions as a quantitative measure of this sampling variability. Simulation-based methods include the randomization tests and bootstrapping methods we will use in this textbook. We can also use a theory-based approach—one which makes use of mathematical modeling—and involves the normal and \\(t\\) probability distributions. All of the theory-based methods discussed in this book work (under certain conditions) because of a very important theorem in Statistics called the Central Limit Theorem. Central Limit Theorem. For large sample sizes, the sampling distribution of a sample proportion (or sample mean) will appear to follow a bell-shaped curve called the normal distribution. An example of a perfect normal distribution is shown in Figure 5.3. While the mean (center) and standard deviation (variability) may change for different scenarios, the general shape remains roughly intact. Figure 5.3: A normal curve. Recall from Chapter 2 that a distribution of a variable is a description of the possible values it takes and how frequently each value occurs. In a sampling distribution, our “variable” is a sample statistic, and the sampling distribution is a description of the possible values a sample statistic takes and how frequently each value occurs when looking across many many possible samples. It is quite amazing that something like a sample proportion, summarizing a categorical variable, will have a bell-shaped sampling distribution if we sample large enough samples! Theory-based methods also give us mathematical expressions for the standard deviation of a sampling distribution. For instance, if the true population proportion is \\(\\pi\\), then the standard deviation of the sampling distribution of sample proportions—how far away we would expect a sample proportion to be away from the population proportion—is74 \\[ SD(\\hat{p}) = \\sqrt{\\frac{\\pi(1-\\pi)}{n}}. \\] Typically, values of parameters such as \\(\\pi\\) are unknown, so we are unable to calculate these standard deviations. In this case, we substitute our “best guess” for \\(\\pi\\) in the formulas, either from a hypothesis or from a point estimate. Standard error. The standard deviation of a sampling distribution for a statistic, denoted by \\(SD\\)(statistic), represents how far away we would expect the statistic to land from the parameter. Since the formulas for these standard deviations depend on unknown parameters, we substitute our “best guess” for \\(\\pi\\) in the formulas, either from a hypothesis or from a point estimate. The resulting estimated standard deviation is called the standard error of the statistic, denoted by \\(SE\\)(statistic). 5.1.3 Hypothesis tests In the Martian alphabet example, we utilized a hypothesis test, which is a formal technique for evaluating two competing possibilities. Each hypothesis test involves a null hypothesis, which represents either a skeptical perspective or a perspective of no difference or no effect, and an alternative hypothesis, which represents a new perspective such as the possibility that there has been a change or that there is a treatment effect in an experiment. The alternative hypothesis is usually the reason the scientists set out to do the research in the first place. Null and alternative hypotheses. When we observe an effect in a sample, we would like to determine if this observed effect represents an actual effect in the population, or whether it was simply due to chance. We label these two competing claims, \\(H_0\\) and \\(H_A\\), which are spoken as “H-naught” and “H_A”. The null hypothesis (\\(H_0\\)) often represents either a skeptical perspective or a claim to be tested. The alternative hypothesis (\\(H_A\\)) represents an alternative claim under consideration and is often represented by a range of possible values for the parameter of interest. In the Martian alphabet example, which of the two competing possibilities was the null hypothesis? the alternative hypothesis?75 The hypothesis testing framework is a very general tool, and we often use it without a second thought. If a person makes a somewhat unbelievable claim, we are initially skeptical. However, if there is sufficient evidence that supports the claim, we set aside our skepticism. The hallmarks of hypothesis testing are also found in the US court system. The US court system A US court considers two possible claims about a defendant: they are either innocent or guilty. If we set these claims up in a hypothesis framework, which would be the null hypothesis and which the alternative? The jury considers whether the evidence is so convincing (strong) that there is no reasonable doubt regarding the person’s guilt. That is, the skeptical perspective (null hypothesis) is that the person is innocent until evidence is presented that convinces the jury that the person is guilty (alternative hypothesis). Analogously, in a hypothesis test, we assume the null hypothesis until evidence is presented that convinces us the alternative hypothesis is true. Jurors examine the evidence to see whether it convincingly shows a defendant is guilty. Notice that if a jury finds a defendant not guilty, this does not necessarily mean the jury is confident in the person’s innocence. They are simply not convinced of the alternative that the person is guilty. This is also the case with hypothesis testing: even if we fail to reject the null hypothesis, we typically do not accept the null hypothesis as truth. Failing to find strong evidence for the alternative hypothesis is not equivalent to providing evidence that the null hypothesis is true. p-value In the Martian alphabet example, we performed a simulation-based hypothesis test of the hypotheses: \\(H_0\\): The chance a human chooses Bumba on the left is 50%. \\(H_A\\): Humans have a preference for choosing Bumba on the left. The research question—can humans read Martian?—was framed in the context of these hypotheses. The null hypothesis (\\(H_0\\)) was a perspective of no effect (no ability to read Martian). The student data provided a point estimate of 89.5% (\\(34/38 \\times 100\\)%) for the true probability of choosing Bumba on the left. We determined that observing such a sample proportion from chance alone (assuming \\(H_0\\)) would be rare—it would only happen in less than 1 out of 1000 samples. When results like these are inconsistent with \\(H_0\\), we reject \\(H_0\\) in favor of \\(H_A\\). Here, we concluded that humans have a preference for choosing Bumba on the left. The less than 1-in-1000 chance is what we call a p-value, which is a probability quantifying the strength of the evidence against the null hypothesis and in favor of the alternative. p-value. The p-value is the probability of observing data at least as favorable to the alternative hypothesis as our current data set, if the null hypothesis were true. We typically use a summary statistic of the data, such as a proportion or difference in proportions, to help compute the p-value and evaluate the hypotheses. This summary value that is used to compute the p-value is often called the test statistic. When interpreting a p-value, remember that the definition of a p-value has three components. It is a (1) probability. What it is the probability of? It is the probability of (2) our observed sample statistic or one more extreme. Assuming what? It is the probability of our observed sample statistic or one more extreme, (3) assuming the null hypothesis is true: probability data76 null hypothesis What was the test statistic in the Martian alphabet example? The test statistic in the the Martian alphabet example was the sample proportion, \\(\\frac{34}{38} = 0.895\\) (or 89.5%). This is also the point estimate of the true probability that humans would choose Bumba on the left. Since the p-value is a probability, its value will always be between 0 and 1. The closer the p-value is to 0, the stronger the evidence we have against the null hypothesis. Why? A small p-value means that our data are unlikely to occur, if the null hypothesis is true. We take that to mean that the null hypothesis isn’t a plausible assumption, and we reject it. This process mimics the scientific method—it is easier to disprove a theory than prove it. If scientists want to find evidence that a new drug reduces the risk of stroke, then they assume it doesn’t reduce the risk of stroke and then show that the observed data are so unlikely to occur that the more plausible explanation is that the drug works. Think of p-values as a continuum of strength of evidence against the null, from 0 (extremely strong evidence) to 1 (no evidence). Beyond around 10%, the data provide no evidence against the null hypothesis. Be careful not to equate this with evidence for the null hypothesis, which is incorrect. The absence of evidence is not evidence of absence. Figure 5.4: Strength of evidence against the null for a continuum of p-values. Once the p-value is beyond around 0.10, the data provide no evidence against the null hypothesis. Regardless of the data structure or analysis method, the hypothesis testing framework always follows the same steps—only the details for how we model randomness in the data change. General steps of a hypothesis test. Every hypothesis test follows these same general steps: Frame the research question in terms of hypotheses. Collect and summarize data using a test statistic. Assume the null hypothesis is true, and simulate or mathematically model a null distribution for the test statistic. Compare the observed test statistic to the null distribution to calculate a p-value. Make a conclusion based on the p-value, and write a conclusion in context, in plain language, and in terms of the alternative hypothesis. Decisions and statistical significance In some cases, a decision to the hypothesis test is needed, with the two possible decisions as follows: Reject the null hypothesis Fail to reject the null hypothesis For which values of the p-value should you “reject” a null hypothesis? “fail to reject” a null hypothesis?77 In order to decide between these two options, we need a previously set threshold for our p-value: when the p-value is less than a previously set threshold, we reject \\(H_0\\); otherwise, we fail to reject \\(H_0\\). This threshold is called the significance level, and when the p-value is less than the significance level, we say the results are statistically significant. This means the data provide such strong evidence against \\(H_0\\) that we reject the null hypothesis in favor of the alternative hypothesis. The significance level, often represented by \\(\\alpha\\) (the Greek letter alpha), is typically set to \\(\\alpha = 0.05\\), but can vary depending on the field or the application and the real-life consequences of an incorrect decision. Using a significance level of \\(\\alpha = 0.05\\) in the Martian alphabet study, we can say that the data provided statistically significant evidence against the null hypothesis. Statistical significance. We say that the data provide statistically significant evidence against the null hypothesis if the p-value is less than some reference value called the significance level, denoted by \\(\\alpha\\). What’s so special about 0.05? We often use a threshold of 0.05 to determine whether a result is statistically significant. But why 0.05? Maybe we should use a bigger number, or maybe a smaller number. If you’re a little puzzled, that probably means you’re reading with a critical eye—good job! The OpenIntro authors have a video to help clarify why 0.05: https://www.openintro.org/book/stat/why05/ Sometimes it’s also a good idea to deviate from the standard. We’ll discuss when to choose a threshold different than 0.05 in Section 5.4.1. Statistical significance has been a hot topic in the news, related to the “reproducibility crisis” in some scientific fields. We encourage you to read more about the debate on the use of p-values and statistical significance. A good place to start would be the Nature article, “Scientists rise up against statistical significance,” from March 20, 2019. 5.1.4 Confidence intervals A point estimate provides a single plausible value for a parameter. However, a point estimate is rarely perfect—usually there is some error in the estimate. In addition to supplying a point estimate of a parameter, a next logical step would be to provide a plausible range of values for the parameter. A plausible range of values for the population parameter is called a confidence interval. Using only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net. We can throw a spear where we saw a fish, but we will probably miss. On the other hand, if we toss a net in that area, we have a good chance of catching the fish. If we report a point estimate, we probably will not hit the exact population parameter. On the other hand, if we report a range of plausible values—a confidence interval—we have a good shot at capturing the parameter. This reasoning also explains why we can never prove a null hypothesis. Sample statistics will vary from sample to sample. While we can quantify this uncertainty (e.g., we are 95% sure the statistic is within 0.15 of the parameter), we can never be certain that the parameter is an exact value. For example, suppose you want to test whether a coin is a fair coin, i.e., \\(H_0: \\pi = 0.50\\) versus \\(H_0: \\pi \\neq 0.50\\), so you toss the coin 10 times to collect data. In those 10 tosses, 6 land on heads and 4 land on tails, resulting in a p-value of 0.75478. We don’t have enough evidence to show that the coin is biased, but surely we wouldn’t say we just proved the coin is fair! There are only two possible decisions in a hypothesis test: (1) reject \\(H_0\\), or (2) fail to reject \\(H_0\\). Since one can never prove a null hypothesis—we can only disprove79 it—we never have the ability to “accept the null.” You may have seen this phrase in other textbooks or articles, but it is incorrect. If we want to be very certain we capture the population parameter, should we use a wider interval or a smaller interval?80 We will explore both simulation-based methods (bootstrapping) and theory-based methods for creating confidence intervals in this text. Though the details change with different scenarios, theory-based confidence intervals will always take the form: \\[ \\mbox{statistic} \\pm (\\mbox{multiplier}) \\times (\\mbox{standard error of the statistic}) \\] The statistic is our best guess for the value of the parameter, so it makes sense to build the confidence interval around that value. The standard error, which is a measure of the uncertainty associated with the statistic, provides a guide for how large we should make the confidence interval. The multiplier is determined by how confident we’d like to be, and tells us how many standard errors we need to add and subtract from the statistic. The amount we add and subtract from the statistic is called the margin of error. General form of a confidence interval. The general form of a theory-based confidence interval for an unknown parameter is \\[ \\mbox{statistic} \\pm (\\mbox{multiplier}) \\times (\\mbox{standard error of the statistic}) \\] The amount we add and subtract to the statistic to calculate the confidence interval is called the margin of error. \\[ \\mbox{margin of error} = (\\mbox{multiplier}) \\times (\\mbox{standard error of the statistic}) \\] In Section 5.3.3 we will discuss different percentages for the confidence interval (e.g., 90% confidence interval or 99% confidence interval). Section 5.4.3 provides a longer discussion on what “95% confidence” actually means. 5.2 The normal distribution Among all the distributions we see in statistics, one is overwhelmingly the most common. The symmetric, unimodal, bell curve is ubiquitous throughout statistics. It is so common that people know it as a variety of names including the normal curve, normal model, or normal distribution.81 Under certain conditions, sample proportions, sample means, and sample differences can be modeled using the normal distribution—the basis for our theory-based inference methods. Additionally, some variables such as SAT scores and heights of US adult males closely follow the normal distribution. Normal distribution facts. Many summary statistics and variables are nearly normal, but none are exactly normal. Thus the normal distribution, while not perfect for any single problem, is very useful for a variety of problems. We will use it in data exploration and to solve important problems in statistics. In this section, we will discuss the normal distribution in the context of data to become more familiar with normal distribution techniques. 5.2.1 Normal distribution model The normal distribution always describes a symmetric, unimodal, bell-shaped curve. However, normal curves can look different depending on the details of the model. Specifically, the normal model can be adjusted using two parameters: mean and standard deviation. As you can probably guess, changing the mean shifts the bell curve to the left or right, while changing the standard deviation stretches or constricts the curve. Figure 5.5 shows the normal distribution with mean \\(0\\) and standard deviation \\(1\\) (which is commonly referred to as the standard normal distribution) on top. A normal distribution with mean \\(19\\) and standard deviation \\(4\\) is shown on the bottom. Figure 5.6 shows the same two normal distributions on the same axis. Figure 5.5: Both curves represent the normal distribution, however, they differ in their center and spread. The normal distribution with mean 0 and standard deviation 1 is called the standard normal distribution. Figure 5.6: The two normal models shown above and now plotted together on the same scale. If a normal distribution has mean \\(\\mu\\) and standard deviation \\(\\sigma\\), we may write the distribution as \\(N(\\mu, \\sigma)\\). The two distributions in Figure 5.6 can be written as \\[\\begin{align*} N(\\mu=0,\\sigma=1)\\quad\\text{and}\\quad N(\\mu=19,\\sigma=4) \\end{align*}\\] Because the mean and standard deviation describe a normal distribution exactly, they are called the distribution’s parameters. Write down the short-hand for a normal distribution with (a) mean 5 and standard deviation 3, (b) mean -100 and standard deviation 10, and (c) mean 2 and standard deviation 9.82 5.2.2 Standardizing with Z-scores Table 5.1 shows the mean and standard deviation for total scores on the SAT and ACT. The distribution of SAT and ACT scores are both nearly normal. Suppose Ann scored 1800 on her SAT and Tom scored 24 on his ACT. Who performed better?83 Table 5.1: Mean and standard deviation for the SAT and ACT. SAT ACT Mean 1500 21 SD 300 5 Figure 5.7: Ann’s and Tom’s scores shown with the distributions of SAT and ACT scores. The solution to the previous example relies on a standardization technique called a Z-score, a method most commonly employed for nearly normal observations (but that may be used with any distribution). The Z-score of an observation is defined as the number of standard deviations it falls above or below the mean. If the observation is one standard deviation above the mean, its Z-score is 1. If it is 1.5 standard deviations below the mean, then its Z-score is -1.5. If \\(x\\) is an observation from a distribution \\(N(\\mu, \\sigma)\\), we define the Z-score mathematically as \\[\\begin{eqnarray*} Z = \\frac{x-\\mu}{\\sigma} \\end{eqnarray*}\\] Using \\(\\mu_{SAT}=1500\\), \\(\\sigma_{SAT}=300\\), and \\(x_{Ann}=1800\\), we find Ann’s Z-score: \\[\\begin{eqnarray*} Z_{Ann} = \\frac{x_{Ann} - \\mu_{SAT}}{\\sigma_{SAT}} = \\frac{1800-1500}{300} = 1 \\end{eqnarray*}\\] The Z-score. The Z-score of an observation is the number of standard deviations it falls above or below the mean. We compute the Z-score for an observation \\(x\\) that follows a distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) by first subtracting its mean, then dividing by its standard deviation: \\[\\begin{eqnarray*} Z = \\frac{x-\\mu}{\\sigma} \\end{eqnarray*}\\] Use Tom’s ACT score, 24, along with the ACT mean and standard deviation to compute his Z-score.84 Observations above the mean always have positive Z-scores while those below the mean have negative Z-scores. If an observation is equal to the mean (e.g., SAT score of 1500), then the Z-score is \\(0\\). Let \\(X\\) represent a random variable from \\(N(\\mu=3, \\sigma=2)\\), and suppose we observe \\(x=5.19\\). (a) Find the Z-score of \\(x\\). (b) Use the Z-score to determine how many standard deviations above or below the mean \\(x\\) falls.85 Head lengths of brushtail possums follow a nearly normal distribution with mean 92.6 mm and standard deviation 3.6 mm. Compute the Z-scores for possums with head lengths of 95.4 mm and 85.8 mm.86 We can use Z-scores to roughly identify which observations are more unusual than others. One observation \\(x_1\\) is said to be more unusual than another observation \\(x_2\\) if the absolute value of its Z-score is larger than the absolute value of the other observation’s Z-score: \\(|Z_1| &gt; |Z_2|\\). This technique is especially insightful when a distribution is symmetric. Which of the two brushtail possum observations in the previous guided practice is more unusual?87 5.2.3 Normal probability calculations in R Ann from the SAT Guided Practice earned a score of 1800 on her SAT with a corresponding \\(Z=1\\). She would like to know what percentile she falls in among all SAT test-takers. Ann’s percentile is the percentage of people who earned a lower SAT score than Ann. We shade the area representing those individuals in Figure 5.8. The total area under the normal curve is always equal to 1, and the proportion of people who scored below Ann on the SAT is equal to the area shaded in Figure 5.8: 0.8413. In other words, Ann is in the \\(84^{th}\\) percentile of SAT takers. Figure 5.8: The normal model for SAT scores, shading the area of those individuals who scored below Ann. We can use the normal model to find percentiles or probabilities. In R, the function to calculate normal probabilities is pnorm(). The normTail() function is available in the openintro R package and will draw the associated curve if it is helpful. In the code below, we find the percentile of \\(Z=0.43\\) is 0.6664, or the \\(66.64^{th}\\) percentile. pnorm(0.43, m = 0, s = 1) #&gt; [1] 0.666 openintro::normTail(0.43, m = 0, s = 1) We can also find the Z-score associated with a percentile. For example, to identify Z for the \\(80^{th}\\) percentile, we use qnorm() which identifies the quantile for a given percentage. The quantile represents the cutoff value.88 We determine the Z-score for the \\(80^{th}\\) percentile using qnorm(): 0.84. qnorm(0.80, m = 0, s = 1) #&gt; [1] 0.842 openintro::normTail(0.80, m = 0, s = 1) We can use these functions with other normal distributions than the standard normal distribution by specifying the mean as the argument for m and the standard deviation as the argument for s. Here we determine the proportion of ACT test takers who scored worse than Tom on the ACT: 0.73. pnorm(24, m = 21, s = 5) #&gt; [1] 0.726 openintro::normTail(24, m = 21, s = 5) Determine the proportion of SAT test takers who scored better than Ann on the SAT.89 5.2.4 Normal probability examples Cumulative SAT scores are approximated well by a normal model, \\(N(\\mu=1500, \\sigma=300)\\). Shannon is a randomly selected SAT taker, and nothing is known about Shannon’s SAT aptitude. What is the probability that Shannon scores at least 1630 on her SATs? First, always draw and label a picture of the normal distribution. (Drawings need not be exact to be useful.) We are interested in the chance she scores above 1630, so we shade the upper tail. See the normal curve below. The picture shows the mean and the values at 2 standard deviations above and below the mean. The simplest way to find the shaded area under the curve makes use of the Z-score of the cutoff value. With \\(\\mu=1500\\), \\(\\sigma=300\\), and the cutoff value \\(x=1630\\), the Z-score is computed as \\[\\begin{eqnarray*} Z = \\frac{x - \\mu}{\\sigma} = \\frac{1630 - 1500}{300} = \\frac{130}{300} = 0.43 \\end{eqnarray*}\\] We use software to find the percentile of \\(Z=0.43\\), which yields 0.6664. However, the percentile describes those who had a Z-score lower than 0.43. To find the area above \\(Z=0.43\\), we compute one minus the area of the lower tail, as seen below. The probability Shannon scores at least 1630 on the SAT is 0.3336. Always draw a picture first, and find the Z-score second. For any normal probability situation, always always always draw and label the normal curve and shade the area of interest first. The picture will provide an estimate of the probability. After drawing a figure to represent the situation, identify the Z-score for the observation of interest. If the probability of Shannon scoring at least 1630 is 0.3336, then what is the probability she scores less than 1630? Draw the normal curve representing this exercise, shading the lower region instead of the upper one.90 Edward earned a 1400 on his SAT. What is his percentile? First, a picture is needed. Edward’s percentile is the proportion of people who do not get as high as a 1400. These are the scores to the left of 1400. Identifying the mean \\(\\mu=1500\\), the standard deviation \\(\\sigma=300\\), and the cutoff for the tail area \\(x=1400\\) makes it easy to compute the Z-score: \\[\\begin{eqnarray*} Z = \\frac{x - \\mu}{\\sigma} = \\frac{1400 - 1500}{300} = -0.3333 \\end{eqnarray*}\\] Using the pnorm() function (either pnorm(-1/3) or pnorm(1400, m=1500, s=300) will give the desired result), the desired probability is \\(0.3694\\). Edward is at the \\(37^{th}\\) percentile. Use the results of the previous example to compute the proportion of SAT takers who did better than Edward. Also draw a new picture.91 Areas to the right. The pnorm() function (and the normal probability table in most books) gives the area to the left. If you would like the area to the right, first find the area to the left and then subtract this amount from one. In R, you can also do this by setting the lower.tail argument to FALSE. Stuart earned an SAT score of 2100. Draw a picture for each part. (a) What is his percentile? (b) What percent of SAT takers did better than Stuart?92 Based on a sample of 100 men,93 the heights of male adults between the ages 20 and 62 in the US is nearly normal with mean 70.0’’ and standard deviation 3.3’’. Mike is 5’7’’ and Jim is 6’4’’. (a) What is Mike’s height percentile? (b) What is Jim’s height percentile? Also draw one picture for each part.94 The last several problems have focused on finding the probability or percentile for a particular observation. What if you would like to know the observation corresponding to a particular percentile? Erik’s height is at the \\(40^{th}\\) percentile. How tall is he? As always, first draw the picture (see below). In this case, the lower tail probability is known (0.40), which can be shaded on the diagram. We want to find the observation that corresponds to this value. As a first step in this direction, we determine the Z-score associated with the \\(40^{th}\\) percentile. Because the percentile is below 50%, we know \\(Z\\) will be negative. Looking in the negative part of the normal probability table, we search for the probability inside the table closest to 0.4000. We find that 0.4000 falls in row \\(-0.2\\) and between columns \\(0.05\\) and \\(0.06\\). Since it falls closer to \\(0.05\\), we take this one: \\(Z=-0.25\\). Knowing \\(Z_{Erik}=-0.25\\) and the population parameters \\(\\mu=70\\) and \\(\\sigma=3.3\\) inches, the Z-score formula can be set up to determine Erik’s unknown height, labeled \\(x_{Erik}\\): \\[\\begin{eqnarray*} -0.25 = Z_{Erik} = \\frac{x_{Erik} - \\mu}{\\sigma} = \\frac{x_{Erik} - 70}{3.3} \\end{eqnarray*}\\] Solving for \\(x_{Erik}\\) yields the height 69.18 inches. That is, Erik is about 5’9’’ (this is notation for 5-feet, 9-inches). qnorm(0.4, m = 0, s = 1) #&gt; [1] -0.253 What is the adult male height at the \\(82^{nd}\\) percentile? Again, we draw the figure first (see below). Next, we want to find the Z-score at the \\(82^{nd}\\) percentile, which will be a positive value. Using qnorm(), the \\(82^{nd}\\) percentile corresponds to \\(Z=0.92\\). Finally, the height \\(x\\) is found using the Z-score formula with the known mean \\(\\mu\\), standard deviation \\(\\sigma\\), and Z-score \\(Z=0.92\\): \\[\\begin{eqnarray*} 0.92 = Z = \\frac{x-\\mu}{\\sigma} = \\frac{x - 70}{3.3} \\end{eqnarray*}\\] This yields 73.04 inches or about 6’1’’ as the height at the \\(82^{nd}\\) percentile. qnorm(0.82, m = 0, s = 1) #&gt; [1] 0.915 What is the \\(95^{th}\\) percentile for SAT scores? What is the \\(97.5^{th}\\) percentile of the male heights? As always with normal probability problems, first draw a picture.95 What is the probability that a randomly selected male adult is at least 6’2’’ (74 inches)? What is the probability that a male adult is shorter than 5’9’’ (69 inches)?96 What is the probability that a random adult male is between 5’9’’ and 6’2’’? These heights correspond to 69 inches and 74 inches. First, draw the figure. The area of interest is no longer an upper or lower tail. The total area under the curve is 1. If we find the area of the two tails that are not shaded (from the previous Guided Practice, these areas are \\(0.3821\\) and \\(0.1131\\)), then we can find the middle area: That is, the probability of being between 5’9’’ and 6’2’’ is 0.5048. What percent of SAT takers get between 1500 and 2000?97 What percent of adult males are between 5’5’’ and 5’7’’?98 5.2.5 68-95-99.7 rule Here, we present a useful general rule for the probability of falling within 1, 2, and 3 standard deviations of the mean in the normal distribution. The rule will be useful in a wide range of practical settings, especially when trying to make a quick estimate without a calculator or Z table. Figure 5.9: Probabilities for falling within 1, 2, and 3 standard deviations of the mean in a normal distribution. Use pnorm() to confirm that about 68%, 95%, and 99.7% of observations fall within 1, 2, and 3, standard deviations of the mean in the normal distribution, respectively. For instance, first find the area that falls between \\(Z=-1\\) and \\(Z=1\\), which should have an area of about 0.68. Similarly there should be an area of about 0.95 between \\(Z=-2\\) and \\(Z=2\\).99 It is possible for a normal random variable to fall 4, 5, or even more standard deviations from the mean. However, these occurrences are very rare if the data are nearly normal. The probability of being further than 4 standard deviations from the mean is about 1-in-30,000. For 5 and 6 standard deviations, it is about 1-in-3.5 million and 1-in-1 billion, respectively. SAT scores closely follow the normal model with mean \\(\\mu = 1500\\) and standard deviation \\(\\sigma = 300\\). (a) About what percent of test takers score 900 to 2100? (b) What percent score between 1500 and 2100?100 5.3 One proportion Notation. \\(n\\) = sample size (number of observational units in the data set) \\(\\hat{p}\\) = sample proportion (number of “successes” divided by the sample size) \\(\\pi\\) = population proportion101 A single proportion is used to summarize data when we measured a single categorical variable on each observational unit—the single variable is measured as either a success or failure (e.g., “surgical complication” vs. “no surgical complication”)102. 5.3.1 Simulation-based test for \\(H_0: \\pi = \\pi_0\\) In Section 5.1.3, we introduced the general steps of a hypothesis test: General steps of a hypothesis test. Every hypothesis test follows these same general steps: Frame the research question in terms of hypotheses. Collect and summarize data using a test statistic. Assume the null hypothesis is true, and simulate or mathematically model a null distribution for the test statistic. Compare the observed test statistic to the null distribution to calculate a p-value. Make a conclusion based on the p-value, and write a conclusion in context, in plain language, and in terms of the alternative hypothesis. People providing an organ for donation sometimes seek the help of a special medical consultant. These consultants assist the patient in all aspects of the surgery, with the goal of reducing the possibility of complications during the medical procedure and recovery. Patients might choose a consultant based in part on the historical complication rate of the consultant’s clients. One consultant tried to attract patients by noting the average complication rate for liver donor surgeries in the US is about 10%, but her clients have had only 3 complications in the 62 liver donor surgeries she has facilitated. She claims this is strong evidence that her work meaningfully contributes to reducing complications (and therefore she should be hired!). Using these data, is it possible to assess the consultant’s claim that her work meaningfully contributes to reducing complications? No. The claim is that there is a causal connection, but the data are observational, so we must be on the lookout for confounding variables. For example, maybe patients who can afford a medical consultant can afford better medical care, which can also lead to a lower complication rate. While it is not possible to assess the causal claim, it is still possible to understand the consultant’s true rate of complications. Steps 1 and 2: Hypotheses and test statistic Regardless of if we use simulation-based methods or theory-based methods, the first two steps of a hypothesis test start out the same: setting up hypotheses and summarizing data with a test statistic. We will let \\(\\pi\\) represent the true complication rate for liver donors working with this consultant. This “true” complication probability is called the parameter of interest103.) The sample proportion for the complication rate is 3 complications divided by the 62 surgeries the consultant has worked on: \\(\\hat{p} = 3/62 = 0.048\\). Since this value is estimated from sample data, it is called a statistic. The statistic \\(\\hat{p}\\) is also our point estimate, or “best guess,” for \\(\\pi\\), and we will use is as our test statistic. Parameters and statistics. A parameter is the “true” value of interest. We typically estimate the parameter using a statistic from a sample of data. When a statistic is used as an estimate of a parameter, it is called a point estimate. For example, we estimate the probability \\(\\pi\\) of a complication for a client of the medical consultant by examining the past complications rates of her clients: \\[\\hat{p} = 3 / 62 = 0.048\\qquad\\text{is used to estimate}\\qquad \\pi\\] Summary measures that summarize a sample of data, such as \\(\\hat{p}\\), are called statistics. Numbers that summarize an entire population, such as \\(\\pi\\), are called parameters. You can remember this distinction by looking at the first letter of each term: Statistics summarize Samples. Parameters summarize Populations. We typically use Roman letters to symbolize statistics (e.g., \\(\\bar{x}\\), \\(\\hat{p}\\)), and Greek letters to symbolize parameters (e.g., \\(\\mu\\), \\(\\pi\\)). Since we rarely can measure the entire population, and thus rarely know the actual parameter values, we like to say, “We don’t know Greek, and we don’t know parameters!” Write out hypotheses in both plain and statistical language to test for the association between the consultant’s work and the true complication rate, \\(\\pi\\), for the consultant’s clients. In words: \\(H_0\\): There is no association between the consultant’s contributions and the clients’ complication rate. \\(H_A\\): Patients who work with the consultant tend to have a complication rate lower than 10%. In statistical language: \\(H_0: \\pi=0.10\\) \\(H_A: \\pi&lt;0.10\\) Steps 3 and 4: Null distribution and p-value To assess these hypotheses, we need to evaluate the possibility of getting a sample proportion as far below the null value, \\(0.10\\), as what was observed (\\(0.048\\)), if the null hypothesis were true. Null value of a hypothesis test. The null value is the reference value for the parameter in \\(H_0\\), and it is sometimes represented with the parameter’s label with a subscript 0 (or “null”), e.g., \\(\\pi_0\\) (just like \\(H_0\\)). The deviation of the sample statistic from the null hypothesized parameter is usually quantified with a p-value104. The p-value is computed based on the null distribution, which is the distribution of the test statistic if the null hypothesis is true. Supposing the null hypothesis is true, we can compute the p-value by identifying the chance of observing a test statistic that favors the alternative hypothesis at least as strongly as the observed test statistic. Null distribution. The null distribution of a test statistic is the sampling distribution of that statistic under the assumption of the null hypothesis. It describes how that statistic would vary from sample to sample, if the null hypothesis were true. The null distribution can be estimated through simulation (simulation-based methods), as in this section, or can be modeled by a mathematical function (theory-based methods), as in Section 5.3.3. We want to identify the sampling distribution of the test statistic (\\(\\hat{p}\\)) if the null hypothesis was true. In other words, we want to see how the sample proportion changes due to chance alone. Then we plan to use this information to decide whether there is enough evidence to reject the null hypothesis. Under the null hypothesis, 10% of liver donors have complications during or after surgery. Suppose this rate was really no different for the consultant’s clients (for all the consultant’s clients, not just the 62 previously measured). If this was the case, we could simulate 62 clients to get a sample proportion for the complication rate from the null distribution. This is a similar scenario to the one we encountered in Section 5.1.1, with one important difference—the null value is 0.10, not 0.50. Thus, a flipping a coin to simulate whether a client had complications would not be simulating under the correct null hypothesis. What physical object could you use to simulate a random sample of 62 clients who had a 10% chance of complications? How would you use this object?105 Assuming the true complication rate for the consultant’s clients is 10%, each client can be simulated using a bag of marbles with 10% red marbles and 90% white marbles. Sampling a marble from the bag (with 10% red marbles) is one way of simulating whether a patient has a complication if the true complication rate is 10% for the data. If we select 62 marbles and then compute the proportion of patients with complications in the simulation, \\(\\hat{p}_{sim}\\), then the resulting sample proportion is calculated exactly from a sample from the null distribution. An undergraduate student was paid $2 to complete this simulation. There were 5 simulated cases with a complication and 57 simulated cases without a complication, i.e., \\(\\hat{p}_{sim} = 5/62 = 0.081\\). Is this one simulation enough to determine whether or not we should reject the null hypothesis? No. To assess the hypotheses, we need to see a distribution of many \\(\\hat{p}_{sim}\\), not just a single draw from this sampling distribution. One simulation isn’t enough to get a sense of the null distribution; many simulation studies are needed. Roughly 10,000 seems sufficient. However, paying someone to simulate 10,000 studies by hand is a waste of time and money. Instead, simulations are typically programmed into a computer, which is much more efficient. Figure 5.10 shows the results of 10,000 simulated studies. The proportions that are equal to or less than \\(\\hat{p}=0.048\\) are shaded. The shaded areas represent sample proportions under the null distribution that provide at least as much evidence as \\(\\hat{p}\\) favoring the alternative hypothesis. There were 1222 simulated sample proportions with \\(\\hat{p}_{sim} \\leq 0.048\\). We use these to construct the null distribution’s left-tail area and find the p-value: \\[\\begin{align} \\text{left tail area }\\label{estOfPValueBasedOnSimulatedNullForSingleProportion} &amp;= \\frac{\\text{Number of observed simulations with }\\hat{p}_{sim}\\leq\\text{ 0.048}}{10000} \\end{align}\\] Of the 10,000 simulated \\(\\hat{p}_{sim}\\), 1222 were equal to or smaller than \\(\\hat{p}\\). Since the hypothesis test is one-sided, the estimated p-value is equal to this tail area: 0.1222. Figure 5.10: The null distribution for \\(\\hat{p}\\), created from 10,000 simulated studies. The left tail, representing the p-value for the hypothesis test, contains 12.22% of the simulations. Step 5: Conclusion Because the estimated p-value is 0.1222, which is not small, we have little to no evidence against the null hypothesis. Explain what this means in plain language in the context of the problem.106 Does the conclusion in the previous Guided Practice imply there is no real association between the surgical consultant’s work and the risk of complications? Explain.107 5.3.2 Bootstrap confidence interval for \\(\\pi\\) A confidence interval provides a range of plausible values for the parameter \\(\\pi\\). If the goal is to produce a range of possible values for a population value, then in an ideal world, we would sample data from the population again and recompute the sample proportion. Then we could do it again. And again. And so on until we have a good sense of the variability of our original estimate. The ideal world where sampling data is free or extremely cheap is almost never the case, and taking repeated samples from a population is usually impossible. So, instead of using a “resample from the population” approach, bootstrapping uses a “resample from the sample” approach. Let’s revisit our medical consultant example from Section 5.3.1. This consultant tried to attract patients by noting the average complication rate for liver donor surgeries in the US is about 10%, but her clients have had only 3 complications in the 62 liver donor surgeries she has facilitated. This data, however, did not provide sufficient evidence that the consultant’s complication rate was less than 10%, since the p-value was approximately 0.122. Does this mean we can conclude that the consultant’s complication rate was equal to 10%? No! Though our decision was to fail to reject the null hypothesis, this does not mean we have evidence for the null hypothesis—we cannot “accept” the null. The sample proportion was \\(\\hat{p} = 3/62 = 0.048\\), which is our point estimate—or “best guess”—of \\(\\pi\\). It wouldn’t make sense that a sample complication rate of 4.8% gives us evidence that the true complication rate was exactly 10%. It`s plausible that the true complication rate is 10%, but there are a range of plausible values for \\(\\pi\\). In this section, we will use a simulation-based method called bootstrapping to generate this range of plausible values for \\(\\pi\\) using the observed data. In the medical consultant case study, the parameter is \\(\\pi\\), the true probability of a complication for a client of the medical consultant. There is no reason to believe that \\(\\pi\\) is exactly \\(\\hat{p} = 3/62\\), but there is also no reason to believe that \\(\\pi\\) is particularly far from \\(\\hat{p} = 3/62\\). By sampling with replacement from the data set (a process called bootstrapping),108 the variability of the possible \\(\\hat{p}\\) values can be approximated, which will allow us to generate a range of plausible values for \\(\\pi\\), i.e., a confidence interval. Most of the inferential procedures covered in this text are grounded in quantifying how one data set would differ from another when they are both taken from the same population. It doesn’t make sense to take repeated samples from the same population because if you have the means to take more samples, a larger sample size will benefit you more than the exact same sample twice. Instead, we measure how the samples behave under an estimate of the population. Figure 5.11 shows how an unknown original population of red and white marbles can be estimated by using multiple copies of a sample of seven marbles. Figure 5.11: An unknown population of red and white marbles. The estimated population on the right is many copies of the observed sample. By taking repeated samples from the estimated population, the variability from sample to sample can be observed. In Figure 5.12 the repeated bootstrap samples are obviously different both from each other, from the original sample, and from the original population. Recall that the bootstrap samples were taken from the same (estimated) population, and so the differences are due entirely to natural variability in the sampling procedure. Figure 5.12: Selecting \\(k\\) random samples from the estimated population created from copies of the observed sample. By summarizing each of the bootstrap samples (here, using the sample proportion), we see, directly, the variability of the sample proportion of red marbles, \\(\\hat{p}\\), from sample to sample. The distribution of bootstrapped \\(\\hat{p}\\)’s for the example scenario is shown in Figure 5.13, and the bootstrap distribution for the medical consultant data is shown in Figure 5.14. Figure 5.13: Calculate the sample proportion of red marbles in each bootstrap resample, then plot these simulated sample proportions in a dot plot. The dot plot of sample proportion provides us a sense of how sample proportions would vary from sample to sample if we could take many samples from our original population. It turns out that in practice, it is very difficult for computers to work with an infinite population (with the same proportional breakdown as in the sample). However, there is a physical and computational model which produces an equivalent bootstrap distribution of the sample proportion in a computationally efficient manner. Consider the observed data to be a bag of marbles 3 of which are red and 4 of which are white. By drawing the marbles out of the bag with replacement, we depict the same sampling process as was done with the infinitely large estimated population. Note that when sampling the original observations with replacement, a particular marble may end up in the new sample one time, multiple times, or not at all. Bootstrapping from one sample. Take a random sample of size \\(n\\) from the original sample, with replacement. This is called a bootstrapped resample. Record the sample proportion (or statistic of interest) from the boostrapped resample. This is called a bootstrapped statistic. Repeat steps (1) and (2) 1000s of times to create a distribution of bootstrapped statistics. If we apply the bootstrap sampling process to the medical consultant example, we consider each client to be one of the marbles in the bag. There will be 59 white marbles (no complication) and 3 red marbles (complication). If we 62 choose marbles out of the bag (one at a time), replacing each chosen marble after its color is recorded, and compute the proportion of simulated patients with complications, \\(\\hat{p}_{bs}\\), then this “bootstrap” proportion represents a single simulated proportion from the “resample from the sample” approach. In a simulation of 62 patients conduced by sampling with replacement from the original sample, about how many would we expect to have had a complication?109 One simulated bootstrap resample isn’t enough to get a sense of the variability from one bootstrap proportion to another bootstrap proportion, so we repeated the simulation 10,000 times using a computer. Figure 5.14 shows the distribution from the 10,000 bootstrap simulations. The bootstrapped proportions vary from about zero to 0.15. By taking the range of the middle 95% of this distribution, we can construct a 95% bootstrapped confidence interval for \\(\\pi\\). The 2.5th percentile is 0, and the 97.5th percentile is 0.113, so the middle 95% of the distribution is the range (0, 0.113). The variability in the bootstrapped proportions leads us to believe that the true risk of complication (the parameter, \\(\\pi\\)) is somewhere between 0 and 11.3%. Figure 5.14: The original medical consultant data is bootstrapped 10,000 times. Each simulation creates a sample from the original data where the probability of a complication is \\(\\hat{p} = 3/62\\). The bootstrap 2.5 percentile proportion is 0 and the 97.5 percentile is 0.113. The result is: we are confident that, in the population, the true probability of a complication is between 0% and 11.3%. 95% Bootstrap confidence interval for a population proportion \\(\\pi\\). The 95% bootstrap confidence interval for the parameter \\(\\pi\\) can be obtained directly using the ordered values \\(\\hat{p}_{boot}\\) values — the bootstrapped sample proportions. Consider the sorted \\(\\hat{p}_{boot}\\) values, and let \\(\\hat{p}_{boot, 0.025}\\) be the 2.5th percentile value and \\(\\hat{p}_{boot, 0.025}\\) be the 97.5th percentile. The 95% confidence interval is given by: (\\(\\hat{p}_{boot, 0.025}\\), \\(\\hat{p}_{boot, 0.975}\\)) You can find confidence intervals of difference confidence levels by changing the percent of the distribution you take, e.g., locate the middle 90% of the bootstrapped statistics for a 90% confidence interval. To find the middle 90% of a distribution, which two percentiles would form its boundaries?110 The original claim was that the consultant’s true rate of complication was under the national rate of 10%. Does the interval estimate of 0 to 11.3% for the true probability of complication indicate that the surgical consultant has a lower rate of complications than the national average? Explain. No. Because the interval overlaps 10%, it might be that the consultant’s work is associated with a lower risk of complciations, or it might be that the consulant’s work is associated with a higher risk (i.e., greater than 10%) of complications! Additionally, as previously mentioned, because this is an observational study, even if an association can be measured, there is no evidence that the consultant’s work is the cause of the complication rate (being higher or lower). 5.3.3 Theory-based methods for \\(\\pi\\) In Section 5.1.2, we introduced the normal distribution and showed how it can be used as a mathematical model to describe the variability of a sample mean or sample proportion as a result of the Central Limit Theorem. We explored the normal distribution further in Section 5.2. Theory-based hypothesis tests and confidence intervals for proportions use the normal distribution to calculate the p-value and to determine the width of the confidence interval. Central Limit Theorem for the sample proportion. When we collect a sufficiently large sample of \\(n\\) independent observations of a categorical variable from a population with \\(\\pi\\) proportion of successes, the sampling distribution of \\(\\hat{p}\\) will be nearly normal with \\[\\begin{align*} &amp;\\text{Mean}=\\pi &amp;&amp;\\text{Standard Deviation }(SD) = \\frac{\\pi(1-\\pi)}{n} \\end{align*}\\] Evaluating the two conditions required for modeling \\(\\hat{p}\\) using theory-based methods There are two conditions required to apply the Central Limit Theorem for a sample proportion \\(\\hat{p}\\). When the sample observations are independent and the sample size is sufficiently large, the normal model will describe the variability in sample proportions quite well; when the observations violate the conditions, the normal model can be inaccurate. Conditions for the sampling distribution of \\(\\hat{p}\\) to be approximately normal. The sampling distribution for \\(\\hat{p}\\) based on a sample of size \\(n\\) from a population with a true proportion \\(\\pi\\) can be modeled using a normal distribution when: Independence. The sample observations are independent, i.e., the outcome of one observation does not influence the outcome of another. This condition is met if data come from a simple random sample of the target population. Success-failure condition. We expected to see at least 10 successes and 10 failures in the sample, i.e., \\(n\\pi\\geq10\\) and \\(n(1-\\pi)\\geq10\\). This condition is met if we have at least 10 successes and 10 failures in the observed data. When these conditions are satisfied, then the sampling distribution of \\(\\hat{p}\\) is approximately normal with mean \\(\\pi\\) and standard deviation \\(\\sqrt{\\frac{\\ \\pi(1-\\pi)\\ }{n}}\\). The success-failure condition listed above is only necessary for the sampling distribution of \\(\\hat{p}\\) to be approximately normal. The mean of the sampling distribution of \\(\\hat{p}\\) is \\(\\pi\\), and the standard deviation is \\(\\sqrt{\\frac{\\ \\pi(1-\\pi)\\ }{n}}\\), regardless of the sample size. Typically we don’t know the true proportion \\(\\pi\\), so we substitute some value to check the success-failure condition and to estimate the standard deviation of the sampling distribution of \\(\\hat{p}\\). The independence condition is a more nuanced requirement. When it isn’t met, it is important to understand how and why it isn’t met. For example, there exist no statistical methods available to truly correct the inherent biases of data from a convenience sample. On the other hand, if we took a cluster random sample (see Section 1.3.4), the observations wouldn’t be independent, but suitable statistical methods are available for analyzing the data (but they are beyond the scope of even most second or third courses in statistics)111. In the examples based on large sample theory, we modeled \\(\\hat{p}\\) using the normal distribution. Why is this not appropriate for the study on the medical consultant? The independence assumption may be reasonable if each of the surgeries is from a different surgical team. However, the success-failure condition is not satisfied. Under the null hypothesis, we would anticipate seeing \\(62\\times 0.10=6.2\\) complications, not the 10 required for the normal approximation. Since theory-based methods cannot be used on the medical consultant example, we’ll turn to another example to demonstrate these methods, where conditions for approximating the distribution of \\(\\hat{p}\\) by a normal distribution are met. Hypothesis test for \\(H_0: \\pi = \\pi_0\\) One possible regulation for payday lenders is that they would be required to do a credit check and evaluate debt payments against the borrower’s finances. We would like to know: would borrowers support this form of regulation? Set up hypotheses to evaluate whether borrowers have a majority support for this type of regulation. We take “majority” to mean greater than 50% of the population. In words, \\(H_0\\): there is not majority support for the regulation \\(H_A\\): the majority of borrowers support the regulation In statistical notation, \\(H_0\\): \\(\\pi = 0.50\\) \\(H_A\\): \\(\\pi &gt; 0.50\\), where \\(\\pi\\) represents the proportion of all payday loan borrowers that would support the regulation. Note that the null hypothesis above was stated as \\(H_0: \\pi = 0.50\\), even though saying there is “not majority support” would imply \\(\\pi \\leq 0.50\\). Indeed, some textbooks would write \\(H_0: \\pi \\leq 0.50\\) in this case, and it is not an incorrect statement. However, when calculating the p-value, we need to assume a particular value for \\(\\pi\\) under the null hypothesis, so in this textbook, our null hypothesis will always be of the form: \\[ H_0: \\mbox{ parameter } = \\mbox{ null value} \\] To apply the normal distribution to model the null distribution, the independence and success-failure conditions must be satisfied. In a hypothesis test, the success-failure condition is checked using the null proportion: we verify \\(n\\pi_0\\) and \\(n(1-\\pi_0)\\) are at least 10, where \\(\\pi_0\\) is the null value. Do payday loan borrowers support a regulation that would require lenders to pull their credit report and evaluate their debt payments? From a random sample of 826 borrowers, 51% said they would support such a regulation. Is it reasonable use a normal distribution to model \\(\\hat{p}\\) for a hypothesis test here?112 Continuing the previous Example, evaluate whether the poll on lending regulations provides convincing evidence that a majority of payday loan borrowers support a new regulation that would require lenders to pull credit reports and evaluate debt payments. With hypotheses already set up and conditions checked, we can move onto calculations. The null standard error in the context of a one proportion hypothesis test is computed using the null value, \\(\\pi_0\\): \\[\\begin{align*} SE_0(\\hat{p}) = \\sqrt{\\frac{\\pi_0 (1 - \\pi_0)}{n}} = \\sqrt{\\frac{0.5 (1 - 0.5)}{826}} = 0.017 \\end{align*}\\] A picture of the normal model for the null distribution of sample proportions in this scenario is shown below in Figure 5.15, with the p-value represented by the shaded region. Note that this null distribution is centered at 0.50, the null value, and has standard deviation 0.017. Under \\(H_0\\), the probability of observing \\(\\hat{p} = 0.51\\) or higher is 0.278, the area above 0.51 on the null distribution. With a p-value of 0.278, the poll does not provide convincing evidence that a majority of payday loan borrowers support regulations around credit checks and evaluation of debt payments. You’ll note that this conclusion is somewhat unsatisfactory because there is no conclusion, as is the case with larger p-values. That is, there is no resolution one way or the other about public opinion. We cannot claim that exactly 50% of people support the regulation, but we cannot claim a majority support it either. Figure 5.15: Approximate sampling distribution of \\(\\hat{p}\\) across all possible samples assuming \\(\\pi = 0.50\\). The shaded area represents the p-value corresponding to an observed sample proportion of 0.51. Often, with theory-based methods, we use a standardized statistic rather than the original statistic as our test statistic. A standardized statistic is computed by subtracting the mean of the null distribution from the original statistic, then dividing by the standard error: \\[ \\mbox{standardized statistic} = \\frac{\\mbox{observed statistic} - \\mbox{null value}}{\\mbox{null standard error}} \\] The null standard error (\\(SE_0(\\text{statistic})\\)) of the observed statistic is its estimated standard deviation assuming the null hypothesis is true. We can interpret the standardized statistic as the number of standard errors our observed statistic is above (if positive) or below (if negative) the null value. When we are modeling the null distribution with a normal distribution, this standardized statistic is called \\(Z\\), since it is the Z-score of the sample proportion. Standardized sample proportion. The standardized statistic for theory-based methods for one proportion is \\[ Z = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}} = \\frac{\\hat{p} - \\pi_0}{SE_0(\\hat{p})} \\] where \\(\\pi_0\\) is the null value. The denominator, \\(SE_0(\\hat{p}) = \\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\), is called the null standard error of the sample proportion. With the standardized statistic as our test statistic, we can find the p-value as the area under a standard normal distribution at or more extreme than our observed \\(Z\\) value. Do payday loan borrowers support a regulation that would require lenders to pull their credit report and evaluate their debt payments? From a random sample of 826 borrowers, 51% said they would support such a regulation. We set up hypotheses and checked conditions previously. Now calculate and interpret the standardized statistic, then use the standard normal distribution to calculate the approximate p-value. Our sample proportion is \\(\\hat{p} = 0.51\\). Since our null value is \\(\\pi_0 = 0.50\\), the null standard error is \\[\\begin{align*} SE_0(\\hat{p}) = \\sqrt{\\frac{\\pi_0 (1 - \\pi_0)}{n}} = \\sqrt{\\frac{0.5 (1 - 0.5)}{826}} = 0.017 \\end{align*}\\] The standardized statistic is \\[\\begin{align*} Z = \\frac{0.51 - 0.50}{0.017} = 0.57 \\end{align*}\\] Interpreting this value, we can say that our sample proportion of 0.51 was only 0.57 standard errors above the null value of 0.50. Shown in Figure 5.16, the p-value is the area above \\(Z = 0.57\\) on a standard normal distribution—0.278—the same p-value we would obtain by finding the area above \\(\\hat{p} = 0.51\\) on a normal distribution with mean 0.50 and standard deviation 0.017, as in Figure 5.15. Figure 5.16: Approximate sampling distribution of \\(Z\\) across all possible samples assuming \\(\\pi = 0.50\\). The shaded area represents the p-value corresponding to an observed standardized statistic of 0.57. Compare to Figure 5.15. Theory-based hypothesis test for a proportion: one-sample \\(Z\\)-test. Frame the research question in terms of hypotheses. Using the null value, \\(\\pi_0\\), verify the conditions for using the normal distribution to approximate the null distribution. Calculate the test statistic: \\[ Z = \\frac{\\hat{p} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}} = \\frac{\\hat{p} - \\pi_0}{SE_0(\\hat{p})} \\] Use the test statistic and the standard normal distribution to calculate the p-value. Make a conclusion based on the p-value, and write a conclusion in context, in plain language, and in terms of the alternative hypothesis. Regardless of the statistical method chosen, the p-value is always derived by analyzing the null distribution of the test statistic. The normal model poorly approximates the null distribution for \\(\\hat{p}\\) when the success-failure condition is not satisfied. As a substitute, we can generate the null distribution using simulated sample proportions and use this distribution to compute the tail area, i.e., the p-value. Neither the p-value approximated by the normal distribution nor the simulated p-value are exact, because the normal distribution and simulated null distribution themselves are not exact, only a close approximation. An exact p-value can be generated using the binomial distribution, but that method will not be covered in this text. Confidence interval for \\(\\pi\\) A confidence interval provides a range of plausible values for the parameter \\(\\pi\\). A point estimate is our best guess for the value of the parameter, so it makes sense to build the confidence interval around that value. The standard error, which is a measure of the uncertainty associated with the point estimate, provides a guide for how large we should make the confidence interval. When \\(\\hat{p}\\) can be modeled using a normal distribution, the 68-95-99.7 rule tells us that, in general, 95% of observations are within 2 standard errors of the mean. Here, we use the value 1.96 to be slightly more precise. The confidence interval for \\(\\pi\\) then takes the form \\[\\begin{align*} \\hat{p} \\pm z^{\\star} \\times SE(\\hat{p}). \\end{align*}\\] We have seen \\(\\hat{p}\\) to be the sample proportion. The value \\(z^{\\star}\\) comes from a standard normal distribution and is determined by the chosen confidence level. The value of the standard error of \\(\\hat{p}\\), \\(SE(\\hat{p})\\), approximates how far we would expect the sample proportion to fall from \\(\\pi\\), and depends heavily on the sample size. Standard error of one proportion, \\(\\hat{p}\\). When the conditions are met so that the distribution for \\(\\hat{p}\\) is nearly normal, the variability of a single proportion, \\(\\hat{p}\\) is well described by its standard deviation: \\[SD(\\hat{p}) = \\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\] Note that we almost never know the true value of \\(\\pi\\), but we can substitute our best guess of \\(\\pi\\) to obtain an approximate standard deviation, called the standard error of \\(\\hat{p}\\): \\[SD(\\hat{p}) \\approx \\hspace{3mm} SE(\\hat{p}) = \\sqrt{\\frac{(\\mbox{best guess of }\\pi)(1 - \\mbox{best guess of }\\pi)}{n}}\\] For hypothesis testing, we often use \\(\\pi_0\\) as the best guess of \\(\\pi\\), as seen in Section 5.3.3. For confidence intervals, we typically use \\(\\hat{p}\\) as the best guess of \\(\\pi\\). Consider taking many polls of registered voters (i.e., random samples) of size 300 and asking them if they support legalized marijuana. It is suspected that about 2/3 of all voters support legalized marijuana. To understand how the sample proportion (\\(\\hat{p}\\)) would vary across the samples, calculate the standard error of \\(\\hat{p}\\).113 A simple random sample of 826 payday loan borrowers was surveyed to better understand their interests around regulation and costs. 51% of the responses supported new regulations on payday lenders. Is it reasonable to model the variability of \\(\\hat{p}\\) from sample to sample using a normal distribution? Calculate the standard error of \\(\\hat{p}\\). Construct a 95% confidence interval for \\(\\pi\\), the proportion of all payday borrowers who support increased regulation for payday lenders. The data are a random sample, so the observations are independent and representative of the population of interest. We also must check the success-failure condition, which we do using \\(\\hat{p}\\) in place of \\(\\pi\\) when computing a confidence interval: \\[\\begin{align*} \\text{Support: } n \\hat{p} &amp; = 826 \\times 0.51 \\approx 421 &amp;\\text{Not: } n (1 - \\hat{p}) &amp; = 826 \\times (1 - 0.51) \\approx 405 \\end{align*}\\] Since both values are at least 10, we can use the normal distribution to model the sampling distribution of \\(\\hat{p}\\). Because \\(\\pi\\) is unknown and the standard error is for a confidence interval, use \\(\\hat{p}\\) as our best guess of \\(\\pi\\) in the formula. \\(SE(\\hat{p}) = \\sqrt{\\frac{0.51 (1 - 0.51)} {826}} = 0.017\\). Using the point estimate \\(0.51\\), \\(z^{\\star} = 1.96\\) for a 95% confidence interval, and the standard error \\(SE = 0.017\\) from the previous Guided Practice, the confidence interval is \\[\\begin{eqnarray*} \\text{point estimate} \\ \\pm\\ z^{\\star} \\times SE \\quad\\to\\quad 0.51 \\ \\pm\\ 1.96 \\times 0.017 \\quad\\to\\quad (0.477, 0.543) \\end{eqnarray*}\\] We are 95% confident that the true proportion of payday borrowers who supported regulation at the time of the poll was between 0.477 and 0.543. Constructing a confidence interval for a single proportion. There are four steps to constructing a confidence interval for \\(p\\). Check independence and the success-failure condition using \\(\\hat{p}\\). If the conditions are met, the sampling distribution of \\(\\hat{p}\\) may be well-approximated by the normal model. Construct the standard error: \\[ SE(\\hat{p}) = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\] Use statistical software to find the multiplier \\(z^{\\star}\\) corresponding to the confidence level. Apply the general confidence interval formula \\(\\mbox{statistic} \\pm (\\mbox{multiplier}) \\times SE\\): \\[ \\hat{p} \\pm z^{\\star}\\times \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\] \\(z^{\\star}\\) and the confidence level Suppose we want to consider confidence intervals where the confidence level is somewhat higher than 95%: perhaps we would like a confidence level of 99%. Think back to the analogy about trying to catch a fish: if we want to be more sure that we will catch the fish, we should use a wider net. To create a 99% confidence level, we must also widen our 95% interval. On the other hand, if we want an interval with lower confidence, such as 90%, we could make our original 95% interval slightly slimmer. The 95% confidence interval structure provides guidance in how to make intervals with new confidence levels. Below is a general 95% confidence interval for a parameter whose point estimate has a nearly normal distribution: \\[\\begin{eqnarray} \\text{point estimate}\\ \\pm\\ 1.96\\times SE \\end{eqnarray}\\] There are three components to this interval: the point estimate, “1.96”, and the standard error. The choice of \\(1.96\\times SE\\) was based on capturing 95% of the sampling distribution of statistics since the point estimate is within 1.96 standard errors of the true parameter about 95% of the time. The choice of 1.96 corresponds to a 95% confidence level. If \\(X\\) is a normally distributed random variable, how often will \\(X\\) be within 2.58 standard deviations of the mean?114 Figure 5.17: The area between -\\(z^{\\star}\\) and \\(z^{\\star}\\) increases as \\(|z^{\\star}|\\) becomes larger. If the confidence level is 99%, we choose \\(z^{\\star}\\) such that 99% of the normal curve is between -\\(z^{\\star}\\) and \\(z^{\\star}\\), which corresponds to 0.5% in the lower tail and 0.5% in the upper tail: \\(z^{\\star}=2.58\\). To create a 99% confidence interval, change 1.96 in the 95% confidence interval formula to be 2.58. The previous Guided Practice highlights that 99% of the time a normal random variable will be within 2.58 standard deviations of its mean. This approach—using the Z-scores in the normal model to compute confidence levels—is appropriate when the point estimate is associated with a normal distribution and we can properly compute the standard error. Thus, the formula for a 99% confidence interval is: \\[\\begin{eqnarray*} \\text{point estimate}\\ \\pm\\ 2.58\\times SE \\end{eqnarray*}\\] The normal approximation is crucial to the precision of the \\(z^\\star\\) confidence intervals. When the normal model is not a good fit, we will use alternative distributions that better characterize the sampling distribution or we will use bootstrapping procedures. Create a 99% confidence interval for the impact of the stent on the risk of stroke using the data from Section 1.1. The point estimate is 0.090, and the standard error is \\(SE = 0.028\\). It has been verified for you that the point estimate can reasonably be modeled by a normal distribution.115 Theory-based \\((1-\\alpha)\\times 100\\)% confidence interval. If the statistic follows the normal model with standard error \\(SE\\), then a confidence interval for the population parameter is \\[\\begin{eqnarray*} \\text{statistic}\\ \\pm\\ z^{\\star} \\times SE \\end{eqnarray*}\\] where \\(z^{\\star}\\) corresponds to the confidence level selected: the middle \\((1-\\alpha)\\times 100\\)% of a standard normal distribution lies between \\(-z^{\\star}\\) and \\(z^{\\star}\\). Using R to find \\(z^{\\star}\\) Figure 5.17 provides a picture of how to identify \\(z^{\\star}\\) based on a confidence level. We select \\(z^{\\star}\\) so that the area between -\\(z^{\\star}\\) and \\(z^{\\star}\\) in the normal model corresponds to the confidence level. In R, you can find \\(z^{\\star}\\) using the qnorm() function: # z* for 90% --&gt; alpha = 0.15 --&gt; need 5% on each size: qnorm(.90 + .05) #&gt; [1] 1.645 # z* for 95% --&gt; alpha = 0.05 --&gt; need 2.5% on each size: qnorm(.95 + .025) #&gt; [1] 1.96 # z* for 99% --&gt; alpha = 0.01 --&gt; need .5% on each size: qnorm(.99 + .005) #&gt; [1] 2.576 Previously, we found that implanting a stent in the brain of a patient at risk for a stroke increased the risk of a stroke. The study estimated a 9% increase in the number of patients who had a stroke, and the standard error of this estimate was about \\(SE = 2.8%\\). Compute a 90% confidence interval for the effect.116 Violating conditions We’ve spent a lot of time discussing conditions for when \\(\\hat{p}\\) can be reasonably modeled by a normal distribution. What happens when the success-failure condition fails? What about when the independence condition fails? In either case, the general ideas of confidence intervals and hypothesis tests remain the same, but the strategy or technique used to generate the interval or p-value change. When the success-failure condition isn’t met for a hypothesis test, we can simulate the null distribution of \\(\\hat{p}\\) using the null value, \\(\\pi_0\\), as seen in Section 5.3.1. Unfortunately, methods for dealing with observations which are not independent are outside the scope of this book. 5.4 Difference of two proportions Notation. \\(n_1\\), \\(n_2\\) = sample sizes of two independent samples \\(\\hat{p}_1\\), \\(\\hat{p}_2\\) = sample proportions of two independent samples \\(\\pi_1\\), \\(\\pi_2\\) = population proportions of two independent samples We now extend the methods from Section 5.3 to apply confidence intervals and hypothesis tests to differences in population proportions that come from two groups: \\(\\pi_1 - \\pi_2\\). In our investigations, we’ll identify a reasonable point estimate of \\(\\pi_1 - \\pi_2\\) based on the sample, and you may have already guessed its form: \\(\\hat{p}_1 - \\hat{p}_2\\). We’ll look at statistical inference for a difference in proportions in two ways: simulation-based methods through a randomization test and bootstrap confidence interval, and theory-based methods through a two sample \\(z\\)-test and \\(z\\)-interval. 5.4.1 Randomization test for \\(H_0: \\pi_1 - \\pi_2 = 0\\) As you learned in Chapter 1, a randomized experiment is done to assess whether or not one variable (the explanatory variable) causes changes in a second variable (the response variable). Every data set has some variability in it, so to decide whether the variability in the data is due to (1) the causal mechanism (the randomized explanatory variable in the experiment) or instead (2) natural variability inherent to the data, we set up a sham randomized experiment as a comparison. That is, we assume that each observational unit would have gotten the exact same response value regardless of the treatment level. By reassigning the treatments many many times, we can compare the actual experiment to the sham experiment. If the actual experiment has more extreme results than any of the sham experiments, we are led to believe that it is the explanatory variable which is causing the result and not inherent data variability. Using a few different studies, let’s look more carefully at this idea of a randomization test. 5.4.1.1 Case study: Gender discrimination We consider a study investigating gender discrimination in the 1970s, which is set in the context of personnel decisions within a bank.117 The research question we hope to answer is, “Are females discriminated against in promotion decisions made by male managers?” Observed data The participants in this study were 48 male bank supervisors attending a management institute at the University of North Carolina in 1972. They were asked to assume the role of the personnel director of a bank and were given a personnel file to judge whether the person should be promoted to a branch manager position. The files given to the participants were identical, except that half of them indicated the candidate was male and the other half indicated the candidate was female. These files were randomly assigned to the subjects. Is this an observational study or an experiment? How does the type of study impact what can be inferred from the results?118 For each supervisor we recorded the gender associated with the assigned file and the promotion decision. Using the results of the study summarized in Table 5.2, we would like to evaluate if females are unfairly discriminated against in promotion decisions. In this study, a smaller proportion of females are promoted than males (0.583 versus 0.875), but it is unclear whether the difference provides convincing evidence that females are unfairly discriminated against. Table 5.2: Summary results for the gender discrimination study. gender male female Total promoted 21 14 35 decision not promoted 3 10 13 Total 24 24 48 The data are visualized in Figure 5.18. Note that the promoted decision is colored in red (promoted) and white(not promoted). Additionally, the observations are broken up into the male and female groups. Figure 5.18: The gender descrimination study can be thought of as 48 red and black cards. Statisticians are sometimes called upon to evaluate the strength of evidence. When looking at the rates of promotion for males and females in this study, why might we be tempted to immediately conclude that females are being discriminated against? The large difference in promotion rates (58.3% for females versus 87.5% for males) suggest there might be discrimination against women in promotion decisions. However, we cannot yet be sure if the observed difference represents discrimination or is just from random chance. Generally there is a little bit of fluctuation in sample data, and we wouldn’t expect the sample proportions to be exactly equal, even if the truth was that the promotion decisions were independent of gender. Additionally, the researchers used a convenience sample—48 male bank supervisors attending a management institute—so we will need to think carefully about to which population we can generalize these results. The previous example is a reminder that the observed outcomes in the sample may not perfectly reflect the true relationships between variables in the underlying population. Table 5.2 shows there were 7 fewer promotions in the female group than in the male group, a difference in promotion rates of 29.2%: \\[ \\hat{p}_M - \\hat{p}_F = \\frac{21}{24} - \\frac{14}{24} = 0.292. \\] This point estimate of the true difference is large, but the sample size for the study is small, making it unclear if this observed difference represents discrimination or whether it is simply due to chance. These two competing claims are our null and alternative hypotheses: \\(H_0\\): Null hypothesis. The variables gender and decision are independent. They have no relationship, and the observed difference between the proportion of males and females who were promoted, 29.2%, was due to chance. \\(H_A\\): Alternative hypothesis. The variables gender and decision are not independent. The difference in promotion rates of 29.2% was not due to chance, and equally qualified females are less likely to be promoted than males. In statistical notation: \\(H_0: \\pi_M - \\pi_F = 0\\) \\(H_A: \\pi_M - \\pi_F &gt; 0\\) What would it mean if the null hypothesis, which says the variables gender and decision are unrelated, is true? It would mean each banker would decide whether to promote the candidate without regard to the gender indicated on the file. That is, the difference in the promotion percentages would be due to the way the files were randomly divided to the bankers, and the randomization just happened to give rise to a relatively large difference of 29.2%. Consider the alternative hypothesis: bankers were influenced by which gender was listed on the personnel file. If this was true, and especially if this influence was substantial, we would expect to see some difference in the promotion rates of male and female candidates. If this gender bias was against females, we would expect a smaller fraction of promotion recommendations for female personnel files relative to the male files. We will choose between these two competing claims by assessing if the data conflict so much with \\(H_0\\) that the null hypothesis cannot be deemed reasonable. If this is the case, and the data support \\(H_A\\), then we will reject the notion of independence and conclude that these data provide strong evidence of discrimination. Variability of the statistic Table 5.2 shows that 35 bank supervisors recommended promotion and 13 did not. Now, suppose the bankers’ decisions were independent of gender. Then, if we conducted the experiment again with a different random assignment of gender to the files, differences in promotion rates would be based only on random fluctuation. We can actually perform this randomization, which simulates what would have happened if the bankers’ decisions had been independent of gender but we had distributed the file genders differently.119 In this simulation, we thoroughly shuffle 48 personnel files, 35 labeled promoted and 13 labeled not promoted, and we deal these files into two stacks. Note that by keeping 35 promoted and 13 not promoted, we are assuming that 35 of the bank managers would have promoted the individual whose content is contained in the file (independent of gender). We will deal 24 files into the first stack, which will represent the 24 “female” files. The second stack will also have 24 files, and it will represent the 24 “male” files. Figure 5.19 highlights both the shuffle and the reallocation to the sham gender groups. Figure 5.19: The gender descrimination data is shuffled and reallocated to the gender groups. Then, as we did with the original data, we tabulate the results and determine the fraction of male and female who were promoted. Since the randomization of files in this simulation is independent of the promotion decisions, any difference in the two promotion rates is entirely due to chance. Table 5.3 show the results of one such simulation. Table 5.3: Simulation results, where the difference in promotion rates between male and female is purely due to chance. gender male female Total promoted 18 17 35 decision not promoted 6 7 13 Total 24 24 48 What is the difference in promotion rates between the two simulated groups in Table 5.3 ? How does this compare to the observed difference 29.2% from the actual study?120 Figure 5.20 shows that the difference in promotion rates is much larger in the original data than it is in the simulated groups (0.292 &gt;&gt;&gt; 0.042). The quantity of interest throughout this case study has been the difference in promotion rates. This summary value is the statistic of interest (or often the test statistic). Figure 5.20: We summarize the randomized data to produce one estimate of the difference in proportions given no gender discrimination. Observed statistic vs. null statistics We computed one possible sample difference in proportions under the null hypothesis in the Guided Practice above, which represents one difference due to chance. While in this first simulation, we physically dealt out files, it is much more efficient to perform this simulation using a computer. Repeating the simulation on a computer, we get another difference due to chance: -0.042. And another: 0.208. And so on until we repeat the simulation enough times that we have a good idea of what represents the distribution of differences in sample proportions from chance alone. Figure 5.21 shows a plot of the differences found from 100 simulations, where each dot represents a simulated difference between the proportions of male and female files recommended for promotion. Figure 5.21: A dot plot of differences from 100 simulations produced under the null hypothesis, \\(H_0\\), where gender_simulated and decision are independent. Two of the 100 simulations had a difference of at least 29.2%, the difference observed in the study, and are shown as solid red dots. Note that the distribution of these simulated differences in proportions is centered around 0. Because we simulated differences in a way that made no distinction between men and women, this makes sense: we should expect differences from chance alone to fall around zero with some random fluctuation for each simulation. How often would you observe a difference of at least 29.2% (0.292) according to Figure 5.21? Often, sometimes, rarely, or never? It appears that a difference of at least 29.2% due to chance alone would only happen about 2% of the time according to Figure 5.21. Such a low probability indicates that observing such a large difference from chance is rare. The difference of 29.2% is a rare event if there really is no impact from listing gender in the candidates’ files, which provides us with two possible interpretations of the study results: \\(H_0\\): Null hypothesis. Gender has no effect on promotion decision, and we observed a difference that is so large that it would only happen rarely. \\(H_A\\): Alternative hypothesis. Gender has an effect on promotion decision, and what we observed was actually due to equally qualified women being discriminated against in promotion decisions, which explains the large difference of 29.2%. When we conduct formal studies, we reject a null position (the idea that the data are a result of chance only) if the data strongly conflict with that null position.121 In our analysis, we determined that there was only a \\(\\approx\\) 2% probability of obtaining a sample where \\(\\geq\\) 29.2% more males than females get promoted by chance alone, so we conclude that the data provide strong evidence of gender discrimination against women by the supervisors. What statistical term is given to the 2% probability of obtaining a sample where \\(\\geq\\) 29.2% more males than females get promoted by chance alone?122 Scope of inference Since the study was a randomized experiment, we can conclude that the effect was due to gender discrimination—the gender of the application caused the lower rate of promotion. However, since this study was a convenience sample, we can only generalize this result to individuals similar to those in the study. Thus, we have evidence of gender discrimination, but only among male bank supervisors attending a management institute at the University of North Carolina in 1972 that are similar to those in the study. 5.4.1.2 Case study: Opportunity cost How rational and consistent is the behavior of the typical American college student? In this section, we’ll explore whether college student consumers always consider the following: money not spent now can be spent later. In particular, we are interested in whether reminding students about this well-known fact about money causes them to be a little thriftier. A skeptic might think that such a reminder would have no impact. We can summarize the two different perspectives using the null and alternative hypothesis framework. \\(H_0\\): Null hypothesis. Reminding students that they can save money for later purchases will not have any impact on students’ spending decisions. \\(H_A\\): Alternative hypothesis. Reminding students that they can save money for later purchases will reduce the chance they will continue with a purchase. How could you design a randomized experiment to test these two hypotheses?123 In statistical notation, we can define parameters \\(\\pi_{ctrl}\\) = the probability a student under a control condition (not reminding them that they can save money for later purchases) refrains from making a purchase, and \\(\\pi_{trmt}\\) = the probability a student under a treatment condition (reminding them that they can save money for later purchases) refrains from makes a purchase. Our hypotheses are then \\(H_0: \\pi_{trmt} - \\pi_{ctrl} = 0\\) \\(H_A: \\pi_{trmt} - \\pi_{ctrl} &gt; 0\\) In this section, we’ll explore an experiment conducted by researchers that investigates this very question for students at a university in the southwestern United States.124 Observed data One-hundred and fifty students were recruited for the study, and each was given the following statement: Imagine that you have been saving some extra money on the side to make some purchases, and on your most recent visit to the video store you come across a special sale on a new video. This video is one with your favorite actor or actress, and your favorite type of movie (such as a comedy, drama, thriller, etc.). This particular video that you are considering is one you have been thinking about buying for a long time. It is available for a special sale price of $14.99. What would you do in this situation? Please circle one of the options below. Half of the 150 students were randomized into a control group and were given the following two options: Buy this entertaining video. Not buy this entertaining video. The remaining 75 students were placed in the treatment group, and they saw a slightly modified option (B): Buy this entertaining video. Not buy this entertaining video. Keep the $14.99 for other purchases. Would the extra statement reminding students of an obvious fact impact the purchasing decision? Table 5.4 summarizes the study results. Table 5.4: Summary of student choices in the opportunity cost study. control group treatment group Total buy DVD 56 41 97 not buy DVD 19 34 53 Total 75 75 150 It might be a little easier to review the results using row proportions, specifically considering the proportion of participants in each group who said they would buy or not buy the DVD. These summaries are given in Table 5.5, and a segmented bar plot is provided in Figure 5.22. Table 5.5: The data above are now summarized using row proportions. Row proportions are particularly useful here since we can view the proportion of buy and not buy decisions in each group. control group treatment group Total buy DVD 0.747 0.547 0.647 not buy DVD 0.253 0.453 0.353 Total 1.00 1.00 1.00 Figure 5.22: Segmented bar plot comparing the proportion who bought and did not buy the DVD between the control and treatment groups. We will define a success in this study as a student who chooses not to buy the DVD.125 Then, the value of interest is the change in DVD purchase rates that results by reminding students that not spending money now means they can spend the money later. We can construct a point estimate for this difference as \\[\\begin{align*} \\hat{p}_{trmt} - \\hat{p}_{ctrl} = \\frac{34}{75} - \\frac{19}{75} = 0.453 - 0.253 = 0.200 \\end{align*}\\] The proportion of students who chose not to buy the DVD was 20% higher in the treatment group than the control group. However, is this result statistically significant? In other words, is a 20% difference between the two groups so prominent that it is unlikely to have occurred from chance alone? Variability of the statistic The primary goal in this data analysis is to understand what sort of differences we might see if the null hypothesis were true, i.e., the treatment had no effect on students. For this, we’ll use the same procedure we applied in Section 5.4.1.1: randomization. Let’s think about the data in the context of the hypotheses. If the null hypothesis (\\(H_0\\)) was true and the treatment had no impact on student decisions, then the observed difference between the two groups of 20% could be attributed entirely to chance. If, on the other hand, the alternative hypothesis (\\(H_A\\)) is true, then the difference indicates that reminding students about saving for later purchases actually impacts their buying decisions. Observed statistic vs. null statistics Just like with the gender discrimination study, we can perform a statistical analysis. Using the same randomization technique from the last section, let’s see what happens when we simulate the experiment under the scenario where there is no effect from the treatment. While we would in reality do this simulation on a computer, it might be useful to think about how we would go about carrying out the simulation without a computer. We start with 150 index cards and label each card to indicate the distribution of our response variable: decision. That is, 53 cards will be labeled “not buy DVD” to represent the 53 students who opted not to buy, and 97 will be labeled “buy DVD” for the other 97 students. Then we shuffle these cards thoroughly and divide them into two stacks of size 75, representing the simulated treatment and control groups. Any observed difference between the proportions of “not buy DVD” cards (what we earlier defined as success) can be attributed entirely to chance. If we are randomly assigning the cards into the simulated treatment and control groups, how many “not buy DVD” cards would we expect to end up with in each simulated group? What would be the expected difference between the proportions of “not buy DVD” cards in each group? Since the simulated groups are of equal size, we would expect \\(53 / 2 = 26.5\\), i.e., 26 or 27, “not buy DVD” cards in each simulated group, yielding a simulated point estimate of 0% . However, due to random fluctuations, we might actually observe a number a little above or below 26 and 27. The results of a single randomization from chance alone is shown in Table 5.6. From this table, we can compute a difference that occurred from chance alone: \\[\\begin{align*} \\hat{p}_{trmt, simulated} - \\hat{p}_{ctrl, simulated} = \\frac{24}{75} - \\frac{29}{75} = 0.32 - 0.387 = - 0.067 \\end{align*}\\] Table 5.6: Summary of student choices against their simulated groups. The group assignment had no connection to the student decisions, so any difference between the two groups is due to chance. control group treatment group Total buy DVD 46 51 97 not buy DVD 29 24 53 Total 75 75 150 Just one simulation will not be enough to get a sense of what sorts of differences would happen from chance alone. We’ll simulate another set of simulated groups and compute the new difference: 0.013. And again: 0.067. And again: -0.173. We’ll do this 1,000 times. The results are summarized in a dot plot in Figure 5.23, where each point represents a simulation. Since there are so many points, it is more convenient to summarize the results in a histogram such as the one in Figure 5.24, where the height of each histogram bar represents the fraction of observations in that group. Figure 5.23: A stacked dot plot of 1,000 chance differences produced under the null hypothesis, \\(H_0\\). Six of the 1,000 simulations had a difference of at least 20% , which was the difference observed in the study. Figure 5.24: A histogram of 1,000 chance differences produced under the null hypothesis, \\(H_0\\). Histograms like this one are a more convenient representation of data or results when there are a large number of observations. If there was no treatment effect, then we’d only observe a difference of at least +20% about 0.6% of the time, or about 1-in-150 times. That is really rare! Instead, we will conclude the data provide strong evidence there is a treatment effect: reminding students before a purchase that they could instead spend the money later on something else lowers the chance that they will continue with the purchase. Notice that we are able to make a causal statement for this study since the study is an experiment. Scope of inference Since the study was a randomized experiment, we can conclude that the effect was due to the reminder about saving money for other purchases—the reminder caused the lower rate of purchase. However, since this study used a volunteer sample (students were “recruited”), we can only generalize this result to individuals similar to those in the study. Thus, we have evidence that reminding students that they can save money for later purchases will reduce the chance they will continue with a purchase, but only among students are similar to those in the study. 5.4.1.3 Case study: Malaria vaccine Observed data We consider a study on a new malaria vaccine called PfSPZ. In this study, volunteer patients were randomized into one of two experiment groups: 14 patients received an experimental vaccine and 6 patients received a placebo vaccine. Nineteen weeks later, all 20 patients were exposed to a drug-sensitive malaria virus strain; the motivation of using a drug-sensitive strain of virus here is for ethical considerations, allowing any infections to be treated effectively. The results are summarized in Table 5.7, where 9 of the 14 treatment patients remained free of signs of infection while all of the 6 patients in the control group patients showed some baseline signs of infection. Table 5.7: Summary results for the malaria vaccine experiment. treatment vaccine placebo Total infection 5 6 11 no infection placebo 9 0 9 Total 14 6 20 Is this an observational study or an experiment? What implications does the study type have on what can be inferred from the results?126 In this study, a smaller proportion of patients who received the vaccine showed signs of an infection (35.7% versus 100%). However, the sample is very small, and it is unclear whether the difference provides convincing evidence that the vaccine is effective. To determine this, we need to perform statistical inference. Instead of using the difference in proportion infected as our summary measure, let’s use the relative risk of infection for this case study. Thus, the parameter of interest is \\(\\pi_{Vac} / \\pi_{Pla}\\), and our point estimate of this parameter is \\[ \\frac{\\hat{p}_{Vac}}{\\hat{p}_{Pla}} = \\frac{5/14}{6/6} = 0.357. \\] Converting this to a percent decrease127, we see that the patients in the vaccine group had a 64.3% reduced risk of infection compared to the placebo group.128 In terms of relative risk, our null and alternative hypotheses are Independence model \\(H_0: \\dfrac{\\pi_{Vac}}{\\pi_{Pla}} = 1\\) Alternative model \\(H_A: \\dfrac{\\pi_{Vac}}{\\pi_{Pla}} &lt; 1\\) Whether we write our hypotheses in terms of a difference in proportions or a ratio of proportions (relative risk), the hypotheses still have the same interpretation. For example, the three null hypotheses \\(H_0: \\pi_{Vac} = \\pi_{Pla}\\), \\(H_0: \\pi_{Vac} - \\pi_{Pla} = 0\\), and \\(H_0: \\pi_{Vac}/\\pi_{Pla} = 1\\), are all algebraicly equivalent. What would it mean if the independence model, which says the vaccine had no influence on the rate of infection, is true? It would mean 11 patients were going to develop an infection no matter which group they were randomized into, and 9 patients would not develop an infection no matter which group they were randomized into. That is, if the vaccine did not affect the rate of infection, the difference in the infection rates was due to chance alone in how the patients were randomized. Now consider the alternative model: infection rates were influenced by whether a patient received the vaccine or not. If this was true, and especially if this influence was substantial, we would expect to see some difference in the infection rates of patients in the groups. We choose between these two competing claims by assessing if the data conflict so much with \\(H_0\\) that the independence model cannot be deemed reasonable. If this is the case, and the data support \\(H_A\\), then we will reject the notion of independence and conclude the vaccine is effective. Variability of the statistic We’re going to implement simulation, where we will pretend we know that the malaria vaccine being tested does work. Ultimately, we want to understand if the large difference we observed is common in these simulations. If it is common, then maybe the difference we observed was purely due to chance. If it is very uncommon, then the possibility that the vaccine was helpful seems more plausible. We can again randomize the responses (infection or no infection) to the treatment conditions under the null hypothesis of independence, but this time, we’ll compute sample relative risks with each simulated sample. How could you use cards to re-randomize one sample into groups? Remember, in this hypothetical world, we believe each patient that got an infection was going to get it regardless of which group they were in, and we would like to see what happens if we randomly assign these patients to the treatment and control groups again.129 Figure 5.25 shows a histogram of the relative risks found from 1,000 randomization simulations, where each dot represents a simulated relative risk of infection (treatment rate divided by control rate). Figure 5.25: A histogram of relative risks of infection from 1,000 simulations produced under the independence model \\(H_0\\), where in these simulations infections are unaffected by the vaccine. Seventeen of the 1,000 simulations (shaded in red) had a relative risk of at most 0.357, the relative risk observed in the study. Observed statistic vs null statistics Note that the distribution of these simulated differences is centered around 1. We simulated the relative risks assuming that the independence model was true, and under this condition, we expect the difference to be near one with some random fluctuation, where near is pretty generous in this case since the sample sizes are so small in this study. How often would you observe a sample relative risk of at most 0.357 (at least a 64.3% reduction in risk on vaccine) according to Figure 5.25? Often, sometimes, rarely, or never? It appears that a 64.3% reduction in risk due to chance alone would only happen about 2% of the time according to Figure 5.25. Such a low probability indicates a rare event. Based on the simulations, we have two options: We conclude that the study results do not provide strong evidence against the independence model. That is, we do not have sufficiently strong evidence to conclude the vaccine had an effect in this clinical setting. We conclude the evidence is sufficiently strong to reject \\(H_0\\) and assert that the vaccine was useful. When we conduct formal studies, usually we reject the notion that we just happened to observe a rare event.130 In this case, we reject the independence model in favor of the alternative. That is, we are concluding the data provide strong evidence that the vaccine provides some protection against malaria in this clinical setting. Statistical inference is built on evaluating whether such differences are due to chance. In statistical inference, data scientists evaluate which model is most reasonable given the data. Errors do occur, just like rare events, and we might choose the wrong model. While we do not always choose correctly, statistical inference gives us tools to control and evaluate how often these errors occur. 5.4.2 Two-sided hypotheses In in the gender discrimination and opportunity cost studies, we explored whether women were discriminated against and whether a simple trick could make students a little thriftier. In these two case studies, we’ve actually ignored some possibilities: What if men are actually discriminated against? What if the money trick actually makes students spend more? The original hypotheses we’ve seen are called one-sided hypothesis tests because they only explored one direction of possibilities. Such hypotheses are appropriate when we are exclusively interested in the single direction, but usually we want to consider all possibilities. To do so, let’s learn about two-sided hypothesis tests in the context of a new study that examines the impact of using blood thinners on patients who have undergone CPR. Case study: CPR and blood thinner {#cpr} Cardiopulmonary resuscitation (CPR) is a procedure used on individuals suffering a heart attack when other emergency resources are unavailable. This procedure is helpful in providing some blood circulation to keep a person alive, but CPR chest compressions can also cause internal injuries. Internal bleeding and other injuries that can result from CPR complicate additional treatment efforts. For instance, blood thinners may be used to help release a clot that is causing the heart attack once a patient arrives in the hospital. However, blood thinners negatively affect internal injuries. Here we consider an experiment with patients who underwent CPR for a heart attack and were subsequently admitted to a hospital.131 Each patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group). The outcome variable of interest was whether the patient survived for at least 24 hours. Form hypotheses for this study in plain and statistical language. Let \\(\\pi_c\\) represent the true survival rate of people who do not receive a blood thinner (corresponding to the control group) and \\(\\pi_t\\) represent the true survival rate for people receiving a blood thinner (corresponding to the treatment group). We want to understand whether blood thinners are helpful or harmful. We’ll consider both of these possibilities using a two-sided hypothesis test. \\(H_0\\): Blood thinners do not have an overall survival effect, i.e., \\(\\pi_t - \\pi_c = 0\\). \\(H_A\\): Blood thinners have an impact on survival, either positive or negative, but not zero, i.e., \\(\\pi_t - \\pi_c \\neq 0\\). Note that if we had done a one-sided hypothesis test, the resulting hypotheses would have been: \\(H_0\\): Blood thinners do not have a positive overall survival effect, i.e., \\(\\pi_t - \\pi_c = 0\\). \\(H_A\\): Blood thinners have a positive impact on survival, i.e., \\(\\pi_t - \\pi_c &gt; 0\\). There were 50 patients in the experiment who did not receive a blood thinner and 40 patients who did. The study results are shown in Table 5.8. Table 5.8: Results for the CPR study. Patients in the treatment group were given a blood thinner, and patients in the control group were not. Treatment Control Total Survived 14 11 25 Died 26 39 65 Total 40 50 90 What is the observed survival rate in the control group? And in the treatment group? Also, provide a point estimate of the difference in survival proportions of the two groups (\\(\\hat{p}_t - \\hat{p}_c\\)) and the relative “risk” of survival (\\(\\hat{p}_t/\\hat{p}_c\\)).132 According to the point estimate, for patients who have undergone CPR outside of the hospital, an additional 13% of these patients survive when they are treated with blood thinners. Interpreting the relative risk, patients in this sample who had undergone CPR outside of the hospital had a 59% higher survival rate when they were treated with blood thinners. However, we wonder if this difference could be easily explainable by chance. As we did in our past studies this chapter, we will simulate what type of differences we might see from chance alone under the null hypothesis. By randomly assigning “simulated treatment” and “simulated control” stickers to the patients’ files, we get a new grouping. If we repeat this simulation 10,000 times, we can build a null distribution of the differences in sample proportions shown in Figure 5.26. Figure 5.26: Null distribution of the point estimate for the difference in proportions, \\(\\hat{p}_t - \\hat{p}_c\\). The shaded right tail shows observations that are at least as large as the observed difference, 0.13. The right tail area is 0.131.133 However, contrary to how we calculated the p-value in previous studies, the p-value of this test is not 0.131! The p-value is defined as the chance we observe a result at least as favorable to the alternative hypothesis as the result (i.e., the difference) we observe. In this case, any differences less than or equal to $-$0.13 would also provide equally strong evidence favoring the alternative hypothesis as a difference of $+$0.13 did. A difference of $-$0.13 would correspond to the survival rate in the control group being 0.13 higher than the treatment group.134 In Figure 5.27 we’ve also shaded these differences in the left tail of the distribution. These two shaded tails provide a visual representation of the p-value for a two-sided test. Figure 5.27: Null distribution of the point estimate for the difference in proportions, \\(\\hat{p}_t - \\hat{p}_c\\). All values that are at least as extreme as +0.13 but in either direction away from 0 are shaded. For a two-sided test, since the null distribution is symmetric, take the single tail (in this case, 0.131) and double it to get the p-value: 0.262. With this large p-value, we do not find statistically significant evidence that the blood thinner has any influence on survival of patients who undergo CPR prior to arriving at the hospital. Default to a two-sided test. We want to be rigorous and keep an open mind when we analyze data and evidence. Use a one-sided hypothesis test only if you truly have interest in only one direction. Computing a p-value for a two-sided test. If your null distribution is symmetric, first compute the p-value for one tail of the distribution, then double that value to get the two-sided p-value. That’s it!135 Consider the situation of the medical consultant. Now that you know about one-sided and two-sided tests, which type of test do you think is more appropriate? The setting has been framed in the context of the consultant being helpful (which is what led us to a one-sided test originally), but what if the consultant actually performed worse than the average? Would we care? More than ever! Since it turns out that we care about a finding in either direction, we should run a two-sided test. The p-value for the two-sided test is double that of the one-sided test, here the simulated p-value would be 0.2444. Generally, to find a two-sided p-value we double the single tail area, which remains a reasonable approach even when the sampling distribution is asymmetric. However, the approach can result in p-values larger than 1 when the point estimate is very near the mean in the null distribution; in such cases, we write that the p-value is 1. Also, very large p-values computed in this way (e.g., 0.85), may also be slightly inflated. Typically, we do not worry too much about the precision of very large p-values because they lead to the same analysis conclusion, even if the value is slightly off. 5.4.3 Bootstrap confidence interval for \\(\\pi_1 - \\pi_2\\) In Section 5.4.1, we worked with the randomization distribution to understand the distribution of \\(\\hat{p}_1 - \\hat{p}_2\\) when the null hypothesis \\(H_0: \\pi_1 - \\pi_2 = 0\\) is true. Now, through bootstrapping, we study the variability of \\(\\hat{p}_1 - \\hat{p}_2\\) without the null assumption. Observed data Reconsider the CPR data from Section 5.4.1 which is provided in Table 5.8. The experiment consisted of two treatments on patients who underwent CPR for a heart attack and were subsequently admitted to a hospital. Each patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group). The outcome variable of interest was whether the patient survived for at least 24 hours. Again, we use the difference in sample proportions as the observed statistic of interest. Here, the value of the statistic is: \\(\\hat{p}_t - \\hat{p}_c = 0.35 - 0.22 = 0.13\\). Variability of the statistic The bootstrap method applied to two samples is an extension of the method described in Section 5.3.2. Now, we have two samples, so each sample estimates the population from which they came. In the CPR setting, the treatment sample estimates the population of all individuals who have gotten (or will get) the treatment; the control sample estimate the population of all individuals who do not get the treatment and are controls. Figure 5.28 extends Figure 5.11 to show the bootstrapping process from two samples simultaneously. Figure 5.28: Creating two estimated populations from two different samples from different populations. As before, once the population is estimated, we can randomly resample observations to create bootstrap samples, as seen in Figure 5.29. Computationally, each bootstrap resample is created by randomly sampling with replacement from the original sample. Figure 5.29: Bootstrapped resamples from two separate estimated populations. The variability of the statistic (the difference in sample proportions) can be calculated by taking one treatment bootstrap sample and one control bootstrap sample and calculating the difference of the bootstrap survival proportions. Figure @(boot2samp2) displays one bootstrap resample from each of the estimated populations, with the difference in sample proportions calculated between the treatment bootstrap sample and the control bootstrap sample. Figure 5.30: The bootstrap resample on the left is from the first estimated population; the one on the right from the second. In this case, the value of the simulated bootstrap statistic would be \\(\\hat{p}_1 - \\hat{p}_2 = \\frac{2}{7}-\\frac{1}{7}\\). As always, the variability of the difference in proportions can only be estimated by repeated simulations, in this case, repeated bootstrap samples. Figure 5.31 shows multiple bootstrap differences calculated for each of the repeated bootstrap samples. Figure 5.31: in this graph, some kind of connection between each of the two sides Repeated bootstrap simulations lead to a bootstrap sampling distribution of the statistic of interest, here the difference in sample proportions. Figure 5.32 visualizes the process in the toy example, and Figure 5.33 shows 1000 bootstrap differences in proportions for the CPR data. Figure 5.32: The process of repeatedly resampling from the estimated population (sampling with replacement from the original sample), computing a difference in sample proportions from each pair of samples, then plotting this distribution. Figure 5.33: A histogram of differences in proportions (treatment \\(-\\) control) from 1000 bootstrap simulations using the CPR data. Percentile vs. SE bootstrap confidence intervals Figure 5.33 provides an estimate for the variability of the difference in survival proportions from sample to sample, The values in the histogram can be used in two different ways to create a confidence interval for the parameter of interest: \\(\\pi_1 - \\pi_2\\). Percentile bootstrap interval As in Section 5.3.2, the bootstrap confidence interval can be calculated directly from the bootstrapped differences in Figure 5.33. The interval created from the percentiles of the distribution is called the percentile interval. Note that here we calculate the 90% confidence interval by finding the 5th and 95th percentile values from the bootstrapped differences. The bootstrap 5 percentile proportion is -0.155 and the 95 percentile is 0.167. The result is: we are 90% confident that, in the population, the true difference in probability of survival (treatment \\(-\\) control) is between -0.155 and 0.167. More clearly, we are 90% confident that the probability of survival for heart attack patients who underwent CPR on blood thinners is between 0.155 less to 0.167 more than that for patients who were not given blood thinners. The interval shows that we do not have much definitive evidence of the affect of blood thinners, one way or another. Figure 5.34: The CPR data is bootstrapped 1000 times. Each simulation creates a sample from the original data where the probability of survival in the treatment group is \\(\\hat{p}_{t} = 14/40\\) and the probability of survival in the control group is \\(\\hat{p}_{c} = 11/50\\). SE bootstrap interval Alternatively, we can use the variability in the bootstrapped differences to calculate a standard error of the difference. The resulting interval is called the SE interval. Section 5.4.4 details the mathematical model for the standard error of the difference in sample proportions, but the bootstrap distribution typically does an excellent job of estimating the variability. \\[SE(\\hat{p}_t - \\hat{p}_c) \\approx SD(\\hat{p}_{bs,t} - \\hat{p}_{bs,c}) = 0.0975\\] The variability of the bootstrapped difference in proportions was calculated in R using the sd() function, but any statistical software will calculate the standard deviation of the differences, here, the exact quantity we hope to approximate. Note that we do not know know the true distribution of \\(\\hat{p}_t - \\hat{p}_c\\), so we will use a rough approximation to find a confidence interval for \\(\\pi_t - \\pi_c\\). As seen in the bootstrap histograms, the shape of the distribution is roughly symmetric and bell-shaped. So for a rough approximation, we will apply the 68-95-99.7 rule which tells us that 95% of observed differences should be roughly no farther than 2 SE from the true parameter difference. An approximate 95% confidence interval for \\(\\pi_t - \\pi_c\\) is given by: \\[\\begin{align*} \\hat{p}_t - \\hat{p}_c \\pm 2 \\cdot SE \\ \\ \\ \\rightarrow \\ \\ \\ 14/40 - 11/50 \\pm 2 \\cdot 0.0975 \\ \\ \\ \\rightarrow \\ \\ \\ (-0.065, 0.325) \\end{align*}\\] We are 95% confident that the true value of \\(\\pi_t - \\pi_c\\) is between -0.065 and 0.325. Again, the wide confidence interval that overlaps zero indicates that the study provides very little evidence about the effectiveness of blood thinners. Since the multiplier “2” in the SE bootstrap interval comes from the 68-95-99.7 rule for normal distributions, these intervals are only valid when the bootstrap sampling distribution is approximately normal. What does 95% mean? Recall that the goal of a confidence interval is to find a plausible range of values for a parameter of interest. The estimated statistic is not the value of interest, but it is typically the best guess for the unknown parameter. The confidence level (often 95%) is a number that takes a while to get used to. Surprisingly, the percentage doesn’t describe the data set at hand, it describes many possible data sets. One way to understand a confidence interval is to think about all the confidence intervals that you have ever made or that you will ever make as a scientist, the confidence level describes those intervals. Figure 5.35 demonstrates a hypothetical situation in which 25 different studies are performed on the exact same population (with the same goal of estimating the true parameter value of \\(\\pi_1 - \\pi_2 = 0.47\\)). The study at hand represents one point estimate (a dot) and a corresponding interval. It is not possible to know whether the interval at hand is to the right of the unknown true parameter value (the black line) or to the left of that line. It is also impossible to know whether the interval captures the true parameter (is blue) or doesn’t (is red). If we are making 95% intervals, then 5% of the intervals we create over our lifetime will not capture the parameter of interest (e.g., will be red as in Figure 5.35 ). What we know is that over our lifetimes as scientists, 95% of the intervals created and reported on will capture the parameter value of interest: thus the language “95% confident.” Figure 5.35: One hypothetical population, parameter value of: \\(\\pi_1 - \\pi_2 = 0.47\\). Twenty-five different studies all which led to a different point estimate, SE, and confidence interval. The study at hand is one of the horizontal lines (hopefully a blue line!). The choice of 95% or 90% or even 99% as a confidence level is admittedly somewhat arbitrary; however, it is related to the logic we used when deciding that a p-value should be declared as significant if it is lower than 0.05 (or 0.10 or 0.01, respectively). Indeed, one can show mathematically, that a 95% confidence interval and a two-sided hypothesis test at a cutoff of 0.05 will provide the same conclusion when the same data and mathematical tools are applied for the analysis. A full derivation of the explicit connection between confidence intervals and hypothesis tests is beyond the scope of this text. 5.4.4 Theory-based methods for \\(\\pi_1 - \\pi_2\\) Like with \\(\\hat{p}\\), the difference of two sample proportions \\(\\hat{p}_1 - \\hat{p}_2\\) can be modeled using a normal distribution when certain conditions are met. Evaluating the two conditions required for modeling \\(\\pi_1 - \\pi_2\\) using theory-based methods First, we require a broader independence condition, and secondly, the success-failure condition must be met by both groups. Conditions for the sampling distribution of \\(\\hat{p}_1 -\\hat{p}_2\\) to be normal. The difference \\(\\hat{p}_1 - \\hat{p}_2\\) can be modeled using a normal distribution when Independence (extended). The data are independent within and between the two groups. Generally this is satisfied if the data come from two independent random samples or if the data come from a randomized experiment. Success-failure condition. The success-failure condition holds for both groups, where we check successes and failures in each group separately. This condition is met if we have at least 10 successes and 10 failures in each sample. If data are displayed in a two-way table, this is equivalent to checking that all cells in the table have at least 10 observations. When these conditions are satisfied, then the sampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\) is approximately normal with mean \\(\\pi_1 - \\pi_2\\) and standard deviation \\[\\begin{eqnarray*} SD(\\hat{p}_1 - \\hat{p}_2) = \\sqrt{\\frac{\\pi_1(1-\\pi_1)}{n_1} + \\frac{\\pi_2(1-\\pi_2)}{n_2}} \\end{eqnarray*}\\] where \\(\\pi_1\\) and \\(\\pi_2\\) represent the population proportions, and \\(n_1\\) and \\(n_2\\) represent the sample sizes. The success-failure condition listed above is only necessary for the sampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\) to be approximately normal. The mean of the sampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\) is \\(\\pi_1 - \\pi_2\\), and the standard deviation is \\(\\sqrt{\\frac{\\pi_1(1-\\pi_1)}{n_1}+\\frac{\\pi_2(1-\\pi_2)}{n_2}}\\), regardless of the two sample sizes. As in the case of one proportion, we typically don’t know the true proportions \\(\\pi_1\\) and \\(\\pi_2\\), so we will substitute some value to check the success-failure condition and to estimate the standard deviation of the sampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\). Confidence interval for \\(\\pi_1 - \\pi_2\\) Standard error of the difference in two proportions, \\(\\hat{p}_1 -\\hat{p}_2\\): confidence intervals. When computing a theory-based confidence interval for \\(\\pi_1 - \\pi_2\\), we substitute \\(\\hat{p}_1\\) for \\(\\pi_1\\) and \\(\\hat{p}_2\\) for \\(\\pi_2\\) in the expression for the standard deviation of the statistic, resulting in its standard error: \\[\\begin{eqnarray*} SE(\\hat{p}_1 -\\hat{p}_2) = \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}} \\end{eqnarray*}\\] This is the standard error formula we will use when computing confidence intervals for the difference in two proportions. If the conditions for the sampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\) to be normal are met, we can apply the generic confidence interval formula for a difference of two proportions, where we use \\(\\hat{p}_1 - \\hat{p}_2\\) as the point estimate and substitute the \\(SE\\) formula above: \\[\\begin{align*} &amp;\\text{point estimate} \\ \\pm\\ z^{\\star} \\times SE &amp;&amp;\\to &amp;&amp;\\hat{p}_1 - \\hat{p}_2 \\ \\pm\\ z^{\\star} \\times \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}} \\end{align*}\\] We reconsider the experiment for patients who underwent cardiopulmonary resuscitation (CPR) for a heart attack and were subsequently admitted to a hospital. These patients were randomly divided into a treatment group where they received a blood thinner or the control group where they did not receive a blood thinner. The outcome variable of interest was whether the patients survived for at least 24 hours. The results are shown in Table 5.8. Check whether we can model the difference in sample proportions using the normal distribution. We first check for independence: since this is a randomized experiment, this condition is satisfied. Next, we check the success-failure condition for each group. We have at least 10 successes and 10 failures in each experiment arm (11, 14, 39, 26), so this condition is also satisfied. With both conditions satisfied, the difference in sample proportions can be reasonably modeled using a normal distribution for these data. Create and interpret a 90% confidence interval of the difference for the survival rates in the CPR study. We’ll use \\(\\pi_t\\) for the true survival rate in the treatment group and \\(\\pi_c\\) for the control group. Our point estimate of \\(\\pi_t - \\pi_c\\) is: \\[\\begin{align*} \\hat{p}_{t} - \\hat{p}_{c} = \\frac{14}{40} - \\frac{11}{50} = 0.35 - 0.22 = 0.13 \\end{align*}\\] We use the standard error formula previously provided. As with the one-sample proportion case, we use the sample estimates of each proportion in the formula in the confidence interval context: \\[\\begin{align*} SE \\approx \\sqrt{\\frac{0.35 (1 - 0.35)}{40} + \\frac{0.22 (1 - 0.22)}{50}} = 0.095 \\end{align*}\\] For a 90% confidence interval, we use \\(z^{\\star} = 1.65\\): \\[\\begin{align*} \\text{point estimate} \\ \\pm\\ z^{\\star} \\times SE \\quad \\to \\quad 0.13 \\ \\pm\\ 1.65 \\times 0.095 \\quad \\to \\quad (-0.027, 0.287) \\end{align*}\\] We are 90% confident that the survival probability for those patients given blood thinners is between 0.027 lower to 0.287 higher than that of patients not given blood thinners, among patients like those in the study. Because 0% is contained in the interval, we do not have enough information to say whether blood thinners help or harm heart attack patients who have been admitted after they have undergone CPR. Note, the problem was set up as 90% to indicate that there was not a need for a high level of confidence (such a 95% or 99%). A lower degree of confidence increases potential for error, but it also produces a more narrow interval. A 5-year experiment was conducted to evaluate the effectiveness of fish oils on reducing cardiovascular events, where each subject was randomized into one of two treatment groups. We will consider heart attack outcomes in the patients listed in Table 5.9. Create a 95% confidence interval for the effect of fish oils on heart attacks for patients who are well-represented by those in the study. Also interpret the interval in the context of the study.136 Table 5.9: Results for the study on n-3 fatty acid supplement and related health benefits. fish oil placebo heart attack 145 200 no event 12788 12738 Total 12933 12938 Hypothesis test for \\(H_0: \\pi_1 - \\pi_2 = 0\\) A mammogram is an X-ray procedure used to check for breast cancer. Whether mammograms should be used is part of a controversial discussion, and it’s the topic of our next example where we learn about two proportion hypothesis tests when \\(H_0\\) is \\(\\pi_1 - \\pi_2 = 0\\) (or equivalently, \\(\\pi_1 = \\pi_2\\)). A 30-year study was conducted with nearly 90,000 female participants. During a 5-year screening period, each woman was randomized to one of two groups: in the first group, women received regular mammograms to screen for breast cancer, and in the second group, women received regular non-mammogram breast cancer exams. No intervention was made during the following 25 years of the study, and we’ll consider death resulting from breast cancer over the full 30-year period. Results from the study are summarized in Figure 5.10. If mammograms are much more effective than non-mammogram breast cancer exams, then we would expect to see additional deaths from breast cancer in the control group. On the other hand, if mammograms are not as effective as regular breast cancer exams, we would expect to see an increase in breast cancer deaths in the mammogram group. Table 5.10: Summary results for breast cancer study. Mammogram Control Death from breast cancer? Yes 500 505 No 44,425 44,405 Is this study an experiment or an observational study?137 Set up hypotheses to test whether there was a difference in breast cancer deaths in the mammogram and control groups.138 The research question describing mammograms is set up to address specific hypotheses (in contrast to a confidence interval for a parameter). In order to fully take advantage of the hypothesis testing structure, we assess the randomness under the condition that the null hypothesis is true (as we always do for hypothesis testing). Using the data from Table 5.10, we will check the conditions for using a normal distribution to analyze the results of the study using a hypothesis test. The details for checking conditions are very similar to that of confidence intervals. However, when the null hypothesis is that \\(\\pi_1 - \\pi_2 = 0\\), we use a special proportion called the pooled proportion to check the success-failure condition and when computing the standard error: \\[\\begin{align*} \\hat{p}_{\\textit{pool}} &amp;= \\frac {\\text{# of patients who died from breast cancer in the entire study}} {\\text{# of patients in the entire study}} \\\\ &amp;\\\\ &amp;= \\frac{500 + 505}{500 + \\text{44,425} + 505 + \\text{44,405}} \\\\ &amp;\\\\ &amp;= 0.0112 \\end{align*}\\] This proportion is an estimate of the breast cancer death rate across the entire study, and it’s our best estimate of the death rates \\(\\pi_{mgm}\\) and \\(\\pi_{ctrl}\\) if the null hypothesis is true that \\(\\pi_{mgm} = \\pi_{ctrl}\\). Use the pooled proportion when \\(H_0\\) is \\(\\pi_1 - \\pi_2 = 0\\). When the null hypothesis is that the proportions are equal, use the pooled proportion (\\(\\hat{p}_{\\textit{pool}}\\)) to verify the success-failure condition and estimate the standard error: \\[\\begin{eqnarray*} \\hat{p}_{\\textit{pool}} = \\frac{\\text{number of &quot;successes&quot;}} {\\text{number of cases}} = \\frac{\\hat{p}_1 n_1 + \\hat{p}_2 n_2}{n_1 + n_2} \\end{eqnarray*}\\] Here \\(\\hat{p}_1 n_1\\) represents the number of successes in sample 1 since \\[\\begin{eqnarray*} \\hat{p}_1 = \\frac{\\text{number of successes in sample 1}}{n_1} \\end{eqnarray*}\\] Similarly, \\(\\hat{p}_2 n_2\\) represents the number of successes in sample 2. Is it reasonable to model the difference in proportions using a normal distribution in this study? Because the patients are randomized, they can be treated as independent, both within and between groups. We also must check the success-failure condition for each group. Under the null hypothesis, the proportions \\(\\pi_{mgm}\\) and \\(\\pi_{ctrl}\\) are equal, so we check the success-failure condition with our best estimate of these values under \\(H_0\\), the pooled proportion from the two samples, \\(\\hat{p}_{\\textit{pool}} = 0.0112\\): \\[\\begin{align*} \\hat{p}_{\\textit{pool}} \\times n_{mgm} &amp;= 0.0112 \\times \\text{44,925} = 503 \\\\ (1 - \\hat{p}_{\\textit{pool}}) \\times n_{mgm} &amp;= 0.9888 \\times \\text{44,925} = \\text{44,422} \\\\ &amp; \\\\ \\hat{p}_{\\textit{pool}} \\times n_{ctrl} &amp;= 0.0112 \\times \\text{44,910} = 503 \\\\ (1 - \\hat{p}_{\\textit{pool}}) \\times n_{ctrl} &amp;= 0.9888 \\times \\text{44,910} = \\text{44,407} \\end{align*}\\] The success-failure condition is satisfied since all values are at least 10. With both conditions satisfied, we can safely model the difference in proportions using a normal distribution. We used the pooled proportion to check the success-failure condition139. We next use it again in the standard error calculation. Standard error of the difference in two proportions, \\(\\hat{p}_1 -\\hat{p}_2\\): hypothesis tests. Since we assume \\(\\pi_1 = \\pi_2\\) when we conduct a theory-based hypothesis test for \\(H_0: \\pi_1 - \\pi_2 = 0\\), we substitute the pooled sample proportion, \\(\\hat{p}_{pool}\\) in for both \\(\\pi_1\\) and \\(\\pi_2\\) in the expression for the standard deviation of the statistic, resulting in its null standard error: \\[\\begin{eqnarray*} SE_0(\\hat{p}_1 -\\hat{p}_2) = \\sqrt{\\frac{\\hat{p}_{pool}(1-\\hat{p}_{pool})}{n_1} + \\frac{\\hat{p}_{pool}(1-\\hat{p}_{pool})}{n_2}} = \\sqrt{\\hat{p}_{pool}(1-\\hat{p}_{pool})\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)} \\end{eqnarray*}\\] This is the standard error formula we will use when computing the test statistic for a hypothesis test of \\(H_0: \\pi_1 - \\pi_2 = 0\\). Compute the point estimate of the difference in breast cancer death rates in the two groups, and use the pooled proportion \\(\\hat{p}_{\\textit{pool}} = 0.0112\\) to calculate the standard error. The point estimate of the difference in breast cancer death rates is \\[\\begin{align*} \\hat{p}_{mgm} - \\hat{p}_{ctrl} &amp;= \\frac{500}{500 + 44,425} - \\frac{505}{505 + 44,405} \\\\ &amp; \\\\ &amp;= 0.01113 - 0.01125 \\\\ &amp; \\\\ &amp;= -0.00012 \\end{align*}\\] The breast cancer death rate in the mammogram group was 0.00012 less than in the control group. Next, the standard error of \\(\\hat{p}_{mgm} - \\hat{p}_{ctrl}\\) is calculated using the pooled proportion, \\(\\hat{p}_{\\textit{pool}}\\): \\[\\begin{align*} SE_0 = \\sqrt{ \\frac{\\hat{p}_{\\textit{pool}}(1-\\hat{p}_{\\textit{pool}})} {n_{mgm}} + \\frac{\\hat{p}_{\\textit{pool}}(1-\\hat{p}_{\\textit{pool}})} {n_{ctrl}} } = 0.00070 \\end{align*}\\] Using the point estimate \\(\\hat{p}_{mgm} - \\hat{p}_{ctrl} = -0.00012\\) and standard error \\(SE = 0.00070\\), calculate a p-value for the hypothesis test and write a conclusion. Just like in past tests, we first compute a test statistic and draw a picture: \\[\\begin{align*} Z = \\frac{\\text{point estimate} - \\text{null value}}{\\mbox{Null }SE} = \\frac{-0.00012 - 0}{0.00070} = -0.17 \\end{align*}\\] The lower tail area below -0.17 on a standard normal distribution is 0.4325, which we double to get the p-value: 0.8650 (see Figure 5.36). With this very large p-value, the difference in breast cancer death rates is reasonably explained by chance, and we have no significant evidence that mammograms either decrease or increase the risk of death by breast cancer compared to regular breast exams, among women similar to those in the study. Figure 5.36: Standard normal distribution with the p-value shaded. The shaded area represents the probability of observing a difference in sample proportions of -0.17 or further away from zero, if the true proportions were equal. Can we conclude that mammograms have no benefits or harm? Here are a few considerations to keep in mind when reviewing the mammogram study as well as any other medical study: We do not accept the null hypothesis. We can only say we don’t have sufficient evidence to conclude that mammograms reduce breast cancer deaths, and we don’t have sufficient evidence to conclude that mammograms increase breast cancer deaths. If mammograms are helpful or harmful, the data suggest the effect isn’t very large. Are mammograms more or less expensive than a non-mammogram breast exam? If one option is much more expensive than the other and doesn’t offer clear benefits, then we should lean towards the less expensive option. The study’s authors also found that mammograms led to over-diagnosis of breast cancer, which means some breast cancers were found (or thought to be found) but that these cancers would not cause symptoms during patients’ lifetimes. That is, something else would kill the patient before breast cancer symptoms appeared. This means some patients may have been treated for breast cancer unnecessarily, and this treatment is another cost to consider. It is also important to recognize that over-diagnosis can cause unnecessary physical or emotional harm to patients. These considerations highlight the complexity around medical care and treatment recommendations. Experts and medical boards who study medical treatments use considerations like those above to provide their best recommendation based on the current evidence. 5.5 Errors, Power, and Practical Importance 5.5.1 Decision errors Hypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free. Similarly, data can point to the wrong conclusion. However, what distinguishes statistical hypothesis tests from a court system is that our framework allows us to quantify and control how often the data lead us to the incorrect conclusion. In a hypothesis test, there are two competing hypotheses: the null and the alternative. We make a statement about which one might be true, but we might choose incorrectly. There are four possible scenarios in a hypothesis test, which are summarized in Table 5.11. Table 5.11: Four different scenarios for hypothesis tests. Test conclusion Fail to reject \\(H_0\\) Reject \\(H_0\\) \\(H_0\\) true good decision Type 1 Error Truth \\(H_A\\) true Type 2 Error good decision A Type 1 Error is rejecting the null hypothesis when \\(H_0\\) is actually true. Since we rejected the null hypothesis in the gender discrimination, opportunity cost, and malaria studies, it is possible that we made a Type 1 Error in one, two, or all three of those studies. A Type 2 Error is failing to reject the null hypothesis when the alternative is actually true. Since we failed to reject the null hypothesis in the medical consultant study, it is possible that we made a Type 2 Error in that study. In a US court, the defendant is either innocent (\\(H_0\\)) or guilty (\\(H_A\\)). What does a Type 1 Error represent in this context? What does a Type 2 Error represent? Table 5.11 may be useful. If the court makes a Type 1 Error, this means the defendant is innocent (\\(H_0\\) true) but wrongly convicted. A Type 2 Error means the court failed to reject \\(H_0\\) (i.e., failed to convict the person) when they were in fact guilty (\\(H_A\\) true). Consider the opportunity cost study where we concluded students were less likely to make a DVD purchase if they were reminded that money not spent now could be spent later. What would a Type 1 Error represent in this context?140 How could we reduce the Type 1 Error rate in US courts? What influence would this have on the Type 2 Error rate? To lower the Type 1 Error rate, we might raise our standard for conviction from “beyond a reasonable doubt” to “beyond a conceivable doubt” so fewer people would be wrongly convicted. However, this would also make it more difficult to convict the people who are actually guilty, so we would make more Type 2 Errors. How could we reduce the Type 2 Error rate in US courts? What influence would this have on the Type 1 Error rate?141 The example and guided practice above provide an important lesson: if we reduce how often we make one type of error, we generally make more of the other type. 5.5.2 Controlling the Type I error rate Choosing a significance level for a test is important in many contexts, and the traditional level is 0.05. However, it is sometimes helpful to adjust the significance level based on the application. We may select a level that is smaller or larger than 0.05 depending on the consequences of any conclusions reached from the test. Significance level = probability of making a Type 1 error. We reject a null hypothesis if the p-value is less than a chosen significance level, \\(\\alpha\\). Therefore, if the null hypothesis is true, but we end up with really unusual data just by chance – a p-value less than \\(\\alpha\\) – then we mistakenly reject the null hypothesis, making a Type 1 error. If making a Type 1 Error is dangerous or especially costly, we should choose a small significance level (e.g., 0.01 or 0.001). If we want to be very cautious about rejecting the null hypothesis, we demand very strong evidence favoring the alternative \\(H_A\\) before we would reject \\(H_0\\). If a Type 2 Error is relatively more dangerous or much more costly than a Type 1 Error, then we should choose a higher significance level (e.g., 0.10). Here we want to be cautious about failing to reject \\(H_0\\) when the null is actually false. Significance levels should reflect consequences of errors. The significance level selected for a test should reflect the real-world consequences associated with making a Type 1 or Type 2 error. Because of the result of increased error rates, it is never okay to change two-sided tests to one-sided tests after observing the data. There are two dangers if we ignore possibilities that disagree with our data or that conflict with our world view: Framing an alternative hypothesis simply to match the direction that the data point will generally inflate the Type 1 Error rate. After all the work we’ve done (and will continue to do) to rigorously control the error rates in hypothesis tests, careless construction of the alternative hypotheses can disrupt that hard work. If we only use alternative hypotheses that agree with our worldview, then we’re going to be subjecting ourselves to confirmation bias, which means we are looking for data that supports our ideas. That’s not very scientific, and we can do better! We explore the consequences of ignoring this advice in the next example. Using \\(\\alpha=0.05\\), we show that freely switching from two-sided tests to one-sided tests will lead us to make twice as many Type 1 Errors as intended. Suppose we are interested in finding any difference from 0. We’ve created a smooth-looking null distribution representing differences due to chance in Figure 5.37. Suppose the sample difference was larger than 0. Then if we can flip to a one-sided test, we would use \\(H_A\\): difference \\(&gt; 0\\). Now if we obtain any observation in the upper 5% of the distribution, we would reject \\(H_0\\) since the p-value would just be a the single tail. Thus, if the null hypothesis is true, we incorrectly reject the null hypothesis about 5% of the time when the sample mean is above the null value, as shown in Figure 5.37. Suppose the sample difference was smaller than 0. Then if we change to a one-sided test, we would use \\(H_A\\): difference \\(&lt; 0\\). If the observed difference falls in the lower 5% of the figure, we would reject \\(H_0\\). That is, if the null hypothesis is true, then we would observe this situation about 5% of the time. By examining these two scenarios, we can determine that we will make a Type 1 Error \\(5\\%+5\\%=10\\%\\) of the time if we are allowed to swap to the “best” one-sided test for the data. This is twice the error rate we prescribed with our significance level: \\(\\alpha=0.05\\) (!). Figure 5.37: The shaded regions represent areas where we would reject \\(H_0\\) under the bad practices considered in when \\(\\alpha = 0.05\\). Hypothesis tests should be set up before seeing the data. After observing data, it is tempting to turn a two-sided test into a one-sided test. Avoid this temptation. Hypotheses should be set up before observing the data. 5.5.3 Power When the null hypothesis is true, the probability of a Type 1 error is our chosen significance level, \\(\\alpha\\), which means the probability of a correct decision is \\(1 - \\alpha\\). If an alternative hypothesis is true, the probability of a Type 2 error, which we denote by \\(\\beta\\), depends on several components: significance level sample size whether the alternative hypothesis is one-sided or two-sided standard deviation of the statistic how far the alternative parameter value is from the null value Only the first three of these components are within the control of the researcher. The probability of a correct decision when the alternative hypothesis is true, \\(1 - \\beta\\), is called the power of the test. Higher power means we are more likely to detect an effect that actually exists. Power. The power of a test is the probability of rejecting a false null hypothesis. Suppose we would like to test whether less than 65% of a large population approves of a new law: \\(H_0: \\pi = 0.65\\) versus \\(H_A: \\pi &lt; 0.65\\). We collect a random sample of \\(n = 200\\) individuals from this population. For what values of the sample proportion, \\(\\hat{p}\\), would we reject \\(H_0\\) using a significance level of \\(\\alpha = 0.05\\)? Under the assumption of the null hypothesis, the standard deviation of \\(\\hat{p}\\) is \\(\\sqrt{0.65(1-0.65)/200} = 0.0337\\). Thus, by the Central Limit Theorem, sample proportions vary according to an approximate normal distribution with mean 0.65 and standard deviation 0.0337. We will reject the null hypothesis that the true proportion is 0.65 if the sample proportion is so low that its probability is less than 0.05, shown in Figure 5.38. To be precise, we will reject \\(H_0\\) if \\(\\hat{p}\\) is less than the 5th percentile of the null distribution: qnorm(0.05, 0.65, 0.0337) = 0.59. Figure 5.38: Shaded area on a null distribution where we would reject the null hypothesis. This area is equal to the significance level. To calculate power, we need to know the true value of the parameter. In the previous example, the alternative was \\(H_0: \\pi &lt; 0.65\\), so if we just say the alternative hypothesis is true, we still do not know the value of \\(\\pi\\). Thus, power calculations are done for a specific value of the parameter, and the power changes if the value of the parameter changes. Consider again the test of whether less than 65% of a large population approves of a new law: \\(H_0: \\pi = 0.65\\) versus \\(H_A: \\pi &lt; 0.65\\). Suppose the population approval rate is actually \\(\\pi = 0.58\\). What is the probability that we will detect this effect? This example asks us to calculate the power – the probability our test will provide evidence that \\(\\pi &lt; 0.65\\) when the true value of \\(\\pi\\) is 0.58. Recall from the previous example that we will reject the null if \\(\\hat{p} &lt; 0.59\\). Thus, the power is the probability that \\(\\hat{p}\\) will be less than 0.59 when the true proportion is 0.58: pnorm(0.59, 0.58, 0.0337) = 0.62. There is only a 62% chance that the data we collect will provide strong enough evidence to conclude \\(\\pi &lt; 0.65\\). This probability is represented by the red area in Figure 5.39. Figure 5.39: The blue distribution is the distribution of sample proportions if the null hypothesis is true, \\(\\pi = 0.65\\) – the blue shaded area represents the probability we reject a true null hypothesis. The red distribution is the distribution of sample proportions under a particular alternative hypothesis, that \\(\\pi = 0.58\\) – the red shaded area represents the power. Increasing power. The power of a test will increase when: the significance level increases the sample size increases we change from a two-sided to a one-sided test the standard deviation of the statistic decreases how far the alternative parameter value is from the null value increases 5.5.4 Statistical Significance versus Practical Importance An Austrian study of heights of 507,125 military recruits reported that men born in spring were statistically significantly taller than men born in the fall (p-value &lt; 0.0001). A confidence interval for the true difference in mean height between men born in spring and men born in fall was (0.598, 0.602) cm. Is this result practically important? No, these results don’t mean much in this context – a difference in average height of around 0.6 cm would not even be noticeable by the human eye! Just because a result is statistically significant does not mean that it is necessarily practically important – meaningful in the context of the problem. In the previous example, we saw two groups of men that differed in average height, and that difference was statistically significant – that is, the observed difference in sample means of 0.6 cm is very unlikely to occur if the true difference in average height was zero. But, a difference of 0.6 cm in height is not meaningful – not practically important. Why did this happen? Recall that the variability in sample statistics decreases as the sample size increases. For example, unknown to you, suppose a slight majority of a population, say 50.5%, support a new ballot measure. You want to test \\(H_0: \\pi = 0.50\\) versus \\(H_0: \\pi &gt; 0.50\\) for this population. Since the true proportion is not exactly 0.50, you can make your p-value smaller than any given significance level as long as you choose a large enough sample size! Figure 5.40 displays this scenario. The distribution of possible sample proportions who support the new ballot measure in samples of size \\(n = 100,000\\) when 50.5% of the population supports the measure is represented by the black normal curve. The dotted red normal curve is the null distribution of sample proportions for \\(H_0: \\pi = 0.5\\). There is very little overlap between the two distributions due to the very large sample size. The shaded blue area represents the power of the test of \\(H_0: \\pi = 0.5\\) versus \\(H_A: \\pi &gt; 0.5\\) when \\(\\alpha = 0.05\\) – 0.885! That is, we have an 88.5% chance that our p-value will be less than 0.05, even though the true proportion is only 0.05 above 0.5! Figure 5.40: Black curve: sampling distribution of sample proportions from samples of size 100,000 when the true proportion is 0.505. Red curve: null distribution of sample proportions for a null value of 0.50. If p-values can be made arbitrarily small with large sample sizes, what might tend to happen with small sample sizes? Would small sample sizes be more likely to give practically important results that are not statistically significant? or statistically significant results that are not practically important?142 Consider the opposite scenario – small sample sizes with a meaningful difference. Suppose again that you would like to determine if a majority of a population support a new ballot measure. However, you only have the time and money to survey 20 people in the community. Unknown to you, 65% of the population support the measure. Examine Figure 5.41. The distribution of possible sample proportions who support the new ballot measure in samples of size \\(n = 20\\) when 65% of the population supports the measure is represented by the black normal curve. The dotted red normal curve is the null distribution of sample proportions for \\(H_0: \\pi = 0.5\\). Even though 0.65 is quite a bit higher than 0.50, there is still a lot of overlap between the two distributions due to the small sample size. The shaded blue area represents the power of the test of \\(H_0: \\pi = 0.5\\) versus \\(H_A: \\pi &gt; 0.5\\) when \\(\\alpha = 0.05\\) – only 0.29! That is, even though 65% of the population supports the measure (much higher than 50%), we only have a 29% chance of detecting that difference with our small sample size. Figure 5.41: Black curve: approximate sampling distribution of sample proportions from samples of size 20 when the true proportion is 0.65. Red curve: approximate null distribution of sample proportions for a null value of 0.50. Statistical significance versus practical importance. For large sample sizes, results may be statistically significant, but not practically important. Since sample statistics vary very little among samples with large sample sizes, it is easy for a hypothesis test to result in a very small p-value, even if the observed effect is practically meaningless. For small sample sizes, results may be practically important, but not statistically significant. Since studies with small sample sizes tend to have very low power, it is difficult for a hypothesis test to result in a very small p-value, even if the observed effect is quite large. 5.6 Summary of Z-procedures So far in this chapter, we have seen the normal distribution applied as the appropriate mathematical model in two distinct settings. Although the two data structures are different, their similarities and differences are worth pointing out. We provide Table 5.12 partly as a mechanism for understanding \\(z\\)-procedures and partly to highlight the extremely common usage of the normal distribution in practice. You will often hear the following two \\(z\\)-procedures referred to as a one sample \\(z\\)-test (\\(z\\)-interval) and two sample \\(z\\)-test (\\(z\\)-interval). Table 5.12: Similarities of \\(z\\)-methods across one sample and two independent samples analysis of a categorical response variable. one sample two indep. samples response variable binary binary explanatory variable none binary parameter of interest proportion: \\(\\pi\\) diff in props:\\(\\pi_1 - \\pi_2\\) statistic of interest proportion: \\(\\hat{p}\\) diff in props: \\(\\hat{p}_1 - \\hat{p}_2\\) null standard error of the statistic (\\(SE_0( ext{statistic})\\)) \\(\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}\\) \\(\\sqrt{\\hat{p}_{pool}(1-\\hat{p}_{pool})\\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}\\) standard error of the statistic (\\(SE( ext{statistic})\\)) \\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) \\(\\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\\) conditions independence, 2. large samples (at least 10 successes and 10 failures) independence, 2. large samples (at least 10 successes and 10 failures in each sample) Hypothesis tests. When applying the normal distribution for a hypothesis test, we proceed as follows: Write appropriate hypotheses. Verify conditions for using the normal distribution. One-sample: the observations must be independent, and you must have at least 10 successes and 10 failures. For a difference of proportions: each sample must separately satisfy the one-sample conditions for the normal distribution, and the data in the groups must also be independent. Compute the statistic of interest, the null standard error, and the degrees of freedom. For \\(df\\), use \\(n-1\\) for one sample, and for two samples use either statistical software or the smaller of \\(n_1 - 1\\) and \\(n_2 - 1\\). Compute the Z-score using the general formula: \\[ Z = \\frac{\\mbox{statistic} - \\mbox{null value}}{\\mbox{null standard error}} = \\frac{\\mbox{statistic} - \\mbox{null value}}{SE_0} \\] Use the statistical software to find the p-value using the standard normal distribution: Sign in \\(H_A\\) is \\(&lt;\\): p-value = area below Z-score Sign in \\(H_A\\) is \\(&gt;\\): p-value = area above Z-score Sign in \\(H_A\\) is \\(\\neq\\): p-value = 2 \\(\\times\\) area below \\(-|\\mbox{Z-score}|\\) Make a conclusion based on the p-value, and write a conclusion in context, in plain language, and in terms of the alternative hypothesis. Confidence intervals. Similarly, the following is how we generally compute a confidence interval using a normal distribution: Verify conditions for using the normal distribution. (See above.) Compute the statistic of interest, the standard error, and \\(z^{\\star}\\). Calculate the confidence interval using the general formula: \\[ \\mbox{statistic} \\pm\\ z^{\\star} SE. \\] Put the conclusions in context and in plain language so even non-data scientists can understand the results. 5.7 R: Inference for categorical data 5.7.1 Inference using R and catstats Making tables from raw data If you are collecting your own data, then your analysis starts with the raw data frame, with one observational unit per row and one variable per column. In this case, you first need to find counts (frequencies) of observations in each category prior to inference. We can use the table() function in R on the raw data to create a contingency table of the counts for each categorical variable. In the one-proportion case, suppose we have a data frame called loans with a variable regulate that contains the Yes/No response of the 826 payday loan borrowers from Section 5.3.3 regarding their support for a regulation to require lenders to pull their credit report and evaluate their debt payments. We can obtain counts of borrowers for each response using the table() function in R: table(loans$regulate) #&gt; #&gt; No Yes #&gt; 404 422 In the R code above, loans is the name of the data set in R, and regulate is the name of the variable. The $ tells R to select the regulate variable from the loans data set. We could have also used the pipe command to generate the table: loans %&gt;% select(regulate) %&gt;% table() #&gt; . #&gt; No Yes #&gt; 404 422 We can also use table() to compute the proportions in each group: #If we know the number of observations: table(loans$regulate)/826 #&gt; #&gt; No Yes #&gt; 0.4891041 0.5108959 #If we don&#39;t know the number of observations: table(loans$regulate)/length(loans$regulate) #&gt; #&gt; No Yes #&gt; 0.4891041 0.5108959 For comparisons of two proportions, we get a two-way table. Consider the case study of the effect of blood thinners on survival after receiving CPR from Section 5.4.2. Data from this study are stored in a data frame called cpr, with variables survival – giving each patient’s outcome decision – and group – indicating whether they were in the treatment (blood thinner) or control (no blood thinner) group. The glimpse() and summary() functions can help us see what variables are in our dataset and the values they take on: glimpse(cpr) #&gt; Rows: 90 #&gt; Columns: 2 #&gt; $ survival &lt;fct&gt; Survived, Survived, Survived, Survived, Survived, Survived, … #&gt; $ group &lt;fct&gt; control, control, control, control, control, control, contro… summary(cpr) #&gt; survival group #&gt; Died :65 control :50 #&gt; Survived:25 treatment:40 To obtain the two-way table of the choices by group, we again use the table() function in R. The key thing to remember here is to put the response variable (outcome) as the first argument and the explanatory variable (grouping) as the second. In our example, survival is the response variable, and group is the explanatory variable. table(cpr$survival, cpr$group) #&gt; #&gt; control treatment #&gt; Died 39 26 #&gt; Survived 11 14 This will set up the table to be useful for making segmented bar plots and using column percentages to compute test statistics. In order to do either of these things, we need to store the table in an R object so we can manipulate it further: data_tbl &lt;- table(cpr$survival, cpr$group) To get column percentages, we use the prop.table() function: prop.table(data_tbl, #Feed in your two-way table margin = 2) #Tell it to compute percentages for columns #&gt; #&gt; control treatment #&gt; Died 0.78 0.65 #&gt; Survived 0.22 0.35 Simulation-based inference for one proportion For simulation-based inference, we will use functions included in the catstats package, created for MSU Statistics courses. See the Statistical computing section at the beginning of this textbook for instructions on how to install catstats if you haven’t already. If the package is installed, you can load it into an R session to make the functions available using the library() function: library(catstats) Once you have loaded the package, you will be able to use the functions for simulation-based inference. For one-proportion inference, this is the one_proportion_test() function and one_proportion_bootstrap_CI() function. Returning to the payday loan regulation example, we can obtain a simulation distribution and p-value using the following function call: one_proportion_test( probability_success = 0.5, #null hypothesis probability of success sample_size = 826, #number of observations number_repetitions = 1000, #number of simulations to create as_extreme_as = 0.51, #observed statistic direction = &quot;greater&quot;, #alternative hypothesis direction report_value = &quot;proportion&quot; #Report number or proportion of successes? ) Note that the observed statistic (as_extreme_as) and the report_value input need to match; since we put in the observed statistic as a proportion, we need to tell the function to report the proportion of successes. If they don’t match, you will almost certainly get a p-value of 0 or 1 – this can happen when there is very strong evidence against the null, but it is always good to check your function inputs when you get an extreme outcome to make sure that is what you should be seeing. In the resulting graph, we see the null distribution of simulated proportions, with those greater than the observed value of 0.51 highlighted in blue. In the figure caption, we see that 308 of our 1000 simulations resulted in a proportion of successes at least as large as the observed value, yielding an approximate p-value of 0.308. To find a confidence interval for the true proportion of payday loan borrowers who support the regulation, we use the one_proportion_bootstrap_CI() function: one_proportion_bootstrap_CI( sample_size = 826, #Number of observations number_successes = 422, #Number of observed successes number_repetitions = 1000, #Number of bootstrap draws confidence_level = 0.95 #Confidence level, as a proportion ) This produces a plot of the bootstrapped proportions with the upper and lower bounds of the confidence interval marked, and gives the interval itself in the figure caption: in this case, we are 95% confident that the true proportion of payday loan borrowers who support the proposed regulation is between 0.479 and 0.546. Simulation-based inference for two proportions For inference about the difference between two proportions, we use the two_proportion_test() and two_proportion_bootstrap_CI() functions. These functions assume you have a data frame with the group and outcome included as variables. Using the CPR and blood thinner study, a call to two_proportion_test() would look like this: two_proportion_test( formula = survival ~ group, #Always do response ~ explanatory data = cpr, #Name of the data set first_in_subtraction = &quot;treatment&quot;, #Value of the explanatory variable that is first in order of subtraction response_value_numerator = &quot;Survived&quot;, #Value of response that is a &quot;success&quot; number_repetitions = 1000, as_extreme_as = 0.13, #Observed statistic direction = &quot;two-sided&quot; #Direction of the alternative hypothesis ) The results give a segmented bar plot of the data — you can check that your formula is correct by making sure the explanatory variable is on the \\(x\\)-axis and the response variable is on the \\(y\\)-axis. Look to the top right of the bar plot to check that you have the correct order of subtraction. Next to the bar plot, we have the null distribution of simulated differences in proportions, with the observed statistic marked with a vertical line and all values as or more extreme than the observed statistic colored red. The figure caption gives the approximate p-value: in this case 181/1000 = 0.181. There are a couple of things to note when using the two_proportion_test function: You need to identify which variable is your outcome and which your group using the formula argument. Specify order of subtraction using first_in_subtraction by putting in EXACTLY the category of the explanatory variable that you want to be first, in quotes — must match capitalization, spaces, etc. for text values! Specify what is a success using response_value_numerator but putting in EXACTLY the category of the response that you consider a success, in quotes. Again, capitalization, spaces, etc. matter here! To produce a confidence interval for the true difference in the proportion of patients that survive after receiving CPR, we use two_proportion_bootstrap_CI(), which uses most of the same arguments as the two_proportion_test() function: two_proportion_bootstrap_CI( formula = survival ~ group, #Always do response ~ explanatory data = cpr, #Name of the data set first_in_subtraction = &quot;treatment&quot;, #Value of the explanatory variable that is first in order of subtraction response_value_numerator = &quot;Survived&quot;, #Value of response that is a &quot;success&quot; number_repetitions = 1000, #Number of bootstrap samples confidence_level = 0.99 #Confidence level, as a proportion ) This produces the distribution of bootstrapped statistics with the bounds of the confidence interval marked, and the value included in the caption. Here, we are 99% confident that the true proportion of patients who survive after receiving CPR is between 0.1 lower and 0.35 higher when patients are given blood thinners compared to when they are not. Theory-based inference for one proportion For theory-based inference, we can use the built-in R function prop.test().143 For a one-proportion test, we need to tell it the number of successes, the number of trials (sample size), and the null value. Using the payday loan regulation example, a call would look like this: prop.test(x = 422, #Number of successes n = 826, #Number of trials p = .5, #Null hypothesis value alternative = &quot;greater&quot;, #Direction of alternative, conf.level = 0.95) #Confidence level as a proportion #&gt; #&gt; 1-sample proportions test with continuity correction #&gt; #&gt; data: 422 out of 826 #&gt; X-squared = 0.34988, df = 1, p-value = 0.2771 #&gt; alternative hypothesis: true p is greater than 0.5 #&gt; 95 percent confidence interval: #&gt; 0.4816939 1.0000000 #&gt; sample estimates: #&gt; p #&gt; 0.5108959 In this output, we can get our observed statistic \\(\\hat{p} = 0.51\\) in the last line of the output (under sample estimates: p), and the p-value of 0.2656 from the second line of the output. You should always check that the null value and the alternative hypothesis in the output matches the problem. You might have noticed that the test statistic reported in the output is X-squared rather than our Z-statistic. This test statistic is equal to our Z-statistic squared, and the p-value is found from what is called a \\(\\chi^2\\) distribution (pronounced “kai squared”). To get the Z-statistic from the X-squared statistic, take the positive square root if \\(\\hat{p} &gt; \\pi_0\\), and the negative square root if \\(\\hat{p} &lt; \\pi_0\\). Use the R output to find the value of the Z-statistic.144 The prop.test() function will also produce the theory-based confidence interval, but to get the correct confidence interval, we need to run the function with a two-sided alternative:145 prop.test(x = 422, #Number of successes n = 826, #Number of trials p = .5, #Null hypothesis value alternative = &quot;two.sided&quot;, #Direction of alternative, conf.level = 0.95, #Confidence level as a proportion correct = FALSE) #We will not use a continuity correction #&gt; #&gt; 1-sample proportions test without continuity correction #&gt; #&gt; data: 422 out of 826 #&gt; X-squared = 0.39225, df = 1, p-value = 0.5311 #&gt; alternative hypothesis: true p is not equal to 0.5 #&gt; 95 percent confidence interval: #&gt; 0.4768346 0.5448563 #&gt; sample estimates: #&gt; p #&gt; 0.5108959 From this output, we obtain a 95% confidence interval for the true proportion of payday loan borrowers who support the new regulation of (0.477, 0.545). Theory-based inference for a difference in two proportions When comparing two proportions, we use the same function, prop.test, but the inputs are now “vectors” rather than “scalars”. As an example, we will again use the CPR and blood thinner study from Section 5.4.2. First, use the table() function to determine the number of successes and the sample size in each category of the explanatory variable: table(cpr$survival, cpr$group) #&gt; #&gt; control treatment #&gt; Died 39 26 #&gt; Survived 11 14 From these results (and the example in Section 5.4.2), we see that the Treatment group (call this group 1) had 14 survive (successes) out of 40 patients (sample size); the Control group (call this group 2) had 11 survive out of 50 patients. These counts are input into the prop.test function as follows: x = vector of number of successes = c(num successes in group 1, num successes in group 2) n = vector of sample sizes = c(sample size for group 1, sample size for group 2) R creates a vector using the c() (or “combine”) function. R will take the order of subtraction to be group 1 \\(-\\) group 2. prop.test(x = c(14, 11), #Number successes in group 1 and group 2 n = c(40, 50), #Sample size of group 1 and group 2 alternative = &quot;two.sided&quot;, #Match sign of alternative #for order of subtraction #group 1 - group 2 conf.level = 0.99, #Confidence level as a proportion correct = FALSE) #No continuity correction #&gt; #&gt; 2-sample test for equality of proportions without continuity #&gt; correction #&gt; #&gt; data: c out of c14 out of 4011 out of 50 #&gt; X-squared = 1.872, df = 1, p-value = 0.1712 #&gt; alternative hypothesis: two.sided #&gt; 99 percent confidence interval: #&gt; -0.1159816 0.3759816 #&gt; sample estimates: #&gt; prop 1 prop 2 #&gt; 0.35 0.22 The observed proportions in each group are given under sample estimates: prop 1 prop 2. R will always take (prop 1 - prop 2) as the order of subtraction. If the observed proportions don’t match your calculations of the proportion of successes or are in the wrong order of subtraction, go back and check your inputs to the x and n arguments. Here, we obtain a p-value of 0.1712 — little to no evidence against the null hypothesis of no difference between the two groups. Since we used a two-sided alternative in prop.testc, this call also produces the correct confidence interval. Our 99% confidence interval for the true difference in the proportion of patients who survive is \\((-0.116, 0.376)\\). We are 99% confident that the true proportion of patients who survive under the treatment is between 0.116 lower and 0.376 higher than the true proportion of patients who survive under the control condition. The fact that this confidence interval contains zero also shows us that we have little to no evidence of a difference in probability of survival between the two groups. The function prop.test will also take a table created by the table function. First, we need to create our table of counts from the raw data, which then becomes our primary input to prop.test(). There is one important step to take before creating our table: R will always put the categories of a categorical variable in alphabetical order when building tables, unless told otherwise. data_tbl &lt;- table(cpr$survival, cpr$group) data_tbl #&gt; #&gt; control treatment #&gt; Died 39 26 #&gt; Survived 11 14 However, prop.test() will assume that the top row is a “success” and the order of subtraction is (column 1 - column 2). Without re-arranging our table, we would get the wrong order of subtraction and/or the wrong proportion of successes in each group. To fix this, we need to relevel() our inputs to tell R to put them in the order we want: #Switch order of subtraction: cpr$group &lt;- relevel(cpr$group, ref = &quot;treatment&quot;) table(cpr$survival, cpr$group) #&gt; #&gt; treatment control #&gt; Died 26 39 #&gt; Survived 14 11 #Switch &quot;success&quot;: cpr$survival &lt;- relevel(cpr$survival, ref = &quot;Survived&quot;) table(cpr$survival, cpr$group) #&gt; #&gt; treatment control #&gt; Survived 14 11 #&gt; Died 26 39 After re-arranging our table, we can use this data_tbl as the first argument in the prop.test() function: data_tbl &lt;- table(cpr$survival, cpr$group) stats::prop.test(x = data_tbl, alternative = &quot;two.sided&quot;, conf.level = 0.99, #Confidence level as a proportion correct = FALSE) #No continuity correction #&gt; #&gt; 2-sample test for equality of proportions without continuity #&gt; correction #&gt; #&gt; data: data_tbl #&gt; X-squared = 1.872, df = 1, p-value = 0.1712 #&gt; alternative hypothesis: two.sided #&gt; 99 percent confidence interval: #&gt; -0.1398193 0.4598193 #&gt; sample estimates: #&gt; prop 1 prop 2 #&gt; 0.56 0.40 5.7.2 catstats function summary In the previous section, you were introduced to four new R functions in the catstats library. Here we provide a summary of these functions. You can also access the help files for these functions using the ? command. For example, type ?one_proportion_test into your R console to bring up the help file for the one_proportion_test function. one_proportion_test: Simulation-based hypothesis test for a single proportion. probability_success = null value sample_size = sample size (\\(n\\)) number_repetitions = number of simulated samples to generate (should be at least 1000!) as_extreme_as = value of observed statistic direction = one of \"greater\", \"less\", or \"two-sided\" (quotations are important here!) to match the sign in \\(H_A\\) report_value = one of \"number\" or \"proportion\" (quotations are important here!) to simulate either sample counts or sample proportions (needs to match the form of the observed statistic) one_proportion_bootstrap_CI: Bootstrap confidence interval for one proportion. sample_size = sample size (\\(n\\)) number_successes = number of successes (note that \\(\\hat{p}\\) = number_successes/sample_size) number_repetitions = number of simulated samples to generate (should be at least 1000!) confidence_level = confidence level as a decimal (e.g., 0.90, 0.95, etc) two_proportion_test: Simulation-based hypothesis test for a single proportion. formula = y ~ x where y is the name of the response variable in the data set and x is the name of the explanatory variable data = name of data set first_in_subtraction = category of the explanatory variable which should be first in subtraction, written in quotations response_value_numerator = category of the response variable which we are counting as a “success”, written in quotations number_repetitions = number of simulated samples to generate (should be at least 1000!) as_extreme_as = value of observed difference in proportions direction = one of \"greater\", \"less\", or \"two-sided\" (quotations are important here!) to match the sign in \\(H_A\\) two_proportion_bootstrap_CI: Bootstrap confidence interval for one proportion. formula = y ~ x where y is the name of the response variable in the data set and x is the name of the explanatory variable data = name of data set first_in_subtraction = category of the explanatory variable which should be first in subtraction, written in quotations response_value_numerator = category of the response variable which we are counting as a “success”, written in quotations number_repetitions = number of simulated samples to generate (should be at least 1000!) confidence_level = confidence level as a decimal (e.g., 0.90, 0.95, etc) 5.7.3 Interactive R tutorials Navigate the concepts you’ve learned in this chapter in R using the following self-paced tutorials. All you need is your browser to get started! Tutorial 5: Introduction to statistical inference Tutorial 5 - Lesson 1: Sampling variability Tutorial 5 - Lesson 2: Randomization test Tutorial 5 - Lesson 3: Errors in hypothesis testing Tutorial 5 - Lesson 4: Parameters and confidence intervals Tutorial 6: Inference for categorical responses Tutorial 6 - Lesson 1: Inference for a single proportion Tutorial 6 - Lesson 2: Hypothesis tests to compare proportions You can also access the full list of tutorials supporting this book here. 5.7.4 R labs Further apply the concepts you’ve learned in this chapter in R with computational labs that walk you through a data analysis case study. Sampling distributions - Does science benefit you? Confidence intervals - Climate change Inference for categorical responses - Texting while driving Full list of labs supporting OpenIntro::Introduction to Modern Statistics 5.8 Chapter 5 review 5.8.1 Terms We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as bolded text. alternative hypothesis null distribution pooled proportion statistical inference bootstrapping null hypothesis practical importance statistical significance Central Limit Theorem null value randomization statistically significant confidence interval one sample \\(z\\)-test sampling distribution success confidence level one-sided hypothesis test SE interval success-failure condition confirmation bias p-value simulation test statistic hypothesis test parameter standard error two sample \\(z\\)-test margin of error percentile standard error for difference in proportions two-sided hypothesis test normal curve percentile interval standard error of single proportion Type 1 Error normal distribution permutation test standard normal distribution Type 2 Error normal model point estimate statistic Z-score We would be assuming that these two variables are independent.↩︎ If you are a STAT 216 student, you will recognize this from our first week’s in-class activity.↩︎ Bumba is the Martian letter on the left!↩︎ A fair coin has a 50% chance of landing on heads, which is the chance a student would guess Bumba correctly if they were just guessing. Thus, toss a coin 38 times with heads representing “guess correctly”; then calculate the proportion of tosses that landed on heads. Another option would be to 10 black cards and 10 red cards, letting red represent “guess correctly”. Shuffle the cards and draw one card, record if it is red or black, then replace the card and shuffle again. Do this 38 times and calculate the proportion of red cards observed.↩︎ To explore this further, watch this TED Talk by neurologist Vilayanur Ramachandran (The synesthesia part begins at roughly 17:40 minutes).↩︎ If you carry out the calculations, you’ll note that the upper bound is actually \\(0.89 + 0.16 = 1.05\\), but since a sample proportion cannot be greater than 1, we truncated the interval to 1.↩︎ The notation \\(SD(\\hat{p})\\) is function notation — we are applying the standard deviation function (\\(SD\\)) to the statistic \\(\\hat{p}\\). This notation does not mean to multiply \\(SD\\) by \\(\\hat{p}\\).↩︎ The first possibility (We can’t read Martian, and these results just occurred by chance.) was the null hypothesis; the second possibility (We can read Martian, and these results reflect this ability.) was the alternative hypothesis.↩︎ Technically, the observed sample statistic or one more extreme in the direction of our alternative. But it is helpful to just remember this as “the data”.↩︎ Since a smaller p-value gives you stronger evidence against the null hypothesis, we reject \\(H_0\\) when the p-value is very small, and fail to reject \\(H_0\\) when the p-value is not small.↩︎ You will get more practice calculating p-values such as these in this Chapter.↩︎ Since statistical methods are grounded in probability, technically we can only find strong evidence against a hypothesis, not disprove it.↩︎ If we want to be more certain we will capture the fish, we might use a wider net. Likewise, we use a wider confidence interval if we want to be more certain that we capture the parameter.↩︎ It is also introduced as the Gaussian distribution after Frederic Gauss, the first person to formalize its mathematical expression.↩︎ (a) \\(N(\\mu=5,\\sigma=3)\\). (b) \\(N(\\mu=-100, \\sigma=10)\\). (c) \\(N(\\mu=2, \\sigma=9)\\).↩︎ We use the standard deviation as a guide. Ann is 1 standard deviation above average on the SAT: \\(1500 + 300=1800\\). Tom is 0.6 standard deviations above the mean on the ACT: \\(21+0.6\\times 5=24\\). In Figure 5.7, we can see that Ann tends to do better with respect to everyone else than Tom did, so her score was better.↩︎ \\(Z_{Tom} = \\frac{x_{Tom} - \\mu_{ACT}}{\\sigma_{ACT}} = \\frac{24 - 21}{5} = 0.6\\)↩︎ (a) Its Z-score is given by \\(Z = \\frac{x-\\mu}{\\sigma} = \\frac{5.19 - 3}{2} = 2.19/2 = 1.095\\). (b) The observation \\(x\\) is 1.095 standard deviations above the mean. We know it must be above the mean since \\(Z\\) is positive.↩︎ For \\(x_1=95.4\\) mm: \\(Z_1 = \\frac{x_1 - \\mu}{\\sigma} = \\frac{95.4 - 92.6}{3.6} = 0.78\\). For \\(x_2=85.8\\) mm: \\(Z_2 = \\frac{85.8 - 92.6}{3.6} = -1.89\\).↩︎ Because the absolute value of Z-score for the second observation is larger than that of the first, the second observation has a more unusual head length.↩︎ To remember the function qnorm() as providing a cutoff, notice that both qnorm() and “cutoff” start with the sound “kuh”. To remember the pnorm() function as providing a probability from a given cutoff, notice that both pnorm() and probability start with the sound “puh”.↩︎ If 84% had lower scores than Ann, the number of people who had better scores must be 16%. (Generally ties are ignored when the normal model, or any other continuous distribution, is used.)↩︎ We found the probability to be 0.6664. A picture for this exercise is represented by the shaded area below “0.6664”.↩︎ If Edward did better than 37% of SAT takers, then about 63% must have done better than him. ↩︎ Numerical answers: (a) 0.9772. (b) 0.0228.↩︎ This sample was taken from the USDA Food Commodity Intake Database.↩︎ First put the heights into inches: 67 and 76 inches. Figures are shown below. (a) \\(Z_{Mike} = \\frac{67 - 70}{3.3} = -0.91\\ \\to\\ 0.1814\\). (b) \\(Z_{Jim} = \\frac{76 - 70}{3.3} = 1.82\\ \\to\\ 0.9656\\). \\↩︎ Remember: draw a picture first, then find the Z-score. (We leave the pictures to you.) The Z-score can be found by using the percentiles and the normal probability table. (a) We look for 0.95 in the probability portion (middle part) of the normal probability table, which leads us to row 1.6 and (about) column 0.05, i.e., \\(Z_{95}=1.65\\). Knowing \\(Z_{95}=1.65\\), \\(\\mu = 1500\\), and \\(\\sigma = 300\\), we setup the Z-score formula: \\(1.65 = \\frac{x_{95} - 1500}{300}\\). We solve for \\(x_{95}\\): \\(x_{95} = 1995\\). (b) Similarly, we find \\(Z_{97.5} = 1.96\\), again setup the Z-score formula for the heights, and calculate \\(x_{97.5} = 76.5\\).↩︎ Numerical answers: (a) 0.1131. (b) 0.3821.↩︎ This is an abbreviated solution. (Be sure to draw a figure!) First find the percent who get below 1500 and the percent that get above 2000: \\(Z_{1500} = 0.00 \\to 0.5000\\) (area below), \\(Z_{2000} = 1.67 \\to 0.0475\\) (area above). Final answer: \\(1.0000-0.5000 - 0.0475 = 0.4525\\).↩︎ 5’5’’ is 65 inches. 5’7’’ is 67 inches. Numerical solution: \\(1.000 - 0.0649 - 0.8183 = 0.1168\\), i.e., 11.68%.↩︎ First draw the pictures. To find the area between \\(Z=-1\\) and \\(Z=1\\), use pnorm() to determine the areas below \\(Z=-1\\) and above \\(Z=1\\). Next verify the area between \\(Z=-1\\) and \\(Z=1\\) is about 0.68. Repeat this for \\(Z=-2\\) to \\(Z=2\\) and also for \\(Z=-3\\) to \\(Z=3\\).↩︎ (a) 900 and 2100 represent two standard deviations above and below the mean, which means about 95% of test takers will score between 900 and 2100. (b) Since the normal model is symmetric, then half of the test takers from part (a) (\\(\\frac{95\\%}{2} = 47.5\\%\\) of all test takers) will score 900 to 1500 while 47.5% score between 1500 and 2100.↩︎ When you see \\(\\pi\\) in this textbook, it will always symbolize a (typically unknown) population proportion, not the value 3.14….↩︎ The terms “success” and “failure” may not actually represent outcomes we view as successful or not, but it is the typical generic way to referring to the possible outcomes of a binary variable. The “success” is whatever we count when calculating our sample proportion.↩︎ Parameters were first introduced in Section 2.3.2↩︎ Now would be a good time to review the definition of a p-value in Section 5.1.3!↩︎ One option would be to use a spinner with 10% shaded red, and the rest shaded green. Each spin of the spinner would represent one client. Spin the spinner 62 times and count the number of times the spinner lands on red. The proportion of times the spinner lands on red represents a simulated \\(\\hat{p}\\) under the assumption that \\(\\pi = 0.10\\). Other objects include: a bag of marbles with 10% red marbles and 90% white marbles, or 10 cards where 1 is red and 9 are white. Sampling 62 times with replacement from these collections would simulate one sample of clients.↩︎ There isn’t sufficiently strong evidence to support the claim that fewer than 10% of the consultant’s clients experience complications. That is, there isn’t sufficiently strong evidence to support an association between the consultant’s work and fewer surgery complications.↩︎ No. It might be that the consultant’s work is associated with a reduction but that there isn’t enough data to convincingly show this connection.↩︎ If you’re curious where the term “bootstrapping” comes from, it comes from the phrase “lift yourself up by your own bootstraps.” Lifting yourself up by your own bootstraps is analogous to creating more samples from the single original sample.↩︎ Since in the original sample, 3 out of 62, or about 5% had complications, we could expect about 5% of the patients (6.2 on average) in the simulation will have a complication, though we will see a little variation from one simulation to the next.↩︎ The the middle 95% of a distribution would range from the 5th percentile (the value with 5% of the distribution below) to the 95th percentile (the value with 5% of the distribution above).↩︎ While this book is scoped to well-constrained statistical problems, do remember that this is just the first book in what is a large library of statistical methods that are suitable for a very wide range of data and contexts.↩︎ Independence holds since the poll is based on a random sample. The success-failure condition also holds, which is checked using the null value (\\(p_0 = 0.5\\)) from \\(H_0\\): \\(np_0 = 826 \\times 0.5 = 413\\), \\(n(1 - p_0) = 826 \\times 0.5 = 413\\). Recall that here, the best guess for \\(\\pi\\) is \\(p_0\\) which comes from the null hypothesis (because we assume the null hypothesis is true when performing the testing procedure steps). \\(H_0\\): there is not support for the regulation; \\(H_0\\): \\(\\pi \\leq 0.50\\). \\(H_A\\): the majority of borrowers support the regulation; \\(H_A\\): \\(\\pi &gt; 0.50\\).↩︎ Because the \\(\\pi\\) is unknown but expected to be around 2/3, we will use 2/3 in place of \\(\\pi\\) in the formula for the standard deviation of \\(\\hat{p}\\) and calculate \\(SE(\\hat{p}) = \\sqrt{\\frac{2/3 (1 - 2/3)} {300}} = 0.027\\).↩︎ This is equivalent to asking how often the \\(Z\\) score will be larger than -2.58 but less than 2.58. (For a picture, see Figure 5.17.) To determine this probability, look up -2.58 and 2.58 in the normal probability table (0.0049 and 0.9951). Thus, there is a \\(0.9951-0.0049 \\approx 0.99\\) probability that the unobserved random variable \\(X\\) will be within 2.58 standard deviations of the mean.↩︎ Since the necessary conditions for applying the normal model have already been checked for us, we can go straight to the construction of the confidence interval: \\(\\text{point estimate}\\ \\pm\\ 2.58 \\times SE \\rightarrow (0.018, 0.162)\\). We are 99% confident that implanting a stent in the brain of a patient who is at risk of stroke increases the risk of stroke within 30 days by a rate of 0.018 to 0.162 (assuming the patients are representative of the population).↩︎ We must find \\(z^{\\star}\\) such that 90% of the distribution falls between -\\(z^{\\star}\\) and \\(z^{\\star}\\) in the standard normal model, \\(N(\\mu=0, \\sigma=1)\\). We can find -\\(z^{\\star}\\) from a standard normal distribution by looking for a lower tail of 5% (the other 5% is in the upper tail), thus \\(z^{\\star}=1.645\\). The 90% confidence interval can then be computed as \\(\\text{point estimate}\\ \\pm\\ 1.65\\times SE \\to (4.4\\%, 13.6\\%)\\). (Note: The conditions for normality had earlier been confirmed for us.) That is, we are 90% confident that implanting a stent in a stroke patient’s brain increased the risk of stroke within 30 days by 4.4% to 13.6%.↩︎ Rosen B and Jerdee T. 1974. “Influence of sex role stereotypes on personnel decisions.” Journal of Applied Psychology 59(1):9-14.↩︎ The study is an experiment, as subjects were randomly assigned a “male” file or a “female” file (remember, all the files were actually identical in content). Since this is an experiment, the results can be used to evaluate a causal relationship between gender of a candidate and the promotion decision.↩︎ The test procedure we employ in this section is sometimes referred to as a permutation test.↩︎ \\(18/24 - 17/24=0.042\\) or about 4.2% in favor of the men. This difference due to chance is much smaller than the difference observed in the actual groups.↩︎ This reasoning does not generally extend to anecdotal observations. Each of us observes incredibly rare events every day, events we could not possibly hope to predict. However, in the non-rigorous setting of anecdotal evidence, almost anything may appear to be a rare event, so the idea of looking for rare events in day-to-day activities is treacherous. For example, we might look at the lottery: there was only a 1 in 176 million chance that the Mega Millions numbers for the largest jackpot in history (March 30, 2012) would be (2, 4, 23, 38, 46) with a Mega ball of (23), but nonetheless those numbers came up! However, no matter what numbers had turned up, they would have had the same incredibly rare odds. That is, any set of numbers we could have observed would ultimately be incredibly rare. This type of situation is typical of our daily lives: each possible event in itself seems incredibly rare, but if we consider every alternative, those outcomes are also incredibly rare. We should be cautious not to misinterpret such anecdotal evidence.↩︎ This probability of the results we see in our study, under the assumption of no discrimination, is the p-value.↩︎ With a sample of students, randomly assign half of them to the control condition, and the other half to the treatment condition. For those in the control condition, present them with a situation where an item is on sale and ask if they would like to buy the item. For those in the treatment condition, present them with the same situation, but also remind them that they can save money for later purchases, then ask if they would like to buy the item. Compute and compare the proportions who refrained from purchasing the item in each group.↩︎ Frederick S, Novemsky N, Wang J, Dhar R, Nowlis S. 2009. Opportunity Cost Neglect. Journal of Consumer Research 36: 553-561.↩︎ Success is often defined in a study as the outcome of interest, and a “success” may or may not actually be a positive outcome. For example, researchers working on a study on HIV prevalence might define a “success” in the statistical sense as a patient who is HIV+. A more complete discussion of the term success will be given in Chapter 5.↩︎ The study is an experiment, as patients were randomly assigned an experiment group. Since this is an experiment, the results can be used to evaluate a causal relationship between the malaria vaccine and whether patients showed signs of an infection.↩︎ \\((0.357 - 1)\\times 100\\)% = -64.3%↩︎ With small sample sizes, researchers often add 0.5 to each of the four cells prior to calculating the sample relative risk in order to avoid dividing by zero. With this adjustment, the sample relative risk is: \\(\\frac{5.5/15}{6.5/7} = 0.395\\). We will use this adjustment when simulating relative risks as well.↩︎ 1. Take 20 notecards to represent the 20 patients, where we write down “infection” on 11 cards and “no infection” on 9 cards. 2. Thoroughly shuffle the notecards and deal 14 into a “vaccine” pile and 6 into a “placebo” pile. 3. Compute the proportion of “infection” cards in the “vaccine” pile and divide it by the proportion of “infection” cards in the “placebo” pile to get the simulated sample relative risk.↩︎ This reasoning does not generally extend to anecdotal observations. Each of us observes incredibly rare events every day, events we could not possibly hope to predict. However, in the non-rigorous setting of anecdotal evidence, almost anything may appear to be a rare event, so the idea of looking for rare events in day-to-day activities is treacherous. For example, we might look at the lottery: there was only a 1 in 292 million chance that the Powerball numbers for the largest jackpot in history (January 13th, 2016) would be (04, 08, 19, 27, 34) with a Powerball of (10), but nonetheless those numbers came up! However, no matter what numbers had turned up, they would have had the same incredibly rare odds. That is, any set of numbers we could have observed would ultimately be incredibly rare. This type of situation is typical of our daily lives: each possible event in itself seems incredibly rare, but if we consider every alternative, those outcomes are also incredibly rare. We should be cautious not to misinterpret such anecdotal evidence.↩︎ B\\(\\ddot{\\text{o}}\\)ttiger et al. “Efficacy and safety of thrombolytic therapy after initially unsuccessful cardiopulmonary resuscitation: a prospective clinical trial.” The Lancet, 2001.↩︎ Observed control survival rate: \\(\\hat{p}_c = \\frac{11}{50} = 0.22\\). Treatment survival rate: \\(\\hat{p}_t = \\frac{14}{40} = 0.35\\). Observed difference: \\(\\hat{p}_t - \\hat{p}_c = 0.35 - 0.22 = 0.13\\). Relative risk: \\(\\hat{p}_t/\\hat{p}_c = 0.35/0.22 = 1.59\\)↩︎ Note: it is only a coincidence that we also have \\(\\hat{p}_t - \\hat{p}_c=0.13\\)!↩︎ Note that the relative risk in the opposite direction is not a decrease of 59%! When comparing control to treatment, the relative risk would be \\(0.22/0.35 = 0.63\\), or a decrease of 37%. These values differ because the quantity we’re comparing to (the “100%” quantity) differs.↩︎ If the null distribution is not symmetric, then the computer will have to count the proportions in each tail separately, since the two tail proportions may differ.↩︎ Because the patients were randomized, the subjects are independent, both within and between the two groups. The success-failure condition is also met for both groups as all counts are at least 10. This satisfies the conditions necessary to model the difference in proportions using a normal distribution. Compute the sample proportions (\\(\\hat{p}_{\\text{fish oil}} = 0.0112\\), \\(\\hat{p}_{\\text{placebo}} = 0.0155\\)), point estimate of the difference (\\(0.0112 - 0.0155 = -0.0043\\)), and standard error \\(SE = \\sqrt{\\frac{0.0112 \\times 0.9888}{12933} + \\frac{0.0155 \\times 0.9845}{12938}} = 0.00145\\). Next, plug the values into the general formula for a confidence interval, where \\(z^{\\star} = 1.96\\) for a 95% confidence level: \\(-0.0043 \\pm 1.96 \\times 0.00145 \\rightarrow (-0.0071, -0.0015)\\). We are 95% confident that fish oils decreases heart attacks by 0.15 to 0.71 percentage points (off of a baseline of about 1.55%) over a 5-year period for subjects who are similar to those in the study. Because the interval is entirely below 0, and the treatment was randomly assigned, the data provide strong evidence that fish oil supplements reduce heart attacks in patients like those in the study.↩︎ This is an experiment. Patients were randomized to receive mammograms or a standard breast cancer exam. We will be able to make causal conclusions based on this study.↩︎ \\(H_0\\): the breast cancer death rate for patients screened using mammograms is the same as the breast cancer death rate for patients in the control, \\(\\pi_{mgm} - \\pi_{ctrl} = 0\\). \\(H_A\\): the breast cancer death rate for patients screened using mammograms is different than the breast cancer death rate for patients in the control, \\(\\pi_{mgm} - \\pi_{ctrl} \\neq 0\\).↩︎ For an example of a two proportion hypothesis test that does not require the success-failure condition to be met, see Section 5.4.1.↩︎ Making a Type 1 Error in this context would mean that reminding students that money not spent now can be spent later does not affect their buying habits, despite the strong evidence (the data suggesting otherwise) found in the experiment. Notice that this does not necessarily mean something was wrong with the data or that we made a computational mistake. Sometimes data simply point us to the wrong conclusion, which is why scientific studies are often repeated to check initial findings.↩︎ To lower the Type 2 Error rate, we want to convict more guilty people. We could lower the standards for conviction from “beyond a reasonable doubt” to “beyond a little doubt”. Lowering the bar for guilt will also result in more wrongful convictions, raising the Type 1 Error rate.↩︎ Since hypothesis tests with small sample sizes typically have low power, small sample sizes can result in practically important results that are not statistically significant.↩︎ By “built-in”, we mean that it is in the R “base” library, so no packages need to be loaded.↩︎ Since our sample proportion \\(\\hat{p} = 0.511\\) is larger than the null value \\(\\pi_0 = 0.5\\), we know our Z-statistic will be positive. Thus, \\(Z = +\\sqrt{0.34988} = 0.592\\). Check these values using the formula for the Z-statistic presented in Section 5.3.3.↩︎ If you run prop.test with a one-sided alternative (\"greater\" or \"less\"), then R will report what is called a “one-sided confidence interval”, where one of the endpoints is either 0 (for a “less” alternative) or 1 (for a “greater” alternative). This is equivalent to only producing an upper bound or a lower bound, respectively, for the parameter of interest.↩︎ "],
["inference-num.html", "Chapter 6 Inference for quantitative data 6.1 One mean 6.2 Paired mean difference 6.3 Difference of two means 6.4 Summary of t-procedures 6.5 R: Inference for quantitative data 6.6 Chapter 6 review", " Chapter 6 Inference for quantitative data Focusing now on statistical inference for quantitative data, we will revisit and expand upon the foundational aspects of hypothesis testing from Section 5.1. The important data structure for this chapter is a quantitative response variable (that is, the outcome is numerical). The three data structures we detail are: one quantitative response variable, summarized by a single mean, one quantitative response variable which is a difference across a pair of observations, summarized by a paired mean difference, and a quantitative response variable broken down by a binary explanatory variable, summarized by a difference in means. When appropriate, each of the data structures will be analyzed using the two methods introduced in Section 5.1: simulation-based and theory-based. As we build on the inferential ideas, we will visit new foundational concepts in statistical inference. One key new idea rests in estimating how the sample mean (as opposed to the sample proportion) varies from sample to sample; the resulting value is referred to as the standard error of the mean. We will also introduce a new important mathematical model, the \\(t\\)-distribution (as the foundation for the \\(t\\)-test). To summarize a quantitative response variable, we focus on the sample mean (instead of, for example, the sample median or the range of the observations) because of the well-studied mathematical model which describes the behavior of the sample mean. The sample mean will be calculated in one group, two paired groups, and two independent groups. We will not cover mathematical models which describe other statistics, but the bootstrap and randomization techniques described below are immediately extendable to any function of the observed data. The techniques described for each setting will vary slightly, but you will be well served to find the structural similarities across the different settings. 6.1 One mean Notation. \\(n\\) = sample size \\(\\bar{x}\\) = sample mean \\(s\\) = sample standard deviation \\(\\mu\\) = population mean \\(\\sigma\\) = population standard deviation A single mean is used to summarize data when we measured a single quantitative variable on each observational unit, e.g., GPA, age, salary. Aside from slight differences in notation, the inferential methods presented in this section will be identical to those for a paired mean difference, as we will see in Section 6.2. 6.1.1 Bootstrap confidence interval for \\(\\mu\\) In this section, we will use bootstrapping, first introduced in Section 5.3.2, to construct a confidence interval for a population mean. Recall that bootstrapping is best suited for modeling studies where the data have been generated through random sampling from a population. Our bootstrapped distribution of sample means will mimic the process of randomly sampling from a population to give us a sense of how sample means will vary from sample to sample. Observed data As an employer who subsidizes housing for your employees, you need to know the average monthly rental price for a three bedroom flat in Edinburgh. In order to walk through the example more clearly, let’s say that you are only able to randomly sample five Edinburgh flats (if this were a real example, you would surely be able to take a much larger sample size, possibly even being able to measure the entire population!). Figure 6.1 presents the details of the random sample of observations where the monthly rent of five flats has been recorded. Figure 6.1: Five randomly sampled flats in Edinburgh. The sample average monthly rent of £1648 is a first guess at the price of three bedroom flats. However, as a student of statistics, you understand that one sample mean based on a sample of five observations will not necessarily equal the true population average rent for all three bedroom flats in Edinburgh. Indeed, you can see that the observed rent prices vary with a standard deviation of £340.232, and surely the average monthly rent would be different if a different sample of size five had been taken from the population. Fortunately, we can use bootstrapping to approximate the variability of the sample mean from sample to sample. Variability of the statistic As with the inferential ideas covered in Chapter 5, the inferential analysis methods in this chapter are grounded in quantifying how one data set differs from another when they are both taken from the same population. Figure 6.2 shows how the unknown original population of all three bedroom flats in Edinburgh can be estimated by using many duplicates of the sample. This estimated population—consisting of infinitely many copies of the original sample—can then be used to generate bootstrapped resamples. Figure 6.2: Using the original sample of five Edinburgh flats to generate an estimated population, which is then used to generate bootstrapped resamples. This process of generating a bootstrapped sample is equivalent to sampling five flats from the original sample, with replacement. In Figure 6.2, the repeated bootstrap resamples are obviously different both from each other and from the original sample. Since the bootstrap resamples are taken from the same (estimated) population, these differences are due entirely to natural variability in the sampling procedure. By summarizing each of the bootstrap resamples (here, using the sample mean), we see, directly, the variability of the sample mean from sample to sample. The distribution of \\(\\bar{x}_{boot}\\), the bootstrapped sample means, for the Edinburgh flats is shown in Figure 6.3. Figure 6.3: Distribution of bootstrapped means from 1,000 simulated bootstrapped samples generated by sampling with replacement from our original sample of five Edinburgh flats. The histogram provides a sense for the variability of the average rent values from sample to sample for samples of size 5. The bootstrapped average rent prices vary from £1250 to £1995 (with a small observed sample of size 5, a bootstrap resample can sometimes, although rarely, include only repeated measurements of the same observation). Bootstrapping from one sample. Take a random sample of size \\(n\\) from the original sample, with replacement. This is called a bootstrapped resample. Record the sample mean (or statistic of interest) from the bootstrapped resample. This is called a bootstrapped statistic. Repeat steps (1) and (2) 1000s of times. Due to theory that is beyond this text, we know that the bootstrap means \\(\\bar{x}_{boot}\\) vary around the original sample mean, \\(\\bar{x}\\), in a similar way to how different sample (i.e., different data sets which would produce different \\(\\bar{x}\\) values) means vary around the true parameter \\(\\mu\\). Therefore, an interval estimate for \\(\\mu\\) can be produced using the \\(\\bar{x}_{boot}\\) values themselves. A 95% bootstrap confidence interval for \\(\\mu\\), the population mean rent price for three bedroom flats in Edinburgh, is found by locating the middle 95% of the bootstrapped sample means in Figure 6.3. 95% Bootstrap confidence interval for a population mean \\(\\mu\\). The 95% bootstrap confidence interval for the parameter \\(\\mu\\) can be obtained directly using the ordered values \\(\\bar{x}_{boot}\\) values — the bootstrapped sample means. Consider the sorted \\(\\bar{x}_{boot}\\) values, and let \\(\\bar{x}_{boot, 0.025}\\) be the 2.5th percentile and \\(\\bar{x}_{boot, 0.025}\\) be the 97.5th percentile. The 95% confidence interval is given by: (\\(\\bar{x}_{boot, 0.025}\\), \\(\\bar{x}_{boot, 0.975}\\)) You can find confidence intervals of difference confidence levels by changing the percent of the distribution you take, e.g., locate the middle 90% of the bootstrapped statistics for a 90% confidence interval. Using Figure 6.3, find the 90% and 95% confidence intervals for the true mean monthly rental price of a three bedroom flat in Edinburgh. A 90% confidence interval is given by (£1429, £1876). The conclusion is that we are 90% confident that the true average rental price for three bedroom flats in Edinburgh lies somewhere between £1429 and £1876. A 95% confidence interval is given by (£1389.75, £1916). The conclusion is that we are 95% confident that the true average rental price for three bedroom flats in Edinburgh lies somewhere between £1389.75 and £1916. Bootstrap percentile confidence interval for \\(\\sigma\\) (special topic) Suppose that the research question at hand seeks to understand how variable the rental price of the three bedroom flats are in Edinburgh. That is, your interest is no longer in the average rental price of the flats but in the standard deviation of the rental prices of all three bedroom flats in Edinburgh, \\(\\sigma\\). You may have already realized that the sample standard deviation, \\(s\\), will work as a good point estimate for the parameter of interest: the population standard deviation, \\(\\sigma\\). The point estimate of the five observations is calculated to be \\(s =\\) £340.23. While \\(s =\\) £340.23 might be a good guess for \\(\\sigma\\), we prefer to have an interval. Although there is a mathematical model which describes how \\(s\\) varies from sample to sample, the mathematical model will not be presented in this text. But even without the mathematical model, bootstrapping can be used to find a confidence interval for the parameter \\(\\sigma\\). Describe the bootstrap distribution for the standard deviation shown in Figure 6.4. The distribution is skewed left and centered near £340.23, which is the point estimate from the original data. Most observations in this distribution lie between £0 and £408.1. Using Figure 6.4, find and interpret a 90% confidence interval for the population standard deviation for three bedroom flat prices in Edinburgh.146 Figure 6.4: The original Edinburgh data is bootstrapped 1,000 times. The histogram provides a sense for the variability of the standard deviation of the rent values from sample to sample. Bootstrapping is not a solution to small sample sizes! The example presented above is done for a sample with only five observations. As with analysis techniques that build on mathematical models, bootstrapping works best when a large random sample has been taken from the population. Bootstrapping is a method for capturing the variability of a statistic when the mathematical model is unknown — it is not a method for navigating small samples. As you might guess, the larger the random sample, the more accurately that sample will represent the target population. 6.1.2 Simulation-based test for \\(H_0: \\mu = \\mu_0\\) We can also use bootstrapping to conduct a simulation-based test of the null hypothesis that the population mean is equal to a specified value, \\(\\mu_0\\), called the null value. In this case, we first shift each value in the data set so that the sample distribution is centered at \\(\\mu_0\\). Then, we bootstrap from the shifted data in order to generate a null distribution of sample means. Consider the following example. In 1851, Carl Wunderlich, a German physician, measured body temperatures of around 25,000 adults and found that the average body temperature was 98.6\\(^{\\circ}\\)F, which we’ve believed ever since. However, a recent study conducted at Stanford University suggests that the average body temperature may actually be lower than 98.6\\(^{\\circ}\\)F.147 Observed data Curious if average body temperature has decreased since 1851, you decided to collect data on a random sample of twenty Montana State University students. The mean body temperature in your sample is \\(\\bar{x}\\) = 97.47\\(^{\\circ}\\)F, and the standard deviation is \\(s\\) = 0.35\\(^{\\circ}\\)F. A dot plot of the data is shown in Figure 6.5, with summary statistics displayed below. favstats(temperatures) #&gt; min Q1 median Q3 max mean sd n missing #&gt; 96.7 97.3 97.5 97.7 98.1 97.5 0.353 20 0 Figure 6.5: Distribution of body temperatures in a random sample of twenty Montana State University students. Shifted bootstrapped null distribution We would like to test the set of hypotheses \\(H_0: \\mu = 98.6\\) versus \\(H_A: \\mu &lt; 98.6\\), where \\(\\mu\\) is the true mean body temperature among all adults (in degrees F). If we were to simulate sample mean body temperatures under \\(H_0\\), we would expect the null distribution to be centered at \\(\\mu_0\\) = 98.6\\(^\\circ\\)F. However, if we bootstrap sample means from our observed sample, the bootstrap distribution will be centered at the sample mean body temperature \\(\\bar{x}\\) = 97.5\\(^\\circ\\)F. To use bootstrapping to generate a null distribution of sample means, we first have to shift the data to be centered at the null value. We do this by adding \\(\\mu_0 - \\bar{x} = 98.6 - 97.5 = 1.1^\\circ\\)F to each body temperature in the sample. This process is displayed in Figure 6.6. Figure 6.6: Distribution of body temperatures in a random sample of twenty Montana State University students (blue) and the shifted body temperatures (red), found by adding 1.1 degree F to each original body temperature. A bootstrapped null distribution generated from sampling 20 shifted temperatures, with replacement, from the shifted data 1,000 times is shown in Figure 6.7. Figure 6.7: Bootstrapped null distribution of sample mean temperatures assuming the true mean temperature is 98.6 degrees F. Shifted bootstrap null distribution for a sample mean. To simulate a null distribution of sample means under the null hypothesis \\(H_0: \\mu = \\mu_0\\): Add \\(\\mu_0 - \\bar{x}\\) to each value in the original sample: \\[ x_1 + \\mu_0 - \\bar{x}, \\hspace{2.5mm} x_2 + \\mu_0 - \\bar{x}, \\hspace{2.5mm} x_3 + \\mu_0 - \\bar{x}, \\hspace{2.5mm} \\ldots, \\hspace{2.5mm} x_n + \\mu_0 - \\bar{x}. \\] Note that if \\(\\bar{x}\\) is larger than \\(\\mu\\), then the quantity \\(\\mu_0 - \\bar{x}\\) will be negative, and you will be subtracting the distance between \\(\\mu\\) and \\(\\bar{x}\\) from each value. Generate 1000s of bootstrap resamples from this shifted distribution, plotting the shifted bootstrap sample mean each time. To calculate the p-value, since \\(H_A: \\mu &lt; 98.6\\), we find the proportion of simulated sample means that were less than or equal to our original sample mean, \\(\\bar{x}\\) = 97.47. As shown in Figure 6.7, none of our simulated sample means were 97.5\\(^\\circ\\)F or lower, giving us very strong evidence that the true mean body temperature among all Montana State University students is less than the commonly accepted 98.6\\(^\\circ\\)F average temperature. 6.1.3 Theory-based inferential methods for \\(\\bar{x}\\) As with the sample proportion, the variability of the sample mean is well described by the mathematical theory given by the Central Limit Theorem. Similar to how we can model the behavior of the sample proportion \\(\\hat{p}\\) using a normal distribution, the sample mean \\(\\bar{x}\\) can also be modeled using a normal distribution when certain conditions are met. However, because of missing information about the inherent variability in the population, a \\(t\\)-distribution is used in place of the standard normal when performing hypothesis test or confidence interval analyses. Central Limit Theorem for the sample mean. When we collect a sufficiently large sample of \\(n\\) independent observations from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the sampling distribution of \\(\\bar{x}\\) will be nearly normal with \\[\\begin{align*} &amp;\\text{Mean}=\\mu &amp;&amp;\\text{Standard Deviation }(SD) = \\frac{\\sigma}{\\sqrt{n}} \\end{align*}\\] Before diving into confidence intervals and hypothesis tests using \\(\\bar{x}\\), we first need to cover two topics: When we modeled \\(\\hat{p}\\) using the normal distribution, certain conditions had to be satisfied. The conditions for working with \\(\\bar{x}\\) are a little more complex, and below, we will discuss how to check conditions for inference using a mathematical model. The standard deviation of the sample mean is dependent on the population standard deviation, \\(\\sigma\\). However, we rarely know \\(\\sigma\\), and instead we must estimate it. Because this estimation is itself imperfect, we use a new distribution called the \\(t\\)-distribution to fix this problem. Evaluating the two conditions required for modeling \\(\\bar{x}\\) using theory-based methods There are two conditions required to apply the Central Limit Theorem for a sample mean \\(\\bar{x}\\). When the sample observations are independent and the sample size is sufficiently large, the normal model will describe the variability in sample means quite well; when the observations violate the conditions, the normal model can be inaccurate. Conditions for the modeling \\(\\bar{x}\\) using theory-based methods. The sampling distribution for \\(\\bar{x}\\) based on a sample of size \\(n\\) from a population with a true mean \\(\\mu\\) and true standard deviation \\(\\sigma\\) can be modeled using a normal distribution when: Independence. The sample observations must be independent, The most common way to satisfy this condition is when the sample is a simple random sample from the population. If the data come from a random process, analogous to rolling a die, this would also satisfy the independence condition. Normality. When a sample is small, we also require that the sample observations come from a normally distributed population. We can relax this condition more and more for larger and larger sample sizes. This condition is obviously vague, making it difficult to evaluate, so next we introduce a couple rules of thumb to make checking this condition easier. When these conditions are satisfied, then the sampling distribution of \\(\\bar{x}\\) is approximately normal with mean \\(\\mu\\) and standard deviation \\(\\frac{\\sigma}{\\sqrt{n}}\\). General rule: how to perform the normality check. There is no perfect way to check the normality condition, so instead we use two general rules: \\(\\mathbf{n &lt; 30}\\): If the sample size \\(n\\) is less than 30 and there are no clear outliers in the data, then we typically assume the data come from a nearly normal distribution to satisfy the condition. \\(\\mathbf{n \\geq 30}\\): If the sample size \\(n\\) is at least 30 and there are no particularly extreme outliers, then we typically assume the sampling distribution of \\(\\bar{x}\\) is nearly normal, even if the underlying distribution of individual observations is not. In this first course in statistics, you aren’t expected to develop perfect judgement on the normality condition. However, you are expected to be able to handle clear cut cases based on the rules of thumb.148 Consider the following two plots that come from simple random samples from different populations. Their sample sizes are \\(n_1 = 15\\) and \\(n_2 = 50\\). Are the independence and normality conditions met in each case? Each sample is from a simple random sample of its respective population, so the independence condition is satisfied. Let’s next check the normality condition for each using the rule of thumb. The first sample has fewer than 30 observations, so we are watching for any clear outliers. None are present; while there is a small gap in the histogram on the right, this gap is small and 20% of the observations in this small sample are represented in that far right bar of the histogram, so we can hardly call these clear outliers. With no clear outliers, the normality condition is reasonably met. The second sample has a sample size greater than 30 and includes an outlier that appears to be roughly 5 times further from the center of the distribution than the next furthest observation. This is an example of a particularly extreme outlier, so the normality condition would not be satisfied. In practice, it’s typical to also do a mental check to evaluate whether we have reason to believe the underlying population would have moderate skew (if \\(n &lt; 30\\)) or have particularly extreme outliers \\((n \\geq 30)\\) beyond what we observe in the data. For example, consider the number of followers for each individual account on Twitter, and then imagine this distribution. The large majority of accounts have built up a couple thousand followers or fewer, while a relatively tiny fraction have amassed tens of millions of followers, meaning the distribution is extremely skewed. When we know the data come from such an extremely skewed distribution, it takes some effort to understand what sample size is large enough for the normality condition to be satisfied. Introducing the \\(t\\)-distribution In practice, we cannot directly calculate the standard deviation for \\(\\bar{x}\\) since we do not know the population standard deviation, \\(\\sigma\\). We encountered a similar issue when computing the standard error for a sample proportion, which relied on the population proportion, \\(\\pi\\). Our solution in the proportion context was to use sample value in place of the population value to calculate a standard error. We’ll employ a similar strategy to compute the standard error of \\(\\bar{x}\\), using the sample standard deviation \\(s\\) in place of \\(\\sigma\\): \\[\\begin{align*} SE(\\bar{x}) = \\frac{s}{\\sqrt{n}} \\approx SD(\\bar{x}) = \\frac{\\sigma}{\\sqrt{n}}. \\end{align*}\\] The standard error of \\(\\bar{x}\\) provides an estimate of the standard deviation of \\(\\bar{x}\\). This strategy tends to work well when we have a lot of data and can estimate \\(\\sigma\\) using \\(s\\) accurately. However, the estimate is less precise with smaller samples, and this leads to problems when using the normal distribution to model \\(\\bar{x}\\) if we do not know \\(\\sigma\\). We’ll find it useful to use a new distribution for inference calculations called the \\(t\\)-distribution. A \\(t\\)-distribution, shown as a solid line in Figure 6.8, has a bell shape. However, its tails are thicker than the normal distribution’s, meaning observations are more likely to fall beyond two standard deviations from the mean than under the normal distribution. The extra thick tails of the \\(t\\)-distribution are exactly the correction needed to resolve the problem (due to extra variability of the test statistic) of using \\(s\\) in place of \\(\\sigma\\) in the \\(SE(\\bar{x})\\) calculation. Figure 6.8: Comparison of a \\(t\\)-distribution and a normal distribution. The \\(t\\)-distribution is always centered at zero and has a single parameter: degrees of freedom (\\(df\\)). The degrees of freedom describes the precise form of the bell-shaped \\(t\\)-distribution. Several \\(t\\)-distributions are shown in Figure 6.9 in comparison to the normal distribution. For inference with a single mean, we’ll use a \\(t\\)-distribution with \\(df = n - 1\\) to model the sample mean when the sample size is \\(n\\). That is, when we have more observations, the degrees of freedom will be larger and the \\(t\\)-distribution will look more like the standard normal distribution; when the degrees of freedom is about 30 or more, the \\(t\\)-distribution is nearly indistinguishable from the normal distribution. Figure 6.9: The larger the degrees of freedom, the more closely the \\(t\\)-distribution resembles the standard normal distribution. Degrees of freedom: \\(df\\). The degrees of freedom describes the shape of the \\(t\\)-distribution. The larger the degrees of freedom, the more closely the distribution approximates the normal model. When modeling \\(\\bar{x}\\) using the \\(t\\)-distribution, use \\(df = n - 1\\). The \\(t\\)-distribution allows us greater flexibility than the normal distribution when analyzing numerical data. In practice, it’s common to use statistical software, such as R, Python, or SAS for these analyses. In R, the function used for calculating probabilities under a \\(t\\)-distribution is pt() (which should seem similar to the previous R function pnorm()). Don’t forget that with the \\(t\\)-distribution, the degrees of freedom must always be specified! For the examples and guided practices below, use R to find the answers. We recommend trying the problems so as to get a sense for how the \\(t\\)-distribution can vary in width depending on the degrees of freedom, and to confirm your working understanding of the \\(t\\)-distribution. What proportion of the \\(t\\)-distribution with 18 degrees of freedom falls below -2.10? Just like a normal probability problem, we first draw the picture in Figure 6.10 and shade the area below -2.10. Using statistical software, we can obtain a precise value: 0.0250. # using pt() to find probability under the $t$-distribution pt(-2.10, df = 18) #&gt; [1] 0.025 Figure 6.10: The \\(t\\)-distribution with 18 degrees of freedom. The area below -2.10 has been shaded. A \\(t\\)-distribution with 20 degrees of freedom is shown in the top panel of Figure 6.11. Estimate the proportion of the distribution falling above 1.65. Note that with 20 degrees of freedom, the \\(t\\)-distribution is relatively close to the normal distribution. With a normal distribution, this would correspond to about 0.05, so we should expect the \\(t\\)-distribution to give us a value in this neighborhood. Using statistical software: 0.0573. # using pt() to find probability under the $t$-distribution pt(1.65, df = 20, lower.tail=FALSE) #&gt; [1] 0.0573 # or 1 - pt(1.65, df = 20) #&gt; [1] 0.0573 Figure 6.11: Top: The \\(t\\)-distribution with 20 degrees of freedom, with the area above 1.65 shaded. Bottom: The \\(t\\)-distribution with 2 degrees of freedom, with the area further than 3 units from 0 shaded. A \\(t\\)-distribution with 2 degrees of freedom is shown in the bottom panel of Figure 6.11. Estimate the proportion of the distribution falling more than 3 units from the mean (above or below). With so few degrees of freedom, the \\(t\\)-distribution will give a more notably different value than the normal distribution. Under a normal distribution, the area would be about 0.003 using the 68-95-99.7 rule. For a \\(t\\)-distribution with \\(df = 2\\), the area in both tails beyond 3 units totals 0.0955. This area is dramatically different than what we obtain from the normal distribution. # using pt() to find probability under the $t$-distribution 2 * pt(-3, df = 2) #&gt; [1] 0.0955 What proportion of the \\(t\\)-distribution with 19 degrees of freedom falls above -1.79 units?149 One sample \\(t\\)-confidence intervals Let’s get our first taste of applying the \\(t\\)-distribution in the context of an example about the mercury content of dolphin muscle. Elevated mercury concentrations are an important problem for both dolphins and other animals, like humans, who occasionally eat them. Figure 6.12: A Risso’s dolphin. Photo by Mike Baird, www.bairdphotos.com. Observed data We will identify a confidence interval for the average mercury content in dolphin muscle using a sample of 19 Risso’s dolphins from the Taiji area in Japan. The data are summarized in Table 6.1. The minimum and maximum observed values can be used to evaluate whether or not there are clear outliers. Table 6.1: Summary of mercury content in the muscle of 19 Risso’s dolphins from the Taiji area. Measurements are in micrograms of mercury per wet gram of muscle (\\(\\mu\\)g/wet g). \\(n\\) \\(\\bar{x}\\) \\(s\\) minimum maximum 19 4.4 2.3 1.7 9.2 Are the independence and normality conditions satisfied for this data set? The observations are a simple random sample, therefore independence is reasonable. The summary statistics in Table 6.1 do not suggest any clear outliers, with all observations within 2.5 standard deviations of the mean. Based on this evidence, the normality condition seems reasonable. In the normal model, we used \\(z^{\\star}\\) and the standard error to determine the width of a confidence interval. We revise the confidence interval formula slightly when using the \\(t\\)-distribution: \\[\\begin{align*} &amp;\\text{point estimate} \\ \\pm\\ t^{\\star}_{df} \\times SE(\\text{point estimate}) &amp;&amp;\\to &amp;&amp;\\bar{x} \\ \\pm\\ t^{\\star}_{df} \\times \\frac{s}{\\sqrt{n}}, \\end{align*}\\] where \\(df = n - 1\\) when computing a one-sample \\(t\\)-interval. Using the summary statistics in Table 6.1, compute the standard error for the average mercury content in the \\(n = 19\\) dolphins. We plug in \\(s\\) and \\(n\\) into the formula: \\(SE(\\bar{x}) = s / \\sqrt{n} = 2.3 / \\sqrt{19} = 0.528\\). The value \\(t^{\\star}_{df}\\) is a cutoff we obtain based on the confidence level and the \\(t\\)-distribution with \\(df\\) degrees of freedom. That cutoff is found in the same way as with a normal distribution: we find \\(t^{\\star}_{df}\\) such that the fraction of the \\(t\\)-distribution with \\(df\\) degrees of freedom within a distance \\(t^{\\star}_{df}\\) of 0 matches the confidence level of interest. When \\(n = 19\\), what is the appropriate degrees of freedom? Find \\(t^{\\star}_{df}\\) for this degrees of freedom and the confidence level of 95%. The degrees of freedom is easy to calculate: \\(df = n - 1 = 18\\). Using statistical software, we find the cutoff where the upper tail is equal to 2.5%: \\(t^{\\star}_{18} =\\) 2.10. The area below -2.10 will also be equal to 2.5%. That is, 95% of the \\(t\\)-distribution with \\(df = 18\\) lies within 2.10 units of 0. # use qt() to find the t-cutoff (with 95% in the middle) qt(0.025, df = 18) #&gt; [1] -2.1 qt(0.975, df = 18) #&gt; [1] 2.1 Compute and interpret the 95% confidence interval for the average mercury content in Risso’s dolphins. We can construct the confidence interval as \\[\\begin{align*} \\bar{x} \\ \\pm\\ t^{\\star}_{18} \\times SE(\\bar{x}) \\quad \\to \\quad 4.4 \\ \\pm\\ 2.10 \\times 0.528 \\quad \\to \\quad (3.29, 5.51) \\end{align*}\\] We are 95% confident the average mercury content of muscles in the population of Risso’s dolphins is between 3.29 and 5.51 \\(\\mu\\)g/wet gram, which is considered extremely high. Finding a \\(t\\)-confidence interval for the mean, \\(\\mu\\). Based on a sample of \\(n\\) independent and nearly normal observations, a confidence interval for the population mean is \\[\\begin{align*} &amp;\\text{point estimate} \\ \\pm\\ t^{\\star}_{df} \\times SE(\\text{point estimate}) &amp;&amp;\\to &amp;&amp;\\bar{x} \\ \\pm\\ t^{\\star}_{df} \\times \\frac{s}{\\sqrt{n}} \\end{align*}\\] where \\(\\bar{x}\\) is the sample mean, \\(t^{\\star}_{df}\\) corresponds to the confidence level and degrees of freedom \\(df\\), and \\(SE\\) is the standard error as estimated by the sample. The FDA’s webpage provides some data on mercury content of fish. Based on a sample of 15 croaker white fish (Pacific), a sample mean and standard deviation were computed as 0.287 and 0.069 ppm (parts per million), respectively. The 15 observations ranged from 0.18 to 0.41 ppm. We will assume these observations are independent. Based on the summary statistics of the data, do you have any objections to the normality condition of the individual observations?150 Calculate the standard error of \\(\\bar{x}\\) using the data summaries in the previous Guided Practice. If we are to use the \\(t\\)-distribution to create a 90% confidence interval for the actual mean of the mercury content, identify the degrees of freedom and \\(t^{\\star}_{df}\\). The standard error: \\(SE(\\bar{x}) = \\frac{0.069}{\\sqrt{15}} = 0.0178\\). Degrees of freedom: \\(df = n - 1 = 14\\). Since the goal is a 90% confidence interval, we choose \\(t_{14}^{\\star}\\) so that the two-tail area is 0.1: \\(t^{\\star}_{14} = 1.76\\). # use qt() to find the t-cutoff (with 90% in the middle) qt(0.05, df = 14) #&gt; [1] -1.76 qt(0.95, df = 14) #&gt; [1] 1.76 Using the information and results of the previous Guided Practice and Example, compute a 90% confidence interval for the average mercury content of croaker white fish (Pacific).151 The 90% confidence interval from the previous Guided Practice is 0.256 ppm to 0.318 ppm. Can we say that 90% of croaker white fish (Pacific) have mercury levels between 0.256 and 0.318 ppm?152 One sample \\(t\\)-tests Now that we’ve used the \\(t\\)-distribution for making a confidence intervals for a mean, let’s speed on through to hypothesis tests for the mean. The test statistic for assessing a single mean is a T. The T score is a ratio of how the sample mean differs from the hypothesized mean as compared to how the observations vary. \\[\\begin{align*} T = \\frac{\\bar{x} - \\mbox{null value}}{s/\\sqrt{n}} \\end{align*}\\] When the null hypothesis is true and the conditions are met, T has a t-distribution with \\(df = n - 1\\). Conditions: independently observed data large samples and no extreme outliers Compare the T score — the standardized sample mean — to the Z score — the standardized sample proportion — presented in Section 5.3.3. Why do we use a “Z” when standardizing proportions, but a “T” when standardizing means?153 Is the typical US runner getting faster or slower over time? We consider this question in the context of the Cherry Blossom Race, which is a 10-mile race in Washington, DC each spring. The average time for all runners who finished the Cherry Blossom Race in 2006 was 93.29 minutes (93 minutes and about 17 seconds). We want to determine using data from 100 participants in the 2017 Cherry Blossom Race whether runners in this race are getting faster or slower, versus the other possibility that there has been no change. What are appropriate hypotheses for this context?154 The data come from a simple random sample of all participants, so the observations are independent. A histogram of the race times is given to evaluate if we can move forward with a t-test. Should we be worried about the normality condition?155 When completing a hypothesis test for the one-sample mean, the process is nearly identical to completing a hypothesis test for a single proportion. First, we find the Z score using the observed value, null value, and standard error; however, we call it a T score since we use a \\(t\\)-distribution for calculating the tail area. Then we find the p-value using the same ideas we used previously: find the area under the \\(t\\)-distribution as or more extreme than our T score. With both the independence and normality conditions satisfied, we can proceed with a hypothesis test using the \\(t\\)-distribution. The sample mean and sample standard deviation of the sample of 100 runners from the 2017 Cherry Blossom Race are 97.32 and 16.98 minutes, respectively. Recall that the sample size is 100 and the average run time in 2006 was 93.29 minutes. Find the test statistic and p-value. What is your conclusion? The hypotheses, found in a previous Guided Practice, are: \\(H_0: \\mu = 93.29\\) minutes \\(H_A: \\mu \\neq 93.29\\) minutes To find the test statistic (T score), we first must determine the standard error: \\[\\begin{align*} SE(\\bar{x}) = 16.98 / \\sqrt{100} = 1.70 \\end{align*}\\] Now we can compute the T score using the sample mean (97.32), null value (98.29), and \\(SE\\): \\[\\begin{align*} T = \\frac{97.32 - 93.29}{1.70} = 2.37 \\end{align*}\\] For \\(df = 100 - 1 = 99\\), we can determine using statistical software that the area under a \\(t\\)-distribution with 99 \\(df\\) that is above our observed T score of 2.37 is 0.01 (see below), which we double to get the p-value: 0.02. Because the p-value is small, the data provide strong evidence that the average run time for the Cherry Blossom Run in 2017 is different than the 2006 average. # using pt() to find the p-value 1 - pt(2.37, df = 99) #&gt; [1] 0.00986 When using a \\(t\\)-distribution, we use a T score (similar to a Z score). To help us remember to use the \\(t\\)-distribution, we use a \\(T\\) to represent the test statistic, and we often call this a T score. The Z score and T score are computed in the exact same way and are conceptually identical: each represents how many standard errors the observed value is from the null value. 6.2 Paired mean difference Notation. \\(n\\) = number of pairs in paired samples \\(\\bar{x}_{d}\\) = sample mean of differences in paired samples \\(s_{d}\\) = sample standard deviation of differences in paired samples \\(\\mu_{d}\\) = population mean of differences in paired samples \\(\\sigma_{d}\\) = population standard deviation of differences in paired samples Paired data represent a particular type of experimental structure where the analysis is somewhat akin to a one-sample analysis (see Section 6.1) but has other features that resemble a two-sample analysis (which we will see in Section 6.3). Quantitative measurements are made on each of two different levels of an explanatory variable, but those measurements are paired — each observational unit consists of two measurements, and the two measurements are subtracted such that only the difference is retained. Table 6.2 presents some examples of studies where paired designs were implemented. Table 6.2: Examples of studies where a paired design is used to measure the difference in the measurement over two conditions. Observational unit Comparison groups Measurement Value of interest Car Smooth Turn vs Quick Spin amount of tire tread after 1,000 miles difference in tread Married heterosexual couple Husband vs Wife age difference in age Textbook UCLA vs Amazon price of new text difference in price Individual person Pre-course vs Post-course exam score difference in score Paired data. Two sets of observations are paired if each observation in one set has a special correspondence or connection with exactly one observation in the other data set. For inferential methods applied to paired data, the analysis is virtually identical to the one-sample approach given in Section 6.1. The key to working with paired data is to consider the measurement of interest to be the difference in measured values across the pair of observations. Thinking about the differences as a single observation on an observational unit changes the paired setting into the one-sample setting. 6.2.1 Simulation-based test for \\(H_0: \\mu_d = 0\\) Consider an experiment done to measure whether tire brand Smooth Turn or tire brand Quick Spin has longer tread wear. That is, after 1,000 miles on a car, which brand of tires has more tread, on average? Observed data The observed data represent 25 tread measurements (in inches) taken on 25 Smooth Turn tires and 25 Quick Spin tires. The study used a total of 25 cars, so on each car, one brand was randomly assigned to the front driver’s side tire and the other to the front passenger’s side tire. Figure 6.13 presents the observed data. The Smooth Turn manufacturer looks at the box plot below and says: clearly the tread on Smooth Turn tires is higher, on average, than the tread on Quick Spin tires after 1,000 miles of driving. The Quick Spin manufacturer is skeptical and retorts: but with only 25 cars, it seems that the variability in road conditions (sometimes one tire hits a pothole, etc.) could be what leads to the small difference in average tread amount. We’d like to be able to systematically distinguish between what the Smooth Turn manufacturer sees in the plot and what the Quick Spin manufacturer sees in the plot. Fortunately for us, we have an excellent way to simulate the natural variability (from road conditions, etc.) that can lead to tires being worn at different rates: bootstrapping. Figure 6.13: Boxplots of the tire tread remaining after 1,000 miles by the brand of tire from which the original measurements came. Gray lines connect the same cars. Since these are paired data, we are only interested in the differences in tire tread between the two brands on each car. The dotplot in Figure 6.14 displays these differences, with summary statistics displayed below. favstats(differences) #&gt; min Q1 median Q3 max mean sd n missing #&gt; -0.00506 -0.000972 0.00205 0.0042 0.0107 0.00196 0.00431 25 0 Figure 6.14: Difference in tire tread (in inches) remaining after 1,000 miles between the two brands (Smooth Turn – Quick Spin). Variability of the statistic A simulation-based test will identify whether the differences seen in the box plot below could plausibly have happened just by chance variability. As before, we will simulate the variability in sample statistics under the assumption that the null hypothesis is true. In this study, the null hypothesis is that average difference in tire tread wear between Smooth Turn and Quick Spin tires is zero. The experiment was conducted to determine whether Smooth Turn or Quick Spin has longer tread wear. Taking the order of differences to be Smooth Turn \\(-\\) Quick Spin, we express the hypotheses as follows. \\(H_0: \\mu_d = 0\\), the true mean difference in tire tread remaining after 1,000 miles between Smooth Turn and Quick Spin (Smooth Turn \\(-\\) Quick Spin) tires is equal to zero. \\(H_A: \\mu_d \\neq 0\\), the true mean difference in tire tread remaining after 1,000 miles between Smooth Turn and Quick Spin (Smooth Turn \\(-\\) Quick Spin) tires is not equal to zero. To simulate the null distribution of mean differences in tread, we will implement the same method used in Section 6.1.2 using a shifted bootstrap distribution. Shifted bootstrap null distribution for a sample mean difference. To simulate a null distribution of sample mean differences under the null hypothesis \\(H_0: \\mu_d = 0\\), Subtract \\(\\bar{x}_d\\) from each difference in the original sample:156 \\[ x_1 - \\bar{x}_d , \\hspace{2.5mm} x_2 - \\bar{x}_d, \\hspace{2.5mm} x_3 - \\bar{x}_d, \\hspace{2.5mm} \\ldots, \\hspace{2.5mm} x_n - \\bar{x}_d. \\] Note that if \\(\\bar{x}_d\\) is a negative number, then you will be adding the distance between \\(0\\) and \\(\\bar{x}_d\\) to each value. Generate 1000s of bootstrap resamples from this shifted distribution, plotting the shifted bootstrap sample mean difference each time. To use bootstrapping to generate a null distribution of sample mean differences in tire tread, we first have to shift the data to be centered at the null value of zero. We shift the data by subtracting \\(\\bar{x}_d\\) = 0.00196 from each tire tread difference in the sample. This process is displayed in Figure 6.15. Figure 6.15: Mean difference in tire tread (in inches) remaining after 1,000 miles between the two brands (Smooth Turn – Quick Spin) (blue), and the shifted mean differences in tire tread (red), found by subtracting 0.00196 to each original difference. Observed statistic vs. null statistics By repeatedly sampling 25 cars with replacement from the shifted bootstrap null distribution, we can create a distribution of the sample mean difference in tire tread, as seen in Figure 6.16. As expected (because the differences were generated under the null hypothesis), the histogram is centered at zero. A line has been drawn at the observed mean difference, \\(\\bar{x}_d\\) = 0.00196, which is nowhere near the differences simulated from natural variability when we assume there is no difference in tire tread wear between brands. Because the observed mean difference in tire tread is so far away from the natural variability of the randomized mean differences in tire tread, we believe that there is a significant difference in tire tread wear between Smooth Turn and Quick Spin brand tires, on average. To be precise, the proportion of simulated \\(\\bar{x}_d\\)’s that are 0.00196 inches or further away from zero is 0.023. This p-value gives us strong evidence in favor of our alternative \\(H_A: \\mu_d \\neq 0\\). Our conclusion is that the extra amount of tire tread remaining in Smooth Turn brand tires after 1,000 miles, on average, is due to more than just natural variability. Data from this experiment suggest that, on average, Smooth Turn tires differ in tread wear compared to Quick Spin tires. Figure 6.16: Histogram of 1000 simulated mean differences in tire tread, assuming that the two brands perform equally, on average. 6.2.2 Bootstrap confidence interval for \\(\\mu_d\\) In an earlier edition of this textbook, we found that Amazon prices were, on average, lower than those of the UCLA Bookstore for UCLA courses in 2010. It’s been several years, and many stores have adapted to the online market, so we wondered, how is the UCLA Bookstore doing today? Observed data We sampled 201 UCLA courses. Of those, 68 required books could be found on Amazon. The ucla_textbooks_f18 data can be found in the openintro package. A portion of the data set from these courses is shown in Table 6.3, where prices are in US dollars. Here the differences are taken as \\[\\begin{align*} \\text{UCLA Bookstore price} - \\text{Amazon price} \\end{align*}\\] It is important that we always subtract using a consistent order; here Amazon prices are always subtracted from UCLA prices. The first difference shown in Table 6.3 is computed as \\(47.97 - 47.45 = 0.52\\). Similarly, the second difference is computed as \\(14.26 - 13.55 = 0.71\\), and the third is \\(13.50 - 12.53 = 0.97\\). Table 6.3: Four cases of the ucla_textbooks_f18 dataset. subject course_num bookstore_new amazon_new price_diff American Indian Studies M10 48.0 47.5 0.52 Anthropology 2 14.3 13.6 0.71 Arts and Architecture 10 13.5 12.5 0.97 Asian M60W 49.3 55.0 -5.69 A dot plot of the data is shown in Figure 6.17, with summary statistics displayed below. favstats(ucla_textbooks_f18$price_diff) #&gt; min Q1 median Q3 max mean sd n missing #&gt; -12.2 -0.992 0.625 2.99 75.2 3.58 13.4 68 0 Figure 6.17: Distribution of differences in new textbook price (UCLA Bookstore – Amazon) in US dollars for 68 required textbooks at UCLA. Each textbook has two corresponding prices in the data set: one for the UCLA Bookstore and one for Amazon. Thus, the two prices for the same textbook are paired, and our analysis need only focus on the differences in textbook price between the two suppliers. Variability of the statistic Following the example of bootstrapping a single mean, the observed mean differences can be bootstrapped in order to understand the variability of the average difference from sample to sample. We can then use the bootstrap distribution of mean differences to calculate bootstrap percentile confidence intervals for the true mean difference in the population. In Figure 6.18, a 99% confidence interval for the mean difference in the cost of a new book at the UCLA Bookstore compared with Amazon has been calculated. The bootstrap percentile interval is computing using the 0.5th percentile and 99.5th percentile of the bootstrapped mean differences and is found to be (-0.044, 8.138). Since this confidence interval contains zero, it does not support the hypothesis that the UCLA Bookstore price is, on average, higher than the Amazon price. That is, since the interval contains both negative and positive values, it is plausible that the prices of UCLA textbooks are lower, on average, than Amazon, and it is also plausible that the prices of UCLA textbooks are higher, on average, than Amazon. We would interpret the interval as follows: We are 99% confident that, on average, new textbook prices at the UCLA Bookstore are between $0.04 lower to $8.14 higher than the same textbook on Amazon. Figure 6.18: Bootstrap distribution for the average difference in new book price at the UCLA Bookstore versus Amazon (UCLA – Amazon). The bounds for a 99% bootstrap percentile confidence interval are superimposed in red, and the observed mean difference in new book price is superimposed in blue. 6.2.3 Theory-based inferential methods for \\(\\bar{x}_d\\) Thinking about the paired differences as a single observation on an observational unit, theory-based inferential methods for a paired mean difference are identical to theory-based methods for a single mean. Theory-based methods for the one sample mean case are covered in Section 6.1.3. The only difference between the methods in Section 6.1 and the methods described in this section is notation, shown below. The subscript “d” stands for “difference” since our variable is a paired difference. One Mean Paired Mean Difference Population mean \\(\\mu\\) \\(\\mu_d\\) Population standard deviation \\(\\sigma\\) \\(\\sigma_d\\) Sample mean \\(\\bar{x}\\) \\(\\bar{x}_d\\) Sample standard deviation \\(s\\) \\(s_d\\) Sample size \\(n\\) \\(n\\) Observed data Consider again the paired textbook price data in the previous section. A histogram of the differences in new textbook price between the UCLA Bookstore and Amazon is shown in Figure 6.19, and summary statistics are displayed in Table 6.4. Table 6.4: Summary statistics for the 68 new textbook price differences (UCLA – Amazon). \\(n\\) \\(\\bar{x}_{d}\\) \\(s_{d}\\) 68 $3.58 $13.42 Figure 6.19: Histogram of the difference in price for each book sampled. Variability of the statistic To analyze a paired data set, we simply analyze the differences using the same one sample \\(t\\)-distribution techniques we applied in Section 6.1.3. Set up a hypothesis test to determine whether, on average, the UCLA Bookstore’s price for a new textbook is higher than the price of the same book on Amazon. Also, check the conditions for whether we can move forward with the test using the \\(t\\)-distribution. We are considering two scenarios: \\(H_0\\): \\(\\mu_{d} = 0\\). The true mean difference in new textbook prices (UCLA – Amazon) is equal to zero. \\(H_A\\): \\(\\mu_{d} &gt; 0\\). The true mean difference in new textbook prices (UCLA – Amazon) is greater than zero. Next, we check the independence and normality conditions: The observations are based on a simple random sample, so independence is reasonable. While there are some outliers, \\(n = 68\\) and none of the outliers are particularly extreme, so the normality of \\(\\bar{x}\\) is satisfied. With these conditions satisfied, we can move forward with the \\(t\\)-distribution. Observed statistic vs. null statistics As mentioned previously, the methods applied to a difference will be identical to the one-sample techniques. Therefore, the full hypothesis test framework is given as an example. Complete the hypothesis test started in the previous Example. To start, compute the standard error associated with \\(\\bar{x}_{d}\\) using the sample standard deviation of the differences (\\(s_{d} = 13.42\\)) and the number of differences (\\(n = 68\\)): \\[\\begin{align*} SE(\\bar{x}_{d}) = \\frac{s_{d}}{\\sqrt{n}} = \\frac{13.42}{\\sqrt{68}} = 1.63 \\end{align*}\\] The test statistic is the T-score of \\(\\bar{x}_{d}\\) under the null condition that the actual mean difference is 0: \\[\\begin{align*} T = \\frac{\\bar{x}_{d} - 0} { SE(\\bar{x}_{d})} = \\frac{3.58 - 0}{1.63} = 2.20 \\end{align*}\\] This value tells us that the sample mean difference in price, $3.58, is 2.20 standard errors above zero (the null value). To visualize the p-value, the approximate sampling distribution of \\(\\bar{x}_{d}\\) is drawn as though \\(H_0\\) is true, and the p-value is represented by the shaded upper tail in Figure 6.20. This area is equivalent to the area above 2.20 on a \\(t\\)-distribution with \\(df = n - 1\\) = 68 \\(-\\) 1 = 67 degrees of freedom. Using the pt function in R, we find the upper tail area of 0.0156. In conclusion, we have strong evidence that Amazon prices are, on average, lower than the UCLA Bookstore prices for UCLA courses. Figure 6.20: Distribution of \\(\\bar{x}_{d}\\) under the null hypothesis of no difference. The observed average difference of 2.98 is marked with the shaded areas more extreme than the observed difference given as the p-value. Create a theory-based 95% confidence interval for the average price difference between books at the UCLA Bookstore and books on Amazon. Conditions for using theory-based methods have already been verified and the standard error computed in the previous Example. To find the confidence interval, identify \\(t^{\\star}_{67}\\) using the R command: qt(0.975, 67) = 2.00, and plug it, the point estimate, and the standard error into the confidence interval formula: \\[\\begin{align*} \\bar{x}_d \\ \\pm\\ t^{\\star} \\times SE(\\bar{x}_d) \\quad\\to\\quad 3.58 \\ \\pm\\ 2.00 \\times 1.63 \\quad\\to\\quad (0.32, 6.84) \\end{align*}\\] We are 95% confident that Amazon is, on average, between $0.32 and $6.84 less expensive than the UCLA Bookstore for UCLA course books. We have strong evidence that Amazon is, on average, less expensive. How should this conclusion affect UCLA student buying habits? Should UCLA students always buy their books on Amazon?157 6.3 Difference of two means Notation. \\(n_1\\), \\(n_2\\) = sample sizes of two independent samples \\(\\bar{x}_1\\), \\(\\bar{x}_2\\) = sample means of two independent samples \\(s_1\\), \\(s_2\\) = sample standard deviations of two independent samples \\(\\mu_1\\), \\(\\mu_2\\) = population means of two independent samples \\(\\sigma_1\\), \\(\\sigma_2\\) = population standard deviations of two independent samples In this section we consider a difference in two population means, \\(\\mu_1 - \\mu_2\\), under the condition that the data are not paired. Just as with a single sample, we identify conditions to ensure we can use the \\(t\\)-distribution with a point estimate of the difference, \\(\\bar{x}_1 - \\bar{x}_2\\), and a new standard error formula. The details for working through inferential problems in the two independent means setting are strikingly similar to those applied to the two independent proportions stetting. We first cover a randomization test where the observations are shuffled under the assumption that the null hypothesis is true. Then we bootstrap the data (with no imposed null hypothesis) to create a confidence interval for the true difference in population means, \\(\\mu_1 - \\mu_2\\). The mathematical model, here the \\(t\\)-distribution, is able to describe both the randomization test and the boostrapping as long as the conditions are met. The inferential tools are applied to three different data contexts: determining whether stem cells can improve heart function, exploring the relationship between pregnant women’s smoking habits and birth weights of newborns, and exploring whether there is statistically significant evidence that one variation of an exam is harder than another variation. This section is motivated by questions like “Is there convincing evidence that newborns from mothers who smoke have a different average birth weight than newborns from mothers who don’t smoke?” 6.3.1 Randomization test for \\(H_0: \\mu_1 - \\mu_2 = 0\\) An instructor decided to run two slight variations of the same exam. Prior to passing out the exams, she shuffled the exams together to ensure each student received a random version. Summary statistics for how students performed on these two exams are shown in Table 6.5 and plotted in Figure 6.21. Anticipating complaints from students who took Version B, she would like to evaluate whether the difference observed in the groups is so large that it provides convincing evidence that Version B was more difficult (on average) than Version A. Observed data Table 6.5: Summary statistics of scores for each exam version. \\(n\\) \\(\\bar{x}\\) s minimum maximum A 58 75.1 13.9 44 100 B 55 72.0 13.8 38 100 Figure 6.21: Exam scores for students given one of three different exams. Construct hypotheses to evaluate whether the observed difference in sample means, \\(\\bar{x}_A - \\bar{x}_B=3.1\\), is due to chance. We will later evaluate these hypotheses using \\(\\alpha = 0.01\\).158 Before moving on to evaluate the hypotheses in the previous Guided Practice, let’s think carefully about the dataset. Are the observations across the two groups independent? Are there any concerns about outliers?159 Variability of the statistic need to talk about the way to randomize is almost identical to chapter 5 &amp; 6. a new plot will probably help (but again, very similar to 5.7) In Section 5.4.1, the variability of the statistic (previously: \\(\\hat{p}_1 - \\hat{p}_2\\)) was visualized after shuffling the observations across the two treatment groups many times. The shuffling process implements the null hypothesis model (that there is no effect of the treatment). In the exam example, the null hypothesis is that exam A and exam B are equally difficult, so the average scores across the two tests should be the same. If the exams were equally difficult, due to natural variability, we would sometimes expect students to do slightly better on exam A (\\(\\bar{x}_A &gt; \\bar{x}_B\\)) and sometimes expect students to do slightly better on exam B (\\(\\bar{x}_B &gt; \\bar{x}_A\\)). The question at hand is: does \\(\\bar{x}_A - \\bar{x}_B=3.1\\) indicate that exam A is easier than exam B. Figure 6.22 shows the process of randomizing the exam to the observed exam scores. If the null hypothesis is true, then the score on each exam should represent the true student ability on that material. It shouldn’t matter whether they were given exam A or exam B. By reallocating which student got which exam, we are able to understand how the difference in average exam scores changes due only to natural variability. There is only one iteration of the randomization process in Figure 6.22, leading to one simulated difference in average scores. Figure 6.22: The version of the test (A or B) is randomly allocated to the test scores, under the null assumption that the tests are equally difficult. Building on Figure 6.22, Figure 6.23 shows the values of the simulated statistics \\(\\bar{x}_{1, sim} - \\bar{x}_{2, sim}\\) over 1000 random simulations. We see that, just by chance, the difference in scores can range anywhere from -10 points to +10 points. Figure 6.23: Histogram of differences in means, calculated from 1000 different randomizations of the exam types. Observed statistic vs. null statistics The goal of the randomization test is to assess the observed data, here the statistic of interest is \\(\\bar{x}_A - \\bar{x}_B=3.1\\). The randomization distribution allows us to identify whether a difference of 3.1 points is more than one would expect by natural variability. By plotting the value of 3.1 on Figure 6.24, we can measure how different or similar 3.1 is to the randomized differences which were generated under the null hypothesis. Figure 6.24: Histogram of differences in means, calculated from 1000 different randomizations of the exam types. The observed difference of 3.1 points is plotted as a vertical line, and the area more extreme than 3.1 is shaded to represent the p-value. Approximate the p-value depicted in Figure 6.24, and provide a conclusion in the context of the case study. Using software, we can find the number of shuffled differences in means that are less than the observed difference (of 3.14) is 19 (our of 1000 randomizations). So 10% of the simulations are larger than the observed difference. To get the p-value, we double the proportion of randomized differences which are larger than the observed difference, p-value = 0.2. Previously, we specified that we would use \\(\\alpha = 0.01\\). Since the p-value is larger than \\(\\alpha\\), we do not reject the null hypothesis. That is, the data do not convincingly show that one exam version is more difficult than the other, and the teacher should not be convinced that she should add points to the Version B exam scores. #&gt; # A tibble: 1 x 1 #&gt; p_value #&gt; &lt;dbl&gt; #&gt; 1 0.1 The large p-value and consistency of \\(\\bar{x}_A - \\bar{x}_B=3.1\\) with the randomized differences leads us to not reject the null hypothesis. Said differently, there is no evidence to think that one of the tests is easier than the other. One might be inclined to conclude that the tests have the same level of difficulty, but that conclusion would be wrong. The hypothesis testing framework is set up only to reject a null claim, it is not set up to validate a null claim. As we concluded, the data are consistent with exams A and B being equally difficult, but the data are also consistent with exam A being 3.1 points “easier” than exam B. The data are not able to adjudicate on whether the exams are equally hard or whether one of them is slightly easier. Indeed, conclusions where the null hypothesis is not rejected often seem unsatisfactory. However, in this case, the teacher and class are probably all relieved that there is no evidence to demonstrate that one of the exams is more difficult than the other. 6.3.2 Bootstrap confidence interval for \\(\\mu_1 - \\mu_2\\) Does treatment using embryonic stem cells (ESCs) help improve heart function following a heart attack? Table 6.6 contains summary statistics for an experiment to test ESCs in sheep that had a heart attack. Each of these sheep was randomly assigned to the ESC or control group, and the change in their hearts’ pumping capacity was measured in the study. Figure 6.29 provides histograms of the two data sets. A positive value corresponds to increased pumping capacity, which generally suggests a stronger recovery. Our goal will be to identify a 95% confidence interval for the effect of ESCs on the change in heart pumping capacity relative to the control group. Observed data Table 6.6: Summary statistics of the embryonic stem cell study. \\(n\\) \\(\\bar{x}\\) s ESCs 9 3.50 5.17 control 9 -4.33 2.76 The point estimate of the difference in the heart pumping variable is straightforward to find: it is the difference in the sample means. \\[\\begin{align*} \\bar{x}_{esc} - \\bar{x}_{control}\\ =\\ 3.50 - (-4.33)\\ =\\ 7.83 \\end{align*}\\] Variability of the statistic As we saw in Section 5.4.3, we will use bootstrapping to estimate the variability associated with the difference in sample means when taking repeated samples. In a method akin to two proportions, a separate sample is taken with replacement from each group (here ESCs and control), the sample means are calculated, and their difference is taken. The entire process is repeated multiple times to produce a bootstrap distribution of the difference in sample means (without the null hypothesis assumption). Figure 6.26 displays the variability of the differences in means with the 90% percentile and SE CIs super imposed. one we have the whole example, write up a bit about how the two different intervals are constructed (including the calculated value of the SE of the difference in means) with the cars, we can compare two different cities. link each bootstrap sample, create the interval, etc. Figure 6.25: first figure with the ? pop, then sample, then estimate of the pop. need to re-do with cars Figure 6.26: Histogram of differences in means after 1000 bootstrap samples from each of the two groups. The observed difference is plotted as a black vertical line at 7.83. The blue and green lines provide the percentile bootstrap and SE boostrap confidence intervals, respectively, for the difference in true population means. Choose one of the boostrap confidence intervals for the true difference in average pumping capacity, \\(\\mu_{esc} - \\mu_{control}\\). Does the interval show that there is a difference across the two treatments? Because the 90% intervals above do not overlap zero (zero is never one of the bootstrapped differences, 95% or 99% intervals would have given the same conclusion!), we conclude tha the ESC treatment is significantly better with respect to heart pumping capacity than the treatment. Beause the study is a randomized controled experiment, we can conclude that it is the treatment (ESC) whic is causing the change in pumping capacity. 6.3.3 Mathematical model 6.3.3.1 \\(t\\)-test for \\(\\mu_1 - \\mu_2\\) Observed data A data set called ncbirths represents a random sample of 150 cases of mothers and their newborns in North Carolina over a year. Four cases from this data set are represented in Table 6.7. We are particularly interested in two variables: weight and smoke. The weight variable represents the weights of the newborns and the smoke variable describes which mothers smoked during pregnancy. We would like to know, is there convincing evidence that newborns from mothers who smoke have a different average birth weight than newborns from mothers who don’t smoke? We will use the North Carolina sample to try to answer this question. The smoking group includes 50 cases and the nonsmoking group contains 100 cases. Table 6.7: Four cases from the ncbirths data set. The value NA, shown for the first two entries of the first variable, indicates that piece of data is missing. fage mage mature weeks premie visits marital gained weight lowbirthweight gender habit whitemom NA 13 younger mom 39 full term 10 not married 38 7.63 not low male nonsmoker not white NA 14 younger mom 42 full term 15 not married 20 7.88 not low male nonsmoker not white 19 15 younger mom 37 full term 11 not married 38 6.63 not low female nonsmoker white 21 15 younger mom 41 full term 6 not married 34 8.00 not low male nonsmoker white Set up appropriate hypotheses to evaluate whether there is a relationship between a mother smoking and average birth weight. The null hypothesis represents the case of no difference between the groups. \\(H_0\\): There is no difference in average birth weight for newborns from mothers who did and did not smoke. In statistical notation: \\(\\mu_{n} - \\mu_{s} = 0\\), where \\(\\mu_{n}\\) represents non-smoking mothers and \\(\\mu_s\\) represents mothers who smoked. \\(H_A\\): There is some difference in average newborn weights from mothers who did and did not smoke (\\(\\mu_{n} - \\mu_{s} \\neq 0\\)). Variability of the statistic We check the two conditions necessary to model the difference in sample means using the \\(t\\)-distribution. Because the data come from a simple random sample, the observations are independent, both within and between samples. With both data sets over 30 observations, we inspect the data in Figure 6.27 for any particularly extreme outliers and find none. Since both conditions are satisfied, the difference in sample means may be modeled using a \\(t\\)-distribution. Figure 6.27: The top panel represents birth weights for infants whose mothers smoked. The bottom panel represents the birth weights for infants whose mothers who did not smoke. The summary statistics in Table 6.8 may be useful for this Guided Practice.160 What is the point estimate of the population difference, \\(\\mu_{n} - \\mu_{s}\\)? Compute the standard error of the point estimate from part (a). Table 6.8: Summary statistics for the ncbirths data set. smoker nonsmoker mean 6.78 7.18 st. dev. 1.43 1.60 samp. size 50.00 100.00 Observed statistic vs. null statistics Complete the hypothesis test started in the previous Example and Guided Practice on ncbirths dataset and research question. Use a significance level of \\(\\alpha=0.05\\). For reference, \\(\\bar{x}_{n} - \\bar{x}_{s} = 0.40\\), \\(SE = 0.26\\), and the sample sizes were \\(n_n = 100\\) and \\(n_s = 50\\). We can find the test statistic for this test using the previous information: \\[\\begin{align*} T = \\frac{\\ 0.40 - 0\\ }{0.26} = 1.54 \\end{align*}\\] The p-value is represented by the two shaded tails in Figure 6.28 We find the single tail area using software. We’ll use the smaller of \\(n_n - 1 = 99\\) and \\(n_s - 1 = 49\\) as the degrees of freedom: \\(df = 49\\). The one tail area is 0.065; doubling this value gives the two-tail area and p-value, 0.135. The p-value is larger than the significance value, 0.05, so we do not reject the null hypothesis. There is insufficient evidence to say there is a difference in average birth weight of newborns from North Carolina mothers who did smoke during pregnancy and newborns from North Carolina mothers who did not smoke during pregnancy. Figure 6.28: The mathematical model for the T statistic when the null hypothesis is true. As expected, the curve is centered at zero (the null value). The observered statistic is also plotted with the area more extreme than the observed statistic plotted to indicate the p-value. We’ve seen much research suggesting smoking is harmful during pregnancy, so how could we fail to reject the null hypothesis in the previous Example?161 If we made a Type 2 Error and there is a difference, what could we have done differently in data collection to be more likely to detect the difference?162 Public service announcement: while we have used this relatively small data set as an example, larger data sets show that women who smoke tend to have smaller newborns. In fact, some in the tobacco industry actually had the audacity to tout that as a benefit of smoking: It’s true. The babies born from women who smoke are smaller, but they’re just as healthy as the babies born from women who do not smoke. And some women would prefer having smaller babies. - Joseph Cullman, Philip Morris’ Chairman of the Board on CBS’ Face the Nation, Jan 3, 1971 Fact check: the babies from women who smoke are not actually as healthy as the babies from women who do not smoke.163 \\(t\\) confidence interval for \\(\\mu_1 - \\mu_2\\) Observed data As with hypothesis testing, for the question of whether we can model the difference using a \\(t\\)-distribution, we’ll need to check new conditions. Like the 2-proportion cases, we will require a more robust version of independence so we are confident the two groups are also independent. Secondly, we also check for normality in each group separately, which in practice is a check for outliers. Using the \\(t\\)-distribution for a difference in means. The \\(t\\)-distribution can be used for inference when working with the standardized difference of two means if Independence (extended). The data are independent within and between the two groups, e.g., the data come from independent random samples or from a randomized experiment. Normality. We check the outliers for each group separately. The standard error may be computed as \\[\\begin{align*} SE = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}} \\end{align*}\\] The official formula for the degrees of freedom is quite complex and is generally computed using software, so instead you may use the smaller of \\(n_1 - 1\\) and \\(n_2 - 1\\) for the degrees of freedom if software isn’t readily available. Variability of the statistic Can the \\(t\\)-distribution be used to make inference using the point estimate, \\(\\bar{x}_{esc} - \\bar{x}_{control} = 7.83\\)? First, we check for independence. Because the sheep were randomized into the groups, independence within and between groups is satisfied. Figure 6.29 does not reveal any clear outliers in either group. (The ESC group does look a bit more variability, but this is not the same as having clear outliers.) With both conditions met, we can use the \\(t\\)-distribution to model the difference of sample means. Figure 6.29: Histograms for both the embryonic stem cell and control group. Generally, we use statistical software to find the appropriate degrees of freedom, or if software isn’t available, we can use the smaller of \\(n_1 - 1\\) and \\(n_2 - 1\\) for the degrees of freedom, e.g., if using a \\(t\\)-table to find tail areas. For transparency in the Examples and Guided Practice, we’ll use the latter approach for finding \\(df\\); in the case of the ESC example, this means we’ll use \\(df = 8\\). Calculate a 95% confidence interval for the effect of ESCs on the change in heart pumping capacity of sheep after they’ve suffered a heart attack. We will use the sample difference and the standard error that we computed earlier calculations: \\[\\begin{align*} \\bar{x}_{esc} - \\bar{x}_{control} = 7.83 &amp;&amp; SE = \\sqrt{\\frac{5.17^2}{9} + \\frac{2.76^2}{9}} = 1.95 \\end{align*}\\] Using \\(df = 8\\), we can identify the multiplier of \\(t^{\\star}_{8} = 2.31\\) for a 95% confidence interval. Finally, we can enter the values into the confidence interval formula: \\[\\begin{align*} \\text{point estimate} \\ \\pm\\ t^{\\star} \\times SE \\quad\\rightarrow\\quad 7.83 \\ \\pm\\ 2.31\\times 1.95 \\quad\\rightarrow\\quad (3.32, 12.34) \\end{align*}\\] We are 95% confident that embryonic stem cells improve the heart’s pumping function in sheep that have suffered a heart attack by 3.32% to 12.34% . 6.4 Summary of t-procedures So far in this chapter, we have seen the \\(t\\)-distribution applied as the appropriate mathematical model in three distinct settings. Although the three data structures are different, their similarities and differences are worth pointing out. We provide Table 6.9 partly as a mechanism for understanding \\(t\\)-procedures and partly to highlight the extremely common usage of the \\(t\\)-distribution in practice. You will often hear the following three \\(t\\)-procedures referred to as a one sample \\(t\\)-test (\\(t\\)-interval), paired \\(t\\)-test (\\(t\\)-interval), and two sample \\(t\\)-test (\\(t\\)-interval). Table 6.9: Similarities of \\(t\\)-methods across one sample, paired sample, and two independent samples analysis of a numeric response variable. one sample paired sample two indep. samples response variable numeric numeric numeric explanatory variable none binary binary parameter of interest mean: \\(\\mu\\) paired mean diff: \\(\\mu_d\\) diff in means: \\(\\mu_1 - \\mu_2\\) statistic of interest mean: \\(\\bar{x}\\) paired mean diff: \\(\\bar{x}_d\\) diff in means: \\(\\bar{x}_1 - \\bar{x}_2\\) standard error \\(\\frac{s}{\\sqrt{n}}\\) \\(\\frac{s_d}{\\sqrt{n}}\\) \\(\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\\) degrees of freedom \\(n-1\\) \\(n -1\\) \\(\\min(n_1 -1, n_2 - 1)\\) conditions independence, 2. normality or large samples independence, 2. normality or large samples independence, 2. normality or large samples Hypothesis tests. When applying the \\(t\\)-distribution for a hypothesis test, we proceed as follows: Write appropriate hypotheses. Verify conditions for using the \\(t\\)-distribution. One-sample or differences from paired data: the observations (or differences) must be independent and nearly normal. For larger sample sizes, we can relax the nearly normal requirement, e.g., slight skew is okay for sample sizes of 15, moderate skew for sample sizes of 30, and strong skew for sample sizes of 60. For a difference of means when the data are not paired: each sample mean must separately satisfy the one-sample conditions for the \\(t\\)-distribution, and the data in the groups must also be independent. Compute the statistic of interest, the standard error, and the degrees of freedom. For \\(df\\), use \\(n-1\\) for one sample, and for two samples use either statistical software or the smaller of \\(n_1 - 1\\) and \\(n_2 - 1\\). Compute the T-score using the general formula: \\[ T = \\frac{\\mbox{statistic} - \\mbox{null value}}{\\mbox{standard error}} \\] Use the statistical software to find the p-value using the appropriate \\(t\\)-distribution: Sign in \\(H_A\\) is \\(&lt;\\): p-value = area below T-score Sign in \\(H_A\\) is \\(&gt;\\): p-value = area above T-score Sign in \\(H_A\\) is \\(\\neq\\): p-value = 2 \\(\\times\\) area below \\(-|\\mbox{T-score}|\\) Make a conclusion based on the p-value, and write a conclusion in context, in plain language, and in terms of the alternative hypothesis. Confidence intervals. Similarly, the following is how we generally computed a confidence interval using a \\(t\\)-distribution: Verify conditions for using the \\(t\\)-distribution. (See above.) Compute the point estimate of interest, the standard error, the degrees of freedom, and \\(t^{\\star}_{df}\\). Calculate the confidence interval using the general formula: \\[ \\mbox{statistic} \\pm\\ t_{df}^{\\star} SE. \\] Put the conclusions in context and in plain language so even non-data scientists can understand the results. 6.5 R: Inference for quantitative data Section on doing inference for categorical data in R. Simulation functions in catstats t.test Simulation-based inference for paired mean difference Simulation-based inference for quantitative data will use functions in the catstats package, as we did for categorical data. library(catstats) The catstats functions for paired data assume that the values for the two groups are in separate columns in a data frame. We’ll work through an example using the tire wear data, which is currently stored in “long format”, with one variable for brand and another for tread depth. First, we’ll convert it to “wide format”, with a column for each brand. tiresWide &lt;- tires %&gt;% select(brand, tread, car) %&gt;% #select only ID, group, and outcome vars pivot_wider(names_from = brand, #name of variable for group values_from = tread) #name of variable for outcome tiresWide &lt;- as.data.frame(tiresWide) Once we have this format, all the paired data functions in catstats should be able to handle the data. First, we can get a look at the pairs of observations: paired_observed_plot(tiresWide) This gives us an idea of the distributions within groups and the differences within pairs. To perform the hypothesis test for a difference in tread depth after 1000 miles, we use the paired_test() function: paired_test( data = tiresWide, #data frame with observed values in groups shift = -0.002, #amount to shift differences to bootstrap null distribution direction = &quot;two-sided&quot;, #Direction of hypothesis test as_extreme_as = 0.002, #Observed statistic number_repetitions = 1000, #number of bootstrap draws for null distribution which_first = 1 #Which column is first in order of subtraction: 1 or 2? ) Note that data could also be a vector of differences. If this is all you have, you can do hypothesis testing and generate a confidence interval, but won’t be able to use paired_observed_plot(). Now let’s take a look at the output of the function: set.seed(1054) paired_test( data = tiresWide, #data frame with observed values in groups shift = -0.002, #amount to shift differences to bootstrap null distribution direction = &quot;two-sided&quot;, #Direction of hypothesis test as_extreme_as = 0.002, #Observed statistic number_repetitions = 1000, #number of bootstrap draws for null distribution which_first = 1 #Which column is first in order of subtraction: 1 or 2? ) This figure displays the bootstrapped null distribution, with the mean and standard deviation of the draws in the upper right corner. We want to see that the mean is close to the null value (almost always zero). If it isn’t, check the value of the shift input, and/or increase the number_repetitions if the shift is correct. The red lines give the cutoffs based on the observed statistic, and values as or more extreme are colored red. If you are doing a one-sided test, there will only be one line. The caption of the figure gives the number and proportion of bootstrapped mean differences that are as or more extreme than the observed statistic. In this case, 20 out of 1000, for a p-value of 0.02. Finally, we will want to generate a confidence interval for the true mean difference using the paired_bootstrap_CI() function. set.seed(2374) paired_bootstrap_CI( data = tiresWide, #Wide-form data set or vector of differences number_repetitions = 1000, #number of draws for bootstrap distribution confidence_level = 0.99, #Confidence level as a proportion which_first = 1 #Order of subtraction: 1st or 2nd set of values come first? ) Here we again have a bootstrap distribution, but now it is the bootstrap distribution of the mean difference itself, rather than a bootstrapped null distribution for the mean difference. We’ve requested a 99% confidence interval, so the relevant percentiles of the bootstrap distribution are highlighted, and the interval itself is given in the caption. In this case, we are 99% confident that the true mean difference in tire tread is between 0 and 0.004 inches greater for Smooth Turn. Theory-based inference for paired mean difference To implement theory-based inference for a paired mean difference in R, we use the t.test() function. As an example, we’ll use the textbook cost data from Section 6.2. There are two ways to put in paired data for a t-test using t.test(). First, we could have the prices of the two groups in two separate variables (in this case, bookstore_new and amazon_new): t.test(x = ucla_textbooks_f18$bookstore_new, #Outcomes for one of each pair y = ucla_textbooks_f18$amazon_new, #Outcomes for other of each pair paired = TRUE, #Tell it to do a paired t-test!! alternative = &quot;two.sided&quot;, #Direction of alternative conf.level = 0.95 #confidence level for interval as a proportion ) Important things to note here: You must include paired = TRUE in your options, or it will do a two-sample t-test. As with categorical data in Chapter 5, if you have a one-sided alternative, you will need to re-run the t.test() with a two-sided alternative to get the correct confidence interval Now let’s take a look at the output of the call: t.test(x = ucla_textbooks_f18$bookstore_new, #Outcomes for first in order of subtraction y = ucla_textbooks_f18$amazon_new, #Outcomes for second in order of subtraction paired = TRUE, #Tell it to do a paired t-test!! alternative = &quot;two.sided&quot;, #Direction of alternative conf.level = 0.95 #confidence level for interval as a proportion ) #&gt; #&gt; Paired t-test #&gt; #&gt; data: ucla_textbooks_f18$bookstore_new and ucla_textbooks_f18$amazon_new #&gt; t = 2, df = 67, p-value = 0.03 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.334 6.832 #&gt; sample estimates: #&gt; mean of the differences #&gt; 3.58 The output tells you right on top that this is a paired test - if it doesn’t, check that you have paired = TRUE in your function call. The next line gives the t-statistic of 2.2012, the degrees of freedom df = 67, and the p-value of 0.03117 (You can look back at Section 6.2.3 to see that these are the same values obtained in the example). We also get the confidence interval; since we had a two-sided alternative, we get the correct interval for the true mean difference in price of $0.33 to $6.83 greater cost from the UCLA Bookstore. The point estimate for the mean difference is the final entry: on average, new bookstore books cost $3.58 more than the same books new from Amazon. You might also have a single variable in your dataset that contains the differences within pairs: we will create this for the textbook data in a variable called price_diff. This format is also usable with the t.test() function: ucla_textbooks_f18 %&gt;% mutate(price_diff = bookstore_new-amazon_new) t.test(x = ucla_textbooks_f18$price_diff, #variable with differences alternative = &quot;two.sided&quot;, #direction of alternative hypothesis conf.level = 0.95) #confidence level as a proportion This requires two fewer arguments: No y input, since the differences are contained in a single variable No paired = TRUE, since we have already accounted for the pairing by taking the differences. The output for this will look almost identical to the two-variable version above: ucla_textbooks_f18 &lt;- ucla_textbooks_f18 %&gt;% mutate(price_diff = bookstore_new-amazon_new) t.test(x = ucla_textbooks_f18$price_diff, #variable with differences alternative = &quot;two.sided&quot;, #direction of alternative hypothesis conf.level = 0.95) #confidence level as a proportion #&gt; #&gt; One Sample t-test #&gt; #&gt; data: ucla_textbooks_f18$price_diff #&gt; t = 2, df = 67, p-value = 0.03 #&gt; alternative hypothesis: true mean is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.334 6.832 #&gt; sample estimates: #&gt; mean of x #&gt; 3.58 Since we only input one variable, t.test() treats it as a one-sample t-test, but note that this works just fine: the t-statistic, df, p-value, confidence interval, and estimated mean are all the same as when we put in the two groups separately and indicated they were paired. 6.5.1 Interactive R tutorials Navigate the concepts you’ve learned in this chapter in R using the following self-paced tutorials. All you need is your browser to get started! Tutorial 7: Inference for categorical responses Tutorial 7 - Lesson 1: Bootstrapping for estimating a parameter Tutorial 7 - Lesson 2: Introducing the t-distribution Tutorial 7 - Lesson 3: Inference for difference in two means Tutorial 7 - Lesson 4: Comparing many means You can also access the full list of tutorials supporting this book here. 6.5.2 R labs Further apply the concepts you’ve learned in this chapter in R with computational labs that walk you through a data analysis case study. Inference for numerical responses - Youth Risk Behavior Surveillance System Full list of labs supporting OpenIntro::Introduction to Modern Statistics 6.6 Chapter 6 review 6.6.1 Terms We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as bolded text. bootstrapping one sample \\(t\\)-test point estimate two sample \\(t\\)-test Central Limit Theorem paired \\(t\\)-test T score degrees of freedom paired data t-distribution By looking at the percentile values in Figure 6.4, the middle 90% of the bootstrap standard deviations are given by the 5th percentile (£153.9) and 95th percentile (£385.6). That is, we are 90% confident that the true standard deviation of rent prices is between £153.9 and £385.6; or that, on average, rent prices tend to be somewhere between £153.9 and £385.6 away from the mean rent price. Note, the problem was set up as 90% to indicate that there was not a need for a high level of confidence (such a 95% or 99%). A lower degree of confidence increases potential for error, but it also produces a more narrow interval.↩︎ Protsiv, Ley,Lankester, Hastie, Parsonnet (2020). Decreasing human body temperature in the United States since the Industrial Revolution. eLife 2020;9:e49555, DOI: 10.7554/eLife.49555. https://elifesciences.org/articles/49555↩︎ More nuanced guidelines would consider further relaxing the particularly extreme outlier check when the sample size is very large. However, we’ll leave further discussion here to a future course.↩︎ We want to find the shaded area above -1.79 (we leave the picture to you). The lower tail area has an area of 0.0447, so the upper area would have an area of \\(1 - 0.0447 = 0.9553\\).↩︎ The sample size is under 30, so we check for obvious outliers: since all observations are within 2 standard deviations of the mean, there are no such clear outliers.↩︎ \\(\\bar{x} \\ \\pm\\ t^{\\star}_{14} \\times SE(\\bar{x}) \\ \\to\\ 0.287 \\ \\pm\\ 1.76 \\times 0.0178 \\ \\to\\ (0.256, 0.318)\\). We are 90% confident that the average mercury content of croaker white fish (Pacific) is between 0.256 and 0.318 ppm.↩︎ No, a confidence interval only provides a range of plausible values for a population parameter, in this case the population mean. It does not describe what we might observe for individual observations.↩︎ The letter “Z” is typically used when its distribution follows a standard normal distribution. We use “T” when the test statistic follows a \\(t\\)-distribution.↩︎ \\(H_0\\): The average 10-mile run time was the same for 2006 and 2017. \\(\\mu = 93.29\\) minutes. \\(H_A\\): The average 10-mile run time for 2017 was different than that of 2006. \\(\\mu \\neq 93.29\\) minutes.↩︎ With a sample of 100, we should only be concerned if there is are particularly extreme outliers. The histogram of the data doesn’t show any outliers of concern (and arguably, no outliers at all).↩︎ Subtracting the sample mean is equivalent to adding \\(\\mu_0 - \\bar{x}_d\\) when the null value is \\(\\mu_d = 0\\). Thus, we are using the same process as that described in Section 6.1.2.↩︎ The average price difference is only mildly useful for this question. Examine the distribution shown in Figure 6.19. There are certainly a handful of cases where Amazon prices are far below the UCLA Bookstore’s, which suggests it is worth checking Amazon (and probably other online sites) before purchasing. However, in many cases the Amazon price is above what the UCLA Bookstore charges, and most of the time the price isn’t that different. Ultimately, if getting a book immediately from the bookstore is notably more convenient, e.g., to get started on reading or homework, it’s likely a good idea to go with the UCLA Bookstore unless the price difference on a specific book happens to be quite large. For reference, this is a very different result from what we (the authors) had seen in a similar data set from 2010. At that time, Amazon prices were almost uniformly lower than those of the UCLA Bookstore’s and by a large margin, making the case to use Amazon over the UCLA Bookstore quite compelling at that time. Now we frequently check multiple websites to find the best price.↩︎ \\(H_0\\): the exams are equally difficult, on average. \\(\\mu_A - \\mu_B = 0\\). \\(H_A\\): one exam was more difficult than the other, on average. \\(\\mu_A - \\mu_B \\neq 0\\).↩︎ (a) Since the exams were shuffled, the “treatment” in this case was randomly assigned, so independence within and between groups is satisfied. (b) The summary statistics suggest the data are roughly symmetric about the mean, and the min/max values don’t suggest any outliers of concern.↩︎ (a) The difference in sample means is an appropriate point estimate: \\(\\bar{x}_{n} - \\bar{x}_{s} = 0.40\\). (b) The standard error of the estimate can be calculated using the standard error formula: \\[\\begin{align*} SE = \\sqrt{\\frac{\\sigma_n^2}{n_n} + \\frac{\\sigma_s^2}{n_s}} \\approx \\sqrt{\\frac{s_n^2}{n_n} + \\frac{s_s^2}{n_s}} = \\sqrt{\\frac{1.60^2}{100} + \\frac{1.43^2}{50}} = 0.26 \\end{align*}\\]↩︎ It is possible that there is a difference but we did not detect it. If there is a difference, we made a Type 2 Error.↩︎ We could have collected more data. If the sample sizes are larger, we tend to have a better shot at finding a difference if one exists. In fact, this is exactly what we would find if we examined a larger data set!↩︎ You can watch an episode of John Oliver on Last Week Tonight to explore the present day offenses of the tobacco industry. Please be aware that there is some adult language.↩︎ "],
["inference-reg.html", "Chapter 7 Inference for regression 7.1 R: Inference for regression 7.2 Chapter 7 review", " Chapter 7 Inference for regression Randomization for slope/correlation t-distribution for regression coefficients - test/CI 7.1 R: Inference for regression Section on doing inference for categorical data in R. - Simulation functions in catstats - lm - Diagnostic plots 7.1.1 Interactive R tutorials Navigate the concepts you’ve learned in this chapter in R using the following self-paced tutorials. All you need is your browser to get started! Tutorial 8: Inference for regression Tutorial 8 - Lesson 1: Inference in regression Tutorial 8 - Lesson 2: Randomization test for slope Tutorial 8 - Lesson 3: t-test for slope Tutorial 8 - Lesson 4: Checking technical conditions for slope inference Tutorial 8 - Lesson 5: Inference beyond the simple linear regression model You can also access the full list of tutorials supporting this book here. 7.1.2 R labs Further apply the concepts you’ve learned in this chapter in R with computational labs that walk you through a data analysis case study. Multiple linear regression - Grading the professor Full list of labs supporting OpenIntro::Introduction to Modern Statistics 7.2 Chapter 7 review 7.2.1 Terms We introduced the following terms in the chapter. If you’re not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as bolded text. "],
["case-studies.html", "Chapter 8 Appendix: Case studies", " Chapter 8 Appendix: Case studies Each week, you will work through a case study and post responses to discussion questions about these case studies in our D2L Discussion forums. We recommend you read through the case study and post responses earlier in the week so that our classroom community can engage in a meaningful discussion about each case study. Week Case Study Article 1 Choose your own adventure! Choose one of: - Agriculture article - Business article - Education article - Nursing article - Psychology article 2 Eating chocolate can help you lose weight! Article 3 Probability Article 4 Introduction to R and RStudio Tutorial "],
["activities.html", "Chapter 9 Appendix: Activities", " Chapter 9 Appendix: Activities Each week, you will work through an in-class activity with your team mates and the guidance of your instructor. We recommend you purchase a printed copy of the activity coursepack from the MSU Bookstore and bring this to class each day. If you cannot purchase the printed coursepack, you may print and activities from this Appendix and bring them to class with you. Activity Title Content 1 Martian Alphabet Introduction to inference 2 Study Design Sampling bias, experiments, observational studies, and confounding variables 3 Current Population Survey Exploratory data analysis: categorical variables 4 IMDb Movie Reviews Exploratory data analysis: quantitative variables 5 Movie Profits Exploratory data analysis: regression and multivariate thinking 6 Handedness of Male Boxers Inference: one proportion 7 Winter Sports Helmet Use and Head Injuries Inference: difference in proportions 8 COVID-19 and Air Pollution Inference: paired mean difference 9 Weather Patterns and Record Snowfall Inference: difference in means 10 Hand Dexterity Inference: regression "],
["references.html", "References", " References Agresti, Alan. 2007. An Introduction to Categorical Data Analysis. 2nd ed. Wiley. Chimowitz, Marc I, Michael J Lynn, Colin P Derdeyn, Tanya N Turan, David Fiorella, Bethany F Lane, L Scott Janis, et al. 2011. “Stenting Versus Aggressive Medical Therapy for Intracranial Arterial Stenosis.” New England Journal of Medicine 365 (11): 993–1003. http://www.nejm.org/doi/full/10.1056/NEJMoa1105335. Slawson, D C, and A F Shaughnessy. 2002. “Teaching Information Mastery: The Case of Baby Jeff and the Importance of Bayes’ Theorem.” Family Medicine 34 (2): 140–42. Tintle, Nathan L, Beth L Chance, George W Cobb, Allan J Rossman, Soma Roy, Todd M Swanson, and Jill L VanderStoep. 2016. Introduction to Statistical Investigations. 1st ed. Wiley. Wickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. O’Reilly Media. Wickham, Hadley, and others. 2014. “Tidy Data.” Journal of Statistical Software 59 (10): 1–23. "]
]

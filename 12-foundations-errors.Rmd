# Errors, power, and practical importance {#foundations-errors}

```{r, include = FALSE}
source("_common.R")
```

<!-- ```{block2, type="todo", echo=TRUE} -->

<!-- - power -->

<!-- - what affects power -->

<!-- - what affects width of CIs -->

<!-- - statistical significance vs practical importance -->

<!-- - effect size? -->

<!-- ``` -->

::: {.chapterintro}
TODO
:::


## Decision errors {#types-of-errors}



<!-- COPIED FROM OLD CHAPTER ON FOUNDATIONS - REPEATS SOME OF WHAT IS SAID IN "P-VALUE AND STATISTICAL SIGNIFICANCE" SECTION OF CHAPTER 9 -->
### Decisions and statistical significance {.unnumbered}

In some cases, a **decision** to the hypothesis test is needed, with the two possible decisions as follows:

-   Reject the null hypothesis
-   Fail to reject the null hypothesis

::: guidedpractice
For which values of the p-value should you "reject" a null hypothesis? "fail to reject" a null hypothesis?[^05-inference-cat-9]
:::

[^05-inference-cat-9]: Since a smaller p-value gives you stronger evidence *against* the null hypothesis, we reject $H_0$ when the p-value is very small, and fail to reject $H_0$ when the p-value is not small.

In order to decide between these two options, we need a previously set threshold for our p-value: when the p-value is less than a previously set threshold, we reject $H_0$; otherwise, we fail to reject $H_0$. This threshold is called the **significance level**\index{hypothesis testing!significance level}\index{significance level}, and when the p-value is less than the significance level, we say the results are **statistically significant**. This means the data provide such strong evidence against $H_0$ that we reject the null hypothesis in favor of the alternative hypothesis. The significance level, often represented by $\alpha$ (the Greek letter *alpha*), is typically set to $\alpha = 0.05$, but can vary depending on the field or the application and the real-life consequences of an incorrect decision. Using a significance level of $\alpha = 0.05$ in the Martian alphabet study, we can say that the data provided statistically significant evidence against the null hypothesis.

::: onebox
**Statistical significance.**

We say that the data provide **statistically significant**\index{hypothesis testing!statistically significant|textbf} evidence against the null hypothesis if the p-value is less than some reference value called the **significance level**\index{significance level}, denoted by $\alpha$.
:::

<!-- END OF COPY -->


\index{hypothesis testing!decision errors|(}

Hypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free. Similarly, data can point to the wrong conclusion. However, what distinguishes statistical hypothesis tests from a court system is that our framework allows us to quantify and control how often the data lead us to the incorrect conclusion.

In a hypothesis test, there are two competing hypotheses: the null and the alternative. We make a statement about which one might be true, but we might choose incorrectly. There are four possible scenarios in a hypothesis test, which are summarized in Table \@ref(tab:fourHTScenarios).

```{r fourHTScenarios}
temptbl <- tribble(
  ~variable,    ~col1, ~col2, ~col3,
 "", "", "Fail to reject $H_0$", "Reject $H_0$",
 "", "$H_0$ true", "good decision", "Type 1 Error",
 "**Truth**", "$H_A$ true", "Type 2 Error", "good decision"
)

temptbl %>%
 kable(caption = "Four different scenarios for hypothesis tests.", 
    col.names = c("", "", "", "")) %>%
 add_header_above( c("", "", "**Test conclusion**" = 2)) %>%
 kable_styling()
```

A **Type 1 Error**\index{Type 1 Error} is rejecting the null hypothesis when $H_0$ is actually true. Since we rejected the null hypothesis in the [Martian alphabet example](#Martian), it is possible that we made a Type I Error in that study.

A **Type 2 Error**\index{Type 2 Error} is failing to reject the null hypothesis when the alternative is actually true. Since we failed to reject the null hypothesis in the [medical consultant](#one-prop-null-boot) and [payday lenders](#payday-lenders) studies, it is possible that we made a Type 2 Error in one or both of those studies.

```{r include=FALSE}
terms_chp_5 <- c(terms_chp_5, "Type 1 Error", "Type 2 Error")
```

::: workedexample
In a US court, the defendant is either innocent ($H_0$) or guilty ($H_A$). What does a Type 1 Error represent in this context? What does a Type 2 Error represent? Table \@ref(tab:fourHTScenarios) may be useful.

------------------------------------------------------------------------

If the court makes a Type 1 Error, this means the defendant is innocent ($H_0$ true) but wrongly convicted. A Type 2 Error means the court failed to reject $H_0$ (i.e., failed to convict the person) when they were in fact guilty ($H_A$ true).
:::

::: guidedpractice
Consider the Martian alphabet study where we concluded students were more likely to say that Bumba was the figure on the left. What would a Type 1 Error represent in this context?[^05-inference-cat-50]
:::

[^05-inference-cat-50]: Making a Type 1 Error in this context would mean that students are equally likely to associate Bumba with the figure on the left as the figure on the right, despite the strong evidence (the data suggesting otherwise) found in the experiment. Notice that this does *not* necessarily mean something was wrong with the data or that we made a computational mistake. Sometimes data simply point us to the wrong conclusion, which is why scientific studies are often repeated to check initial findings.

::: workedexample
How could we reduce the Type 1 Error rate in US courts? What influence would this have on the Type 2 Error rate?

------------------------------------------------------------------------

To lower the Type 1 Error rate, we might raise our standard for conviction from "beyond a reasonable doubt" to "beyond a conceivable doubt" so fewer people would be wrongly convicted. However, this would also make it more difficult to convict the people who are actually guilty, so we would make more Type 2 Errors.
:::

::: guidedpractice
How could we reduce the Type 2 Error rate in US courts? What influence would this have on the Type 1 Error rate?[^05-inference-cat-51]
:::

[^05-inference-cat-51]: To lower the Type 2 Error rate, we want to convict more guilty people. We could lower the standards for conviction from "beyond a reasonable doubt" to "beyond a little doubt". Lowering the bar for guilt will also result in more wrongful convictions, raising the Type 1 Error rate.

\index{hypothesis testing!decision errors|)}

The example and guided practice above provide an important lesson: if we reduce how often we make one type of error, we generally make more of the other type.

```{=html}
<!--
%Hypothesis testing is built around rejecting or failing to reject the null hypothesis. That is, we do not reject $H_0$ unless the data provide strong evidence against it. But what precisely does *strong evidence* mean? As a general rule of thumb, for those cases where the null hypothesis is actually true, we do not want to incorrectly reject $H_0$ more than 5% of the time. This corresponds to our default significance level of $\alpha = 0.05$, which we use as a comparison with the p-value. In the next section, we discuss the appropriateness of different significance levels.
-->
```
## Controlling the Type I error rate

\index{hypothesis testing!significance level|(} \index{significance level|(}

Choosing a significance level for a test is important in many contexts, and the traditional level is 0.05. However, it is sometimes helpful to adjust the significance level based on the application. We may select a level that is smaller or larger than 0.05 depending on the consequences of any conclusions reached from the test.

::: onebox
**Significance level = probability of making a Type 1 error.**

We reject a null hypothesis if the p-value is less than a chosen significance level, $\alpha$. Therefore, if the null hypothesis is true, but we end up with really unusual data just by chance -- a p-value less than $\alpha$ -- then we mistakenly reject the null hypothesis, making a Type 1 error.
:::

If making a Type 1 Error is dangerous or especially costly, we should choose a small significance level (e.g., 0.01 or 0.001). If we want to be very cautious about rejecting the null hypothesis, we demand very strong evidence favoring the alternative $H_A$ before we would reject $H_0$.

If a Type 2 Error is relatively more dangerous or much more costly than a Type 1 Error, then we should choose a higher significance level (e.g., 0.10). Here we want to be cautious about failing to reject $H_0$ when the null is actually false.

::: importantbox
**Significance levels should reflect consequences of errors.**

The significance level selected for a test should reflect the real-world consequences associated with making a Type 1 or Type 2 error.
:::

Because of the result of increased error rates, it is never okay to change two-sided tests to one-sided tests after observing the data. There are two dangers if we ignore possibilities that disagree with our data or that conflict with our world view:

1.  Framing an alternative hypothesis simply to match the direction that the data point will generally inflate the Type 1 Error rate. After all the work we've done (and will continue to do) to rigorously control the error rates in hypothesis tests, careless construction of the alternative hypotheses can disrupt that hard work.

2.  If we only use alternative hypotheses that agree with our worldview, then we're going to be subjecting ourselves to **confirmation bias**\index{confirmation bias}, which means we are looking for data that supports our ideas. That's not very scientific, and we can do better!

We explore the consequences of ignoring this advice in the next example.

::: workedexample
Using $\alpha=0.05$, we show that freely switching from two-sided tests to one-sided tests will lead us to make twice as many Type 1 Errors as intended.

------------------------------------------------------------------------

Suppose we are interested in finding if a proportion differs from 0.5. We've created a smooth-looking **null distribution** representing proportions due to chance in Figure \@ref(fig:type1ErrorDoublingExampleFigure).

Suppose the sample proportion was larger than 0.5. Then if we can flip to a one-sided test, we would use $H_A$: proportion $> 0.5$. Now if we obtain any observation in the upper 5% of the distribution, we would reject $H_0$ since the p-value would just be a the single tail. Thus, if the null hypothesis is true, we incorrectly reject the null hypothesis about 5% of the time when the sample proportion is above the null value, as shown in Figure \@ref(fig:type1ErrorDoublingExampleFigure).

Suppose the sample proportion was smaller than 0.5. Then if we change to a one-sided test, we would use $H_A$: proportion $< 0.5$. If the observed difference falls in the lower 5% of the figure, we would reject $H_0$. That is, if the null hypothesis is true, then we would observe this situation about 5% of the time.

By examining these two scenarios, we can determine that we will make a Type 1 Error $5\%+5\%=10\%$ of the time if we are allowed to swap to the "best" one-sided test for the data. This is twice the error rate we prescribed with our significance level: $\alpha=0.05$ (!).
:::

```{r type1ErrorDoublingExampleFigure, fig.cap="The shaded regions represent areas where we would reject $H_0$ under the bad practices considered in when $\\alpha = 0.05$.", warning=FALSE, fig.width=10}
par(las=1, mar=c(2,0,0.5,0), mgp=c(3,0.9,0))
X <- seq(-4,4,0.01)
Y <- dnorm(X)
plot(X, Y, type='l', axes=F, xlim=c(-4,4))
axis(1, at=c(-5,0,5), label=expression(0, 0.5, 1), cex.axis=1.2)
these <- which(X >= 1.65)
polygon(c(X[these[1]], X[these],X[rev(these)[1]]), c(0,Y[these],0), col=COL[1])
these <- which(X <= -1.65)
polygon(c(X[these[1]], X[these],X[rev(these)[1]]), c(0,Y[these],0), col=COL[1])

arrows(-2,0.17, -1.9, 0.08, length=0.07, col=COL[1], lwd=1.5)
text(-1.92, 0.21, '5%', cex=1.2, col=COL[1])
arrows(2,0.17, 1.9, 0.08, length=0.07, col=COL[1], lwd=1.5)
text(2.08, 0.21, '5%', cex=1.2, col=COL[1])

lines(X, Y, lwd=1.5)
abline(h=0)
```

::: importantbox
**Hypothesis tests should be set up *before* seeing the data.**

After observing data, it is tempting to turn a two-sided test into a one-sided test. Avoid this temptation. Hypotheses should be set up *before* observing the data.
:::

```{=html}
<!--
%\Comment{Should we scrap this subsection and example and just leave the caution box? Downside: weakens item 1 near the start of Section \@ref(IntroducingTwoSidedHypotheses).}
-->
```
## Power

When the null hypothesis is true, the probability of a Type 1 error is our chosen significance level, $\alpha$, which means the probability of a correct decision is $1 - \alpha$.

If an alternative hypothesis is true, the probability of a Type 2 error, which we denote by $\beta$, depends on several components:

-   significance level
-   sample size
-   whether the alternative hypothesis is one-sided or two-sided
-   standard deviation of the statistic
-   how far the alternative parameter value is from the null value

Only the first three of these components are within the control of the researcher.

The probability of a correct decision when the alternative hypothesis is true, $1 - \beta$, is called the **power** of the test. Higher **power** means we are more likely to detect an effect that actually exists.

::: onebox
**Power.**

The **power** of a test is the probability of rejecting a false null hypothesis.
:::

::: workedexample
Suppose we would like to test whether less than 65% of a large population approves of a new law: $H_0: \pi = 0.65$ versus $H_A: \pi < 0.65$. We collect a random sample of $n = 200$ individuals from this population. For what values of the sample proportion, $\hat{p}$, would we reject $H_0$ using a significance level of $\alpha = 0.05$?

------------------------------------------------------------------------

Under the assumption of the null hypothesis, the standard deviation of $\hat{p}$ is $\sqrt{0.65(1-0.65)/200} = 0.0337$. Thus, by the Central Limit Theorem, sample proportions vary according to an approximate normal distribution with mean 0.65 and standard deviation 0.0337. We will reject the null hypothesis that the true proportion is 0.65 if the sample proportion is so low that its probability is less than 0.05, shown in Figure \@ref(fig:power-example).

To be precise, we will reject $H_0$ if $\hat{p}$ is less than the 5th percentile of the null distribution: `qnorm(0.05, 0.65, 0.0337)` = 0.59.
:::

```{r power-example, out.width = "80%", fig.cap = "Shaded area on a null distribution where we would reject the null hypothesis. This area is equal to the significance level."}
include_graphics("05/figures/PowerExample.png")
```

To calculate power, we need to know the true value of the parameter. In the previous example, the alternative was $H_0: \pi < 0.65$, so if we just say the alternative hypothesis is true, we still do not know the value of $\pi$. Thus, power calculations are done for a specific value of the parameter, and the power changes if the value of the parameter changes.

::: workedexample
Consider again the test of whether less than 65% of a large population approves of a new law: $H_0: \pi = 0.65$ versus $H_A: \pi < 0.65$. Suppose the population approval rate is actually $\pi = 0.58$. What is the probability that we will detect this effect?

------------------------------------------------------------------------

This example asks us to calculate the power -- the probability our test will provide evidence that $\pi < 0.65$ when the true value of $\pi$ is 0.58. Recall from the previous example that we will reject the null if $\hat{p} < 0.59$. Thus, the power is the probability that $\hat{p}$ will be less than 0.59 when the true proportion is 0.58: `pnorm(0.59, 0.58, 0.0337)` = 0.62. There is only a 62% chance that the data we collect will provide strong enough evidence to conclude $\pi < 0.65$. This probability is represented by the red area in Figure \@ref(fig:power-example-2).
:::

```{r power-example-2, out.width = "80%", fig.cap = "The blue distribution is the distribution of sample proportions if the null hypothesis is true, $\\pi = 0.65$ -- the blue shaded area represents the probability we reject a true null hypothesis. The red distribution is the distribution of sample proportions under a particular alternative hypothesis, that $\\pi = 0.58$ -- the red shaded area represents the power."}
include_graphics("05/figures/PowerExample2.png")
```

::: onebox
**Increasing power.**

The **power** of a test will *increase* when:

-   the significance level *increases*
-   the sample size *increases*
-   we change from a two-sided to a one-sided test
-   the standard deviation of the statistic *decreases*
-   how far the alternative parameter value is from the null value *increases*
:::

```{r, out.width="40%"}
include_graphics("05/images/Whalberg.png")
```

## Statistical significance vs. practical importance

::: workedexample
An Austrian study of heights of 507,125 military recruits reported that men born in spring were statistically significantly taller than men born in the fall (p-value \< 0.0001). A confidence interval for the true difference in mean height between men born in spring and men born in fall was (0.598, 0.602) cm. Is this result practically important?

------------------------------------------------------------------------

No, these results don't mean much in this context -- a difference in average height of around 0.6 cm would not even be noticeable by the human eye! Just because a result is statistically significant does not mean that it is necessarily practically important -- meaningful in the context of the problem.
:::

```{r include=FALSE}
terms_chp_5 <- c(terms_chp_5, "practical importance", "statistical significance")
```

In the previous example, we saw two groups of men that differed in average height, and that difference was **statistically significant** -- that is, the observed difference in sample means of 0.6 cm is very unlikely to occur *if the true difference in average height was zero*. But, a difference of 0.6 cm in height is not meaningful -- not **practically important**.

Why did this happen? Recall that the variability in sample statistics decreases as the sample size increases. For example, unknown to you, suppose a slight majority of a population, say 50.5%, support a new ballot measure. You want to test $H_0: \pi = 0.50$ versus $H_0: \pi > 0.50$ for this population. Since the true proportion is not exactly 0.50, you can make your p-value smaller than any given significance level as long as you choose a large enough sample size!

Figure \@ref(fig:stat-signif) displays this scenario. The distribution of possible sample proportions who support the new ballot measure in samples of size $n = 100,000$ when 50.5% of the population supports the measure is represented by the black normal curve. The dotted red normal curve is the null distribution of sample proportions for $H_0: \pi = 0.5$. There is very little overlap between the two distributions due to the very large sample size. The shaded blue area represents the power of the test of $H_0: \pi = 0.5$ versus $H_A: \pi > 0.5$ when $\alpha = 0.05$ -- 0.885! That is, we have an 88.5% chance that our p-value will be less than 0.05, even though the true proportion is only 0.05 above 0.5!

```{r stat-signif, out.width="100%", fig.cap = "Black curve: sampling distribution of sample proportions from samples of size 100,000 when the true proportion is 0.505. Red curve: null distribution of sample proportions for a null value of 0.50." }
n <- 100000
m <- 0.505
s <- sqrt(m*(1-m)/n)
drawnormal(m = m, s = s, 
           xlabel = "Sample proportion",
           shade = TRUE, six.sigma = FALSE, 
           cex.axis=1.2,cex.lab=1.3, from = 0.49, to = 0.52,
           x = qnorm(.975, 0.5, s), dir = "upper")
curve(dnorm(x, 0.5, s), add=TRUE, lty=2, col = "red")

text(x = 0.515, y = 100, paste("power = ", round(pnorm(qnorm(.975, 0.5, s), 0.505, s,lower.tail=FALSE),3)), col = "blue", cex = 1.2)
```

<!-- Someday - annotate these plots better -->

::: guidedpractice
If p-values can be made arbitrarily small with large sample sizes, what might tend to happen with small sample sizes? Would small sample sizes be more likely to give practically important results that are not statistically significant? or statistically significant results that are not practically important?[^05-inference-cat-52]
:::

[^05-inference-cat-52]: Since hypothesis tests with small sample sizes typically have low power, small sample sizes can result in practically important results that are not statistically significant.

Consider the opposite scenario -- small sample sizes with a meaningful difference. Suppose again that you would like to determine if a majority of a population support a new ballot measure. However, you only have the time and money to survey 20 people in the community. Unknown to you, 65% of the population support the measure.

Examine Figure \@ref(fig:prac-signif). The distribution of possible sample proportions who support the new ballot measure in samples of size $n = 20$ when 65% of the population supports the measure is represented by the black normal curve. The dotted red normal curve is the null distribution of sample proportions for $H_0: \pi = 0.5$. Even though 0.65 is quite a bit higher than 0.50, there is still a lot of overlap between the two distributions due to the small sample size. The shaded blue area represents the power of the test of $H_0: \pi = 0.5$ versus $H_A: \pi > 0.5$ when $\alpha = 0.05$ -- only 0.29! That is, even though 65% of the population supports the measure (much higher than 50%), we only have a 29% chance of detecting that difference with our small sample size.

```{r prac-signif, out.width="100%", fig.cap = "Black curve: approximate sampling distribution of sample proportions from samples of size 20 when the true proportion is 0.65. Red curve: approximate null distribution of sample proportions for a null value of 0.50." }
n <- 20
m <- 0.65
s <- sqrt(m*(1-m)/n)
drawnormal(m = m, s = s, 
           xlabel = "Sample proportion",
           shade = TRUE, six.sigma = FALSE, 
           cex.axis=1.2,cex.lab=1.3, from = 0, to = 1,
           x = qnorm(.975, 0.5, s), dir = "upper")
curve(dnorm(x, 0.5, s), add=TRUE, lty=2, col = "red")

text(x = 0.2, y = 3, paste("power = ", round(pnorm(qnorm(.975, 0.5, s), 0.65, s,lower.tail=FALSE),3)), col = "blue", cex = 1.2)
```

::: onebox
**Statistical significance versus practical importance.**

*For large sample sizes, results may be statistically significant, but not practically important.* Since sample statistics vary very little among samples with large sample sizes, it is easy for a hypothesis test to result in a very small p-value, even if the observed effect is practically meaningless.

*For small sample sizes, results may be practically important, but not statistically significant.* Since studies with small sample sizes tend to have very low power, it is difficult for a hypothesis test to result in a very small p-value, even if the observed effect is quite large.
:::




## Chapter review {#chp12-review}

### Summary {-}

::: {.underconstruction}
TODO
:::

### Terms {-}

We introduced the following terms in the chapter. If you're not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as **bolded text**.

```{r}
make_terms_table(terms_chp_12)
```

### Key ideas {-}

::: {.underconstruction}
TODO
:::



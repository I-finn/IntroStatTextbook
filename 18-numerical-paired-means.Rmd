# Inference for comparing paired means {#inference-paired-means}

```{r, include = FALSE}
source("_common.R")
```

::: chapterintro
In Chapter \@ref(inference-one-mean), analysis was done to summarize a quantitative variable measured on a single population.
What if we were to measure that variable twice on the same observational unit? 
In this case, the two measurements are **dependent**---knowledge of the observations in one group gives us information about what we would expect to happen in the other group.
Sometimes dependency is not something that can be addressed through a statistical method.
However, this particular dependency, **pairing**, can be modeled quite effectively by using the same tools for a single quantitative variable from Chapter \@ref(inference-one-mean) applied to the differences between two measurements on each pair.
:::

Paired data represent a particular type of experimental structure where the analysis is somewhat akin to a one-sample analysis (see Chapter \@ref(inference-one-mean)) but has other features that resemble a two-sample analysis (which we will see in Chapter \@ref(inference-two-means)).  Quantitative measurements are made on each of two different levels of an explanatory variable, but those measurements are **paired** --- each observational unit consists of two measurements, and the two measurements are subtracted such that only the difference is retained.  Table \@ref(tab:pairedexamples) presents some examples of studies where paired designs were implemented.

```{r pairedexamples}
temptbl <- tribble(
 ~variable,    ~col1, ~col2, ~col3, 
 "Car", "Smooth Turn vs Quick Spin", "amount of tire tread after 1,000 miles", "difference in tread",
 "Married heterosexual couple", "Husband vs Wife", "age", "difference in age",
 "Textbook", "UCLA vs Amazon", "price of new text", "difference in price",
 "Individual person", "Pre-course vs Post-course", "exam score", "difference in score"
)

temptbl %>%
 kable(caption = "Examples of studies where a paired design is used to measure the difference in the measurement over two conditions.",
    col.names = c("Observational unit", "Comparison groups", "Measurement", "Value of interest")) %>%
 kable_styling() 
```


::: {.onebox}
**Paired data.**

  Two sets of observations are *paired* if each
  observation in one set has a special correspondence
  or connection with exactly one observation in the other
  data set.
:::

```{r include=FALSE}
terms_chp_18 <- c("paired data")
```


<!-- Paired data can arise from two different situations: -->

<!-- * **Repeated measures**.  Paired data using repeated measures occurs when we measure the same person or unit twice and take the difference between those measurements. -->

<!-- * **Matching**. Paired data using matching occurs when we first match people or units into similar pairs, and then take the difference between the two measurements taken on each person/unit. -->

<!-- ::: {.guidedpractice} -->
<!-- Which of the examples in Table \@ref(tab:pairedexamples) are paired using repeated measures? paired using matching?^[The examples with cars and people are paired using repeated measures --- each car or person was measured twice (Smooth Turn tire and Quick Spin tire, or pre-score and post-score). Textbooks would need to be matched since they are not physically in the same location; married couples are matched since we measure the age on each member of the couple separately.] -->
<!-- :::  -->

For inferential methods applied to paired data, the analysis is virtually identical to the one-sample approach given in Chapter \@ref(inference-one-mean).
The key to working with paired data is to consider the measurement of interest to be the _difference_ in measured values across the pair of observations.
Thinking about the differences as a single observation on an observational unit changes the paired setting into the one-sample setting. 

A comparison of the notation used in Chapter \@ref(inference-one-mean) and the notation used in this chapter is shown below. The subscript "d" stands for "difference" since our variable is now a paired difference.

::: {.onebox}
**Notation for a single quantitative variable (left) and paired differences (right).**

|  | **One Mean** | **Paired Mean Difference** |
|-|-|-|
| Population mean | $\mu$ | $\mu_d$ |
| Population standard deviation | $\sigma$ | $\sigma_d$ |
| Sample mean | $\bar{x}$ | $\bar{x}_d$ |
| Sample standard deviation | $s$ | $s_d$ |
| Sample size | $n$ | $n$ |
:::

Instead of $n$ representing the number of observational units, with paired data, $n$ represents the number of *pairs* in paired samples.
Similarly, $\mu_d$, $\sigma_d$, $\bar{x}_d$ and $s_d$ are all calculated
using the differences in measured values within pairs.

## Shifted bootstrap test for $H_0: \mu_d = 0$

Consider an experiment done to measure whether tire brand Smooth Turn or tire brand Quick Spin has longer tread wear.  That is, after 1,000 miles on a car, which brand of tires has more tread, on average?  

### Observed data

The observed data represent 25 tread measurements (in inches) taken on 25 Smooth Turn tires and 25 Quick Spin tires.
The study used a total of 25 cars, so on each car, one brand was randomly assigned to the front driver's side tire and the other to the front passenger's side tire. 
Figure \@ref(fig:tiredata) presents the observed data.
The Smooth Turn manufacturer looks at the box plot below and says:

> clearly the tread on Smooth Turn tires is higher, on average, than the tread on Quick Spin tires after 1,000 miles of driving.

The Quick Spin manufacturer is skeptical and retorts:  

> but with only 25 cars, it seems that the variability in road conditions (sometimes one tire hits a pothole, etc.) could be what leads to the small difference in average tread amount.

We'd like to be able to systematically distinguish between what the Smooth Turn manufacturer sees in the plot and what the Quick Spin manufacturer sees in the plot.  Fortunately for us, we have an excellent way to simulate the natural variability (from road conditions, etc.) that can lead to tires being worn at different rates:  bootstrapping.


```{r tiredata, fig.cap = "Boxplots of the tire tread remaining after 1,000 miles by the brand of tire from which the original measurements came. Gray lines connect the same cars.", warning = FALSE, fig.width=12}
# inches  about 1/32" per 3500 mi
set.seed(47)
brandA <- rnorm(25, .310, .003)
brandB <- rnorm(25, .308, .003)

tires <- data.frame(tread = c(brandA, brandB), 
                    car = rep(1:25,2), 
                    brand = c(rep("Smooth Turn", 25), rep("Quick Spin", 25)))

tires %>%
  mutate(`Tire Brand` = brand) %>%
ggplot(aes( x = `Tire Brand`, y = tread, color = `Tire Brand`)) +
  geom_boxplot() +
  geom_point() +
  geom_line(aes(group = car), color = "grey") +
  ylab("Tire tread (inches)") + 
  xlab("Brand of tires.")
```

Since these are paired data, we are only interested in the _differences_ in tire tread between the two brands on each car. The dotplot in Figure \@ref(fig:tiredata-diff) displays these differences, with summary statistics displayed below.

```{r pivot-tires, echo=FALSE}
tires.diff <- pivot_wider(tires, id_cols = "car", names_from = "brand", values_from = "tread") %>% mutate(difference = `Smooth Turn` - `Quick Spin`)
differences <- tires.diff$difference
```

```{r, echo=TRUE}
favstats(differences)
```

```{r tiredata-diff, fig.cap="Difference in tire tread (in inches) remaining after 1,000 miles between the two brands (Smooth Turn -- Quick Spin).", out.width = "75%"}
tires.diff %>% ggplot(aes(x = difference)) +
  geom_dotplot(fill = "blue") +
  labs(x = "Difference in tire tread (Smooth Turn - Quick Spin) in inches") +
    theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank(),
        text = element_text(size = 15))
```


### Variability of the statistic

```{r, echo = FALSE}
# Set digits to 5 to display mean difference in tire tread more accurately
options(digits = 5)
```
A simulation-based test will identify whether the differences seen in the box plot below could plausibly have happened just by chance variability.
As before, we will simulate the variability in sample statistics under the assumption that the null hypothesis is true.
In this study, the null hypothesis is that average difference in tire tread wear between Smooth Turn and Quick Spin tires is zero. The experiment was conducted to determine whether Smooth Turn or Quick Spin has longer tread wear. Taking the order of differences to be Smooth Turn $-$ Quick Spin, we express the hypotheses as follows.

* $H_0: \mu_d = 0$, the true mean difference in tire tread remaining after 1,000 miles between Smooth Turn and Quick Spin (Smooth Turn $-$ Quick Spin) tires is equal to zero.  
* $H_A: \mu_d \neq 0$, the true mean difference in tire tread remaining after 1,000 miles between Smooth Turn and Quick Spin (Smooth Turn $-$ Quick Spin) tires is not equal to zero.

To simulate the null distribution of mean differences in tread, we will implement the same method used in Section \@ref(one-mean-null-boot) using a shifted bootstrap distribution.

::: {.onebox}
**Shifted bootstrap null distribution for a sample mean difference.**

To simulate a null distribution of sample mean differences under the null hypothesis $H_0: \mu_d = 0$,

1. Subtract $\bar{x}_d$ from each difference in the original sample:^[Subtracting the sample mean is equivalent to adding $\mu_0 - \bar{x}_d$ when the null value is $\mu_d = 0$. Thus, we are using the same process as that described in Section \@ref(one-mean-null-boot).]  
  \[
    x_1 - \bar{x}_d , \hspace{2.5mm} x_2 - \bar{x}_d, \hspace{2.5mm}  x_3 - \bar{x}_d, \hspace{2.5mm}  \ldots, \hspace{2.5mm}  x_n - \bar{x}_d.
  \]
  Note that if $\bar{x}_d$ is a negative number, then you will be _adding_ the distance between $0$ and $\bar{x}_d$ to each value.
2. Generate 1000s of bootstrap resamples from this shifted distribution, plotting the shifted bootstrap sample mean difference each time.
:::


To use bootstrapping to generate a null distribution of sample mean differences in tire tread, we first have to **shift the data** to be centered at the null value of zero. We shift the data by subtracting $\bar{x}_d$ = `r round(mean(differences), 5)` from each tire tread difference in the sample. This process is displayed in Figure \@ref(fig:tiredata-diff-shift).

```{r tiredata-diff-shift, fig.cap="Mean difference in tire tread (in inches) remaining after 1,000 miles between the two brands (Smooth Turn -- Quick Spin) (blue), and the shifted mean differences in tire tread (red), found by subtracting 0.00196 to each original difference.", out.width = "75%"}
tires.diff$shift <- tires.diff$difference - mean(differences)  # Add column of shifted differences to wide data set
tires.tmp <- data.frame(difference = c(tires.diff$difference, tires.diff$shift),
                        Shifted = c(rep("No",25), rep("Yes",25))) # Stack data
tires.tmp %>% ggplot(aes(x = difference, fill = Shifted)) +
  geom_dotplot() +
  labs(x = "Difference in tire tread (Smooth Turn - Quick Spin) in inches") +
    theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank(),
        text = element_text(size = 15))  +
  geom_vline(xintercept = mean(differences), color = "blue") +
  geom_vline(xintercept = 0, color = "red") +
  scale_fill_manual(values=c("blue", "red"))
```

### Observed statistic vs. null value


By repeatedly sampling 25 cars with replacement from the shifted bootstrap null distribution, we can create a distribution of the sample mean difference in tire tread, as seen in Figure \@ref(fig:pairRandomiz).
As expected (because the differences were generated under the null hypothesis), the histogram is centered at zero.
A line has been drawn at the observed mean difference, $\bar{x}_d$ = `r round(mean(differences), 5)`, which is nowhere near the differences simulated from natural variability when we assume there is no difference in tire tread wear between brands.
Because the observed mean difference in tire tread is so far away from the natural variability of the randomized mean differences in tire tread, we believe that there is a significant difference in tire tread wear between Smooth Turn and Quick Spin brand tires, on average. 

To be precise, the proportion of simulated $\bar{x}_d$'s that are `r round(mean(differences), 5)` inches or further away from zero is `r round(2*pnorm(mean(differences), 0, sd(differences)/sqrt(25), lower.tail=FALSE), 3)`. This p-value gives us strong evidence in favor of our alternative $H_A: \mu_d \neq 0$.
Our conclusion is that the extra amount of tire tread remaining in Smooth Turn brand tires after 1,000 miles, on average, is due to more than just natural variability. Data from this experiment suggest that, on average, Smooth Turn tires differ in tread wear compared to Quick Spin tires.

```{r pairRandomiz, fig.cap = "Histogram of 1000 simulated mean differences in tire tread, assuming that the two brands perform equally, on average.", warning = FALSE, fig.width=10}
# inches  about 1/32" per 3500 mi

obs_diff <- mean(brandA) - mean(brandB)

set.seed(4747)
diffRand <- c()
for (i in 1:1000){

tirediff <- data.frame(brandA, brandB) %>%
  mutate(diffs = brandA - brandB) %>%
  mutate(grpPerm = sample(c(-1,1), 25, replace = TRUE)) %>%
  mutate(diffsPerm = diffs * grpPerm) %>%
  summarize(mean(diffsPerm)) %>% pull()

diffRand <- c(diffRand, tirediff)
}
  
diff.df = data.frame(diffRand)
  
ggplot(diff.df, aes( x = diffRand)) +
  geom_histogram() +
  ylab("") + 
  xlab("Mean difference in tire tread (inches) when true difference is zero.") +
  geom_vline(xintercept = obs_diff, color = "#F05133") 
```

```{r, echo = FALSE}
# Set digits back to 3
options(digits = 3)
```


## Bootstrap confidence interval for $\mu_d$

In an earlier edition of this textbook,
we found that Amazon prices were, on average,
lower than those of the UCLA Bookstore for UCLA courses
in 2010.
It's been several years, and many stores have adapted
to the online market, so we wondered,
how is the UCLA Bookstore doing today?

### Observed data

We sampled 201 UCLA courses.
Of those, 68
required books could be found on Amazon.

::: {.data}
The `ucla_textbooks_f18` data can be found in the [openintro](http://openintrostat.github.io/openintro/reference/index.html) package.
:::

A portion of the data set from these courses
is shown in Table \@ref(tab:textbooksDF),
where prices are in US dollars. Here the differences are taken as
\begin{align*}
\text{UCLA Bookstore price} - \text{Amazon price}
\end{align*}

It is important that we always subtract using
a consistent order;
here Amazon prices are always subtracted from UCLA prices.
The first difference shown in Table \@ref(tab:textbooksDF)
is computed as $47.97 - 47.45 = 0.52$.
Similarly, the second difference is computed as
$14.26 - 13.55 = 0.71$,
 and the third is $13.50 - 12.53 = 0.97$.

```{r textbooksDF}
data("ucla_textbooks_f18")

ucla_textbooks_f18 <- ucla_textbooks_f18 %>% 
  select(subject, course_num, bookstore_new, amazon_new) %>%
  mutate(price_diff = bookstore_new - amazon_new) %>%
  filter(!is.na(bookstore_new) & !is.na(amazon_new)) 

ucla_textbooks_f18 %>%
  head(4) %>%
   kable(caption = "Four cases of the `ucla_textbooks_f18` dataset.") %>%
 kable_styling() 

```

A dot plot of the data is shown in Figure \@ref(fig:text-price-hist), with summary statistics displayed below.

```{r, echo=TRUE, collapse = FALSE}
favstats(ucla_textbooks_f18$price_diff)
```

```{r text-price-hist, fig.cap="Distribution of differences in new textbook price (UCLA Bookstore -- Amazon) in US dollars for 68 required textbooks at UCLA.", out.width = "75%"}
ucla_textbooks_f18 %>% ggplot(aes(x = price_diff)) +
  geom_dotplot(fill = COL["blue", "full"]) +
  labs(x = "Difference in new textbook price (UCLA - Amazon) in US dollars") +
    theme(axis.title.y = element_blank(),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank(),
        text = element_text(size = 13)) 
```

<!--
\newcommand{\uclabookN}{68}
\newcommand{\uclabookDF}{67}
\newcommand{\uclabookM}{3.58}
\newcommand{\uclabookSD}{13.42}
\newcommand{\uclabookSE}{1.63}
-->

\index{paired|(}
\index{data!textbooks|(}


Each textbook has two corresponding prices in the data set:
one for the UCLA Bookstore and one for Amazon.
Thus, the two prices for the same textbook are **paired**,
and our analysis need only focus on the _differences_
in textbook price between the two suppliers. 



### Variability of the statistic

Following the example of bootstrapping a single mean, the observed mean differences can be bootstrapped in order to understand the variability of the average difference from sample to sample. We can then use the bootstrap distribution of mean differences to calculate bootstrap percentile confidence intervals for the true mean difference in the population.

```{r, echo=FALSE}
# Simulate bootstrapped mean differences 
# and find 99% percentile interval
set.seed(216)
boot_diff <- ucla_textbooks_f18 %>% 
  select(subject, course_num, bookstore_new, amazon_new) %>%
  mutate(price_diff = bookstore_new - amazon_new) %>%
  filter(!is.na(bookstore_new) & !is.na(amazon_new)) %>%
  specify(response = price_diff) %>%
  generate(reps = 1000, type = "bootstrap") %>%
  calculate(stat = "mean") 

ci_perc_diff <- boot_diff %>%
  get_confidence_interval(level = 0.99, type = "percentile")
```

In Figure \@ref(fig:pairboot), a 99% confidence interval for the mean difference in the cost of a new book at the UCLA Bookstore compared with Amazon has been calculated.
The bootstrap percentile interval is computing using the 0.5^th^ percentile and 99.5^th^ percentile of the bootstrapped mean differences and is found to be (`r ci_perc_diff$lower_ci`, `r ci_perc_diff$upper_ci`).
Since this confidence interval contains zero, it does not support the hypothesis that the UCLA Bookstore price is, on average, higher than the Amazon price. That is, since the interval contains both negative and positive values, it is plausible that the prices of UCLA textbooks are lower, on average, than Amazon, and it is also plausible that the prices of UCLA textbooks are higher, on average, than Amazon. We would interpret the interval as follows: We are 99% confident that, on average, new textbook prices at the UCLA Bookstore are between \$0.04 lower to \$8.14 higher than the same textbook on Amazon.


```{r pairboot, fig.cap = "Bootstrap distribution for the average difference in new book price at the UCLA Bookstore versus Amazon (UCLA -- Amazon). The bounds for a 99% bootstrap percentile confidence interval are superimposed in red, and the observed mean difference in new book price is superimposed in blue.", warning = FALSE, fig.width=12}
boot_diff %>%
  infer::visualize(bins = 20) +
  infer::shade_confidence_interval(ci_perc_diff, 
                                   color = "red", 
                                   fill = NULL, size = .5) +
  geom_vline(xintercept = mean(ucla_textbooks_f18$price_diff, na.rm=TRUE), color = "blue") + 
  labs(x = "Mean difference in textbook price (UCLA - Amazon) in US dollars",
       y = "Frequency") +
  ggtitle("") +
  ylim(c(0,200))
```


## Theory-based inferential methods for $\mu_d$ {#paired-mean-math}

As with the simulation-based inferential methods for a paired mean difference,
theory-based inferential methods for this scenario are identical
to the theory-based inferential methods for a single mean---only the notation differs.

### Observed data

Consider again the paired textbook price data in the previous section. 
A histogram of the differences in new textbook price between the UCLA Bookstore and Amazon is shown in
Figure \@ref(fig:diffInTextbookPricesF18), and summary statistics
are displayed in Table \@ref(tab:textbooksSummaryStats).

```{r textbooksSummaryStats}
temptbl <- tribble(
  ~col0,    ~col1, ~col2,
  "68", "$3.58", "$13.42"
)

temptbl %>%
 kable(caption = "Summary statistics for the 68 new textbook price differences (UCLA -- Amazon).",
    col.names = c("$n$", "$\\bar{x}_{d}$", "$s_{d}$")) %>%
 kable_styling() 
```


```{r diffInTextbookPricesF18, fig.cap = "Histogram of the difference in price for each book sampled.", warning = FALSE, fig.width = 10}

histPlot(ucla_textbooks_f18$price_diff, axes = FALSE, # breaks = 20,
         xlab = "UCLA Bookstore Price - Amazon Price (USD)",
         ylab = "",
         col = COL[1])
AxisInDollars(1, at = pretty(ucla_textbooks_f18$price_diff), tck = -0.03)
axis(2, at = seq(0, 30, 10), tck = -0.02)
# axis(2, at = seq(0, 15, 5), tck = -0.02)
par(las = 0)
mtext("Frequency", 2, 2.4)
```

### Variability of the statistic

To analyze a paired data set,
we simply analyze the differences using the same one-sample $t$-distribution techniques
we applied in
Section \@ref(one-mean-math).


::: {.workedexample}
Set up a hypothesis test
    to determine whether, on average, the UCLA Bookstore's price for
    a new textbook is higher than the price of the same
    book on Amazon.
    Also, check the conditions for whether we can move
    forward with the test using the $t$-distribution.
  
---
      
We are considering two scenarios: 

* $H_0$: $\mu_{d} = 0$.
      The true mean difference in new textbook prices (UCLA -- Amazon)
      is equal to zero.
* $H_A$: $\mu_{d} > 0$.
      The true mean difference in new textbook prices (UCLA -- Amazon)
      is greater than zero.

Next, we check the independence and normality conditions:

* The observations are based on a simple random sample,
  so independence is reasonable.

* While there are some outliers,
  $n = 68$ and none of the outliers
  are particularly extreme, so the normality
  of $\bar{x}$ is satisfied.

With these conditions satisfied,
  we can move forward with the $t$-distribution.
:::

### Observed statistic vs. null statistics

As mentioned previously, the methods applied to a difference will be identical to the one-sample techniques.  Therefore, the full hypothesis test framework is given as an example.

::: {.workedexample}

Complete the hypothesis test started
    in the previous Example.

---

To start, compute the standard error associated with
  $\bar{x}_{d}$ using the sample standard
  deviation of the differences
  ($s_{d} = 13.42$)
  and the number of differences
  ($n = 68$):
  \begin{align*}
  SE(\bar{x}_{d})
    = \frac{s_{d}}{\sqrt{n}}
    = \frac{13.42}{\sqrt{68}} = 1.63
  \end{align*}
  The test statistic is the T-score of
  $\bar{x}_{d}$
  under the null condition that the actual mean
  difference is 0:
  \begin{align*}
  T
    = \frac{\bar{x}_{d} - 0}
        { SE(\bar{x}_{d})}
    = \frac{3.58 - 0}{1.63} = 2.20
  \end{align*}
  This value tells us that the sample mean difference in price, \$3.58,
  is 2.20 standard errors above zero (the null value).
  
  To visualize the p-value, the approximate sampling distribution
  of $\bar{x}_{d}$ is drawn as though
  $H_0$ is true,
  and the p-value is represented by the shaded upper tail in Figure \@ref(fig:textbooksF18HTTails). This area is equivalent
  to the area above 2.20 on a $t$-distribution with $df = n - 1$ = 68 $-$ 1 = 67 degrees of freedom.

  Using the `pt` function in R, we find the
  upper tail area of 0.0156.

  In conclusion, we have strong evidence that
  Amazon prices are, on average, lower than the
  UCLA Bookstore prices for UCLA courses.
:::


```{r textbooksF18HTTails, fig.cap = "Distribution of $\\bar{x}_{d}$ under the null hypothesis of no difference.  The observed average difference of 2.98 is marked with the shaded areas more extreme than the observed difference given as the p-value.", warning = FALSE, echo = FALSE}

m <- mean(ucla_textbooks_f18$price_diff)
s <- sd(ucla_textbooks_f18$price_diff)
se <- s / sqrt(length(ucla_textbooks_f18$price_diff))
z <- m / se

normTail(U = abs(m),
         s = se,
         df = 20,
         # xlim = 5 * c(-1, 1),
         col = COL[1],
         # border = COL[4],
         axes = FALSE)
at <- c(-100, 0, m, 100)
labels <- expression(0, mu[0]*' = 0', bar(x)[d]*" = 3.58", 0)
axis(1, at, labels, cex.axis = 0.9)

```

<!-- TODO later - add note to compare to 99% CI in previous section, which contained zero. -->

::: {.workedexample}
Create a theory-based 95% confidence interval for the average price difference between books at the UCLA Bookstore and books on Amazon.

---

Conditions
  for using theory-based methods have already been verified and the standard error
  computed in the previous Example.
  
To find the confidence interval,
  identify $t^{\star}_{67}$ using the R command: `qt(0.975, 67)` = 2.00,
  and plug it, the point estimate,
  and the standard error into the confidence
  interval formula:
  \begin{align*}
  \bar{x}_d \ \pm\ t^{\star} \times SE(\bar{x}_d)
      \quad\to\quad
          3.58 \ \pm\ 2.00 \times 1.63
      \quad\to\quad (0.32, 6.84)
  \end{align*}
  We are 95% confident that Amazon is, on average,
  between $0.32 and $6.84 less expensive
  than the UCLA Bookstore for UCLA course books.
:::

::: {.guidedpractice}
We have strong evidence that Amazon is,
on average, less expensive.
How should this conclusion affect UCLA student
buying habits?
Should UCLA students always buy their books
on Amazon?^[The average price difference
  is only mildly useful for this question.
  Examine the distribution shown in
  Figure \@ref(fig:diffInTextbookPricesF18).
  There are certainly a handful of cases where
  Amazon prices are far below the UCLA Bookstore's,
  which suggests it is worth checking Amazon
  (and probably other online sites) before purchasing.
  However, in many cases the Amazon price is
  above what the UCLA Bookstore charges,
  and most of the time the price isn't that different.
  Ultimately, if getting a book immediately from
  the bookstore is notably more convenient,
  e.g., to get started on reading or homework,
  it's likely a good idea to go with the UCLA
  Bookstore unless the price difference on a
  specific book happens to be quite large.
  For reference, this is a very different result
  from what we (the authors) had seen in a similar
  data set from 2010.
  At that time, Amazon prices were almost uniformly
  lower than those of the UCLA Bookstore's and by
  a large margin, making the case to use Amazon over
  the UCLA Bookstore quite compelling at that time.
  Now we frequently check multiple websites
  to find the best price.]
:::


\index{data!textbooks|)}
\index{paired|)}



## Chapter review {#chp18-review}

<!-- ### Summary {-} -->

<!-- ::: {.underconstruction} -->
<!-- TODO -->
<!-- ::: -->

### Terms {-}

We introduced the following terms in the chapter. If you're not sure what some of these terms mean, we recommend you go back in the text and review their definitions. We are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate. However you should be able to easily spot them as **bolded text**.

```{r}
make_terms_table(terms_chp_18)
```

<!-- ### Key ideas {-} -->

<!-- ::: {.underconstruction} -->
<!-- TODO -->
<!-- ::: -->


